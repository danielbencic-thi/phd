% This file was created with Citavi 6.11.0.0

@incollection{Marcu.2019,
 author = {Marcu, Alina and Costea, Drago{\c{s}} and Licăreţ, Vlad and P{\^i}rvu, Mihai and Slu{\c{s}}anschi, Emil and Leordeanu, Marius},
 title = {SafeUAV: Learning to Estimate Depth and Safe Landing Areas for UAVs from Synthetic Data},
 pages = {43--58},
 volume = {11130},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-11011-6},
 series = {SpringerLink B{\"u}cher},
 editor = {Leal-Taix{\'e}, Laura and Roth, Stefan},
 booktitle = {Computer Vision -- ECCV 2018 Workshops},
 year = {2019},
 address = {Cham},
 doi = {10.1007/978-3-030-11012-3{\textunderscore }4},
 file = {Marcu, Costea et al 2019 - SafeUAV Learning to Estimate Depth:Attachments/Marcu, Costea et al 2019 - SafeUAV Learning to Estimate Depth.pdf:application/pdf}
}


@article{Creswell.2018,
 author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
 year = {2018},
 title = {Generative Adversarial Networks: An Overview},
 pages = {53--65},
 volume = {35},
 number = {1},
 issn = {1053-5888},
 journal = {IEEE Signal Processing Magazine},
 doi = {10.1109/MSP.2017.2765202},
 file = {Creswell, White et al. 2018 - Generative Adversarial Networks:Attachments/Creswell, White et al. 2018 - Generative Adversarial Networks.pdf:application/pdf;Creswell, White et al 2018 - Generative Adversarial Networks:Attachments/Creswell, White et al 2018 - Generative Adversarial Networks.pdf:application/pdf}
}


@incollection{Shah.2018,
 author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
 title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
 pages = {621--635},
 volume = {5},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-67360-8},
 series = {Springer Proceedings in Advanced Robotics},
 editor = {Hutter, Marco and Siegwart, Roland},
 booktitle = {Field and Service Robotics},
 year = {2018},
 address = {Cham},
 doi = {10.1007/978-3-319-67361-5{\textunderscore }40}
}


@inproceedings{Schweiger.10320211072021,
 author = {Schweiger, Andreas and Annighoefer, Bjoern and Reich, Marina and Regli, Christoph and Moy, Yannick and Soodt, Thomas and de Cacqueray, Alexis and Redon, Romaric},
 title = {Classification for Avionics Capabilities Enabled by Artificial Intelligence},
 pages = {1--10},
 publisher = {IEEE},
 isbn = {978-1-6654-3420-1},
 booktitle = {2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)},
 year = {10/3/2021 - 10/7/2021},
 doi = {10.1109/DASC52595.2021.9594364},
 file = {Schweiger, Annighoefer et al 10 3 2021 - 10 7 2021 - Classification for Avionics Capabilities Enabled:Attachments/Schweiger, Annighoefer et al 10 3 2021 - 10 7 2021 - Classification for Avionics Capabilities Enabled.pdf:application/pdf}
}


@article{DurrantWhyte.2006,
 author = {Durrant-Whyte, H. and Bailey, T.},
 year = {2006},
 title = {Simultaneous localization and mapping: part I},
 pages = {99--110},
 volume = {13},
 number = {2},
 issn = {1070-9932},
 journal = {IEEE Robotics {\&} Automation Magazine},
 doi = {10.1109/MRA.2006.1638022},
 file = {Durrant-Whyte, Bailey 2006 - Simultaneous localization and mapping:Attachments/Durrant-Whyte, Bailey 2006 - Simultaneous localization and mapping.pdf:application/pdf}
}


@misc{Mittal.04.06.2019,
 abstract = {Unmanned Aerial Vehicles (UAVs) equipped with bioradars are a life-saving technology that can enable identification of survivors under collapsed buildings in the aftermath of natural disasters such as earthquakes or gas explosions. However, these UAVs have to be able to autonomously navigate in disaster struck environments and land on debris piles in order to accurately locate the survivors. This problem is extremely challenging as pre-existing maps cannot be leveraged for navigation due to structural changes that may have occurred. Furthermore, existing landing site detection algorithms are not suitable to identify safe landing regions on debris piles. In this work, we present a computationally efficient system for autonomous UAV navigation and landing that does not require any prior knowledge about the environment. We propose a novel landing site detection algorithm that computes costmaps based on several hazard factors including terrain flatness, steepness, depth accuracy, and energy consumption information. We also introduce a first-of-a-kind synthetic dataset of over 1.2 million images of collapsed buildings with groundtruth depth, surface normals, semantics and camera pose information. We demonstrate the efficacy of our system using experiments from a city scale hyperrealistic simulation environment and in real-world scenarios with collapsed buildings.},
 author = {Mittal, Mayank and Mohan, Rohit and Burgard, Wolfram and Valada, Abhinav},
 date = {04.06.2019},
 title = {Vision-Based Autonomous UAV Navigation and Landing for Urban Search and  Rescue},
 url = {http://arxiv.org/pdf/1906.01304v2},
 file = {Mittal, Mohan et al. 04.06.2019 - Vision-Based Autonomous UAV Navigation:Attachments/Mittal, Mohan et al. 04.06.2019 - Vision-Based Autonomous UAV Navigation.pdf:application/pdf}
}


@misc{Bayerlein2020,
 author = {Bayerlein, Harald and Theile, Mirco and Caccamo, Marco and Gesbert, David},
 year = {2020},
 title = {UAV Path Planning for Wireless Data Harvesting: A Deep Reinforcement Learning Approach},
 file = {Bayerlein, Theile et al 2020 - UAV Path Planning for Wireless:Attachments/Bayerlein, Theile et al 2020 - UAV Path Planning for Wireless.pdf:application/pdf}
}


@inproceedings{Deng2019,
 author = {Deng, Chengqi and Qiu, Kaitao and Xiong, Rong and Zhou, Chunlin},
 title = {Comparative Study of Deep Learning Based Features in SLAM},
 pages = {250--254},
 publisher = {IEEE},
 isbn = {978-1-7281-2229-8},
 booktitle = {2019 4th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)},
 year = {2019},
 doi = {10.1109/ACIRS.2019.8935995},
 file = {Deng, Qiu et al 2019 - Comparative Study of Deep Learning:Attachments/Deng, Qiu et al 2019 - Comparative Study of Deep Learning.pdf:application/pdf}
}


@article{Dosovitskiy2020,
 abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
 author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
 year = {2020},
 title = {An Image is Worth 16x16 Words: Transformers for Image Recognition  at Scale},
 volume = {abs/2010.11929},
 journal = {CoRR},
 file = {Dosovitskiy, Beyer et al 2020 - An Image is Worth 16x16:Attachments/Dosovitskiy, Beyer et al 2020 - An Image is Worth 16x16.pdf:application/pdf}
}


@inproceedings{Geiger2012,
 author = {Geiger, A. and Lenz, P. and Urtasun, R.},
 title = {Are we ready for autonomous driving? The KITTI vision benchmark suite},
 pages = {3354--3361},
 publisher = {IEEE},
 isbn = {978-1-4673-1228-8},
 booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2012},
 doi = {10.1109/CVPR.2012.6248074},
 file = {Geiger, Lenz et al 2012 - Are we ready for autonomous:Attachments/Geiger, Lenz et al 2012 - Are we ready for autonomous.pdf:application/pdf}
}


@article{Hossain2019,
 abstract = {In recent years, demand has been increasing for target detection and tracking from aerial imagery via drones using onboard powered sensors and devices. We propose a very effective method for this application based on a deep learning framework. A state-of-the-art embedded hardware system empowers small flying robots to carry out the real-time onboard computation necessary for object tracking. Two types of embedded modules were developed: one was designed using a Jetson TX or AGX Xavier, and the other was based on an Intel Neural Compute Stick. These are suitable for real-time onboard computing power on small flying drones with limited space. A comparative analysis of current state-of-the-art deep learning-based multi-object detection algorithms was carried out utilizing the designated GPU-based embedded computing modules to obtain detailed metric data about frame rates, as well as the computation power. We also introduce an effective target tracking approach for moving objects. The algorithm for tracking moving objects is based on the extension of simple online and real-time tracking. It was developed by integrating a deep learning-based association metric approach with simple online and real-time tracking (Deep SORT), which uses a hypothesis tracking methodology with Kalman filtering and a deep learning-based association metric. In addition, a guidance system that tracks the target position using a GPU-based algorithm is introduced. Finally, we demonstrate the effectiveness of the proposed algorithms by real-time experiments with a small multi-rotor drone.},
 author = {Hossain, Sabir and Lee, Deok-jin},
 year = {2019},
 title = {Deep Learning-Based Real-Time Multiple-Object Detection and Tracking from Aerial Imagery via a Flying Robot with GPU-Based Embedded Devices},
 url = {https://www.mdpi.com/1424-8220/19/15/3371},
 volume = {19},
 number = {15},
 issn = {1424-8220},
 journal = {Sensors},
 doi = {10.3390/s19153371},
 file = {Hossain, Lee 2019 - Deep Learning-Based Real-Time Multiple-Object Detection:Attachments/Hossain, Lee 2019 - Deep Learning-Based Real-Time Multiple-Object Detection.pdf:application/pdf}
}


@article{Kakaletsis2022,
 author = {Kakaletsis, Efstratios and Symeonidis, Charalampos and Tzelepi, Maria and Mademlis, Ioannis and Tefas, Anastasios and Nikolaidis, Nikos and Pitas, Ioannis},
 year = {2022},
 title = {Computer Vision for Autonomous UAV Flight Safety: An Overview and a Vision-based Safe Landing Pipeline Example},
 pages = {1--37},
 volume = {54},
 number = {9},
 issn = {0360-0300},
 journal = {ACM Computing Surveys},
 doi = {10.1145/3472288},
 file = {Kakaletsis, Symeonidis et al 2022 - Computer Vision for Autonomous UAV:Attachments/Kakaletsis, Symeonidis et al 2022 - Computer Vision for Autonomous UAV.pdf:application/pdf}
}


@misc{Koch11.04.2018,
 abstract = {Autopilot systems are typically composed of an {\textquotedbl}inner loop{\textquotedbl} providing stability and control, while an {\textquotedbl}outer loop{\textquotedbl} is responsible for mission-level objectives, e.g. way-point navigation. Autopilot systems for UAVs are predominately implemented using Proportional, Integral Derivative (PID) control systems, which have demonstrated exceptional performance in stable environments. However more sophisticated control is required to operate in unpredictable, and harsh environments. Intelligent flight control systems is an active area of research addressing limitations of PID control most recently through the use of reinforcement learning (RL) which has had success in other applications such as robotics. However previous work has focused primarily on using RL at the mission-level controller. In this work, we investigate the performance and accuracy of the inner control loop providing attitude control when using intelligent flight control systems trained with the state-of-the-art RL algorithms, Deep Deterministic Gradient Policy (DDGP), Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO). To investigate these unknowns we first developed an open-source high-fidelity simulation environment to train a flight controller attitude control of a quadrotor through RL. We then use our environment to compare their performance to that of a PID controller to identify if using RL is appropriate in high-precision, time-critical flight control.},
 author = {Koch, William and Mancuso, Renato and West, Richard and Bestavros, Azer},
 date = {11.04.2018},
 title = {Reinforcement Learning for UAV Attitude Control},
 url = {http://arxiv.org/pdf/1804.04154v1},
 file = {Koch, Mancuso et al 11042018 - Reinforcement Learning for UAV Attitude:Attachments/Koch, Mancuso et al 11042018 - Reinforcement Learning for UAV Attitude.pdf:application/pdf}
}


@inproceedings{Lai2020,
 abstract = {Using the camera as the main device for collecting environmental information, visual simultaneous localization and mapping（SLAM）aims to determine the robot's current location and build its surroundings. SLAM system works well in static environments, but it doesn't work well enough in dynamic environments. With many advances in computer vision achieved through deep learning approach, and considering SLAM problem is similar with the computer vision tasks, there is a growing tendency among scholars to consider how to solve the problem of SLAM by using deep learning methods, and put forward some good solutions. The goal of this article is to briefly introduce the SLAM system first and then introduce the dynamic SLAM system in detail proposed in recent years.},
 author = {Lai, Dongcheng and Zhang, Yunjian and Li, Congduan},
 title = {A Survey of Deep Learning Application in Dynamic Visual SLAM},
 pages = {279--283},
 publisher = {IEEE},
 isbn = {978-1-7281-9619-0},
 booktitle = {2020 International Conference on Big Data {\&} Artificial Intelligence {\&} Software Engineering (ICBASE)},
 year = {2020},
 doi = {10.1109/ICBASE51474.2020.00065},
 file = {Lai, Zhang et al 2020 - A Survey of Deep Learning:Attachments/Lai, Zhang et al 2020 - A Survey of Deep Learning.pdf:application/pdf}
}


@misc{MacielPearson12.12.2019,
 abstract = {With the rapidly growing expansion in the use of UAVs, the ability to autonomously navigate in varying environments and weather conditions remains a highly desirable but as-of-yet unsolved challenge. In this work, we use Deep Reinforcement Learning to continuously improve the learning and understanding of a UAV agent while exploring a partially observable environment, which simulates the challenges faced in a real-life scenario. Our innovative approach uses a double state-input strategy that combines the acquired knowledge from the raw image and a map containing positional information. This positional data aids the network understanding of where the UAV has been and how far it is from the target position, while the feature map from the current scene highlights cluttered areas that are to be avoided. Our approach is extensively tested using variants of Deep Q-Network adapted to cope with double state input data. Further, we demonstrate that by altering the reward and the Q-value function, the agent is capable of consistently outperforming the adapted Deep Q-Network, Double Deep Q- Network and Deep Recurrent Q-Network. Our results demonstrate that our proposed Extended Double Deep Q-Network (EDDQN) approach is capable of navigating through multiple unseen environments and under severe weather conditions.},
 author = {Maciel-Pearson, Bruna G. and Marchegiani, Letizia and Akcay, Samet and Atapour-Abarghouei, Amir and Garforth, James and Breckon, Toby P.},
 date = {12.12.2019},
 title = {Online Deep Reinforcement Learning for Autonomous UAV Navigation and  Exploration of Outdoor Environments},
 url = {http://arxiv.org/pdf/1912.05684v1},
 file = {Maciel-Pearson, Marchegiani et al 12122019 - Online Deep Reinforcement Learning:Attachments/Maciel-Pearson, Marchegiani et al 12122019 - Online Deep Reinforcement Learning.pdf:application/pdf}
}


@inproceedings{Milz2018,
 abstract = {Deep learning has become the standard model for object detection and recognition. Recently, there is progress on using CNN models for geometric vision tasks like depth estimation, optical flow prediction or motion segmentation. However, Visual SLAM remains to be one of the areas of automated driving where CNNs are not mature for deployment in commercial automated driving systems. In this paper, we explore how deep learning can be used to replace parts of the classical Visual SLAM pipeline. Firstly, we describe the building blocks of Visual SLAM pipeline composed of standard geometric vision tasks. Then we provide an overview of Visual SLAM use cases for automated driving based on the authors' experience in commercial deployment. Finally, we discuss the opportunities of using Deep Learning to improve upon state-of-the-art classical methods.},
 author = {Milz, Stefan and Arbeiter, Georg and Witt, Christian and Abdallah, Bassam and Yogamani, Senthil},
 title = {Visual SLAM for Automated Driving: Exploring the Applications of Deep Learning},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
 year = {2018},
 file = {Milz, Arbeiter et al 2018 - Visual SLAM for Automated Driving:Attachments/Milz, Arbeiter et al 2018 - Visual SLAM for Automated Driving.pdf:application/pdf}
}


@article{Mittal2020,
 author = {Mittal, Payal and Singh, Raman and Sharma, Akashdeep},
 year = {2020},
 title = {Deep learning-based object detection in low-altitude UAV datasets: A survey},
 pages = {104046},
 volume = {104},
 journal = {Image and Vision computing},
 doi = {10.1016/j.imavis.2020.104046},
 file = {Mittal, Singh et al 2020 - Deep learning-based object detection (2):Attachments/Mittal, Singh et al 2020 - Deep learning-based object detection (2).pdf:application/pdf}
}


@inproceedings{Olsner2020,
 abstract = {Automated flight, e.g. first person view drone racing is a challenging task involving many sub-problems like monocular object detection, 3D pose estimation, mapping, optimal path planning and collision avoidance. Treating this problem, we propose an intuitive solution for the NeurIPS (2019) Game of Drones competition, especially the perception focused tier. We formulate a modular system composed of three layers: machine learning based perception, mapping and planning. Fundamental is a robust gate detection for target guidance accompanied with a monocular depth estimation for collision avoidance. The estimated targets are used to create and update the 3D gate positions within a map. Rule based trajectory planning is finally used for optimal flying. Our approach runs in real-time on a state of the art GPU and is able to robustly navigate through different simulated race tracks under challenging conditions, e.g. high speeds, confusing gate positioning and irregular shapes.Our approach ranks on the 3rd place on the final leader board. In this paper we present our system design in detail and provide additional experimental results.},
 author = {{\"O}lsner, Florian and Milz, Stefan},
 title = {Catch Me, If You Can! A Mediated Perception Approach Towards Fully Autonomous Drone Racing},
 url = {https://proceedings.mlr.press/v123/olsner20a.html},
 pages = {90--99},
 volume = {123},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Escalante, Hugo Jair and Hadsell, Raia},
 booktitle = {Proceedings of the NeurIPS 2019 Competition and Demonstration Track},
 year = {2020},
 file = {{\"O}lsner, Milz 2020 - Catch Me, If You Can:Attachments/{\"O}lsner, Milz 2020 - Catch Me, If You Can.pdf:application/pdf}
}


@misc{Parmar15.02.2018,
 abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
 author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, {\L}ukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
 date = {15.02.2018},
 title = {Image Transformer},
 url = {http://arxiv.org/pdf/1802.05751v3},
 file = {Parmar, Vaswani et al 15022018 - Image Transformer:Attachments/Parmar, Vaswani et al 15022018 - Image Transformer.pdf:application/pdf}
}


@inproceedings{Schirmer2018,
 abstract = {Unmanned aircraft systems promise to be useful for a multitude of applications such as cargo transport and disaster recovery.

The research on increased autonomous decision-making capabilities is therefore rapidly growing and advancing.

However, the safe use, certification, and airspace integration for unmanned aircraft in a broad fashion is  still unclear.

Standards for development and verification of manned aircraft are either only partially applicable or resulting safety and verification efforts are unrealistic in practice due to the higher level of autonomy required by unmanned aircraft.

Machine learning techniques are hard to interpret for a human and their outcome is strongly dependent on the training data.

This work presents the current certification practices in unmanned aviation in the context of autonomy and artificial intelligence.

Specifically,  the recently introduced categories of unmanned aircraft systems and the specific operation risk assessment are described, which  provide  means for flight permission not solely focusing on the aircraft but also incorporating the target operation.

Exemplary, we show how the specific operation risk assessment might be used as an enabler for hard-to-certify techniques by taking the operation into account during system design.},
 author = {Schirmer, Sebastian and Torens, Christoph and Nikodem, Florian and Dauer, Johann},
 title = {Considerations of Artificial Intelligence Safety Engineering in Aerospace},
 url = {https://elib.dlr.de/124727/},
 keywords = {Aerospace;AI-based System;Certification;Unmanned Aircraft Systems;Validation;Verification},
 publisher = {Springer},
 editor = {Gallina, Barbara and Skavhaug, Amund and Schoitsch, Erwin and Bitsch, Friedemann},
 booktitle = {Computer Safety, Reliability, and Security SAFECOMP 2018},
 year = {2018},
 file = {Schirmer, Torens et al 2018 - Considerations of Artificial Intelligence Safety:Attachments/Schirmer, Torens et al 2018 - Considerations of Artificial Intelligence Safety.pdf:application/pdf}
}


@inproceedings{Simon2018,
 abstract = {Lidar based 3D object detection is inevitable for autonomous driving, because it directly links to environmental understanding and therefore builds the base for prediction and motion planning. The capacity of inferencing highly sparse 3D data in real-time is an ill-posed problem for lots of other application areas besides automated vehicles, e.g. augmented reality, personal robotics or industrial automation. We introduce Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, we describe a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. Thus, we propose a specific Euler-RegionProposal Network (E-RPN) to estimate the pose of the object by adding an imaginary and a real fraction to the regression network. This ends up in a closed complex space and avoids singularities, which occur by single angle estimations. The E-RPN supports to generalize well during training. Our experiments on the KITTI benchmark suite show that we outperform current leading methods for 3D object detection specifically in terms of efficiency. We achieve state of the art results for cars, pedestrians and cyclists by being more than five times faster than the fastest competitor. Further, our model is capable of estimating all eight KITTIclasses, including Vans, Trucks or sitting pedestrians simultaneously with high accuracy.},
 author = {Simon, Martin and Milz, Stefan and Amendey, Karl and Gross, Horst-Michael},
 title = {Complex-yolo: An euler-region-proposal for real-time 3d object detection on point clouds},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
 year = {2018},
 file = {Simon, Milz et al 2018 - Complex-yolo:Attachments/Simon, Milz et al 2018 - Complex-yolo.pdf:application/pdf}
}


@article{Theile2020,
 abstract = {Coverage path planning (CPP) is the task of designing a trajectory that enables a mobile agent to travel over every point of an area of interest. We propose a new method to control an unmanned aerial vehicle (UAV) carrying a camera on a CPP mission with random start positions and multiple options for landing positions in an environment containing no-fly zones. While numerous approaches have been proposed to solve similar CPP problems, we leverage end-to-end reinforcement learning (RL) to learn a control policy that generalizes over varying power constraints for the UAV. Despite recent improvements in battery technology, the maximum flying range of small UAVs is still a severe constraint, which is exacerbated by variations in the UAV's power consumption that are hard to predict. By using map-like input channels to feed spatial information through convolutional network layers to the agent, we are able to train a double deep Q-network (DDQN) to make control decisions for the UAV, balancing limited power budget and coverage goal. The proposed method can be applied to a wide variety of environments and harmonizes complex goal structures with system constraints.},
 author = {Theile, Mirco and Bayerlein, Harald and Nai, Richard and Gesbert, David and Caccamo, Marco},
 year = {2020},
 title = {UAV Coverage Path Planning under Varying Power Constraints using Deep  Reinforcement Learning},
 url = {http://arxiv.org/pdf/2003.02609v2},
 pages = {1444--1449},
 doi = {10.1109/IROS45743.2020.9340934},
 file = {Theile, Bayerlein et al 2020 - UAV Coverage Path Planning:Attachments/Theile, Bayerlein et al 2020 - UAV Coverage Path Planning.pdf:application/pdf}
}


@article{Wilson2021,
 author = {Wilson, A. N. and Kumar, Abhinav and Jha, Ajit and Cenkeramaddi, Linga Reddy},
 year = {2021},
 title = {Embedded Sensors, Communication Technologies, Computing Platforms and Machine Learning for UAVs: A Review},
 pages = {1},
 issn = {1530-437X},
 journal = {IEEE Sensors Journal},
 doi = {10.1109/JSEN.2021.3139124},
 file = {Wilson, Kumar et al 2021 - Embedded Sensors:Attachments/Wilson, Kumar et al 2021 - Embedded Sensors.pdf:application/pdf}
}


@inproceedings{Yang2014,
 abstract = {3D path planning of unmanned aerial vehicle (UAV) targets at finding an optimal and collision free path in a 3D cluttered environment while taking into account the geometric, physical and temporal constraints. Although a lot of works have been done to solve UAV 3D path planning problem, there lacks a comprehensive survey on this topic, let alone the recently published works that focus on this field. This paper analyses the most successful UAV 3D path planning algorithms that developed in recent years. This paper classifies the UAV 3D path planning methods into five categories, sampling-based algorithms, node-based algorithms, mathematical model based algorithms, Bio-inspired algorithms, and multi-fusion based algorithms. For each category a critical analysis and comparison is given. Furthermore a comprehensive applicable analysis for each kind of method is presented after considering its working mechanism and time complexity.},
 author = {Yang, Liang and Qi, Juntong and Xiao, Jizhong and Yong, Xia},
 title = {A literature review of UAV 3D path planning},
 pages = {2376--2381},
 booktitle = {Proceeding of the 11th World Congress on Intelligent Control and Automation},
 year = {2014},
 doi = {10.1109/WCICA.2014.7053093},
 file = {Yang, Qi et al 2014 - A literature review of UAV:Attachments/Yang, Qi et al 2014 - A literature review of UAV.pdf:application/pdf}
}


