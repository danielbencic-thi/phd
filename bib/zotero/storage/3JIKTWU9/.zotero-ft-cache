Received April 20, 2020, accepted May 2, 2020, date of publication May 6, 2020, date of current version May 21, 2020. Digital Object Identifier 10.1109/ACCESS.2020.2992749
An Experiment-Based Review of Low-Light Image Enhancement Methods
WENCHENG WANG 1, (Member, IEEE), XIAOJIN WU1, XIAOHUI YUAN 2, (Senior Member, IEEE), AND ZAIRUI GAO 1
1College of Information and Control Engineering, Weifang University, Weifang 261061, China 2College of Engineering, University of North Texas, Denton, TX 76207, USA
Corresponding authors: Wencheng Wang (wwcwfu@126.com) and Xiaojin Wu (wfuwxj@163.com) This work was supported in part by the Shandong Provincial Natural Science Foundation under Grant ZR2019FM059, in part by the Science and Technology Plan for the Youth Innovation of Shandong’s Universities under Grant 2019KJN012, and in part by the National Natural Science Foundation of China under Grant 61403283.
ABSTRACT Images captured under poor illumination conditions often exhibit characteristics such as low brightness, low contrast, a narrow gray range, and color distortion, as well as considerable noise, which seriously affect the subjective visual effect on human eyes and greatly limit the performance of various machine vision systems. The role of low-light image enhancement is to improve the visual effect of such images for the beneﬁt of subsequent processing. This paper reviews the main techniques of low-light image enhancement developed over the past decades. First, we present a new classiﬁcation of these algorithms, dividing them into seven categories: gray transformation methods, histogram equalization methods, Retinex methods, frequency-domain methods, image fusion methods, defogging model methods and machine learning methods. Then, all the categories of methods, including subcategories, are introduced in accordance with their principles and characteristics. In addition, various quality evaluation methods for enhanced images are detailed, and comparisons of different algorithms are discussed. Finally, the current research progress is summarized, and future research directions are suggested.
INDEX TERMS Review, survey, low-light image enhancement, Retinex method, image enhancement, quality evaluation.

I. INTRODUCTION With the rapid development of computer vision technology, digital image processing systems have been widely used in many ﬁelds, such as industrial production [1], video monitoring [2], intelligent transportation [3], and remote sensing monitoring, and thus play important roles in industrial production [4], daily life [5], military applications [6], etc. However, some uncontrollable factors often exist during the process of image acquisition, resulting in various image defects. In particular, under poor illumination conditions, such as indoors, nighttime, or cloudy days, the light reﬂected from the object surface may be weak; consequently, the image quality of such a low-light image may be seriously degraded due to color distortions and noise [7]–[10]. After image conversion, storage, transmission and other operations,
The associate editor coordinating the review of this manuscript and approving it for publication was Yudong Zhang .

the quality of this kind of low-light image is seriously further reduced.
Low light, as the name implies, refers to the environmental conditions where illuminance does not meet the normal standard [11]. Any images captured in an environment with relatively weak light are often regarded as low-light images [12], [13]. Nevertheless, it has thus far been impossible to identify speciﬁc theoretical values that deﬁne a lowlight environment in practical applications, and consequently, no uniﬁed standard exists. Therefore, each image-sensor manufacturer has its own standards; for example, Hikvision usually classiﬁes low-light environments into the following categories: dark level (0.01 Lux - 0.1 Lux), moonlight level (0.001 Lux - 0.01 Lux) and starlight level (less than 0.001 Lux). Images captured in these types of environments exhibit characteristics such as low brightness, low contrast, a narrow gray range and color distortion as well as considerable noise [14], [15]. Fig. 1 shows three images with low brightness and their corresponding gray histograms, where

87884

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

FIGURE 1. Examples of low-light images.
the X-axis shows the grayscale values and the Y-axis represents the number of pixels. The pixel values of these images are mainly focused in the lower range due to the lack of illumination, and the gray difference of the corresponding pixels between the various channels of the color image is limited. There is only a small gap between the maximum and minimum gray levels of the image. The whole color layer exhibits deviations, and the edge information is weak; consequently, it is difﬁcult to distinguish details of the image. These characteristics reduce the usability of such images, seriously degrade their subjective visual effect, and greatly limit the functionality of various visual systems [16]–[18].
To weaken the impact of video/image acquisition from low-illumination environments, researchers have pursued various improvements from both the hardware and software perspectives. One approach is to improve the image acquisition system hardware [19]–[21]. Another is to process the images after they are generated. Because low-illumination cameras use high-performance charge-coupled device (CCD) or complementary metal–oxide–semiconductor (CMOS) technology, professional low-light circuits, and ﬁlters as the core components to improve the imaging quality for lowlight-level imaging, their manufacturing process is highly rigorous, and the technology is complex [22]. Although some professional low-light cameras produced by companies such as Sony, Photonis, SiOnyx and Texas Instruments (TI) have appeared on the market, they are not widely used in daily life because of their high prices. As an alternative approach, the improvement of software algorithms offers great ﬂexibility, and improving the quality of low-light videos and images by means of digital image processing has always been an important direction of research. Therefore, it is of great signiﬁcance and practical value to study enhancement algorithms for low-light images to improve the performance of imaging devices.
The main purpose of low-light image enhancement is to improve the overall and local contrast of the image, improve its visual effect, and transform the image into a form more suitable for human observation or computer processing, while avoiding noise ampliﬁcation and achieving good real-time performance. [23]–[27]. To this end, it is essential to enhance the validity and availability of data captured
VOLUME 8, 2020

under low illumination to obtain clear images or videos [28]. Such enhancement can not only render images more consistent with the subjective visual perception of individuals and improve the reliability and robustness of outdoor visual systems but also allow such images to be more conveniently analyzed and processed by computer vision equipment, which is of great importance for promoting the development of image information mining [29], [30]. Related research results can be widely applied in ﬁelds such as urban trafﬁc monitoring, outdoor video acquisition, satellite remote sensing, and military aviation investigation and can be used as a reference for studies on topics such as underwater image analysis and haze image clarity [31]. Moreover, as an important branch of research in the ﬁeld of image processing, low-light image enhancement has interdisciplinary and innovative appeal and broad application prospects and has become a focus of interdisciplinary research in recent years [32]. A large number of researchers at home and abroad have been paying increasing attention to this ﬁeld for quite some time [33]–[38].
In the real world, color images are most commonly used, so most of the algorithms are either designed for color image enhancement or derived from gray image enhancement methods. The major methods are listed below.
(i) Enhancement based on the RGB (red, green, blue) color space. The speciﬁc steps are as follows. The three color components (R, G and B) are extracted from the original RGB color image. Then, these three components are each individually enhanced using a grayscale image enhancement method. Finally, the three components are merged, and the enhanced results are output. The speciﬁc principle is visually summarized in Fig. 2. This method is simple but can result in serious color deviations in the enhanced images because it neglects the correlations between the components.
FIGURE 2. Image enhancement in the RGB color space.
(ii) Enhancement based on the HSI (hue, saturation, intensity) color space (or the YCbCr, L∗a∗b, YUV color space) [39]–[42]. The brightness component I in the HSI color space is separate from and unrelated to the chrominance component H , i.e., the color information of an image. When the chrominance does not change, the brightness and saturation will determine all of the image information. Hence, to enhance a color image, the I and S components are usually enhanced separately while maintaining the same chromaticity H . Fig. 3 shows the ﬂow chart of this enhancement approach.
87885

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

FIGURE 4. Classification of low-light image enhancement algorithms.

FIGURE 3. Image enhancement in the HSI color space.
In recent years, a common method used to process color images has been to leave one component unchanged while enhancing the other components based on a space transformation. Notably, the available transformations of the color space are diverse, and the form of an image is not limited to a certain space. Regardless of the color space used, the processing steps are similar to those of an enhancement method based on the HSI space [43]–[45]. Studies on low-light image enhancement technology are currently still being conducted by many researchers. Although promising ﬁndings have been obtained, this technology is still not mature. In particular, the available algorithms often have a better effect in a certain aspect than in others. Thus, the ﬁeld still has signiﬁcant research value and offers a large development space that is attractive to researchers.
The remainder of this paper is organized as follows. Section II introduces a classiﬁcation of low-light image enhancement algorithms according to their underlying principles, and the characteristics of the different categories of algorithms are analyzed in detail. In Section III, related quality assessment criteria for enhanced images are described. Several experiments implemented to test the performance of the representative methods are described in Section IV, and the conclusions are summarized and future research directions suggested in the last section.
II. CLASSIFICATION OF LOW-LIGHT IMAGE ENHANCEMENT ALGORITHMS Scholars worldwide have proposed many image enhancement algorithms for images captured under low-illumination conditions to improve low-light videos and images from different perspectives [1], [3], [7]. In accordance with the algorithms used for brightness enhancement, this paper divides these processing methods into seven classes: gray transformation methods, histogram equalization (HE) methods, Retinex methods, frequency-domain methods, image fusion methods,
87886

defogging model methods and machine learning methods. These methods can be further divided into different subclasses in accordance with the differences in their principles. The overall classiﬁcation is depicted in Fig. 4.

A. GRAY TRANSFORMATION METHODS
A gray transformation method is a spatial-domain image enhancement algorithm based on the principle of transforming the gray values of single pixels into other gray values by means of a mathematical function [46], which is usually called a mapping-based approach. Such a method enhances an image by modifying the distribution and dynamic range of the gray values of the pixels [1], [7]. The main subclasses of this type method include linear and nonlinear transformations.

1) LINEAR TRANSFORMATION A linear transformation of gray values, also known as a linear stretching, is a linear function of the gray values of the input image [1], and the formula is as follows:

g(x, y) = C · f (x, y) + R

(1)

where f (x, y) and g(x, y) represent the input and output

images, respectively, and C and R are the coefﬁcients of the

linear transformation. An image can be enhanced to different

degrees by adjusting the values of the coefﬁcients in the above

formula. A corresponding transformation curve is shown in

Fig. 5(a). A common formula for a linear gray stretch is as

follows:

g(x, y)

=

f

(x, y) − fmin fmax − fmin

(gmax

−

gmin)

+

gmin

(2)

where fmax and fmin represent the maximum and minimum gray values of the input image, respectively, and gmax and gmin represent the maximum and minimum gray values of the output image, respectively [7]. Thus, the dynamic range of the image is transformed from [fmin, fmax] to [gmin, gmax] to enhance the brightness and contrast. Sometimes, the gray

values in only a speciﬁc area of the image need to be stretched

VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

pixels with higher gray values [54]. The typical formula is as follows:

g(x, y) = log(1 + c × f (x, y))

(4)

where c is a control parameter. The shapes of several logarithmic transformation functions
are shown in Fig. 6(a). A logarithmic transformation function stretches the gray values of pixels in low-gray-value areas and compresses the values of pixels in high-gray-value areas.

FIGURE 5. Linear transformation curves.

or compressed by applying a piecewise linear transformation to adjust the contrast; the formula for such a transformation is as follows.

 

c

f

(x,

y)

 

a



g(x,

y)

=

 

d

b 

− −

c a

(f

(x,

y)

−

a)

+

c

    

h

−

d

(f

(x,

y)

−

b)

+

d

e−b

0 ≤ f (x, y) ≤ a a ≤ f (x, y) ≤ b (3) b ≤ f (x, y) ≤ e

The various functions in the piecewise formula are represented by colored polylines in the coordinate system corresponding to the transformation. The positions of the discontinuity points must be determined individually for each speciﬁc image. An example of such a piecewise linear transformation curve is shown in Fig. 5(b).
In the piecewise linear transformation method, parameter optimization can be performed only based on experience or with considerable human participation; thus, it lacks an adaptive mechanism. Additionally, it is difﬁcult to achieve the optimal enhancement effect [46], [47], [47]–[49]. To overcome these problems, a hybrid genetic algorithm combined with differential evolution has been applied for image enhancement processing [50]. The optimal transformation curve was obtained through the adaptive mutation and quick search capabilities of this algorithm. In summary, the principle of linear image enhancement is simple and fast to execute, but the effect is not satisfactory, with some image details typically being lost due to uneven image enhancement.

2) NONLINEAR TRANSFORMATION
The basic idea of a nonlinear gray transformation is to use a nonlinear function to transform the gray values of an image [51]. Frequently used nonlinear transformation functions include logarithmic functions, gamma functions and various other improved functions [52], [53]. A logarithmic transformation function implies that there is a logarithmic relationship between the value of each pixel in the output image and the value of the corresponding pixel in the input image. This type of transformation is suitable for an excessively dark image because it can stretch the lower gray values of the image while compressing the dynamic range of the

VOLUME 8, 2020

FIGURE 6. Nonlinear transformation curves.

The gamma function is a nonlinear transformation with broad application whose formula is as follows:

g(x, y) = f (x, y)γ

(5)

where γ denotes the gamma correction parameter, which is usually a constant. Several gamma transformation curves are shown in Fig. 6(b).
As shown in Fig. 6(b), several different transformation curves can be obtained by varying the parameter γ . When γ > 1, the transformation will stretch the dynamic range of the low-gray-value areas of the image and compress the range of the high-gray-value areas. In contrast, when γ < 1, the transformation will compress low gray values and stretch high gray values. When γ = 1, the output remains the same [55]. Therefore, different gray regions of an image can be selectively stretched or compressed by adjusting this parameter to obtain a better enhancement effect. Drago et al. suggested that the dynamic range of an image can be effectively compressed by mapping the gray values of the image using an adaptive logarithmic function [56]. Tao et al. used a gray value corresponding to the cumulative histogram of an image with a value of 0.1 to self-adaptively obtain a nonlinear mapping function that can enhance the brightness of dark regions while inhibiting the enhancement of bright regions [57]. Likewise, a low-light color image enhancement algorithm based on a logarithmic processing model was proposed by Tian et al. [58]. First, this algorithm applies a nonlinear enhancement process to the brightness components of an image; then, the membership function is amended by introducing an enhancement operator. Finally, the enhanced image is obtained by means of the inverse transform of the membership function. Huang et al. [59] proposed an adaptive gamma correction algorithm in which the gamma

87887

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

correction parameter is adaptively obtained in accordance with the cumulative probability distribution histogram. However, because this gamma correction method relies on a single parameter, it is prone to cause overenhancement of bright areas. To overcome this shortcoming, a double gamma function was constructed by Zhi et al. and adjusted based on the distribution characteristics of the illumination map [60], thus improving the gray values in low-brightness areas while suppressing the gray values in local high-brightness regions. Moreover, an arctangent hyperbola has been used to map the hue component of an image to an appropriate range by Yu et al. [61], and later, low-light image enhancement based on the optimal hyperbolic tangent proﬁle was proposed [62]. Nonlinear transformation requires more complex calculations and consequently a longer time than linear transformation [63], [64].
In summary, gray transformation can highlight gray areas of interest and has the advantages of simple implementation and fast speed. However, such methods do not consider the overall gray distribution of an image; consequently, their enhancement ability is limited, and their adaptability is poor.

B. HISTOGRAM EQUALIZATION (HE) METHODS
If the pixel values of an image are evenly distributed across all possible gray levels, then the image shows high contrast and a large dynamic range. On the basis of this characteristic, the HE algorithm uses the cumulative distribution function (CDF) to adjust the output gray levels to have a probability density function that corresponds to a uniform distribution; in this way, hidden details in dark areas can be made to reappear, and the visual effect of the input image can be effectively enhanced [65], [66].

1) PRINCIPLE OF HE

In the HE method, the CDF is used as the transformation

curve for the image gray values [67]–[71]. Let I and L denote

an image and its gray levels, respectively. I (i, j) represents the

gray value at the position with coordinates (i, j), N represents

the total number of pixels in the image, and nk represents the number of pixels of gray level k. Then, the gray-level

probability density function of image I is deﬁned as

p(k) = nk , (k = 0, 1, 2, . . . , L − 1)

(6)

N

The CDF of the gray levels of image I is

k

c(k) = p(r), k = 0, 1, 2, . . . , L − 1

(7)

r =0

The standard HE algorithm maps the original image to an

enhanced image with an approximately uniform gray-level

distribution based on the CDF. The mapping relationship is

as follows:

f (k) = (L − 1) × c(k)

(8)

An example of HE is shown in Fig. 7, in which (a) presents the input low-light image, (b) displays the histogram of the

87888

FIGURE 7. Example of HE on a grayscale image.
input low-light image, (c) presents the enhanced image after HE, and (d) displays the histogram of the enhanced image. The principle of the standard HE algorithm is simple and can be executed in real time. However, the brightness of the enhanced image will be uneven, and some details may be lost due to gray-level merging.
FIGURE 8. Basic model of GHE algorithms.
2) BASIC MODELS OF HE METHODS Depending on the regions considered in the calculation, HE methods can be divided into global histogram equalization (GHE) and local histogram equalization (LHE) [72]. The general concept of a GHE algorithm is illustrated by the model shown in Fig. 8, where X represents the original image, Y represents the enhanced image generated by the HE algorithm, Y = f (X ) represents the traditional HE process or an improved version, and X1, X2, X3, · · · , Xn represent n subimages composed of pixels in the original image that satisfy certain conditions according to a given property, which is deﬁned as Q(x). The parameter x represents the magnitude of the image gray value, Y1, Y2, Y3, · · · , Yn denote the equalized images corresponding to the n subimages, and the image Y after equalization is obtained by merging the subimages in accordance with the pixel positions.
The GHE model has several advantages, such as relatively few calculations and high efﬁciency, and it is especially suitable for the enhancement of overall darker or brighter images [58]. However, it is difﬁcult for a global algorithm, which conducts statistical operations based on the gray values of the whole image, to obtain the optimal recovered values for each local region. Such an algorithm is unable to adapt to the local brightness characteristics of the input image, and consequently, the sense of depth in the image will be decreased after processing.
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

FIGURE 9. Basic model of LHE algorithms.
To solve this problem, many scholars have proposed that an LHE algorithm should be used instead, and such algorithms have hence been put into wide practice. The basic idea of LHE is to apply the HE operation separately to various local areas of an image. The original image is spatially divided into multiple subblocks, and equalization is conducted separately on each subblock to adaptively enhance the local information of the image to achieve the desired enhancement effect. LHE methods can be further divided into three approaches [72], namely, LHE with nonoverlapping subblocks, LHE with overlapping subblocks and LHE with partially overlapping subblocks, as shown in Fig. 9. The implementation process for these algorithms is as follows.
(i) For an input image of a given size, M × N , a subblock with dimensions of m × n is deﬁned in the upper left corner of the image, and additional subblocks are deﬁned by moving along the horizontal and vertical directions with step sizes of h and w, respectively.
(ii) HE processing is applied to each subblock in the same manner as for a GHE algorithm.
Then, the results are added to the output image, and the cumulative number of subblock processing rounds for each pixel is recorded.
(iii) The next subblocks are deﬁned by moving horizontally with the horizontal step size h and vertically with the vertical step size w. For each subblock that does not exceed the image boundary, step (ii) is repeated; if no such unprocessed subblocks remain, the method proceeds to the next step.
(iv) The output image is obtained by dividing the gray value of each pixel in the output image by the corresponding cumulative number of subblock processing rounds. The disadvantages of this algorithm are the local block effects and the large number of calculations.
3) HE ALGORITHMS Many algorithms have been developed based on the classic HE approach. For example, Kim ﬁrst proposed
VOLUME 8, 2020

brightness-preserving bi-histogram equalization (BBHE) to maintain the image brightness [73], in which the input image is divided into two subimages IL and IU (satisfying the conditions I = IL ∪IU and I = IL ∩IU = ) using the mean brightness of the original image as a threshold. HE is then applied to each subimage to address the issue of uneven brightness in local areas of the enhanced image. Subsequently, Wang et al. proposed the equal-area dualistic subimage histogram equalization (DSIHE) algorithm [74]. This algorithm uses the median gray value of the original image as a threshold to divide the image into two parts of the same size to maximize its entropy value, thus overcoming the loss of image information caused by the standard HE algorithm. Later, Chen proposed the minimum mean brightness error bi-histogram equalization (MMBEBHE) model [75], which minimizes the mean brightness error between the output image and the original image. Furthermore, Shen et al. proposed the iterative brightness bi-histogram equalization (IBBHE) algorithm [76], in which the segmentation threshold is selected through an iterative method to drive the mean to converge to the optimum while avoiding the confusion between target and background that can occur in traditional HE. Similarly, a BBHE algorithm that preserves color information was proposed by Tian et al. [77]. This algorithm not only retains the color information of the input image but also enhances the image details. Other optimization methods, including local approaches, have also been continuously emerging. For example, a standard adaptive histogram equalization (AHE) algorithm was proposed in [78], while in [79], a block iterative histogram method was used to enhance the image contrast, and a moving template was used for partially overlapped subblock histogram equalization (POSHE) processing for each part of the image. Liu et al. [80] proposed an LHE method that uses a histogram-number-based gray-level protection mechanism on the basis of nonoverlapping subblocks. The spatial positions of the pixels in each block are taken into account when setting the weights, thus effectively eliminating the block effect. Huang and Yeh [81] proposed a novel LHE algorithm that can improve the contrast of an image while maintaining its brightness. Reza [82] proposed the contrastlimited adaptive histogram equalization (CLAHE) algorithm. This algorithm effectively mitigates the block effect that arises in the enhancement process and limits local contrast enhancement by setting a threshold, thus avoiding excessive enhancement of the image contrast. The CLAHE method can be combined with the Wiener ﬁlter (WF) or a ﬁnite impulse response ﬁlter (FIRF) for image contrast enhancement, as discussed in [83] and [84], respectively. Based on BBHE and recursive mean-separate histogram equalization (RMSHE) [85], an LHE algorithm that maintains image brightness was proposed in [86]. In this algorithm, RMSHE is applied to each subblock, thus effectively maintaining the mean brightness of the subblocks. Based on DSIHE and RMSHE, a POSHE method with equal-area recursive decomposition was proposed in [87]. In this algorithm, multiple equal-area recursive decompositions are implemented on the
87889

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

subblocks in combination with DSIHE and RMSHE to more effectively maintain the image brightness compared with DSIHE. Simultaneously, a bilinear difference measure is used to eliminate the differences in contrast between each subblock and the two externally adjacent subblocks, thus improving the visual effect of the image. In addition, an LHE algorithm based on entropy maximization and brightness maintenance was proposed in [88] that maximizes the entropy of the subblocks while leaving their mean brightness unchanged before and after equalization, thus effectively enhancing the image details. In [89], the contextual and variational contrast (CVC) enhancement algorithm was proposed; in this algorithm, the two-dimensional histogram and context information model of the input image are used to implement nonlinear data mapping to enhance a weakly lighted image. In [59], an HE method with gamma correction was proposed and achieved a balance between the quality of the output image and the computing time. However, the HE methods discussed above typically fail to effectively eliminate the potentially severe interference of noise in weakly illuminated images; in fact, they may even amplify such noise. Therefore, researchers have proposed interpolationbased HE algorithms, in which linear interpolation methods are used to determine the transformation function for the current pixels, thus overcoming the "block effect" caused by nonoverlapping subblocks in HE and achieving a better enhancement effect. In recent years, the newly proposed algorithms have all been combined with image analysis. The background brightness-preserving histogram equalization (BBPHE) algorithm [90] divides the input image into the background region and the target region, while the dominant orientation-based texture histogram equalization (DOTHE) algorithm [91] divides the image into textured and smooth regions. Other typical algorithms include gain-controllable clipped histogram equalization (GCCHE) [72], recursive subimage histogram equalization (RSIHE) [92], entropybased dynamic subhistogram equalization (EDSHE) [93], dynamic histogram equalization (DHE) [94], brightnesspreserving dynamic histogram equalization (BPDHE) [95], bi-histogram equalization with a plateau limit (BHEPL) [96], median-mean-based subimage-clipped histogram equalization (MMSICHE) [97], exposure-based subimage histogram equalization (ESIHE) [98], adaptively modiﬁed histogram equalization (AMHE) [99], weighted histogram equalization (WHE) [100], a histogram modiﬁcation framework (HMF) [101], gap adjustment for histogram equalization (CegaHE) [102], and unsharp masking with histogram equalization (UMHE) [103].
To illustrate the performance of HE methods on color images, the AMHE [99], BBHE [73], CLAHE [82], DSIHE [74], HE [66], RMSHE [85], RSIHE [92], and WHE [100] algorithms are tested here in both the RGB and HSI color spaces. The test image and its results are shown in Figs. 10-12.
(i) Equalization and merging of the three R, G and B subimages
87890

FIGURE 10. Image decomposition (figure best viewed in color).
FIGURE 11. Image enhancement using RGB model (figure best viewed in color).
FIGURE 12. Image enhancement using HSI model.
The contrast of gray images can be effectively enhanced; however, for a color image with three components (R, G and B), serious color distortion of the image may be
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

observed if the ﬁnal image is obtained by simply equalizing and merging the R, G, and B subimages after equalization. The main reason is that the traditional HE algorithm excessively enhances the image brightness. If the mean brightness of one of the three R, G and B subimages of a color image is too dark or too bright, then the mean brightness of this subimage after equalization will be near the median gray value of this component. As a result, the color corresponding to this subimage will be either strengthened or weakened after enhancement, resulting in obvious color distortion and inconsistency in the ﬁnal color image. Therefore, the primary goal of HE for a color image using this method is to maintain the mean brightness of the image while enhancing the image contrast.
(ii) In the HSI model, only the brightness component is equalized. In this method, the input color image is ﬁrst converted from the RGB color space to the HSI color space, and then HE enhancement is applied to the brightness component I. Finally, the color image is converted back to the RGB space. In this way, the number of equalizations is reduced from 3 to 1. However, some calculations are still needed for the transformation between the color spaces, and there is still a risk of excessive image enhancement.
In summary, HE algorithms can effectively enhance lowlight images and are often used in combination with other methods. The visual effect of such an image can be improved based on the contrast and detail enhancement provided by an HE algorithm. However, these methods can also easily cause a loss of color ﬁdelity and the generation of noise, resulting in image distortion.

C. RETINEX METHODS
The Retinex theory, namely, the theory of the retinal cortex, established by Land and McCann, is based on the perception of color by the human eye and the modeling of color invariance [104]. The essence of this theory is to determine the reﬂective nature of an object by removing the effects of the illuminating light from the image. According to Retinex theory, the human visual system processes information in a speciﬁc way during the transmission of visual information, thus removing a series of uncertain factors such as the intensity of the light source and unevenness of light. Consequently, only information that reﬂects essential characteristics of the object, such as the reﬂection coefﬁcient, is retained [105]–[109]. Based on the illumination-reﬂection model (as shown in Fig. 13), an image can be expressed as the product of a reﬂection component and an illumination component [110]:

I (x, y) = R(x, y)L(x, y)

(9)

where R(x, y) is the reﬂection component, which represents the reﬂective characteristics of the object surface; L(x, y)
is the illumination component, which depends on the environmental light characteristics; and I (x, y) is the received image. L(x, y) determines the dynamic range of the image, whereas R(x, y) determines the inherent nature of the image. According to Retinex theory, if L(x, y) can be estimated from

VOLUME 8, 2020

FIGURE 13. Light reflection model.

FIGURE 14. General process of the Retinex algorithm.

I (x, y), then the reﬂection component can be separated from the total amount of light, and the inﬂuence of the illumination component on the image can be reduced to enhance the image [111]. The Retinex algorithm features a sharpening capability, color constancy, large dynamic range compression and high color ﬁdelity. The general process of the Retinex algorithm is shown in Fig. 14, where Log denotes the logarithmic operation and Exp denotes the exponential operation.
Many researchers have proposed effective image enhancement algorithms based on the Retinex theory. First, Land proposed that the illumination component could be estimated by using a random path algorithm to reduce the effect of uneven illumination. However, this random path algorithm is complex and has a common effect. Later, a two-dimensional path selection method, namely, the central Retinex algorithm, was proposed. Its core idea is as follows: an appropriate surround function is selected to determine the weighting of the pixel values in the neighborhood of the current pixel, which are then used to replace the current pixel value. Subsequently, Jobson et al. proposed the single-scale Retinex (SSR) algorithm [112], [113], followed by the multiscale Retinex (MSR) algorithm and the multiscale Retinex algorithm with color restoration (MSRCR) [114], [115].

1) SINGLE-SCALE RETINEX (SSR) Essentially, the SSR algorithm obtains a reﬂection image by estimating the ambient brightness. The formula is as follows:

log Ri(x, j) = log Ii(x, y) − log[G(x, y)∗Ii(x, y)] (10)

where I (x, y) represents the input image, R(x, y) represents

the reﬂection image, i represents the various color channels,

(x, y) represents the position of a pixel in the image, G(x, y)

represents the Gaussian surround function, and ∗ represents

the convolution operator.

The formula for the Gaussian surround function is

G(x,

y)

=

Ke(−

x 2 +y2 σ2

)

(11)

where σ is a scale parameter. The smaller the value of this parameter is, the larger the dynamic range compression of

87891

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

the image is, and the clearer the local values are. K is a normalization factor to ensure that the Gaussian function satisﬁes

G(x, y)dxdy = 1

(12)

To better mimic the characteristics of the human visual system, automatic gain compensation is often required; that is, the output image is mapped to [0, 255] using a linear gray stretching algorithm. The mathematical formula is as follows:

Ri(x, y)

=

255

×

Ri(x, y) − Rmin Rmax − Rmin

(13)

where Ri(x, y) is the output after gray stretching of the ith color channel, and Rmax and Rmin are the maximum and minimum gray levels of the original image, respectively.
Fig. 15 shows the enhancement results obtained using the SSR method when σ is 15, 80 and 250, respectively.

FIGURE 15. Enhancement with the SSR algorithm (figure best viewed in color).

However, the SSR algorithm has some limitations. It is difﬁcult to maintain a balance between detailed information enhancement and color ﬁdelity in images processed with this algorithm due to the use of a single scale parameter.

2) MULTISCALE RETINEX (MSR) To maintain a balance between dynamic range compression and color constancy, Jobson, Rahman et al. extended the single-scale algorithm to a multiscale algorithm, namely, the MSR algorithm [114], which is expressed as follows:

MSR = log Ri(x, y)

k =1

= ωk {log Ii(x, y) − log[Gk (x, y)∗Ii(x, y)]} (14)

N

N

ωk = 1

(15)

k =1

where i represents the three color channels; k represents the Gaussian surround scales; N is the number of scales, generally 3; and the ω parameters are the scale weights. Compared with the SSR algorithm, the MSR algorithm can take advantage of the beneﬁts of multiple scales. The MSR algorithm not only enhances image details and contrast but also produces enhanced images that exhibit better color consistency and an improved visual effect.

87892

3) MULTISCALE RETINEX WITH COLOR RESTORATION (MSRCR)
During the process of image enhancement, the SSR or MSR algorithm is applied separately to the three color channels, R, G and B. Therefore, compared with the original image, the relative proportions of the three color channels may change after enhancement, thus resulting in color distortion. To overcome this problem, MSRCR has been proposed. This algorithm includes a color recovery factor C for each channel, which is calculated based on the proportional relationship among the three color channels in the input image and is then used to correct the color of the output image to eliminate color distortion.
The formula for the color recovery factor is as follows:

Ci(x, y)

=

f(

Ii(x, y)
3

)

(16)

Ii(x, y)

i

where f denotes the mapping function, and C(x, y) is the color recovery factor. Jobson et al. found that the best color recovery effect is achieved when the mapping function is a logarithmic function, namely,

Ci(x, y) = β × log(α ×

Ii(x, y)
3

)

(17)

Ii(x, y)

i

The mathematical expression for the MSRCR algorithm can be obtained by combining formulas (18) and (15):

MSRCR = log Ri(x, y)
k =1
= Ciωk {log Ii(x, y) − log[Gk (x, y)∗Ii(x, y)]}
N
(18)

The algorithm takes advantage of the convolution operation with Gaussian functions. Dynamic range compression and color constancy are achieved for features at large, medium and small scales, thus yielding a relatively ideal visual effect. Experimental results obtained with the SSR, MSR and MSRCR algorithms are shown in Fig. 16.

FIGURE 16. Enhancement with different Retinex algorithms.
4) OTHER RETINEX ALGORITHMS Retinex theory conforms to the characteristics of human visual perception; consequently, it has been widely applied and developed. Many enhancement algorithms have been proposed based on Retinex theory [116]–[119]. Kimmel et al. used the transcendental hypothesis to propose
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

a Retinex algorithm based on a variational framework [120], in which the problem of illumination estimation is transformed into an optimal quadratic programming problem. Despite its high complexity, this algorithm achieves good results. Elad et al. proposed a noniterative Retinex algorithm that can process the edges in an image and suppress noise in dark areas [121]. Meylan et al. proposed a new model for presenting images with a high dynamic range and adapting images both globally and locally to the human visual system. In this algorithm, an adaptive ﬁlter is applied to reduce the chromatic aberrations caused by halo effects and brightness modiﬁcation [122]. Xu et al. proposed a rapid Retinex image enhancement method that eliminates the halo phenomenon encountered with the traditional Retinex algorithm in areas of high light and dark contrast [123].
Likewise, Marcelo et al. proposed a kernel-based Retinex (KBR) method, which relies on calculating the expected value of a suitable random-variable-weighted kernel function to reduce color error and improve details in a shadow image [124]. In 2011, Ng et al. proposed a total variation model based on the Retinex algorithm; in this model, the illumination component is spatially smooth, and the reﬂection component is piecewise continuous. Moreover, a fast calculation method was used to solve the minimization problem posed by the variation model. Finally, the validity of the proposed model was veriﬁed through experiments [125]. In addition, Fu et al. proposed a weighted variation model for the simultaneous reﬂectivity and illumination estimation (SRIE) of observed images; this model can precisely retain the estimated reﬂectivity while inhibiting noise to some extent [126]. Petro et al. proposed a multiscale Retinex algorithm with chromaticity preservation (MSRCP) [127]. First, the image brightness data are processed using the MSR algorithm; then, the results are mapped to each channel in accordance with the original proportional relationship among the R, G and B channels. Thus, the image is enhanced while retaining the original color distribution, and the grayish color typically observed in images enhanced using the MSRCR algorithm is effectively improved [128]. Later, Matin et al. optimized the MSRCP method using particle swarm optimization (PSO) to avoid manual adjustment of the parameters [129]. Chen and Beghdadi [130] proposed an image enhancement algorithm based on Retinex and a histogram stretch method to maintain the natural color of images. Shen and Hwang [131] proposed an image enhancement algorithm based on Retinex with a robust envelope. To avoid color distortion, Jang et al. [132] proposed an image enhancement algorithm based on the use of MSR to estimate the main color of an image. Inspired by Retinex theory, Wang et al. [133], Xiao et al. [134] used a bionic method to enhance images. Chang Hsing Lee et al. proposed an adaptive MSR algorithm [135] based on brightness classiﬁcation. For pixels in dark areas and bright areas, higher weights were given to larger-scale SSR components to enhance the overall visual effect of the image. Wang et al. [136] proposed

a bright-pass ﬁlter in combination with neighborhood brightness information to maintain image naturalness, not only improving the image contrast but also better maintaining natural brightness without requiring naturalness preserved enhancement (NPE). Based on Retinex theory, some scholars have separated the reﬂection and illumination components and then enhanced the latter using a local nonlinear transformation model to render a brighter and more natural image [137]. As an alternative, an enhancement adjustment factor has been introduced [138] to adjust the enhancement degrees of different brightness values to avoid noise ampliﬁcation and color distortion. Fu et al. [139] proposed a Retinex algorithm based on a variation framework that effectively enhances the contour details of an image while suppressing abnormal enhancement. In 2014, Zhao proposed a Retinex algorithm based on a Markov random ﬁeld model. This algorithm estimates the illumination component of an image by means of guided ﬁltering and solves for the reﬂection component of the object of interest based on the Markov random ﬁeld model; additionally, this algorithm solves problems such as detail loss, color distortion and halo effects encountered when the MSRCR algorithm is used to process nighttime color images [140]. Later, Zhao et al. [141] proposed a Retinex algorithm based on weighted least squares. Jae et al. [142] proposed an MSR algorithm based on subband decomposition with a fusion strategy. Moreover, Liu et al. [143] combined the Retinex algorithm with bilateral ﬁltering, thereby effectively improving the color distortion and detail loss in the ﬁnal image but also increasing the complexity of the algorithm [144], [145]. Yin et al. [146], Mulyantini and Choi [147], Zhang et al. [148], Ji et al. [149], and Zhang et al. [150] proposed Retinex-based algorithms combined with guided ﬁlters [151]. Particularly during the early stage of research on Retinex algorithms, scholars obtained many fruitful ﬁndings.
In short, Retinex algorithms have clear beneﬁts and can be easily implemented. These methods can not only increase the contrast and brightness of an image but also has obvious advantages in terms of color image enhancement. However, these algorithms use the Gaussian convolution template for illumination estimation and do not have the ability to preserve edges; consequently, they may lead to halo phenomena in some regions with sharp boundaries or cause the whole image to be too bright.
D. FREQUENCY-DOMAIN METHODS With the development of multiscale image analysis technology, research on image enhancement algorithms has been extended from the spatial domain to the frequency domain [152]. Image enhancement methods based on the frequency domain transform an image into the frequency domain for ﬁltering by means of Fourier analysis, and the ﬁnal image is then inversely transformed back into the spatial domain. Typical frequency-domain methods include homomorphic ﬁltering (HF) and wavelet transform (WT) methods.

VOLUME 8, 2020

87893

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

1) HOMOMORPHIC FILTERING (HF)
HF-based enhancement methods use the characteristics of the illumination-reﬂection model to transform the illumination and reﬂection components in the form of a sum in the logarithmic domain rather than a product. A high-pass ﬁlter is used to enhance the high-frequency reﬂection component and suppress the low-frequency illumination component in the Fourier transform domain [7].
The speciﬁc steps of the HF process are listed as follows. (i) In the illumination-reﬂection model, the illumination component is multiplied by the reﬂection component, which cannot be transformed into the frequency domain. Therefore, to allow these components to be processed separately, the logarithmic transformation should ﬁrst be implemented to transform these multiplicative components into additive components. Taking the logarithm of both sides of equation (10) yields the following:

ln I (x, y) = ln L(x, y) + ln R(x, y)

(19)

(ii) The image is transformed from the spatial domain into the frequency domain by means of the Fourier transform, i.e., the Fourier transform is applied to both sides of the above equation:

F[ln I (x, y)] = F[ln L(x, y) + ln R(x, y)] (20)

This equation can be written more concisely as

I (u, v) = L(u, v) + R(u, v)

(21)

where I (u, v), L(u, v) and R(u, v) are the Fourier transforms of I (u, v), L(u, v) and R(u, v), respectively. The spectral function L(u, v) is mainly concentrated in the low-frequency range, while the spectral function R(u, v) is mainly concentrated in the high-frequency range.
(iii) For contrast enhancement, an appropriate high-pass ﬁlter is selected, and the R(u, v) component in the frequency domain is enhanced by the transfer function H (u, v). The resulting expression is as follows:

S(u, v) = H (u, v)I (u, v) = H (u, v)L(u, v) + H (u, v)R(u, v) (22)

(iv) The inverse Fourier transform is used to transform the image from the frequency domain back into the spatial domain. Let s(u, v) denote the inverse Fourier transform corresponding to S(u, v); then, the inverse Fourier transform of equation (10) is

s(u, v) = F−1(H (u, v)L(u, v)) + F−1(H (u, v)R(u, v))

= hL (x, y) + hR(u, v)

(23)

Therefore, the enhanced image corresponds to the superposition of the illumination component and reﬂection component.
(v) The inverse logarithmic transform G(x, y) = exp[s(x, y)] is applied to equation (24) to obtain the ﬁnal corrected image. Thus, by taking the exponent of both sides of

87894

equation (24), the image after frequency-domain correction is obtained as follows:

G(x, y) = exp|hL (x, y)| · exp|hR(x, y)|

(24)

Therefore, the core of the HF technique is to design an appropriate ﬁlter H (u, v) based on the image properties characterized by the illumination component and the reﬂection component in combination with a frequency ﬁlter and a gray transformation to compress the dynamic range and enhance the contrast. A homomorphic ﬁlter has the following general form:

H (u, v) = (γH − γL )Hhp(u, v) + γL

(25)

where γL < 1 and γH < 1; the purpose of these parameters is to control the scope of the ﬁlter amplitude. Hhp is usually a high-pass ﬁlter, such as a Gaussian high-pass ﬁlter, a Butterworth high-pass ﬁlter, or a Laplacian ﬁlter. If a Gaussian ﬁlter is used as Hhp, then

Hhp = 1 − exp[−c × (D2(u, v)/D20)]

(26)

where c is a constant that controls the form of the ﬁlter. The larger the value of the transition gradient from low
frequency to high frequency is, the steeper the slope, as shown in Fig. 17.

FIGURE 17. Amplitude-frequency curve of a homomorphic filter.
FIGURE 18. Flowchart of the HF process.
The speciﬁc algorithm ﬂow of the HF method is shown in Fig. 18. In this ﬁgure, Log is the logarithmic transform, FFT is the fast Fourier transform, H (u,v) is the frequency ﬁltering function, IFFT is the inverse FFT, and Exp is the exponential operation.
The traditional HF algorithm requires two Fourier transforms and thus is not suitable for real-time processing. To address this issue, some scholars have proposed an HF algorithm based on a spatial ﬁlter [153], [154]. The main idea is similar to that of the traditional HF algorithm. First, the original image is transformed into the logarithmic domain, and then, the output of a low-pass ﬁlter is used to estimate the illumination component. Finally, the reﬂection component is added to obtain the enhanced image. Because the traditional HF algorithm does not account for the local
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

features of the image space, Zhang and Xie [155] proposed an HF algorithm based on the block discrete cosine transform (DCT) and removed the block effect after HF by considering the average boundaries with adjacent subimages as well as the characteristics of the DCT. Images processed with this algorithm show a good effect in terms of local contrast. A two-channel HF color image enhancement method based on the HSV color space was proposed in [41] by Han Lina et al. First, the input color image is transformed from RGB space into HSV space, thus obtaining separate chroma-, saturation- and brightness-channel images. Then, the saturation (S)-channel image is enhanced via Butterworth HF, and the brightness (V)-channel image is enhanced via Gaussian HF. Finally, the image is transformed back into RGB space to obtain the enhanced image. Fig. 19 shows the effects of enhancement processing with a Gaussian highpass ﬁlter and presents the histograms corresponding to the images. The image brightness is improved after the HF process, but the image details are fuzzy.
FIGURE 19. HF effect (figure best viewed in color).
Each coin has two sides: HF has the advantage of better maintaining the original image content, but its disadvantage is that it requires two Fourier transforms, namely, one exponential operation and one logarithmic operation, and therefore involves more calculation [156]–[158]. If the cut-off frequency of the high-pass ﬁlter is too high, then the dynamic range will be compressed and details will be lost. If the cutoff frequency is too low, the dynamic range compression will be minimal, and the algorithm will lack self-adaptability. This method is based on the premise of uniform illumination; consequently, the enhancement effect is poor for nighttime images with both bright and dark areas.
HF algorithms can remove uneven regions generated by light while maintaining the contour information of an image. However, such an algorithm requires two Fourier transformations, i.e., one exponential operation and one logarithmic operation, for each pixel in an image; therefore, its computational burden is large.
2) WAVELET TRANSFORM (WT) Similar to the Fourier transform, the WT is a mathematical transform that uses a group of functions called a wavelet function basis to represent or approximate a signal [159]. The WT can be used not only to characterize the local features of signals in the time and frequency domains but also to conduct a multiscale analysis of functions or signals through operations such as scaling and translation. Thus, great progress in image contrast enhancement has been achieved using
VOLUME 8, 2020

WT methods. In a WT-based image enhancement algorithm, the input image is ﬁrst decomposed into low-frequency and high-frequency image components; then, image components at different frequencies are separately enhanced to highlight the details of the image. The main idea of the wavelet analysis method is to apply wavelet decomposition to the original image to obtain the wavelet coefﬁcients for different subbands, adjust these wavelet coefﬁcients, and then apply the inverse transformation to the new coefﬁcients to obtain the processed image. Such an image enhancement algorithm can enhance an image at multiple scales based on the WT. It is believed that low-illumination conditions have a greater inﬂuence on high-frequency image components, which are generally concentrated at the edges of an image and in contour regions [160]. Therefore, a WT-based algorithm will enhance the high-frequency components of the input image and suppress its low-frequency components. In particular, the dual-tree complex WT can usually achieve satisfactory results [161]–[165].
The basic process of WT-based image enhancement is as follows. Processing of the displacement τ is carried out for the function ψ(t) describing the basic wavelet (parent wavelet); then, a wavelet sequence can be obtained by taking the inner product between the processed ψ(t) and the signal x(t) to be analyzed at various scales a.

WTx (a, τ )

=

1 √
a

+∞
x (t )ψ ∗
−∞

t −τ a

dt

(a > 0) (27)

The equivalent expression in the frequency domain is

√

WTx (a, τ )

=

a 2π

+∞
X (ω)ψ∗(aω)ejωτ dω
−∞

(28)

where X (ω) and ψ(ω) represent the Fourier transforms of x(t) and ψ(t), respectively.
In standard WT-based image enhancement, the input image is usually ﬁrst decomposed into one low-pass subimage and three directional high-pass subimages, namely, a horizontal detail image, a vertical detail image, and a diagonal detail image. The low-pass subimage represents the lowfrequency information in the image, which corresponds to smooth regions. The high-pass subimages represent the highfrequency information in the image, which correspond to detailed image information. Based on the characteristics of these subimages, the most effective method is selected to enhance the coefﬁcients of the different frequency components. Finally, the enhanced image in the spatial domain is obtained through inverse transformation.
The steps of WT-based image enhancement are as follows [163], [164].
(i) The original image is input. (ii) The low-frequency and high-frequency components of the original image are obtained via wavelet decomposition.
87895

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

(iii) The wavelet coefﬁcients are nonlinearly enhanced with a functional relationship that satisﬁes

 Wi + G × (T − 1)

Wi > T



Wo = G × Wi

|Wi| ≤ T

(29)

Wi − G × (T − 1) Wi < −T

where G is the gain for the wavelet coefﬁcient, T is the threshold for the wavelet coefﬁcient, Wi is the wavelet coefﬁcient after image decomposition, and Wo is the wavelet coefﬁcient after enhancement.
(iv) The enhanced wavelet coefﬁcients are inversely transformed to obtain the reconstructed enhanced image.
The basic ﬂow of the WT-based image enhancement process is illustrated in Fig. 20.

FIGURE 20. Flowchart of WT-based image enhancement.

FIGURE 21. WT-based low-light image enhancement (figure best viewed in color).
The results of WT-based image enhancement are shown in Fig. 21, where n is the wavelet scale.
In low-light images, it is difﬁcult to distinguish image noise from image details. A high-frequency analysis is conducted on the WT-decomposed image, and then, to process the decomposed wavelet coefﬁcients, various thresholds and enhancement factors are applied to effectively remove noise while enhancing detail components. Generally, the enhancement effect is better than that of traditional image enhancement algorithms [166]. Zong et al. proposed a contrast enhancement method based on multiscale wavelet analysis [167]. In this method, a multiscale nonlinear high-pass function is used to process the wavelet coefﬁcients, thus enabling the enhancement of ultrasonic images. Loza et al. proposed an adaptive contrast enhancement method based on the statistics of local wavelet coefﬁcients [168]. A model for the local wavelet coefﬁcients was established on the basis of the binary Cauchy distribution, thus yielding a nonlinear enhancement function for wavelet coefﬁcient compression. A WT-based image enhancement algorithm based on a knee function and gamma correction (KGWT) has been proposed in which an improved knee function and a gamma transform function are used to enhance the low-frequency coefﬁcients [169]. Then, after enhancement, the low-frequency coefﬁcients are combined with the high-frequency coefﬁcients, and ﬁnally, the inverse WT is applied to obtain
87896

the enhanced image. The KGWT algorithm improves the overall brightness and contrast of images. A WT-based image enhancement algorithm based on contrast entropy was proposed in [170]. After wavelet decomposition, the lowfrequency components of the image are enhanced via HE, and the high-frequency components are enhanced by maximizing the contrast entropy. Likewise, in [171], the singular value matrices of low-frequency images were obtained with an enhanced wavelet decomposition approach, which also achieved an improved image enhancement effect. A fast and adaptive enhancement algorithm for low-light images based on the WT was proposed in [172]. In this algorithm, the RGB input image is transformed into HSV space, and the discrete wavelet transform (DWT) is applied to the brightness (V ) image to separate high-frequency and low-frequency subbands. The illumination components in the low-frequency WT subbands of the image are rapidly estimated and removed using bilateral ﬁltering, while a fuzzy transformation is used to realize the enhancement and denoising of edge and texture information. WT-based image enhancement theory is often combined with other approaches, such as fuzzy theory, image fusion, and HE. As discussed in [173], [174], after wavelet decomposition is performed on the original image, HE can be performed on each subband image individually. Finally, the inverse WT can be used to reconstruct the enhanced, noise-reduced image [175]. The WT approach has also been combined with Retinex theory to enhance low-light images, thus achieving a better enhancement effect [176], [177]. Russo [178] proposed a method of improving image quality by means of multiscale equalization in the wavelet domain. Chen [179] proposed an image enhancement method that combines wavelet and fractional differential models. The WT can reﬂect both the time-domain and frequency-domain features of an image. Speciﬁcally, this model not only extracts edge information from an image but also extracts its overall structure, which is consistent with the needs of low-light image enhancement. However, because the wavelet basis needs to be deﬁned in advance, the application of this algorithm is limited.
The curvelet transform is a multiscale analysis method developed based on the WT that can overcome the limitations of the WT by enhancing the curved edges in an image [180]. Starck et al. [181] proposed a multiscale analysis method based on the curvelet transform and compared it with the WT algorithm to demonstrate its superiority for color image enhancement. The curvelet-transform-based enhancement algorithm achieves a better effect for noisy images; however, it is not as effective as the WT method for noiseless or nearly noiseless images. In [182], the WT was combined with the curvelet transform to achieve image enhancement with edge preservation. The speciﬁc steps are as follows. First, the curvelet transform is applied to remove noise without the loss of edge details; then, the image is enhanced using the WT. In [183], an improved enhancement algorithm for noisy lowlight color images based on the second-generation curvelet transform was proposed. A compromise factor for the YUV
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

(luminance and chromaticity) color space and an improved gain function were used to suppress, elevate or maintain the curvelet coefﬁcients. This approach effectively suppresses noise while optimally recovering both the edges and smooth parts in an image acquired under low illumination. Thus, the image enhancement quality is effectively improved. The main advantage of image enhancement methods based on the WT is that they allow the time- and frequency-domain features of images to be analyzed on multiple scales [184]. Another advantage of wavelet analysis lies in the reﬁned local analysis capabilities, as such methods have better local characteristics in both the spatial and frequency domains and thus are beneﬁcial for analyzing and highlighting the details of an image. Wavelet analysis is mainly used for infrared images [185], [186] and medical image enhancement [187]. The disadvantage is that overbright illumination cannot be avoided.
In summary, frequency-domain-based algorithms can effectively highlight the details of an image through enhancement of the wavelet coefﬁcients, but they can also easily magnify the noise in the image. Like other frequency-domain transformation methods, these image enhancement methods require large amounts of calculation, and the selection of the transformation parameters often requires manual intervention.
E. METHODS BASED ON IMAGE FUSION Another direction of research on low-light image enhancement involves methods based on image fusion techniques [188]. In these methods, many images of the same scene are obtained with different sensors, or additional images are obtained with the same sensor using various imaging methods or at different times. Finally, as much useful information as possible is extracted from every image to synthesize a high-quality image, thus improving the utilization rate of the image information. The synthesized image can reﬂect multilevel information from the original images to comprehensively describe the scene, thus allowing the available image information to better meet the requirements of both human observers and computer vision systems.
1) MULTISPECTRAL IMAGE FUSION Multispectral image fusion is an improved method of obtaining the details of a low-light imaged scene by fusing a visible image with an infrared image. Near-infrared (NIR) light has a longer wavelength and stronger penetration ability than does visible light, allowing redundant information to be removed from a ﬁltered infrared image. Additionally, a low-light visible image can provide rich background information; consequently, better images can be obtained through image fusion. For example, in a method developed by the US Naval Research Laboratory (NRL), images obtained with an infrared thermal imager are integrated with the R-, G- and B-channel images obtained with a low-light night vision device to obtain night vision color images [189]–[191]. Toet et al. proposed a pseudocolor fusion algorithm for
VOLUME 8, 2020

infrared and visible images [192]; this algorithm enhances the clarity of the image details while retaining the unique information captured by various sensors. Additionally, this algorithm involves only addition and subtraction operations and thus can be implemented using simple hardware in real time [193]. Zhu et al. proposed a fusion framework for night vision applications called night vision context enhancement (FNCE) [194], in which the fused result is obtained by combining decomposed images using three different rules.
Furthermore, many scholars have studied the use of night vision technology for single- and double-channel low-light color fusion based on bispectral and trispectral features. Vision technology has been developed based on low-light and infrared thermal image fusion, low-light and longwave infrared image fusion, ultraviolet and low-light image fusion, and even trispectral color fusion based on low-light, mediumwave and longwave infrared images [195]–[197]. However, the visible and infrared images need to be acquired simultaneously, which constrains such algorithms in terms of the hardware conditions necessary to support them. Moreover, the intelligence and adaptability of these algorithms are poor, and their parameters need to be artiﬁcially set. Therefore, these algorithms have still not been widely adopted.
2) IMAGE FUSION BASED ON BACKGROUND HIGHLIGHTING Generally, image fusion methods based on background highlighting rely on the integration of low-light images with daytime images to enhance the image details, thus improving the visual effect of the low-light images [198]. The general process is described as follows. First, an image is obtained in the daytime under reasonably sufﬁcient lighting conditions for use as the source of the background for the fused image. Then, another image is obtained in the same position under low illumination, and the background of this image is removed. The remainder of the latter image is taken as the foreground of the fused image. Finally, the background and foreground are integrated into a single image using a suitable algorithm. For example, Raskar et al. estimated the intensity of the mixed gradient ﬁeld of multiple low-light images and daytime images of the same scene, thus improving the visual effect of the low-light images [199]. Rao et al. proposed a low-light enhancement method based on video frame fusion [200]. The foreground area of each low-light video frame was fused with the background area from a daytime video frame of the same scene to improve the brightness of the low-light video and compensate for detail loss. In [201], daytime images from the same site at various times were fused, and the ﬁnal fused image was obtained using a moving object extraction technique and weighting processing based on brightness estimation theory. This process is shown in Fig. 22.
Multi-image fusion methods such as those presented in [202], [203] achieve a better enhancement effect but require high-quality daytime video information from the same scene. For example, such methods are not suitable for use in
87897

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

FIGURE 22. Fusion based on background highlighting.
underground mines, because no high-quality daytime video information is available for such areas. Therefore, the applications of these algorithms are limited. Moreover, the large number of iterations required complicates the calculation.
3) FUSION BASED ON MULTIPLE EXPOSURES Image fusion is the process of combining multiple images of the same scene into a single high-quality image that contains more information than any single input image. Petschnigg et al. proposed a method of obtaining various images with both ﬂash and nonﬂash technologies and then realizing low-light image enhancement through image fusion [204]. In this method, a ﬂash image is captured to record detailed information of the scene, and a nonﬂash image is captured to record the brightness information of the background. Then, the image detail information is integrated with the background brightness information. The resulting image contains not only the details from the ﬂash image but also the brightness information from the nonﬂash image. Similarly, high-dynamic-range (HDR) [205]–[207] imaging using multiexposure fusion (MEF) techniques has become very popular in recent years. MEF methods use multiple images of the same scene with different exposures. The ﬁnal HDR image is obtained by synthesizing the best details from the images corresponding to each exposure time. A gradientdomain HDR compression algorithm was proposed in [208]. In this algorithm, different gradients are proportionally compressed in the gradient domain of the images, and Poisson’s equation is solved in the modiﬁed gradient domain to obtain output images with a low dynamic range. This algorithm can also reveal detailed information in areas of various brightness in HDR night images. Li et al. proposed an image enhancement algorithm based on multiple image fusion [209]. In this algorithm, multiple images of the same scene are ﬁrst acquired with different exposure times, and then, various details are extracted from each image. Finally, these details are integrated to generate an enhanced HDR image [210]. Merianos and Mitianoudis combined two image fusion methods, one for the fusion of the luminance channel and one for the fusion of the color channels. The fusion output thus derived outperforms both individual methods [211]. In Ref. [212], the author proposed a new fusion approach in the spatial domain using a propagated image ﬁlter. In the proposed approach, a weight map is calculated for every input image using the propagated image ﬁlter and gradient-domain postprocessing.
87898

The fusion of multiple images acquired from the same scene can be applied to effectively enhance low-light images. Because good-quality image information from the same scene is needed, methods of this kind have stringent requirements in terms of image acquisition; in particular, the camera equipment needs to be stable. Since a long shooting time is required, this method cannot be applied for real-time imaging or video enhancement. Moreover, the enhancement effect for images of globally low brightness is poor.
FIGURE 23. Fusion based on a single image.
4) FUSION BASED ON A SINGLE IMAGE Many scholars have studied the synthesis of the entire dynamic range of a scene [213], [214], including details extracted in a variety of ways from a single image, to break the dependence on image sequences, as shown in Fig. 23. Le and Li [215] improved the contrast of an image by fusing the original image with the image obtained after a logarithmic transformation. Yamakawa and Sugita presented an image fusion technique that used a source image and the Retinexprocessed image to achieve high visibility in both bright and dark areas [216]. In Ref. [217], Wang et al. adaptively generated two new images based on nonlinear functional transformations in accordance with the illumination-reﬂection model and multiscale theory and used a principal component analysis (PCA)-based fusion method to enhance a low-light image. In [218], an adaptive histogram separation method was used to construct underexposed and overexposed images from an original image sequence; these images were then separately processed, and ﬁnally, HDR images were generated via multiexposure image fusion. In addition, Fu et al. [219] proposed an image enhancement algorithm based on the fusion of the results of multiple enhancement techniques. This algorithm integrates multiple image enhancement techniques by means of a linear weighted fusion strategy to improve the enhancement effect. However, this strategy is too complex to satisfy real-time requirements. The algorithm proposed in [220] integrates the color contrast, saturation and exposure brightness of an original or preprocessed image by incorporating MSRCR into a pyramid algorithm using the gold tower technique and specifying different weight parameters depending on the image information to achieve the effective color enhancement of a traditional low-light image. A camera response model (CRM) is often adopted for generating multiple images [221]. In [222], the authors proposed a single-image-based method of generating HDR images based on camera response function (CRF) reconstruction. Ying et al. [223] proposed a novel bioinspired
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

enhancement model in which the source image is generated on the basis of a simulated CRM, and the weight matrix for image fusion is designed using illumination estimation techniques [224], [225]. Unlike the model presented in [223], the model presented in the later cited papers avoids any heuristic judgment of whether an image pixel is underexposed and thus is more ﬂexible in generating more intermediate enhancement results. In Ref. [226], a framework based on a CRM and a weighted least squares strategy was proposed in which every pixel is adjusted in accordance with a calculated exposure map and Retinex theory; this framework can preserve details while improving contrast, color correction, and noise suppression. In addition, Zhou et al. [227] generated multiple enhanced images based on a lightness-aware CRM and then performed mid-level fusion of these images based on a patch-based image decomposition model. This model, however, has a limited ability to improve images in which one area is already overenhanced. In this case, the overenhanced area is even more strongly enhanced, resulting in the loss of important details.
In short, the main idea of methods based on image fusion is that useful information on the same target collected from multiple sources can be further utilized, without requiring a physical model, to obtain a ﬁnal high-quality image through image processing and computer technology.
These fusion-based methods are simple and can achieve good results. However, they require two or more different images of the same scene; therefore, it is difﬁcult to realize image enhancement within a short time, as is needed for realtime monitoring situations, and these methods are difﬁcult to apply and popularize in practice.
F. METHODS BASED ON DEFOGGING MODELS As one branch of the ﬁeld of image enhancement, image defogging techniques have seen great progress and produced good results in recent years. In 2009, He Kaiming proposed the dark channel prior theory for images, which has been widely applied [228]. In 2011, a low-light enhancement algorithm [229], also called a bright channel prior method [230], [231], was proposed by Dong et al. based on defogging theory; this method relies on the statistical analysis of a dark primary color version of a low-light inverted image and a dark primary color version of a foggy image. The main idea of the algorithm is that when an RGB image captured in a dark environment is inverted, the visual effect is similar to that of a daytime image acquired in a foggy environment (as shown in Fig. 24). Hence, a defogging algorithm based on a dark channel prior can be used to process the inverted lowlight image; then, the image can be inverted again to obtain an enhanced low-light image.
This enhancement method greatly improves the image brightness and enhances the visual details of the image by analyzing the features of the low-light image and modeling them with a foggy image degradation model. The basic process is shown in Fig. 25. Low-light image enhancement methods based on dark primary color defogging techniques
VOLUME 8, 2020

FIGURE 24. Comparison of histograms of foggy, low-light and inverted images.
FIGURE 25. Framework of low-light image enhancement based on a dark channel prior.
have certain issues. For example, low-light images inevitably contain noise; however, the foggy image degradation model used in such an algorithm does not consider the effect of noise. Therefore, the image noise will typically be ampliﬁed, which will visually impact the results of image enhancement [232]–[234]. Considering the need for noise processing, Liu Yang et al. optimized the processing speed of such an algorithm without accurately extracting the transmittance; therefore, the ﬁnal enhanced images exhibited a blocky effect. Zhang et al. also proposed an optimized algorithm, in which the parameters for transmittance estimation are selected directly based on experience; consequently, the robustness of this algorithm is poor [235]. Jiang et al. used ﬁlters to remove details and introduced a pyramid operation to calculate a smooth transmission coefﬁcient, which not only improved the processing speed but also yielded better naturalness [236]. Simultaneously, the noise was suppressed. Later, Song et al. [237] improved upon this model to overcome an issue related to block artifacts. Then, Pang introduced a gamma transformation to improve the image contrast [238]. By combining the defogging approach with bilateral ﬁltering, Zhang et al. proposed a low-light image enhancement method that can operate in real time. After the
87899

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

parameters are initially estimated using a dark channel prior, they are optimized using a bilateral ﬁlter; thus, the effect of noise is reduced [239]. Tao et al. combined a bright channel prior with a convolutional neural network (CNN) [240], and Park et al. combined a bright channel prior [241] with a Retinex enhancement algorithm. Both achieved improved results [242]. A fast enhancement algorithm for low-light video has been proposed by combining Retinex theory with dark channel prior theory [243], and this algorithm can be further combined with scene detection, edge compensation and interframe compensation techniques for video enhancement. In [244], a method was proposed to solve the transmittance problem based on a foggy degradation model and a CNN, in which the transmission map and atmospheric light map are amended by means of guided ﬁltering to obtain an enhanced low-light image. Recently, an enhancement method with strong illumination mitigation and bright halo suppression has been presented, which combines a dehazing algorithm with a dark channel prior and a denoising method to achieve a better visual effect [245].
Algorithms based on defogging models offer good performance with low computational complexity. However, their physical interpretation is somewhat lacking, and they are still susceptible to overenhancement in some detailed areas. Inverted low-light images have their own unique characteristics, and the direct application of defogging algorithms to such images is still not an ideal approach for image enhancement.
G. METHODS BASED ON MACHINE LEARNING Most existing low-light image enhancement techniques are model-based techniques rather than data-driven techniques. Only in recent years have methods based on machine learning for image enhancement begun to emerge in signiﬁcant numbers [246]–[248]. For example, in [249], the reﬂection component of an object was represented using a sparse representation method, and the image details contained in the reﬂection component were learned using a dictionary learning method, thus achieving an improved enhancement effect. However, noise can easily be introduced during the machine learning process. An image enhancement method based on a color estimation model (CEM) was proposed by Fu et al. [250], in which the dynamic range of color images in the RGB color space was controlled by adjusting the CEM parameters to effectively inhibit oversaturation of the enhanced images. Fotiadou et al. proposed a low-light image enhancement algorithm based on a sparse image representation [251] in which both a low-light condition dictionary and a daylight condition dictionary were established. The sparse constraint was used as prior knowledge to update the dictionaries, and low-light image blocks were used to approximately estimate the corresponding daylight images. An image enhancement algorithm based on fuzzy rule reasoning [252] was proposed in which three traditional enhancement methods were combined by applying fuzzy theory and machine learning to establish a set of fuzzy rules, and the best
87900

enhancement algorithm was adaptively selected for different images to achieve image enhancement. This method can also be used to objectively and accurately evaluate the image enhancement effect.
Since 2016, several deep-learning-based methods for image enhancement have also emerged. For example, Yan et al. proposed the ﬁrst deep-learning-based method for photo adjustment [253]. Lore et al. adopted a stacked sparse denoising autoencoder in a framework for training an LLNet for low-light image enhancement [254]. In this framework, a sparsity regularized reconstruction loss was taken as the loss function, and deep learning based on the self-encoder approach was used to learn the features of image signals acquired under various low-illumination conditions to realize adaptive brightness adjustment and denoising. Park et al. proposed a dual autoencoder network model based on Retinex theory [255]; in this model, a stacked autoencoder was combined with a convolutional autoencoder to realize low-light enhancement and noise reduction. The stacked autoencoder, with a small number of hidden units, was used to estimate the smooth illumination component in the space, and the convolutional autoencoder was used to process two-dimensional image information to reduce the ampliﬁcation of noise during the process of brightness enhancement.
CNNs have been used as the basis of deep learning frameworks in many research works [256]–[259]. Tao et al. proposed a low-light CNN (LLCNN) in which a multistage characteristic map was used to generate an enhanced image by learning from low-light images with different nuclei [260]. In [261], a global illumination-aware and detail-preserving network (GLADNet) was designed. In this network, the input image is ﬁrst scaled to a certain size and then passed to encoder and decoder networks to generate global prior knowledge of the illumination. Based on this prior information and the original images, a convolutional network is then used to reconstruct the image details. Ignatov et al. took a different approach of learning a mapping between images acquired by a mobile phone camera and a digital single-lens reﬂex (DSLR) camera. They built a dataset consisting of images of the same scene taken by the different cameras [262] and presented an end-to-end deep learning approach for translating ordinary photos into DSLR-quality images. Lv et al. proposed a new network (MBLLEN) consisting of a feature extraction module (FEM), an enhancement module (EM) and a fusion module (FM) [263], which produces output images via feature fusion. Gabriel et al. designed a deep convolutional neural network (DCNN) [264] based on a large dataset of HDR images, and Liu et al. trained the DCNN using only synthetic data to recover the details lost due to quantization [265]. Gharbi et al. constructed a learning framework based on a deep bilateral network, thus achieving real-time processing for image enhancement [266]. Later, Chen et al. [267] introduced a dataset of raw short-exposure low-light images (the See-in-the-Dark (SID) database) and developed a pipeline for processing these images based on a fully convolutional network (FCN). Through end-to-end
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

training, a good improvement over the traditional method of low-light image processing was achieved.
Based on Retinex theory, Shen et al. [268] analyzed the performance of the MSR algorithm from the perspective of a CNN framework and proposed a method of enhancing lowlight images by using an MSR network (MSR-net) based on a CNN architecture, while Guo et al. [269] proposed our pipeline neural network consisting of a denoising net and low light image enhancement net (LLIE-net). Wei et al. assumed that observed images could be decomposed into their reﬂectance and illumination components, collected a LOw-Light (LOL) dataset containing low-/normal-light image pairs, and proposed a deep network called RetinexNet [270]. Li et al. designed a network called LightenNet based on a CNN architecture [271]; this network takes a weakly illuminated image as input and outputs a corresponding illumination map, which is subsequently used to generate an enhanced image based on the Retinex model. Zhang et al. built a simple yet effective network called the Kindling the Darkness (KinD) network [272], which is composed of a layer decomposition net, a reﬂectance restoration net, and an illumination adjustment net, and trained it on pairs of images captured under different exposure conditions.
Inspired by the multiple image fusion method, Cai et al. [273] proposed a framework based on a CNN trained to enhance single images. In this work, thirteen different MEF and HDR compression methods were used to generate an enhanced image for each series of images from a large-scale multiexposure image dataset. Finally, low-light images were enhanced by the CNN after end-to-end training on the lowcontrast and high-contrast image dataset. Yang et al. used two CNNs to build a tool for RGB image enhancement [274] in which intermediate HDR images are ﬁrst generated from the input RGB images to ultimately produce high-quality LDR images. Nevertheless, generating HDR images from single images is a challenging problem. To handle both local and global features, Kinoshita and Kiya proposed an architecture consisting of a local encoder, a global encoder, and a decoder trained on tone-mapped images obtained from existing HDR images [275]. Experimental results showed its excellent performance compared with conventional image enhancement methods, including CNN-based methods.
In contrast to supervised learning methods, generative adversarial network (GAN)-based methods can be used for image enhancement without training on pairs of images [276], [277]. For example, Meng et al. proposed a GAN-based framework for nighttime image enhancement, which takes advantage of GANs’ powerful ability to generate images from real data distributions, and the results demonstrate its effectiveness. To our knowledge, this was the ﬁrst time that GANs were applied for the purpose of nighttime image enhancement [278]. In Ref. [279], the authors fully utilized the advantages of both GANs and CNNs, using a transitive CNN to map the enhanced images back to the space of the source images to relax the need for paired groundtruth photos. Kim et al. [280] applied local illumination to
VOLUME 8, 2020

make the training images and used an advanced generative adversarial network to build Low-Lightgan. The key advantages of such networks are that they can be trained easily and can achieve better experimental results than traditional enhancers [279].
Undoubtedly, deep-learning-based methods can achieve excellent performance in low-light image enhancement, and they also represent a major trend of current development in image processing research. However, such methods must be supported by large datasets, and an increase in the complexity of a model will lead to a sharp increase in the time complexity of the corresponding algorithm. With the steady growth of research on low-light image enhancement, not only are some low-light data available from widely used public benchmark datasets such as PASCAL VOC [281], ImageNet [282], and Microsoft COCO [283], but researchers are also building public datasets speciﬁcally designed for low-light image processing, such as SID [267] and EDD (Exclusively Dark Dataset) [284].
III. EVALUATION METHODS Image quality assessment (IQA) focuses mainly on two aspects, namely, the ﬁdelity of the image and the readability of the image, which can be regarded as subjective and objective evaluation standards, respectively. A subjective evaluation method measures image quality on the basis of the subjective perception of the human visual system, i.e., whether the image conveys a certain experience. However, it is still difﬁcult to accurately simulate the human visual system. Therefore, current subjective evaluation systems based on the human visual system can evaluate image quality only qualitatively rather than quantitatively [285].
A. SUBJECTIVE EVALUATION In a subjective evaluation method, human observers are asked to evaluate the quality of processed images in accordance with their visual effects based on a predetermined evaluation scale. Such an evaluation depends on subjective assessment of the image processing results to determine the advantages and disadvantages of a particular algorithm. The score is typically divided into 5 grades (1-5 points), and the number of raters should typically be no fewer than 20 [286]. Some of the raters should have experience in image processing, while some should not. The raters will evaluate the visual effects of the images in accordance with their personal experience or agreed-upon evaluation criteria. To ensure fairness and equity, the ﬁnal scores will be weighted to obtain the ﬁnal subjective quality evaluation result for each image. The typical evaluation standards are summarized in Table 1.
This method is simple and can reﬂect the visual quality of images. Such a subjective evaluation can accurately represent the visual perception of the majority of observers. However, such an evaluation lacks stability and can be easily affected by the experimental conditions as well as the knowledge background, emotional state, motivation and degree of fatigue of the observer. In studies related to image enhancement,
87901

TABLE 1. Criteria for subjective assessment.

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods TABLE 2. NIQA metrics and related references.

it is necessary to provide key details of different magniﬁed parts of images for comparison to assess, e.g., lack of uniformity. However, this process is time consuming and arduous in practice and thus often cannot be applied in engineering applications.

B. OBJECTIVE EVALUATION An objective evaluation is an evaluation using speciﬁc data and based on certain objective criteria. To the best of our knowledge, there are no IQA methods that have been speciﬁcally designed for the evaluation of low-light image enhancement methods. Hence, different researchers utilize different strategies to evaluate their results. At present, the objective evaluation methods for image enhancement can be divided into full-reference methods and no-reference methods depending on whether they require reference images (ground-truth images or synthetic images). Objective evaluation methods have various advantages, such as simple calculations, fast execution, ease of quantitative calculation based on a constructed model, and high stability; therefore, data from objective evaluations are generally adopted as image quality scores [287].
1) NO-REFERENCE IQA (NIQA) METRICS Since no objective reference image is available in the case of a low-light input image, most methods that are suitable for low-light image enhancement assessment are based on NIQA metrics. The most common NIQA metrics include the mean value (MV), standard difference (STD), average gradient (AG), and information entropy (IE). In addition, there are several general methods available for image quality evaluation, including the Blind/Referenceless Image Spatial QUality Evaluator (BRISQUE) [288], the Naturalness Image Quality Evaluator (NIQE) [289], the BLind Image Integrity Notator using DCT Statistics (BLIINDS-II) [290], the blind tonemapped image quality index (BTMQI) [291], gradient ratioing at visible edges (GRVE) [292], the autoregressive-based image sharpness metric (ARISM) [293], the no-reference image quality metric for contrast distortion (NIQMC) [294], the Global Contrast Factor (GCF) [295], the average information content (AIC) [296], the effective measure of enhancement (EME) [297], PixDist [298], and the no-reference free-energy-based robust metric (NFERM) [299]. The commonly used NIQA metrics and related references are shown in Table 2. Descriptions of several of these metrics follow.
87902

(i) Mean value (MV). The MV mainly refers to the mean of the gray values of an image, and it mainly reﬂects the color or degree of brightness of the image. The smaller the image mean is, the darker the image. Conversely, the larger the mean is, the brighter the image, and the lighter the colors. The formula is as follows:

µ= 1

MN
f (i, j)

(30)

M ×N

i=1 j=1

where M and N are the width and height, respectively, of the image and f (i, j) is the gray value at pixel point (i, j).
(ii) Standard difference (STD). The variance of the gray values reﬂects the degree of dispersion of the image relative to the mean and thus is a measure of the contrast within a certain range. The larger the variance is, the more information is contained in the image, and the better the visual effect. When the variance is smaller, the information contained in the image is less, and the image is more monochromatic and uniform. The formula is

MN
f (i, j)(f (i, j) − µ)2

i=1 j=1
STD =

(31)

M ×N

where M and N are the image width and height, respectively; f (i, j) is the gray value at pixel point (i, j); and µ is the MV of the image
(iii) Average gradient (AG). The AG represents the clarity of an image, reﬂecting the image’s ability to express contrasting details. This metric measures the rate of change in the image values based on changes in the contrast of minute details or the relative clarity of the image. In an image, faster gray changes in a certain direction result in larger image gradients; therefore, this metric can be used to determine

VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

whether or not an image is clear. The AG can be expressed as

1 M N (∂f /∂x)2 + (∂f /∂y)2

AG =

(32)

M ×N

2

i=1 j=1

where M and N are the image width and height, respectively, and ∂f /∂x and ∂f /∂x are the horizontal and vertical gradients, respectively.
(iv) Information entropy (IE). Entropy can be used as a measure of an amount of information and is widely used to evaluate image quality [300,48]. A static image is regarded as an information source with random output; the set A of source symbols is deﬁned as the set of all possible symbols {ai}, and the probability of source symbol ai is P(ai). Thus, the average information quantity of an image can be expressed as

L

H = − P(ai) log2 P(ai)

(33)

i=1

According to entropy theory, the larger the IE value is, the larger the amount of information contained in the image, and the richer the image detail.

2) FULL-REFERENCE IQA (FIQA) METRICS
The most common FIQA metrics include the mean square error (MSE), the peak signal-to-noise ratio (PSNR), the structural similarity index metric (SSIM) [301], and the lightness order error (LOE) [136]. Other available FIQA metrics include the patch-based contrast quality index (PCQI) [302], the colorfulness-based PCQI (CPCQI) [303], the Gradient Magnitude Similarity Deviation (GMSD) [304], the visual information ﬁdelity (VIF) [305], the visual saliency index (VSI) [306], the tone-mapped image quality index (TMQI) [307], the Statistical Naturalness Measure (SNM) [307], and the Feature SIMilarity Index (FSIM) [308]. The commonly used NIQA metrics and related references are shown in Table 3. Descriptions of several of these metrics follow.
(i) Mean square error (MSE). This metric represents the direct deviation between the enhanced image and the original image; it has the same meaning as the absolute mean brightness error (AMBE) [75].

1M MSE =
M ×N

N
[f (i, j) − fe(i, j)]

(34)

i=1 j=1

where M and N are the width and height, respectively, of the image; f (i, j) represents the input image; and fe(i, j) represents the enhanced image. In an image quality evaluation, a smaller MSE value indicates higher similarity between the enhanced and original images.
(ii) Peak signal-to-noise ratio (PSNR). The PSNR of an image is the most extensively and commonly used objective evaluation method for measuring the image denoising effect. The larger the PSNR value is, the smaller the difference

VOLUME 8, 2020

TABLE 3. FIQA metrics and related references.

between the images before and after processing. An excessively high PSNR indicates that the effect of the denoising algorithm is not obvious. A smaller PSNR indicates a greater difference between the images before and after processing. An excessively low PSNR may suggest that the image is distorted. The speciﬁc expression is as follows:

PSNR = 10 lg fm2ax

(35)

MSE

where fmax is the maximum gray value, fmax = 255. (iii) Structural similarity index metric (SSIM). The above

methods do not consider the characteristics of the human

visual system when assessing image quality; they compute

only a simple random error between the input image and

the processed image and analyze the difference between the

input and output images from a mathematical perspective.

Therefore, the above metrics cannot fully and accurately

reﬂect the image quality. Researchers have found that nat-

ural images exhibit certain special structural features, such

as strong correlations between pixels, and these correlations

capture a large amount of important structural information for

an image. Therefore, Wang et al. proposed a method based

on structural similarity for evaluating image quality [301].

The SSIM evaluates the quality of a processed image relative

to the reference image based on comparisons of luminance

(l(f, fe)), contrast (c(f, fe)) and structure (s(f, fe)) between the two images. These three values are combined to obtain the

overall similarity measure. The formula is as follows:

SSIM = F[1(f , fe), c(f , fe), s(f , fe)]

(36)

The ﬂow of the SSIM algorithm is shown in Fig. 26. The degree of similarity between the two images is reﬂected by the value of the SSIM; the minimum value is 0, and the maximum value is 1. A value closer to 1 indicates

87903

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

FIGURE 26. Flowchart of the SSIM algorithm.

FIGURE 27. Low-light images under three illumination conditions (figure best viewed in color).

that the two images are more similar. Taking the human visual system as the starting point, this method can effectively simulate human visual perception to extract information about the structure of an image. The evaluation result is very close to the subjective perception of the human eye; therefore, this metric is widely used in image quality evaluations.
(iv) Lightness order error (LOE). Considering that the relative order of lightness of different image areas reﬂects the direction of the light source and the variation in illumination, Ref. [136] proposed the LOE metric to measure the discrepancy in lightness order between an original image I and its enhanced version Ie. The LOE is deﬁned as

1 LOE =

MN
RD(i, j)

(37)

M ×N

i=1 j=1

where RD(i, j) is the difference in the relative lightness order
between the original image f and its enhanced version fe at pixel (i, j). This difference is deﬁned as follows:

M
RD(i, j) =
x

N
U (L(i, j), L(x, y)) ⊕ U (Le(i, j), Le(x, y))
y
(38)

where M and N are the image width and height, respectively; ⊕ is the exclusive-or operator; and L(i, j) and Le(i, j) are the maximum values among the three color channels at location (i, j) for f and fe, respectively. The function U (p, q) returns a value of 1 if p >= q; otherwise, it returns 0. The smaller the LOE value is, the better the lightness order is preserved.
The above measures have the following advantages: they are simple to calculate, have clear physical meanings, and enable mathematically convenient optimizations.

IV. ANALYSIS OF DIFFERENT ENHANCEMENT METHODS To compare the enhancement effects of various algorithms as well as the consistency of subjective and objective evaluation, experiments using many methods are presented in this paper for illustration. A test platform was built based on a desktop computer to verify the algorithms. This system includes an Intel(R) Core(TM) i7-6700 CPU @3.4 GHz with 16 GB RAM and the Windows 10 operating system.
87904

FIGURE 28. Two pairs of images with different exposures (figure best viewed in color).
A. SUBJECTIVE EVALUATION The test images shown in Fig. 27 represent three different illumination conditions, namely, uniform low light, uneven illumination and nighttime; the source images are named ‘Flowers.bmp’, ‘Building.bmp’ and ‘Lawn.bmp’, respectively. In addition, we adopt two pairs of images for referencebased comparisons, where each pair consists of a low-light image, as shown in the top row of Fig. 28, and a corresponding well-exposed image, as shown in the bottom row. In Fig. 28, the image on the left is named ‘Desk.bmp’, and the image on the right is ‘Road.bmp’.
The experimental results are shown in Figs. 29-33. In these ﬁgures, panel (a) contains the original image, and panels (b)-(r) display the results of many enhancement methods: Gamma correction [3], AHE [78], WT [166], BIMEF [223], CegaHE [102], CRM [225], CVC [89], Dong et al. [229], MBLLEN [263], HMF [101], LIME [13], MF [219], HE [66], MSRCP [127], MSRCR [115], NPE [136], and SRIE [126]. As shown in panels (b)-(r), all of these image enhancement methods improve the visual effect of the original image to some degree. The details become clearer with the Gamma, WT, AHE, HMF, CVC, MSRCP and SRIE methods, but the overall level of brightness is dark. Especially when the Gamma and CVC methods are used, the enhancement effects for the three types of images are similar, while the WT method makes the output image blurred. The AHE method achieves a better effect when processing uniformly illuminated lowlight images, but a wheel halo effect appears in unevenly illuminated low-light images. Although the CegaHE, HE, and MSRCR methods can brighten the entire image, the hue changes dramatically for an image with uneven illumination, resulting in the loss of the real color of the original scene. Although the image brightness after processing with the SRIE method is not high, this method achieves a consistent image processing effect for all three types of images, and the tone recovery effect is superior. By comparison, the Dong, MBLLEN, MF, NPE, LIME and CRM methods demonstrate
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

FIGURE 29. Experimental results on ‘Flowers.bmp’ (figure best viewed in color).

FIGURE 30. Experimental results on ‘Building.bmp’ (figure best viewed in color).

outstanding performance in color and detail enhancement, and their visual effects are obviously superior to those of the other abovementioned image enhancement methods. However, when the Dong and NPE methods are used to process unevenly illuminated low-light images such as the ‘Building’ and ‘Road’ images, overenhancement appears at the boundaries. The MF method, MBLLEN method and CRM method better maintain the color of the original images compared with the above methods, but their overall effect is no better than that of the LIME method. The LIME method considers both brightness and hue information and maintains excellent realistic effects. Hence, the LIME method achieves higher color ﬁdelity from the perspective of human visual perception.

B. OBJECTIVE EVALUATION Based on the above images, experiments for objective quality evaluation were performed using various IQA methods, including both NIQA and FIQA metrics.
1) NIQA-BASED EVALUATION Eight metrics, namely, STD, IE, AG, BLIINDS-II [290], NIQE [289], BRISQUE [288], the contrast enhancementbased contrast-changed image quality measure (CEIQ) [311], and the spatial–spectral entropy-based quality measure (SSEQ) [312], were employed for NIQA-based evaluation. The experimental results obtained on ‘Flowers.bmp’, ‘Building.bmp’ and ‘Lawn.bmp’ are shown in Tables 4–6, and the best score in terms of each metric is highlighted in bold.

VOLUME 8, 2020

87905

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

FIGURE 31. Experimental results on ‘Lawn.bmp’ (figure best viewed in color).

FIGURE 32. Experimental results on ‘Desk.bmp’ (figure best viewed in color).

These data show that the different evaluation metrics assign different scores to the same image enhancement algorithm and that the interpretations of the evaluation results are completely opposite in some cases. The reason is that the traditional IQA metrics used in this evaluation consider different aspects of the image obtained after enhancement. For the three images, no method gets the best score on all metrics. The HE method achieves the highest scores in terms of the IE and CEIQ metrics on the three images. The LIME method achieves the best scores in terms of the AG metric for ‘Building.bmp’ and ‘Lawn.bmp’. The CVC method achieves the best score on STD metric for ‘‘Flower.bmp’’ and ‘Lawn.bmp’. Overall, the HE and CVC methods are the

top two scoring methods based on these metrics. However, to some extent, the distortion of chrominance information causes the results of the objective evaluation to be opposite to those of the subjective evaluation.
2) FIQA-BASED EVALUATION For the FIQA-based evaluation, eleven metrics were selected, namely, MSE, PSNR, SSIM [301], LOE [136], PCQI [302], GMSD [304], VIF [305], VSI [306], FSIM [308], RVSIM [313], and IFC [314]. The reference images for ‘Desk.bmp’ and ‘Road.bmp’ are shown in panel (a) of Fig. 33 and Fig. 34, and the experimental data are listed in Tables 7 - 8, where the best score in terms of each metric

87906

VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

FIGURE 33. Experimental results on ‘Road.bmp’ (figure best viewed in color). TABLE 4. Objective evaluation of various methods on ‘Flowers.bmp’ using NIQA metrics.
TABLE 5. Objective evaluation of various methods on ‘Building.bmp’ using NIQA metrics.
TABLE 6. Objective evaluation of various methods on ‘Lawn.bmp’ using NIQA metrics.

is highlighted in bold. From these data, it can be seen that the best scores in terms of the different metrics are relatively concentrated among certain image enhancement algorithms.

For example, the CegaHE method earns the best scores according to seven of the above eleven metrics on ‘Desk.bmp’. For the image ‘Road.bmp’, BIMEF, CRM and

VOLUME 8, 2020

87907

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods TABLE 7. Objective evaluation of various methods on ‘Desk.bmp’ using FIQA metrics.
TABLE 8. Objective evaluation of various methods on ‘Road.bmp’ using FIQA metrics.
TABLE 9. Comparison of time complexity (unit: seconds).

MF achieve four, four and two of the best scores, respectively. In addition, these three methods have the same scores in terms of the GMSD and VSI metrics for the evaluation on ‘Road.bmp’. To some extent, FIQA-based evaluations provide a more accurate description of the images and are more consistent with subjective evaluation results than NIQA-based evaluations are for cases in which reference images are available.

C. TIME COMPLEXITY To test the processing speeds of the various methods, experiments were performed using images of various sizes, and all algorithms were run using MATLAB except the MELLEN method [263]. Table 9 shows that the Retinex-based methods (MSR, MSRCR, and MSRCP) have high computational complexities because of their multiscale Gaussian ﬁltering operations. The NPE, SRIE and MELLEN methods have

87908

VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

the lowest computational efﬁciency for processing a single image because of the use of iterative computations to ﬁnd the optimal solution. When processing an image with pixel dimensions of 3200 × 2400, their processing times are 206 seconds, 649 seconds and 890 seconds, respectively. In contrast, gamma correction and the various HE-based methods (AHE, HMF, and CegaHE) are faster, and their run times are only slightly affected by an increase in the image size. In particular, when the gamma correction method is run on an image of 3200 × 2400 pixels, it needs only 70 milliseconds to run, which is 1/12700th of the run time of the SRIE method. Therefore, the gamma correction method has an absolute advantage in terms of run time. For images with pixel dimensions of 1600 × 1200, the gamma method methods and the HE-based methods can be used under realtime conditions.
The IQA metrics considered above are not completely consistent with subjective human perception and thus are not suitable for the direct evaluation of enhanced low-light images. They need to be combined with subjective evaluations based on human vision. Therefore, there is a great need to design and develop an objective quality assessment method for low-light image enhancement that shows good agreement with the mechanism of human vision.
V. CONCLUSION This paper summarizes seven widely used classes of low-light image enhancement algorithms and their improved versions and describes the underlying principles of the different methods. Then, it introduces the current quality evaluation system for low-light images and identiﬁes the problems with this existing system. Finally, many representative image enhancement methods are evaluated using both subjective and objective evaluation methods. The characteristics and performance of the existing methods are analyzed and summarized, and the shortcomings of the present work in this ﬁeld are further revealed. The essential purpose of low-light image enhancement is to improve the image contrast both globally and locally in a certain range of the gray space in accordance with the distribution of the gray values of the original image pixels. Simultaneously, it should be ensured that the enhanced image shows good image quality with regard to the characteristics of human visual perception, noise suppression, image entropy maximization, brightness maintenance, etc. The merits and shortcomings of the various methods are summarized in Table 10.
Based on the limitations of the current methods, care must be taken in image enhancement to ensure an appropriate balance among several factors, such as the image color, visual effect and information entropy, while attempting to improve the visibility of the image contrast. However, the existing algorithms all have certain disadvantages, such as loss of detail, color distortion, or high computational complexity; thus, current low-light image enhancement techniques cannot guarantee the performance of a vision system in a lowlight environment. In future research on low-illumination
VOLUME 8, 2020

TABLE 10. Merits and shortcomings of different methods.
image enhancement, researchers should focus on the following tasks:
(i) Improve the robustness and adaptive capabilities of low-light image enhancement algorithms. The robustness and adaptive capabilities of the existing methods are insufﬁcient to meet the requirements of practical applications. The ideal method should be able to adaptively adjust to different application conditions and different types of low-light images.
(ii) Reduce the computational complexity of the available algorithms. To satisfy the needs of practical applications, realtime methods are often in demand; however, most of the existing methods currently require a long processing time. In addition, the results of the existing methods are still susceptible to certain problems, such as color deviations and detail ambiguity. The high-performance processors in graphics processing units (GPUs) allow such algorithms to be parallelized, which can signiﬁcantly improve their processing speed and may ultimately enable real-time image enhancement.
(iii) Establish a standard quality evaluation system. At present, there are too few specialized low-light image datasets, and the quality evaluation system is not mature. This limits the further development of this research ﬁeld and the selection of suitable enhancement and restoration methods for practical applications.
(iv) Develop a video-based enhancement algorithm. Currently, most of the research in this ﬁeld has focused on single images, and research on video enhancement has not received sufﬁcient attention; by contrast, video processing plays a greater role in practical applications. There is an urgent need to solve the problems related to the efﬁciency of low-illumination video processing, interframe consistency and so on.
In summary, thus far, no image enhancement algorithm exists that is optimal in terms of all of the above issues simultaneously. Therefore, it is necessary to select the most suitable image enhancement algorithm based on
87909

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

application-speciﬁc requirements. It is hoped that image enhancement technology can be advanced to a higher level through in-depth studies of these enhancement algorithms, thus allowing this technology to play an important role in multiple disciplines.

APPENDIX Abbreviation HE CDF GHE LHE BBHE
DSIHE
MMBEBHE
IBBHE
AHE POSHE
CLAHE
FIRF RMSHE
CVC BBPHE
GCCHE
RSIHE
DHE BPDHE
EDSHE
BHEPL
MMSICHE
ESIHE
AMHE
WHE HMF CegaHE
SSR MSR MSRCR KBR

Phrase

Histogram equalization

Cumulative distribution function

Global histogram equalization

Local histogram equalization

Brightness-preserving

bi-histogram

equalization

Dualistic subimage histogram

equalization

Minimum mean brightness error

bi-histogram equalization

Iterative of brightness bi-histogram

equalization

Adaptive histogram equalization

Partially overlapped subblock histogram

equalization

Contrast-limited adaptive histogram

equalization

Finite impulse response ﬁlter

Recursive mean-separate histogram

equalization

Contextual and variational contrast

Background

brightness-preserving

histogram equalization

Gain-controllable clipped histogram

equalization

Recursive subimage histogram

equalization

Dynamic histogram equalization

Brightness-preserving dynamic histogram

equalization

Entropy- based dynamic subhistogram

equalization

Bi-histogram equalization with a plateau

limit

Median-mean based subimage-clipped

histogram equalization

Exposure-based subimage histogram

equalization

Adaptively modiﬁed histogram

equalization

Weighted histogram equalization

Histogram modiﬁcation framework

Gap Adjustment for Histogram

Equalization

Single-scale Retinex

Multiscale Retinex

Multiscale Retinex with color restoration

Kernel-based Retinex

87910

SRIE
MSRCP
NPE WT DCT KGWT HDR CNN DCP CEM LLCNN MEF IQA HVS AG MSE PSNR SSIM BIQI BRISQUE
NIQE

Simultaneous reﬂectivity and illumination estimation Multiscale Retinex with chromaticity preservation Naturalness preserved enhancement Wavelet transform Discrete cosine transform Knee function and Gamma correction High dynamic range Convolutional neural network Dark channel priori Color estimation model Low-light CNN Multiple exposure image fusion Image quality assessment Human visual system Average gradient Mean square error Peak signal-to-noise Structural similarity index Blind image quality index Blind/referenceless image spatial quality evaluation Naturalness image quality evaluator

ACKNOWLEDGMENT The authors thank AJE for linguistic assistance during the preparation of this manuscript.

REFERENCES
[1] H. Wang, Y. Zhang, and H. Shen, ‘‘Review of image enhancement algorithms,’’ (in Chinese), Chin. Opt., vol. 10, no. 4, pp. 438–448, 2017.
[2] W. Wang, X. Yuan, X. Wu, and Y. Liu, ‘‘Fast image dehazing method based on linear transformation,’’ IEEE Trans. Multimedia, vol. 19, no. 6, pp. 1142–1155, Jun. 2017.
[3] M. Fang, H. Li, and L. Lei, ‘‘A review on low light video image enhancement algorithms,’’ (in Chinese), J. Changchun Univ. Sci. Technol., vol. 39, no. 3, pp. 56–64, 2016.
[4] J. Yu, D. Li, and Q. Liao, ‘‘Color constancy-based visibility enhancement of color images in low-light conditions,’’ (in Chinese), Acta Automatica Sinica, vol. 37, no. 8, pp. 923–931, 2011.
[5] S. Ko, S. Yu, W. Kang, C. Park, S. Lee, and J. Paik, ‘‘Artifact-free lowlight video enhancement using temporal similarity and guide map,’’ IEEE Trans. Ind. Electron., vol. 64, no. 8, pp. 6392–6401, Aug. 2017.
[6] X. Fu, G. Fan, Y. Zhao, and Z. Wang, ‘‘A new image enhancement algorithm for low illumination environment,’’ in Proc. IEEE Int. Conf. Comput. Sci. Autom. Eng., Jun. 2011, pp. 625–627.
[7] S. Park, K. Kim, S. Yu, and J. Paik, ‘‘Contrast enhancement for low-light image enhancement: A survey,’’ IEIE Trans. Smart Process. Comput., vol. 7, no. 1, pp. 36–48, Feb. 2018.
[8] K. Yang, X. Zhang, and Y. Li, ‘‘A biological vision inspired framework for image enhancement in poor visibility conditions,’’ IEEE Trans. Image Process., vol. 29, pp. 1493–1506, Sep. 2019, doi: 10.1109/TIP.2019.2938310.
[9] C. Dai, M. Lin, J. Wang, and X. Hu, ‘‘Dual-purpose method for underwater and low-light image enhancement via image layer separation,’’ IEEE Access, vol. 7, pp. 178685–178698, 2019.
[10] Y.-F. Wang, H.-M. Liu, and Z.-W. Fu, ‘‘Low-light image enhancement via the absorption light scattering model,’’ IEEE Trans. Image Process., vol. 28, no. 11, pp. 5679–5690, Nov. 2019.
[11] M. Kim, D. Park, D. K. Han, and H. Ko, ‘‘A novel framework for extremely low-light video enhancement,’’ in Proc. IEEE Int. Conf. Consum. Electron., Jan. 2014, pp. 91–92.
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

[12] M. H. Conde, B. Zhang, K. Kagawa, and O. Loffeld, ‘‘Low-light image enhancement for multiaperture and multitap systems,’’ IEEE Photon. J., vol. 8, no. 2, pp. 1–25, Apr. 2016.
[13] X. Guo, Y. Li, and H. Ling, ‘‘LIME: Low-light image enhancement via illumination map estimation,’’ IEEE Trans. Image Process., vol. 26, no. 2, pp. 982–993, Feb. 2017.
[14] Q. Mu, Y. Wei, and J. Li, ‘‘Research on the improved Retinex algorithm for low illumination image enhancement,’’ (in Chinese), J. Harbin Eng. Univ., vol. 39, no. 12, pp. 1–7, Jan. 2018.
[15] K. Aditya, V. Reddy, and R. Hariharan, ‘‘Enhancement technique for improving the reliability of disparity map under low light condition,’’ in Proc. Int. Conf. Innov. Autom. Mechatronics Eng., 2014, pp. 236–243.
[16] Z. Shi, M. Zhu, B. Guo, and M. Zhao, ‘‘A photographic negative imaging inspired method for low illumination night-time image enhancement,’’ Multimedia Tools Appl., vol. 76, no. 13, pp. 15027–15048, Jul. 2017.
[17] R. Chandrasekharan and M. Sasikumar, ‘‘Fuzzy transform for contrast enhancement of nonuniform illumination images,’’ IEEE Signal Process. Lett., vol. 25, no. 6, pp. 813–817, Jun. 2018.
[18] Y. Chen, X. Xiao, H.-L. Liu, and P. Feng, ‘‘Dynamic color image resolution compensation under low light,’’ Optik, vol. 126, no. 6, pp. 603–608, Mar. 2015.
[19] J. Zhu, L. Li, and W. Jin, ‘‘Natural-appearance colorization and enhancement for the low-light-level night vision imaging,’’ (in Chinese), Acta Photonica Sinica, vol. 47, no. 4, pp. 159–198, 2018.
[20] L. Jinhong and Z. Mei, ‘‘Design and realization of low-light-level CMOS image sensor,’’ (in Chinese), Infr. Laser Eng., vol. 47, no. 7, 2018, Art. no. 720002.
[21] N. Faramarzpour, M. J. Deen, S. Shirani, Q. Fang, L. W. C. Liu, F. de Souza Campos, and J. W. Swart, ‘‘CMOS-based active pixel for low-light-level detection: Analysis and measurements,’’ IEEE Trans. Electron Devices, vol. 54, no. 12, pp. 3229–3237, Nov. 2007.
[22] Z. Yuantao, C. Mengyang, S. Dexin, and L. Yinnian, ‘‘Digital TDI technology based on global shutter sCMOS image sensor for low-lightlevel imaging,’’ (in Chinese), Acta Optica Sinica, vol. 38, no. 9, 2018, Art. no. 0911001.
[23] S. Hao, Z. Feng, and Y. Guo, ‘‘Low-light image enhancement with a reﬁned illumination map,’’ Multimedia Tools Appl., vol. 77, no. 22, pp. 29639–29650, Nov. 2018.
[24] F. Zhang, X. Wei, and S. Qiang, ‘‘A perception-inspired contrast enhancement method for low-light images in gradient domain,’’ (in Chinese), J. Comput.-Aided Des. Comput. Graph., vol. 26, no. 11, pp. 1981–1988, 2014.
[25] Y.-H. Shiau, P.-Y. Chen, H.-Y. Yang, and S.-Y. Li, ‘‘A low-cost hardware architecture for illumination adjustment in real-time applications,’’ IEEE Trans. Intell. Transp. Syst., vol. 16, no. 2, pp. 934–946, Sep. 2015.
[26] C.-C. Leung, K.-S. Chan, H.-M. Chan, and W.-K. Tsui, ‘‘A new approach for image enhancement applied to low-contrast-low-illumination IC and document images,’’ Pattern Recognit. Lett., vol. 26, no. 6, pp. 769–778, May 2005.
[27] S.-Y. Yu and H. Zhu, ‘‘Low-illumination image enhancement algorithm based on a physical lighting model,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 29, no. 1, pp. 28–37, Jan. 2019.
[28] H.-J. Yun, Z.-Y. Wu, G.-J. Wang, G. Tong, and H. Yang, ‘‘A novel enhancement algorithm combined with improved fuzzy set theory for low illumination images,’’ Math. Problems Eng., vol. 2016, no. 8, 2016, Art. no. 8598917.
[29] S. Sun, Y. Dong, and C. Tang, ‘‘An enhanced algorithm for single nighttime low illuminated vehicle-mounted video image,’’ (in Chinese), Comput. Technol. Develop., vol. 28, no. 4, pp. 50–54, 2018.
[30] W. Wang, F. Chang, T. Ji, and X. Wu, ‘‘A fast single-image dehazing method based on a physical model and gray projection,’’ IEEE Access, vol. 6, pp. 5641–5653, 2018.
[31] J. Lim, M. Heo, C. Lee, and C.-S. Kim, ‘‘Contrast enhancement of noisy low-light images based on structure-texture-noise decomposition,’’ J. Vis. Commun. Image Represent., vol. 45, pp. 107–121, May 2017.
[32] G. Lyu, H. Huang, H. Yin, S. Luo, and X. Jiang, ‘‘A novel visual perception enhancement algorithm for high-speed railway in the low light condition,’’ in Proc. 12th Int. Conf. Signal Process., Oct. 2014, pp. 1022–1025.
[33] S.-C. Pei and C.-T. Shen, ‘‘Color enhancement with adaptive illumination estimation for low-backlighted displays,’’ IEEE Trans. Multimedia, vol. 19, no. 8, pp. 1956–1961, Aug. 2017.
VOLUME 8, 2020

[34] J. Zhang, P. Zhou, and Q. Zhang, ‘‘Low-light image enhancement based on iterative multi-scale guided ﬁlter Retinex,’’ (in Chinese), J. Graph., vol. 39, no. 1, pp. 1–11, 2018.
[35] Y. Li, J. Wang, R. Xing, X. Hong, and R. Feng, ‘‘A new graph morphological enhancement operator for low illumination color image,’’ in Proc. 7th Int. Symp. Comput. Intell. Design, Dec. 2014, pp. 505–508.
[36] H. Su and C. Jung, ‘‘Perceptual enhancement of low light images based on two-step noise suppression,’’ IEEE Access, vol. 6, pp. 7005–7018, 2018.
[37] D. Mao, Z. Xie, and X. He, ‘‘Adaptive bilateral logarithm transformation with bandwidth preserving and low-illumination image enhancement,’’ (in Chinese), J. Image Graph., vol. 22, no. 10, pp. 1356–1363, 2017.
[38] X. Sun, H. Liu, S. Wu, Z. Fang, C. Li, and J. Yin, ‘‘Low-light image enhancement based on guided image ﬁltering in gradient domain,’’ Int. J. Digit. Multimedia Broadcast., vol. 2017, Aug. 2017, Art. no. 9029315.
[39] R. Song, D. Li, and X. Wang, ‘‘Low illumination image enhancement algorithm based on HSI color space,’’ (in Chinese), J. Graph., vol. 38, no. 2, pp. 217–223, 2017.
[40] B. Gupta and T. K. Agarwal, ‘‘New contrast enhancement approach for dark images with non-uniform illumination,’’ Comput. Electr. Eng., vol. 70, pp. 616–630, Aug. 2018.
[41] L. Han, J. Xiong, and G. Geng, ‘‘Using HSV space real-color image enhanced by homomorphic ﬁlter in two channels,’’ (in Chinese), Comput. Eng. Appl., vol. 45, no. 27, pp. 18–20, 2009.
[42] M. Iqbal, S. S. Ali, M. M. Riaz, A. Ghafoor, and A. Ahmad, ‘‘Color and white balancing in low-light image enhancement,’’ Optik, vol. 209, May 2020, Art. no. 164260.
[43] F. Wu and U. KinTak, ‘‘Low-light image enhancement algorithm based on HSI color space,’’ in Proc. 10th Int. Congr. Image Signal Process., Biomed. Eng. Informat., Oct. 2017, pp. 1–6.
[44] A. Nandal, V. Bhaskar, and A. Dhaka, ‘‘Contrast-based image enhancement algorithm using grey-scale and colour space,’’ IET Signal Process., vol. 12, no. 4, pp. 514–521, Jun. 2018.
[45] Q. Mu, Y. Wei, and Z. Li, ‘‘Color image enhancement method based on weighted image guided ﬁltering,’’ 2018, arXiv:1812.09930. [Online]. Available: http://arxiv.org/abs/1812.09930
[46] Q. Xu, H. Jiang, R. Scopigno, and M. Sbert, ‘‘A novel approach for enhancing very dark image sequences,’’ Signal Process., vol. 103, pp. 309–330, Oct. 2014.
[47] Z. Feng and S. Hao, ‘‘Low-light image enhancement by reﬁning illumination map with self-guided ﬁltering,’’ in Proc. IEEE Int. Conf. Big Knowl., Aug. 2017, pp. 183–187.
[48] L. Florea, C. Florea, and C. Ionascu, ‘‘Avoiding the deconvolution: Framework oriented color transfer for enhancing low-light images,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, Jun. 2016, pp. 936–944.
[49] Z. Zhou, N. Sang, and X. Hu, ‘‘Global brightness and local contrast adaptive enhancement for low illumination color image,’’ Optik, vol. 125, no. 6, pp. 1795–1799, Mar. 2014.
[50] D. Mu, C. Xu, and H. Ge, ‘‘Hybrid genetic algorithm based image enhancement technology,’’ in Proc. Int. Conf. Internet Technol. Appl., Aug. 2011, pp. 1–4.
[51] J. Wang, ‘‘An enhancement algorithm for low-illumination color image with preserving edge,’’ (in Chinese), Comput. Technol. Develop., vol. 28, no. 1, pp. 116–120, 2018.
[52] K. Srinivas and A. K. Bhandari, ‘‘Low light image enhancement with adaptive sigmoid transfer function,’’ IET Image Process., vol. 14, no. 4, pp. 668–678, Mar. 2020.
[53] W. Kim, R. Lee, M. Park, and S.-H. Lee, ‘‘Low-light image enhancement based on maximal diffusion values,’’ IEEE Access, vol. 7, pp. 129150–129163, 2019.
[54] K. Panetta, S. Agaian, Y. Zhou, and E. J. Wharton, ‘‘Parameterized logarithmic framework for image enhancement,’’ IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 41, no. 2, pp. 460–473, Apr. 2011.
[55] Z. Xiao, X. Zhang, F. Zhang, L. Geng, J. Wu, L. Su, and L. Chen, ‘‘Diabetic retinopathy retinal image enhancement based on gamma correction,’’ J. Med. Imag. Health Informat., vol. 7, no. 1, pp. 149–154, Feb. 2017.
[56] F. Drago, K. Myszkowski, T. Annen, and N. Chiba, ‘‘Adaptive logarithmic mapping for displaying high contrast scenes,’’ Comput. Graph. Forum, vol. 22, no. 3, pp. 419–426, Sep. 2003.
[57] L. Tao and V. Asari, ‘‘An integrated neighborhood dependent approach for nonlinear enhancement of color images,’’ in Proc. Int. Conf. Inf. Technol. Coding Comput., 2004, p. 138.
87911

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

[58] X. Tian, X. Xu, and C. Wu, ‘‘Low illumination color image enhancement algorithm based on LIP model,’’ (in Chinese), J. Xian Univ. Posts Telecommun., vol. 20, no. 1, pp. 9–13, 2015.
[59] S.-C. Huang, F.-C. Cheng, and Y.-S. Chiu, ‘‘Efﬁcient contrast enhancement using adaptive gamma correction with weighting distribution,’’ IEEE Trans. Image Process., vol. 22, no. 3, pp. 1032–1041, Mar. 2013.
[60] N. Zhi, S. Mao, and M. Li, ‘‘An enhancement algorithm for coal mine low illumination images based on bi-Gamma function,’’ (in Chinese), J. Liaoning Tech. Univ., vol. 37, no. 1, pp. 191–197, 2018.
[61] C.-Y. Yu, Y.-C. Ouyang, C.-M. Wang, and C.-I. Chang, ‘‘Adaptive inverse hyperbolic tangent algorithm for dynamic contrast adjustment in displaying scenes,’’ EURASIP J. Adv. Signal Process., vol. 2010, no. 1, Dec. 2010, Art. no. 485151.
[62] S. C. Liu, S. Liu, H. Wu, M. A. Rahman, S. C.-F. Lin, C. Y. Wong, N. Kwok, and H. Shi, ‘‘Enhancement of low illumination images based on an optimal hyperbolic tangent proﬁle,’’ Comput. Electr. Eng., vol. 70, pp. 538–550, Aug. 2018.
[63] D. David, ‘‘Low illumination image enhancement algorithm using iterative recursive ﬁlter and visual gamma transformation function,’’ in Proc. 5th Int. Conf. Adv. Comput. Commun., Sep. 2015, pp. 408–411.
[64] Y. Huang, ‘‘A Retinex image enhancement based on L channel illumination estimation and Gamma function,’’ Techn. Autom. Appl., vol. 37, no. 5, pp. 56–60, 2018.
[65] S. Lee, N. Kim, and J. Paik, ‘‘Adaptively partitioned block-based contrast enhancement and its application to low light-level video surveillance,’’ SpringerPlus, vol. 4, no. 1, Dec. 2015, Art. no. 431.
[66] L. Li, S. Sun, and C. Xia, ‘‘Survey of histogram equalization technology,’’ Comput. Syst. Appl., vol. 23, no. 3, pp. 1–8, 2014.
[67] K. Singh, R. Kapoor, and S. K. Sinha, ‘‘Enhancement of low exposure images via recursive histogram equalization algorithms,’’ Optik, vol. 126, no. 20, pp. 2619–2625, Oct. 2015.
[68] A. Singh and K. Gupta, ‘‘A contrast enhancement technique for low light images,’’ in Proc. Int. Conf. Commun. Syst., 2016, pp. 220–230.
[69] Q. Wang and R. Ward, ‘‘Fast image/video contrast enhancement based on weighted thresholded histogram equalization,’’ IEEE Trans. Consum. Electron., vol. 53, no. 2, pp. 757–764, Jul. 2007.
[70] R. Dale-Jones and T. Tjahjadi, ‘‘A study and modiﬁcation of the local histogram equalization algorithm,’’ Pattern Recognit., vol. 26, no. 9, pp. 1373–1381, Sep. 1993.
[71] M. F. Khan, E. Khan, and Z. A. Abbasi, ‘‘Segment dependent dynamic multi-histogram equalization for image contrast enhancement,’’ Digit. Signal Process., vol. 25, pp. 198–223, Feb. 2014.
[72] L. Li, S. Sun, and C. Xia, ‘‘Survey of histogram equalization technology,’’ (in Chinese), Comput. Syst. Appl., vol. 23, no. 3, pp. 1–8, 2014.
[73] Y.-T. Kim, ‘‘Contrast enhancement using brightness preserving bihistogram equalization,’’ IEEE Trans. Consum. Electron., vol. 43, no. 1, pp. 1–8, Feb. 1997.
[74] Y. Wang, Q. Chen, and B. Zhang, ‘‘Image enhancement based on equal area dualistic sub-image histogram equalization method,’’ IEEE Trans. Consum. Electron., vol. 45, no. 1, pp. 68–75, Feb. 1999.
[75] S.-D. Chen and A. R. Ramli, ‘‘Minimum mean brightness error bihistogram equalization in contrast enhancement,’’ IEEE Trans. Consum. Electron., vol. 49, no. 4, pp. 1310–1319, Nov. 2003.
[76] H. Shen, S. Sun, and B. Lei, ‘‘An adaptive brightness preserving bihistogram equalization,’’ Proc. SPIE, vol. 8005, Dec. 2011, Art. no. 8005.
[77] X. Tian, D. Qiao, and C. Wu, ‘‘Color image enhancement based on bihistogram equalization,’’ (in Chinese), J. Xian Univ. Posts Telecommun., vol. 20, no. 2, pp. 58–63, 2015.
[78] T. K. Kim, J. K. Paik, and B. S. Kang, ‘‘Contrast enhancement system using spatially adaptive histogram equalization with temporal ﬁltering,’’ IEEE Trans. Consum. Electron., vol. 44, no. 1, pp. 82–87, Feb. 1998.
[79] J.-Y. Kim, L.-S. Kim, and S.-H. Hwang, ‘‘An advanced contrast enhancement using partially overlapped sub-block histogram equalization,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 11, no. 4, pp. 475–484, Apr. 2001.
[80] B. Liu, W. Jin, Y. Chen, C. Liu, and L. Li, ‘‘Contrast enhancement using non-overlapped sub-blocks and local histogram projection,’’ IEEE Trans. Consum. Electron., vol. 57, no. 2, pp. 583–588, May 2011.
[81] S.-C. Huang and C.-H. Yeh, ‘‘Image contrast enhancement for preserving mean brightness without losing image features,’’ Eng. Appl. Artif. Intell., vol. 26, nos. 5–6, pp. 1487–1492, May 2013.
87912

[82] A. M. Reza, ‘‘Realization of the contrast limited adaptive histogram equalization (CLAHE) for real-time image enhancement,’’ J. VLSI Signal Process.-Syst. Signal, Image, Video Technol., vol. 38, no. 1, pp. 35–44, Aug. 2004.
[83] M. F. Al-Sammaraie, ‘‘Contrast enhancement of roads images with foggy scenes based on histogram equalization,’’ in Proc. 10th Int. Conf. Comput. Sci. Educ. (ICCSE), Jul. 2015, pp. 95–101.
[84] G. Yadav, S. Maheshwari, and A. Agarwal, ‘‘Foggy image enhancement using contrast limited adaptive histogram equalization of digitally ﬁltered image: Performance improvement,’’ in Proc. Int. Conf. Adv. Comput., Commun. Informat., Sep. 2014, pp. 2225–2231.
[85] S.-D. Chen and A. R. Ramli, ‘‘Contrast enhancement using recursive mean-separate histogram equalization for scalable brightness preservation,’’ IEEE Trans. Consum. Electron., vol. 49, no. 4, pp. 1301–1309, Nov. 2003.
[86] J. Jiang, Y. Zhang, and F. Xue, ‘‘Local histogram equalization with brightness preservation,’’ (in Chinese), Acta Electronica Sinica, vol. 34, no. 5, pp. 861–866, 2006.
[87] C. Sun and F. Yuan, ‘‘Partially overlapped sub-block histogram equalization based on recursive equal area separateness,’’ (in Chinese), Opt. Precis. Eng., vol. 17, no. 9, pp. 2292–2300, 2009.
[88] Y. Tian, Q. Wan, and F. Wu, ‘‘Local histogram equalization based on the minimum brightness error,’’ in Proc. 4th Int. Conf. Image Graph. (ICIG), Aug. 2007, pp. 58–61.
[89] T. Celik and T. Tjahjadi, ‘‘Contextual and variational contrast enhancement,’’ IEEE Trans. Image Process., vol. 20, no. 12, pp. 3431–3441, Dec. 2011.
[90] T. L. Tan, K. S. Sim, and C. P. Tso, ‘‘Image enhancement using background brightness preserving histogram equalisation,’’ Electron. Lett., vol. 48, no. 3, pp. 155–157, 2012.
[91] K. Singh, D. K. Vishwakarma, G. S. Walia, and R. Kapoor, ‘‘Contrast enhancement via texture region based histogram equalization,’’ J. Mod. Opt., vol. 63, no. 15, pp. 1444–1450, Aug. 2016.
[92] K. S. Sim, C. P. Tso, and Y. Y. Tan, ‘‘Recursive sub-image histogram equalization applied to gray scale images,’’ Pattern Recognit. Lett., vol. 28, no. 10, pp. 1209–1221, Jul. 2007.
[93] A. S. Parihar and O. P. Verma, ‘‘Contrast enhancement using entropybased dynamic sub-histogram equalisation,’’ IET Image Process., vol. 10, no. 11, pp. 799–808, Nov. 2016.
[94] M. Abdullah-Al-Wadud, M. Kabir, and M. Dewan, ‘‘A dynamic histogram equalization for image contrast enhancement,’’ IEEE Trans. Consum. Electron., vol. 53, no. 2, pp. 593–600, May 2007.
[95] H. Ibrahim and N. P. Kong, ‘‘Brightness preserving dynamic histogram equalization for image contrast enhancement,’’ IEEE Trans. Consum. Electron., vol. 53, no. 4, pp. 1752–1758, Nov. 2007.
[96] C. Ooi, N. P. Kong, and H. Ibrahim, ‘‘Bi-histogram equalization with a plateau limit for digital image enhancement,’’ IEEE Trans. Consum. Electron., vol. 55, no. 4, pp. 2072–2080, Nov. 2009.
[97] K. Singh and R. Kapoor, ‘‘Image enhancement via median-mean based sub-image-clipped histogram equalization,’’ Optik, vol. 125, no. 17, pp. 4646–4651, Sep. 2014.
[98] K. Singh and R. Kapoor, ‘‘Image enhancement using exposure based sub image histogram equalization,’’ Pattern Recognit. Lett., vol. 36, no. 1, pp. 10–14, Jan. 2014.
[99] H. Kim, J. Lee, and J. Lee, ‘‘Contrast enhancement using adaptively modiﬁed histogram equalization,’’ in Proc. Paciﬁc-Rim Symp. Image Video Technol., 2006, pp. 1150–1158.
[100] M. Kim and M. Chung, ‘‘Recursively separated and weighted histogram equalization for brightness preservation and contrast enhancement,’’ IEEE Trans. Consum. Electron., vol. 54, no. 3, pp. 1389–1397, Aug. 2008.
[101] T. Arici, S. Dikbas, and Y. Altunbasak, ‘‘A histogram modiﬁcation framework and its application for image contrast enhancement,’’ IEEE Trans. Image Process., vol. 18, no. 9, pp. 1921–1935, Sep. 2009.
[102] C.-C. Chiu and C.-C. Ting, ‘‘Contrast enhancement algorithm based on gap adjustment for histogram equalization,’’ Sensors, vol. 16, no. 6, 2016, Art. no. 936.
[103] S. Kansal, S. Purwar, and R. K. Tripathi, ‘‘Image contrast enhancement using unsharp masking and histogram equalization,’’ Multimedia Tools Appl., vol. 77, no. 20, pp. 26919–26938, Oct. 2018.
[104] E. H. Land and J. J. McCann, ‘‘Lightness and Retinex theory,’’ J. Opt. Soc. Amer., vol. 61, no. 1, pp. 1–11, Jan. 1971.
[105] S. Park, S. Yu, B. Moon, S. Ko, and J. Paik, ‘‘Low-light image enhancement using variational optimization-based Retinex model,’’ IEEE Trans. Consum. Electron., vol. 63, no. 2, pp. 178–184, May 2017.
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

[106] H. Tanaka, Y. Waizumi, and T. Kasezawa, ‘‘Retinex-based signal enhancement for image dark regions,’’ in Proc. IEEE Int. Conf. Signal Image Process. Appl., Sep. 2017, pp. 205–209.
[107] S. Liao, Y. Piao, and B. Li, ‘‘Low illumination color image enhancement based on improved retinex,’’ in Proc. LIDAR Imag. Detection Target Recognit., Nov. 2017, p. 160.
[108] M. Li, J. Liu, W. Yang, X. Sun, and Z. Guo, ‘‘Structure-revealing lowlight image enhancement via robust Retinex model,’’ IEEE Trans. Image Process., vol. 27, no. 6, pp. 2828–2841, Jun. 2018.
[109] H. Hu and G. Ni, ‘‘Color image enhancement based on the improved retinex,’’ in Proc. Int. Conf. Multimedia Technol., Oct. 2010, pp. 1–4.
[110] H.-G. Lee, S. Yang, and J.-Y. Sim, ‘‘Color preserving contrast enhancement for low light level images based on retinex,’’ in Proc. Asia–Paciﬁc Signal Inf. Process. Assoc. Annu. Summit Conf., Dec. 2015, pp. 884–887.
[111] H. Liu, X. Sun, H. Han, and W. Cao, ‘‘Low-light video image enhancement based on multiscale retinex-like algorithm,’’ in Proc. Chin. Control Decis. Conf., May 2016, pp. 3712–3715.
[112] D. J. Jobson, Z. Rahman, and G. A. Woodell, ‘‘Properties and performance of a center/surround retinex,’’ IEEE Trans. Image Process., vol. 6, no. 3, pp. 451–462, Mar. 1997.
[113] Z. Rahman, D. J. Jobson, and G. A. Woodell, ‘‘Multi-scale retinex for color image enhancement,’’ in Proc. 3rd IEEE Int. Conf. Image Process., Sep. 1996, pp. 1003–1006.
[114] D. J. Jobson, Z. Rahman, and G. A. Woodell, ‘‘A multiscale retinex for bridging the gap between color images and the human observation of scenes,’’ IEEE Trans. Image Process., vol. 6, no. 7, pp. 965–976, Jul. 2002.
[115] D. J. Jobson, ‘‘Retinex processing for automatic image enhancement,’’ J. Electron. Imag., vol. 13, no. 1, pp. 100–110, Jan. 2004.
[116] X. Ren, W. Yang, W. Cheng, and J. Liu, ‘‘LR3M: Robust lowlight enhancement via low-rank regularized retinex model,’’ IEEE Trans. Image Process., vol. 29, pp. 5862–5876, Apr. 2020, doi: 10.1109/TIP.2020.2984098.
[117] S. Hao, X. Han, Y. Guo, X. Xu, and M. Wang, ‘‘Low-light image enhancement with semi-decoupled decomposition,’’ IEEE Trans. Multimedia, early access, Jan. 27, 2020, doi: 10.1109/TMM.2020.2969790.
[118] Z. Gu, F. Li, F. Fang, and G. Zhang, ‘‘A novel retinex-based fractionalorder variational model for images with severely low light,’’ IEEE Trans. Image Process., vol. 29, pp. 3239–3253, Dec. 2019, doi: 10.1109/TIP.2019.2958144.
[119] P. Hao, S. Wang, S. Li, and M. Yang, ‘‘Low-light image enhancement based on retinex and saliency theories,’’ in Proc. Chin. Autom. Congr., Hangzhou, China, Nov. 2019, pp. 2594–2597.
[120] R. Kimmel, M. Elad, D. Shaked, R. Keshet, and I. Sobel, ‘‘A variational framework for Retinex,’’ Int. J. Comput. Vis., vol. 52, no. 1, pp. 7–23, 2003.
[121] M. Elad, ‘‘Retinex by two bilateral ﬁlters,’’ in Proc. 5th Int. Conf. Scale Space PDE Methods Comput. Vis., Hofgeismar, Germany, Apr. 2005, pp. 217–229.
[122] L. Meylan and S. Susstrunk, ‘‘High dynamic range image rendering with a retinex-based adaptive ﬁlter,’’ IEEE Trans. Image Process., vol. 15, no. 9, pp. 2820–2830, Sep. 2006.
[123] X. Xu, Q. Chen, and P. Wang, ‘‘A fast halo-free image enhancement method based on Retinex,’’ (in Chinese), J. Comput.-Aided Des. Comput. Graph., vol. 20, no. 10, pp. 1325–1331, 2008.
[124] M. Bertalmío, V. Caselles, and E. Provenzi, ‘‘Issues about Retinex theory and contrast enhancement,’’ Int. J. Comput. Vis., vol. 83, no. 1, pp. 101–119, Jun. 2009.
[125] M. K. Ng and W. Wang, ‘‘A total variation model for Retinex,’’ SIAM J. Imag. Sci., vol. 4, no. 1, pp. 345–365, Jan. 2011.
[126] X. Fu, D. Zeng, Y. Huang, X.-P. Zhang, and X. Ding, ‘‘A weighted variational model for simultaneous reﬂectance and illumination estimation,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2016, pp. 2782–2790.
[127] A. B. Petro, C. Sbert, and J.-M. Morel, ‘‘Multiscale Retinex,’’ Image Process. Line, vol. 4, pp. 71–88, Apr. 2014.
[128] H. Lin and Z. Shi, ‘‘Multi-scale Retinex improvement for nighttime image enhancement,’’ Optik, vol. 125, no. 24, pp. 7143–7148, Dec. 2014.
[129] F. Matin, Y. Jeong, K. Kim, and K. Park, ‘‘Color image enhancement using multiscale Retinex based on particle swarm optimization method,’’ J. Phys., Conf. Ser., vol. 960, no. 1, Jan. 2018, Art. no. 012026.
[130] S. Chen and A. Beghdadi, ‘‘Natural rendering of color image based on Retinex,’’ in Proc. 16th IEEE Int. Conf. Image Process., Nov. 2009, pp. 1813–1816.
VOLUME 8, 2020

[131] C.-T. Shen and W.-L. Hwang, ‘‘Color image enhancement using Retinex with robust envelope,’’ in Proc. 16th IEEE Int. Conf. Image Process., Nov. 2009, pp. 3141–3144.
[132] I.-S. Jang, K.-H. Park, and Y.-H. Ha, ‘‘Color correction by estimation of dominant chromaticity in multi-scaled Retinex,’’ J. Imag. Sci. Technol., vol. 53, no. 5, pp. 501–512, 2009.
[133] S. Wang, X. Ding, and Y. Liao, ‘‘A novel bio-inspired algorithm for color image enhancement,’’ (in Chinese), Acta Electronica Sinica, vol. 36, no. 10, pp. 1970–1973, 2008.
[134] Q. Xiao, X. Ding, S. Wang, Y. Liao, and D. Guo, ‘‘A halo-free and hue preserving algorithm for color image enhancement,’’ (in Chinese), J. Comput.-Aided Des. Comput. Graph., vol. 22, no. 8, pp. 1246–1252, Sep. 2010.
[135] C.-H. Lee, J.-L. Shih, C.-C. Lien, and C.-C. Han, ‘‘Adaptive multiscale Retinex for image contrast enhancement,’’ in Proc. Int. Conf. SignalImage Technol. Internet-Based Syst., Dec. 2013, pp. 43–50.
[136] S. Wang, J. Zheng, H.-M. Hu, and B. Li, ‘‘Naturalness preserved enhancement algorithm for non-uniform illumination images,’’ IEEE Trans. Image Process., vol. 22, no. 9, pp. 3538–3548, Sep. 2013.
[137] D. Wang, X. Niu, and Y. Dou, ‘‘A piecewise-based contrast enhancement framework for low lighting video,’’ in Proc. IEEE Int. Conf. Secur., Pattern Anal., Cybern., Oct. 2014, pp. 235–240.
[138] J. Xiao, S. Shan, and P. Duan, ‘‘A fast image enhancement algorithm based on fusion of different color spaces,’’ Acta Automatica Sinica, vol. 40, no. 4, pp. 697–705, 2014.
[139] X. Fu, Y. Liao, D. Zeng, Y. Huang, X.-P. Zhang, and X. Ding, ‘‘A probabilistic method for image enhancement with simultaneous illumination and reﬂectance estimation,’’ IEEE Trans. Image Process., vol. 24, no. 12, pp. 4965–4977, Dec. 2015.
[140] H. Zhao, C. Xiao, and J. Yu, ‘‘A Retinex algorithm for night color image enhancement by MRF,’’ (in Chinese), Opt. Precis. Eng., vol. 22, no. 4, pp. 1048–1055, 2014.
[141] H. Zhao, C. Xiao, and J. Yu, ‘‘Retinex algorithm for night color image enhancement based on WLS,’’ (in Chinese), J. Beijing Univ. Technol., vol. 40, no. 3, pp. 404–410, 2014.
[142] J. Ho Jang, Y. Bae, and J. Beom Ra, ‘‘Contrast-enhanced fusion of multisensor images using subband-decomposed multiscale Retinex,’’ IEEE Trans. Image Process., vol. 21, no. 8, pp. 3479–3490, Aug. 2012.
[143] X. Liu, T. Qiao, and Z. Qiao, ‘‘Image enhancement method of mine based on bilateral ﬁltering and Retinex algorithm,’’ (in Chinese), Ind. Mine Autom., vol. 43, no. 2, pp. 49–54, 2017.
[144] S. Feng, ‘‘Image enhancement algorithm based on real-time Retinex and bilateral ﬁltering,’’ (in Chinese), Comput. Appl. Softw., vol. 26, no. 11, pp. 234–238, 2009.
[145] M.-R. Wang and S.-Q. Jiang, ‘‘Image enhancement algorithm combining multi-scale retinex and bilateral ﬁlter,’’ in Proc. Int. Conf. Autom., Mech. Control Comput. Eng., 2015, pp. 1221–1226.
[146] J. Yin, H. Li, J. Du, and P. He, ‘‘Low illumination image Retinex enhancement algorithm based on guided ﬁltering,’’ in Proc. IEEE 3rd Int. Conf. Cloud Comput. Intell. Syst., Nov. 2014, pp. 639–644.
[147] A. Mulyantini and H.-K. Choi, ‘‘Color image enhancement using a Retinex algorithm with bilateral ﬁltering for images with poor illumination,’’ J. Korea Multimedia Soc., vol. 19, no. 2, pp. 233–239, Feb. 2016.
[148] Y. Zhang, W. Huang, W. Bi, and G. Gao, ‘‘Colorful image enhancement algorithm based on guided ﬁlter and Retinex,’’ in Proc. IEEE Int. Conf. Signal Image Process., Aug. 2016, pp. 33–36.
[149] J. Wei, Q. Zhijie, X. Bo, and Z. Dean, ‘‘A nighttime image enhancement method based on Retinex and guided ﬁlter for object recognition of apple harvesting robot,’’ Int. J. Adv. Robot. Syst., vol. 15, no. 1, pp. 1–12, Jan. 2018.
[150] S. Zhang, G.-J. Tang, X.-H. Liu, S.-H. Luo, and D.-D. Wang, ‘‘Retinex based low-light image enhancement using guided ﬁltering and variational framework,’’ (in Chinese), Optoelectron. Lett., vol. 14, no. 2, pp. 156– 160, Mar. 2018.
[151] D. Zhu, G. Chen, P. N. Michelini, and H. Liu, ‘‘Fast image enhancement based on maximum and guided ﬁlters,’’ in Proc. IEEE Int. Conf. Image Process. (ICIP), Taipei, Taiwan, Sep. 2019, pp. 4080–4084.
[152] M. Wang, Z. Tian, W. Gui, X. Zhang, and W. Wang, ‘‘Low-light image enhancement based on nonsubsampled shearlet transform,’’ IEEE Access, vol. 8, pp. 63162–63174, 2020.
[153] S. Wen and Z. You, ‘‘Homomorphic ﬁltering space domain algorithm for performance optimization,’’ (in Chinese), Comput. Appl. Res., vol. 17, no. 3, pp. 62–65, 2000.
87913

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

[154] J. Xiao, S. Song, and L. Ding, ‘‘Research on the fast algorithm of spatial homomorphic ﬁltering,’’ (in Chinese), J. Image Graph., vol. 13, no. 12, pp. 2302–2306, 2008.
[155] Y. Zhang and M. Xie, ‘‘Colour image enhancement algorithm based on HSI and local homomorphic ﬁltering,’’ (in Chinese), Comput. Appl. Softw., vol. 30, no. 12, pp. 303–307, 2013.
[156] Y. Zhang and M. Xie, ‘‘Block-DCT based homomorphic ﬁltering algorithm for color image enhancement,’’ Comput. Eng. Des., vol. 34, no. 5, pp. 1752–1756, 2013.
[157] L. Xiao, C. Li, Z. Wu, and T. Wang, ‘‘An enhancement method for X-ray image via fuzzy noise removal and homomorphic ﬁltering,’’ Neurocomputing, vol. 195, pp. 56–64, Jun. 2016.
[158] X. Tian, X. Cheng, and W. Chengmao, ‘‘Color image enhancement method based on Homomorphic ﬁltering,’’ (in Chinese), J. Xian Univ. Posts Telecommun., vol. 20, no. 6, pp. 51–55, 2015.
[159] A. Loza, D. Bull, and A. Achim, ‘‘Automatic contrast enhancement of low-light images based on local statistics of wavelet coefﬁcients,’’ in Proc. IEEE Int. Conf. Image Process., Sep. 2013, pp. 3553–3556.
[160] T. Sun, C. Jung, P. Ke, H. Song, and J. Hwang, ‘‘Readability enhancement of low light videos based on discrete wavelet transform,’’ in Proc. IEEE Int. Symp. Multimedia, Dec. 2017, pp. 342–345.
[161] C. Jung, Q. Yang, T. Sun, Q. Fu, and H. Song, ‘‘Low light image enhancement with dual-tree complex wavelet transform,’’ J. Vis. Commun. Image Represent., vol. 42, pp. 28–36, Jan. 2017.
[162] T. Sun and C. Jung, ‘‘Readability enhancement of low light images based on dual-tree complex wavelet transform,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., Mar. 2016, pp. 1741–1745.
[163] T.-C. Hsung, D. P.-K. Lun, and W. W. L. Ng, ‘‘Efﬁcient fringe image enhancement based on dual-tree complex wavelet transform,’’ Appl. Opt., vol. 50, no. 21, pp. 3973–3986, Jul. 2011.
[164] M. Z. Iqbal, A. Ghafoor, and A. M. Siddiqui, ‘‘Satellite image resolution enhancement using dual-tree complex wavelet transform and nonlocal means,’’ IEEE Geosci. Remote Sens. Lett., vol. 10, no. 3, pp. 451–455, May 2013.
[165] M.-X. Yang, G.-J. Tang, X.-H. Liu, L.-Q. Wang, Z.-G. Cui, and S.-H. Luo, ‘‘Low-light image enhancement based on retinex theory and dualtree complex wavelet transform,’’ Optoelectron. Lett., vol. 14, no. 6, pp. 470–475, Nov. 2018.
[166] X. Zhou, S. Zhou, and F. Huang, ‘‘New algorithm of image enhancement based on wavelet transform,’’ (in Chinese), Comput. Appl., vol. 25, no. 3, pp. 606–608, 2005.
[167] X. Zong, A. F. Laine, and E. A. Geiser, ‘‘Speckle reduction and contrast enhancement of echocardiograms via multiscale nonlinear processing,’’ IEEE Trans. Med. Imag., vol. 17, no. 4, pp. 532–540, Aug. 1998.
[168] A. Łoza, D. R. Bull, P. R. Hill, and A. M. Achim, ‘‘Automatic contrast enhancement of low-light images based on local statistics of wavelet coefﬁcients,’’ Digit. Signal Process., vol. 23, no. 6, pp. 1856–1866, Dec. 2013.
[169] A. K. Bhandari, A. Kumar, and G. K. Singh, ‘‘Improved knee transfer function and gamma correction based method for contrast and brightness enhancement of satellite image,’’ AEU-Int. J. Electron. Commun., vol. 69, no. 2, pp. 579–589, Feb. 2015.
[170] S. E. Kim, J. J. Jeon, and I. K. Eom, ‘‘Image contrast enhancement using entropy scaling in wavelet domain,’’ Signal Process., vol. 127, no. 1, pp. 1–11, Oct. 2016.
[171] H. Demirel, C. Ozcinar, and G. Anbarjafari, ‘‘Satellite image contrast enhancement using discrete wavelet transform and singular value decomposition,’’ IEEE Geosci. Remote Sens. Lett., vol. 7, no. 2, pp. 333–337, Apr. 2010.
[172] Q. Li and Q. Liu, ‘‘Adaptive enhancement algorithm for low illumination images based on wavelet transform,’’ (in Chinese), Chin. J. Lasers, vol. 42, no. 2, pp. 280–286, 2015.
[173] Y. Jin and A. F. Laine, ‘‘Contrast enhancement by multiscale adaptive histogram equalization,’’ Proc. SPIE, vol. 4478, pp. 206–213, Dec. 2001.
[174] W. L. Jun and Z. Rong, ‘‘Image defogging algorithm of single color image based on wavelet transform and histogram equalization,’’ Appl. Math. Sci., vol. 7, pp. 3913–3921, 2013.
[175] L. Huang, W. Zhao, and J. Wang, ‘‘Combination of contrast limited adaptive histogram equalization and discrete wavelet transform for image enhancement,’’ IET Image Process., vol. 9, no. 10, pp. 908–915, 2015.
[176] Q. Xu, J. Cui, and B. Chen, ‘‘Low-light image enhancement algorithm based on the wavelet transform and Retinex theory,’’ (in Chinese), J. Hunan Univ. Arts Sci., vol. 29, no. 2, pp. 41–46, 2017.
87914

[177] K. Kawasaki and A. Taguchi, ‘‘A multiscale Retinex based on wavelet transformation,’’ in Proc. IEEE Asia–Paciﬁc Conf. Circuits Syst., Nov. 2014, pp. 33–36.
[178] F. Russo, ‘‘An image enhancement technique combining sharpening and noise reduction,’’ IEEE Trans. Instrum. Meas., vol. 51, no. 4, pp. 824–828, Aug. 2002.
[179] L. Chen, ‘‘The application of wavelet transform in the image enhancement processing,’’ (in Chinese), J. Shaanxi Univ. Technol., vol. 30, no. 1, pp. 32–37, 2014.
[180] M. H. Asmare, V. S. Asirvadam, and A. F. M. Hani, ‘‘Image enhancement based on contourlet transform,’’ Signal, Image Video Process., vol. 9, no. 7, pp. 1679–1690, Oct. 2015.
[181] J.-L. Starck, F. Murtagh, E. J. Candes, and D. L. Donoho, ‘‘Gray and color image contrast enhancement by the curvelet transform,’’ IEEE Trans. Image Process., vol. 12, no. 6, pp. 706–717, Jun. 2003.
[182] G. G. Bhutada, R. S. Anand, and S. C. Saxena, ‘‘Edge preserved image enhancement using adaptive fusion of images denoised by wavelet and curvelet transform,’’ Digit. Signal Process., vol. 21, no. 1, pp. 118–130, Jan. 2011.
[183] X. Si, J. Wen, and X. Wang, ‘‘Image enhancement algorithm based on curvelet transform II for low illumination colorful and noising images,’’ (in Chinese), Command Inf. Syst. Technol., vol. 7, no. 4, pp. 87–90, 2016.
[184] T. Y. Han, D. H. Kim, S. H. Lee, and B. C. Song, ‘‘Infrared image superresolution using auxiliary convolutional neural network and visible image under low-light conditions,’’ J. Vis. Commun. Image Represent., vol. 51, pp. 191–200, Feb. 2018.
[185] T. Mikami, D. Sugimura, and T. Hamamoto, ‘‘Capturing color and nearinfrared images with different exposure times for image enhancement under extremely low-light scene,’’ in Proc. IEEE Int. Conf. Image Process., Oct. 2014, pp. 669–673.
[186] H. Yamashita, D. Sugimura, and T. Hamamoto, ‘‘Enhancing low-light color images using an RGB-NIR single sensor,’’ in Proc. Vis. Commun. Image Process., Dec. 2015, pp. 1–4.
[187] L. Li, Y. Si, and Z. Jia, ‘‘Medical image enhancement based on CLAHE and unsharp masking in NSCT domain,’’ J. Med. Imag. Health Informat., vol. 8, no. 3, pp. 431–438, Mar. 2018.
[188] L. Wang, G. Fu, Z. Jiang, G. Ju, and A. Men, ‘‘Low-light image enhancement with attention and multi-level feature fusion,’’ in Proc. IEEE Int. Conf. Multimedia Expo Workshops, Jul. 2019, pp. 276–281.
[189] A. Toet, M. A. Hogervorst, R. van Son, and J. Dijk, ‘‘Augmenting full colour-fused multi-band night vision imagery with synthetic imagery in real-time,’’ Int. J. Image Data Fusion, vol. 2, no. 4, pp. 287–308, Dec. 2011.
[190] M. Aguilar, D. A. Fay, and A. M. Waxman, ‘‘Real-time fusion of lowlight CCD and uncooled IR imagery for color night vision,’’ Proc. SPIE, vol. 3364, pp. 124–135, Jul. 1998.
[191] B. Qi, G. Kun, Y.-X. Tian, and Z.-Y. Zhu, ‘‘A novel false color mapping model-based fusion method of visual and infrared images,’’ in Proc. Int. Conf. Opt. Instrum. Technol., Optoelectron. Imag. Process. Technol., Dec. 2013, Art. no. 904519.
[192] A. Toet, ‘‘Colorizing single band intensiﬁed nightvision images,’’ Displays, vol. 26, no. 1, pp. 15–21, Jan. 2005.
[193] W. Yang, J. Zhang, and H. Xu, ‘‘Study of infrared and LLL image fusion algorithm based on the target characteristics,’’ (in Chinese), Laser Infr., vol. 44, no. 1, pp. 56–60, 2014.
[194] J. Zhu, W. Jin, and L. Li, ‘‘Fusion of the low-light-level visible and infrared images for night-vision context enhancement,’’ (in Chinese), Chin. Opt. Lett., vol. 16, no. 1, pp. 94–99, 2018.
[195] Y. Zhang, L. Bai, and Q. Chen, ‘‘Dual-band low level light image realtime registration based on pixel spatial correlation degree,’’ (in Chinese), J. Nanjing Univ. Sci. Technol., vol. 33, no. 4, pp. 506–510, 2009.
[196] S. Yang and W. Liu, ‘‘Color fusion method for low-level light and infrared images,’’ (in Chinese), Infr. Laser Eng., vol. 43, no. 5, pp. 1654–1659, 2014.
[197] X. Qian, L. Han, and B. Wang, ‘‘A fast fusion algorithm of visible and infrared images,’’ (in Chinese), J. Comput.-Aided Des. Comput. Graph., vol. 23, no. 7, pp. 1211–1216, 2011.
[198] J. Li, S. Z. Li, Q. Pan, and T. Yang, ‘‘Illumination and motion-based video enhancement for night surveillance,’’ in Proc. IEEE Int. Workshop Vis. Surveill. Perform. Eval. Tracking Surveill., Jun. 2005, pp. 169–175.
[199] R. Raskar, J. Yu, and A. Llie, ‘‘Image fusion for context enhancement and video surrealism,’’ in Proc. 3rd Int. Symp. Non-Photorealistic Animation Rendering, 2004, pp. 85–94.
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

[200] Y. Rao, ‘‘Image-based fusion for video enhancement of night-time surveillance,’’ Opt. Eng., vol. 49, no. 12, pp. 120501–120503, Dec. 2010.
[201] Y. Pu, L. Liu, and X. Liu, ‘‘Enhancement technology of video under low illumination,’’ (in Chinese), Infr. Laser Eng., vol. 43, no. 6, pp. 2021–2026, 2014.
[202] J. Zhu and Z. Wang, ‘‘Low-illumination surveillance image enhancement based on similar scenes,’’ (in Chinese), Comput. Appl. Softw., vol. 32, no. 1, pp. 203–205, 2015.
[203] Y. Rao, Z. Chen, and M. Sun, ‘‘An effective night video enhancement algorithm,’’ in Proc. IEEE Conf. Vis. Commun. Image Process., Jun. 2011, pp. 1–4.
[204] G. Petschnigg, R. Szeliski, M. Agrawala, M. Cohen, H. Hoppe, and K. Toyama, ‘‘Digital photography with ﬂash and no-ﬂash image pairs,’’ ACM Trans. Graph., vol. 23, no. 3, pp. 664–672, Aug. 2004.
[205] E. Reinhard et al., High Dynamic Range Imaging: Acquisition, Display and Image-Based Lighting, 2nd ed. San Francisco, CA, USA: Morgan Kaufmann, 2010.
[206] T. Stathaki, Image Fusion: Algorithms and Applications. New York, NY, USA: Academic, 2008.
[207] H. Zhang, E. Zhu, and Y. Wu, ‘‘High dynamic range image generating algorithm based on detail layer separation of a single exposure image,’’ (in Chinese), Acta Automatica Sinica, vol. 45, no. 11, pp. 2159–2170, 2019.
[208] R. Fattal, D. Lischinski, and M. Werman, ‘‘Gradient domain high dynamic range compression,’’ ACM Trans. Graph., vol. 21, no. 3, pp. 249–256, Jul. 2002.
[209] Z. Guo Li, J. H. Zheng, and S. Rahardja, ‘‘Detail-enhanced exposure fusion,’’ IEEE Trans. Image Process., vol. 21, no. 11, pp. 4672–4676, Nov. 2012.
[210] B.-J. Yun, H.-D. Hong, and H.-H. Choi, ‘‘A contrast enhancement method for HDR image using a modiﬁed image formation model,’’ IEICE Trans. Inf. Syst., vol. E95-D, no. 4, pp. 1112–1119, 2012.
[211] I. Merianos and N. Mitianoudis, ‘‘A hybrid multiple exposure image fusion approach for HDR image synthesis,’’ in Proc. IEEE Int. Conf. Imag. Syst. Techn., Oct. 2016, pp. 222–226.
[212] D. Patel, B. Sonane, and S. Raman, ‘‘Multi-exposure image fusion using propagated image ﬁltering,’’ in Proc. Int. Conf. Comput. Vis. Image Process., 2017, pp. 431–441.
[213] Y. Huo and Q. Peng, ‘‘High dynamic range images and reverse tone mapping operators,’’ (in Chinese), Syst. Eng. Electron., vol. 34, no. 4, pp. 821–826, 2012.
[214] Y. Huo, F. Yang, L. Dong, and V. Brost, ‘‘Physiological inverse tone mapping based on retina response,’’ Vis. Comput., vol. 30, no. 5, pp. 507–517, May 2014.
[215] H.-S. Le and H. Li, ‘‘Fused logarithmic transform for contrast enhancement,’’ Electron. Lett., vol. 44, no. 1, pp. 19–20, 2008.
[216] M. Yamakawa and Y. Sugita, ‘‘Image enhancement using Retinex and image fusion techniques,’’ Electron. Commun. Jpn., vol. 101, no. 8, pp. 52–63, Aug. 2018.
[217] W. Wang, Z. Chen, X. Yuan, and X. Wu, ‘‘Adaptive image enhancement method for correcting low-illumination images,’’ Inf. Sci., vol. 496, pp. 25–41, Sep. 2019.
[218] A. T. Celebi, R. Duvar, and O. Urhan, ‘‘Fuzzy fusion based high dynamic range imaging using adaptive histogram separation,’’ IEEE Trans. Consum. Electron., vol. 61, no. 1, pp. 119–127, Feb. 2015.
[219] X. Fu, D. Zeng, Y. Huang, Y. Liao, X. Ding, and J. Paisley, ‘‘A fusionbased enhancing method for weakly illuminated images,’’ Signal Process., vol. 129, pp. 82–96, Dec. 2016.
[220] W. Xie et al., ‘‘Color reduction and detail extraction of low illumination image with improved pyramid fusion,’’ (in Chinese), Appl. Res. Comput., vol. 36, no. 2, pp. 606–610, 2019.
[221] Y. Ren, Z. Ying, T. H. Li, and G. Li, ‘‘LECARM: Low-light image enhancement using the camera response model,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 29, no. 4, pp. 968–981, Apr. 2019.
[222] Y. Huo and X. Zhang, ‘‘Single image-based HDR image generation with camera response function estimation,’’ IET Image Process., vol. 11, no. 12, pp. 1317–1324, Dec. 2017.
[223] Z. Ying, G. Li, and W. Gao, ‘‘A bio-inspired multi-exposure fusion framework for low-light image enhancement,’’ 2017, arXiv:1711.00591. [Online]. Available: http://arxiv.org/abs/1711.00591
[224] Z. Ying, G. Li, and Y. Ren, ‘‘A new image contrast enhancement algorithm using exposure fusion framework,’’ in Proc. Int. Conf. Comput. Anal. Images Patterns, 2017, pp. 36–46.
VOLUME 8, 2020

[225] Z. Ying, G. Li, Y. Ren, R. Wang, and W. Wang, ‘‘A new low-light image enhancement algorithm using camera response model,’’ in Proc. IEEE Int. Conf. Comput. Vis. Workshops, Oct. 2017, pp. 3015–3022.
[226] Z. Rahman, M. Aamir, Y.-F. Pu, F. Ullah, and Q. Dai, ‘‘A smart system for low-light image enhancement with color constancy and detail manipulation in complex light environments,’’ Symmetry, vol. 10, no. 12, 2018, Art. no. 10120718.
[227] Z. Zhou et al., ‘‘Single-image low-light enhancement via generating and fusing multiple sources,’’ Neural Comput. Appl., Nov. 2018, doi: 10.1007/s00521-018-3893-3.
[228] K. He, J. Sun, and X. Tang, ‘‘Single image haze removal using dark channel prior,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2009, pp. 1956–1963.
[229] X. Dong, G. Wang, and Y. Pang, ‘‘Fast efﬁcient algorithm for enhancement of low lighting video,’’ in Proc. IEEE Int. Conf. Multimedia Expo Washington, Jul. 2011, pp. 1–6.
[230] G. Li, G. Li, and G. Han, ‘‘Illumination compensation using Retinex model based on bright channel prior,’’ (in Chinese), Opt. Precis. Eng., vol. 26, no. 5, pp. 1191–1200, 2018.
[231] X. Fu, D. Zeng, Y. Huang, X. Ding, and X.-P. Zhang, ‘‘A variational framework for single low light image enhancement using bright channel prior,’’ in Proc. IEEE Global Conf. Signal Inf. Process., Dec. 2013, pp. 1085–1088.
[232] X. Wang, H. Zhang, and Y. Wu, ‘‘Low-illumination image enhancement based on physical model,’’ J. Comput. Appl., vol. 35, no. 8, pp. 2301–2304, 2015.
[233] Z. Shi, M. M. Zhu, B. Guo, M. Zhao, and C. Zhang, ‘‘Nighttime low illumination image enhancement with single image using bright/dark channel prior,’’ EURASIP J. Image Video Process., vol. 2018, no. 1, Dec. 2018, Art. no. 13.
[234] X. Wei, L. Xueling, T. Zhigang, Y. Jin, and X. Ke, ‘‘Low light image enhancement based on luminance map and haze removal model,’’ in Proc. 10th Int. Symp. Comput. Intell. Design, Dec. 2017, pp. 143–146.
[235] X. Zhang, P. Shen, and L. Luo, ‘‘Enhancement and noise reduction of very low light level images,’’ in Proc. IEEE Int. Conf. Pattern Recognit., Nov. 2012, pp. 2034–2037.
[236] X. Jiang, H. Yao, S. Zhang, X. Lu, and W. Zeng, ‘‘Night video enhancement using improved dark channel prior,’’ in Proc. IEEE Int. Conf. Image Process., Sep. 2013, pp. 553–557.
[237] J. Song, L. Zhang, and P. Shen, ‘‘Single low-light image enhancement using luminance map,’’ in Proc. Chin. Conf. Pattern Recognit., Nov. 2016, pp. 101–110.
[238] J. Pang, S. Zhang, and W. Bai, ‘‘A novel framework for enhancement of the low lighting video,’’ in Proc. IEEE Symp. Comput. Commun., Jul. 2017, pp. 1366–1371.
[239] L. Zhang, P. Shen, X. Peng, G. Zhu, J. Song, W. Wei, and H. Song, ‘‘Simultaneous enhancement and noise reduction of a single low-light image,’’ IET Image Process., vol. 10, no. 11, pp. 840–847, Nov. 2016.
[240] L. Tao, C. Zhu, J. Song, T. Lu, H. Jia, and X. Xie, ‘‘Low-light image enhancement using CNN and bright channel prior,’’ in Proc. IEEE Int. Conf. Image Process., Sep. 2017, pp. 3215–3219.
[241] H. Lee, K. Sohn, and D. Min, ‘‘Unsupervised low-light image enhancement using bright channel prior,’’ IEEE Signal Process. Lett., vol. 27, pp. 251–255, Jan. 2020, doi: 10.1109/LSP.2020.2965824.
[242] S. Park, B. Moon, S. Ko, S. Yu, and J. Paik, ‘‘Low-light image restoration using bright channel prior-based variational retinex model,’’ EURASIP J. Image Video Process., vol. 2017, no. 1, Dec. 2017, Art. no. 44
[243] Y. Hu, Y. Shang, and X. Fu, ‘‘Low-illumination video enhancement algorithm based on combined atmospheric physical model and luminance transmission map,’’ (in Chinese), J. Image Graph., vol. 21, no. 8, pp. 1010–1020, 2016.
[244] C. Yu, X. Xu, and H. Lin, ‘‘Low-illumination image enhancement method based on a fog-degraded model,’’ (in Chinese), J. Image Graph., vol. 22, no. 9, pp. 1194–1205, 2017.
[245] C. Tang, Y. Wang, H. Feng, Z. Xu, Q. Li, and Y. Chen, ‘‘Low-light image enhancement with strong light weakening and bright halo suppressing,’’ IET Image Process., vol. 13, no. 3, pp. 537–542, Feb. 2019.
[246] P. Gómez, M. Semmler, A. Schützenberger, C. Bohr, and M. Döllinger, ‘‘Low-light image enhancement of high-speed endoscopic videos using a convolutional neural network,’’ Med. Biol. Eng. Comput., vol. 57, no. 7, pp. 1451–1463, Jul. 2019.
[247] W. Ren, S. Liu, L. Ma, Q. Xu, X. Xu, X. Cao, J. Du, and M.-H. Yang, ‘‘Low-light image enhancement via a deep hybrid network,’’ IEEE Trans. Image Process., vol. 28, no. 9, pp. 4364–4375, Sep. 2019.
87915

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

[248] Y. Cai and U. Kintak, ‘‘Low-light image enhancement based on modiﬁed U-net,’’ in Proc. Int. Conf. Wavelet Anal. Pattern Recognit., Jul. 2019, pp. 1–7.
[249] H. Chang, M. K. Ng, W. Wang, and T. Zeng, ‘‘Retinex image enhancement via a learned dictionary,’’ Opt. Eng., vol. 54, no. 1, Jan. 2015, Art. no. 013107.
[250] H. Fu, H. Ma, and S. Wu, ‘‘Night removal by color estimation and sparse representation,’’ in Proc. IEEE Int. Conf. Pattern Recognit., Nov. 2012, pp. 3656–3659.
[251] K. Fotiadou, G. Tsagkatakis, and P. Tsakalides, ‘‘Low light image enhancement via sparse representations,’’ in Proc. Int. Conf. Image Anal. Recognit., Vilamoura, Portugal, Oct. 2014, pp. 84–93.
[252] J. Cepeda-Negrete and R. E. Sanchez-Yanez, ‘‘Automatic selection of color constancy algorithms for dark image enhancement by fuzzy rulebased reasoning,’’ Appl. Soft Comput., vol. 28, pp. 1–10, Mar. 2015.
[253] Z. Yan, H. Zhang, B. Wang, S. Paris, and Y. Yu, ‘‘Automatic photo adjustment using deep neural networks,’’ ACM Trans. Graph., vol. 35, no. 2, May 2016, Art. no. 11.
[254] K. G. Lore, A. Akintayo, and S. Sarkar, ‘‘LLNet: A deep autoencoder approach to natural low-light image enhancement,’’ Pattern Recognit., vol. 61, pp. 650–662, Jan. 2017.
[255] S. Park, S. Yu, M. Kim, K. Park, and J. Paik, ‘‘Dual autoencoder network for retinex-based low-light image enhancement,’’ IEEE Access, vol. 6, pp. 22084–22093, 2018.
[256] Y. Endo, Y. Kanamori, and J. Mitani, ‘‘Deep reverse tone mapping,’’ ACM Trans. Graph., vol. 36, no. 6, Nov. 2017, Art. no. 177.
[257] E. Ha, H. Lim, S. Yu, and J. Paik, ‘‘Low-light image enhancement using dual convolutional neural networks for vehicular imaging systems,’’ in Proc. IEEE Int. Conf. Consum. Electron., Jan. 2020, pp. 1–2.
[258] H. Ma, S. Ma, and Y. Xu, ‘‘Low-light image enhancement based on deep convolutional neural network,’’ (in Chinese), Acta Optica Sinica, vol. 39, no. 2, pp. 91–100, 2019.
[259] M. Kim, ‘‘Improvement of low-light image by convolutional neural network,’’ in Proc. IEEE 62nd Int. Midwest Symp. Circuits Syst., Aug. 2019, pp. 189–192.
[260] L. Tao, C. Zhu, G. Xiang, Y. Li, H. Jia, and X. Xie, ‘‘LLCNN: A convolutional neural network for low-light image enhancement,’’ in Proc. IEEE Vis. Commun. Image Process., Dec. 2017, pp. 1–4.
[261] W. Wang, C. Wei, W. Yang, and J. Liu, ‘‘GLADNet: Low-light enhancement network with global awareness,’’ in Proc. 13th IEEE Int. Conf. Autom. Face Gesture Recognit., May 2018, pp. 751–755.
[262] A. Ignatov, N. Kobyshev, R. Timofte, and K. Vanhoey, ‘‘DSLR-quality photos on mobile devices with deep convolutional networks,’’ in Proc. IEEE Int. Conf. Comput. Vis., Oct. 2017, pp. 3297–3305.
[263] F. Lv, F. Lu, and J. Wu, ‘‘MBLLEN: Low-light image/video enhancement using CNNs,’’ in Proc. Brit. Mach. Vis. Conf., 2018, pp. 1–13.
[264] G. Eilertsen, J. Kronander, G. Denes, R. K. Mantiuk, and J. Unger, ‘‘HDR image reconstruction from a single exposure using deep CNNs,’’ ACM Trans. Graph., vol. 36, no. 6, Nov. 2017, Art. no. 178
[265] C. Liu, X. Wu, and X. Shu, ‘‘Learning-based dequantization for image restoration against extremely poor illumination,’’ 2018, arXiv:1803.01532. [Online]. Available: http://arxiv.org/abs/1803.01532
[266] M. Gharbi, J. Chen, J. T. Barron, S. W. Hasinoff, and F. Durand, ‘‘Deep bilateral learning for real-time image enhancement,’’ ACM Trans. Graph., vol. 36, no. 4, Jul. 2017, Art. no. 118.
[267] C. Chen, Q. Chen, J. Xu, and V. Koltun, ‘‘Learning to see in the dark,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 3291–3300.
[268] L. Shen, Z. Yue, F. Feng, Q. Chen, S. Liu, and J. Ma, ‘‘MSR-net:Lowlight image enhancement using deep convolutional network,’’ 2017, arXiv:1711.02488. [Online]. Available: http://arxiv.org/abs/1711.02488
[269] Y. Guo, X. Ke, J. Ma, and J. Zhang, ‘‘A pipeline neural network for lowlight image enhancement,’’ IEEE Access, vol. 7, pp. 13737–13744, 2019.
[270] C. Wei, W. Wang, and W. Yang, ‘‘Deep Retinex decomposition for lowlight enhancement,’’ in Proc. Brit. Mach. Vis. Conf., 2018, pp. 1–12.
[271] C. Li, J. Guo, F. Porikli, and Y. Pang, ‘‘LightenNet: A convolutional neural network for weakly illuminated image enhancement,’’ Pattern Recognit. Lett., vol. 104, pp. 15–22, Mar. 2018.
[272] Y. Zhang, J. Zhang, and X. Guo, ‘‘Kindling the darkness: A practical low-light image enhancer,’’ 2019, arXiv:1905.04161. [Online]. Available: http://arxiv.org/abs/1905.04161
[273] J. Cai, S. Gu, and L. Zhang, ‘‘Learning a deep single image contrast enhancer from multi-exposure images,’’ IEEE Trans. Image Process., vol. 27, no. 4, pp. 2049–2062, Apr. 2018.
87916

[274] X. Yang, K. Xu, Y. Song, Q. Zhang, X. Wei, and R. Lau, ‘‘Image correction via deep reciprocating HDR transformation,’’ 2018, arXiv:1804.04371. [Online]. Available: http://arxiv.org/abs/1804.04371
[275] Y. Kinoshita and H. Kiya, ‘‘Convolutional neural networks considering local and global features for image enhancement,’’ 2019, arXiv:1905.02899. [Online]. Available: http://arxiv.org/abs/1905.02899
[276] Y.-S. Chen, Y.-C. Wang, M.-H. Kao, and Y.-Y. Chuang, ‘‘Deep photo enhancer: Unpaired learning for image enhancement from photographs with GANs,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 6306–6314.
[277] J. Wang, W. Tan, X. Niu, and B. Yan, ‘‘RDGAN: Retinex decomposition based adversarial learning for low-light enhancement,’’ in Proc. IEEE Int. Conf. Multimedia Expo, Jul. 2019, pp. 1186–1191.
[278] Y. Meng, D. Kong, Z. Zhu, and Y. Zhao, ‘‘From night to day: GANs based low quality image enhancement,’’ Neural Process. Lett., vol. 50, no. 1, pp. 799–814, Aug. 2019.
[279] A. Ignatov, N. Kobyshev, R. Timofte, K. Vanhoey, and L. Van Gool, ‘‘WESPE: Weakly supervised photo enhancer for digital cameras,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, Jun. 2018, pp. 804–809.
[280] G. Kim, D. Kwon, and J. Kwon, ‘‘Low-lightgan: Low-light enhancement via advanced generative adversarial network with task-driven training,’’ in Proc. IEEE Int. Conf. Image Process., Sep. 2019, pp. 2811–2815.
[281] Y. P. Loh and C. S. Chan, ‘‘Getting to know low-light images with the exclusively dark dataset,’’ Comput. Vis. Image Understand., vol. 178, pp. 30–42, Jan. 2019.
[282] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, ‘‘The Pascal visual object classes (VOC) challenge,’’ Int. J. Comput. Vis., vol. 88, no. 2, pp. 303–338, Jun. 2010.
[283] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, ‘‘ImageNet large scale visual recognition challenge,’’ Int. J. Comput. Vis., vol. 115, no. 3, pp. 211–252, Dec. 2015.
[284] T. Lin et al., ‘‘Microsoft COCO: Common objects in context,’’ in Proc. Eur. Conf. Comput. Vis., Zürich, Switzerland, Sep. 2014, pp. 740–755.
[285] N. He, K. Xie, and T. Li, ‘‘Overview of image quality assessment,’’ (in Chinese), J. Beijing Inst. Graphic Commun., vol. 25, no. 2, pp. 47–50, 2017.
[286] S. Liu et al., ‘‘Overview of image quality assessment,’’ (in Chinese), Sciencepaper Online, vol. 6, no. 7, pp. 501–506, 2011.
[287] W. Huang, Y. Zhang, and B. Wei, ‘‘Low illumination image decomposition and details enhancement under gradient sparse and least square constraint,’’ (in Chinese), Acta Electronica Sinica, vol. 46, no. 2, pp. 424–432, 2018.
[288] A. Mittal, A. K. Moorthy, and A. C. Bovik, ‘‘No-reference image quality assessment in the spatial domain,’’ IEEE Trans. Image Process., vol. 21, no. 12, pp. 4695–4708, Dec. 2012.
[289] A. Mittal, R. Soundararajan, and A. C. Bovik, ‘‘Making a ‘completely blind’ image quality analyzer,’’ IEEE Signal Process. Lett., vol. 20, no. 3, pp. 209–212, Mar. 2013.
[290] M. A. Saad, A. C. Bovik, and C. Charrier, ‘‘Blind image quality assessment: A natural scene statistics approach in the DCT domain,’’ IEEE Trans. Image Process., vol. 21, no. 8, pp. 3339–3352, Aug. 2012.
[291] K. Gu, S. Wang, G. Zhai, S. Ma, X. Yang, W. Lin, W. Zhang, and W. Gao, ‘‘Blind quality assessment of tone-mapped images via analysis of information, naturalness, and structure,’’ IEEE Trans. Multimedia, vol. 18, no. 3, pp. 432–443, Mar. 2016.
[292] N. Hautière, J.-P. Tarel, D. Aubert, and É. Dumont, ‘‘Blind contrast enhancement assessment by gradient ratioing at visible edges,’’ Image Anal. Stereol., vol. 27, no. 2, pp. 87–95, 2008.
[293] K. Gu, G. Zhai, W. Lin, X. Yang, and W. Zhang, ‘‘No-reference image sharpness assessment in autoregressive parameter space,’’ IEEE Trans. Image Process., vol. 24, no. 10, pp. 3218–3231, Oct. 2015.
[294] K. Gu, W. Lin, G. Zhai, X. Yang, W. Zhang, and C. W. Chen, ‘‘Noreference quality metric of contrast-distorted images based on information maximization,’’ IEEE Trans. Cybern., vol. 47, no. 12, pp. 4559–4565, Dec. 2017.
[295] K. Matkovic, L. Neumann, and A. Neumann, ‘‘Global contrast factor-a new approach to image contrast,’’ in Proc. Comput. Aesthetics Graph., Vis. Imag., 2005, pp. 159–167.
[296] Y.-C. Chang and C.-M. Chang, ‘‘A simple histogram modiﬁcation scheme for contrast enhancement,’’ IEEE Trans. Consum. Electron., vol. 56, no. 2, pp. 737–742, May 2010.
VOLUME 8, 2020

W. Wang et al.: Experiment-Based Review of Low-Light Image Enhancement Methods

[297] S. S. Agaian, B. Silver, and K. A. Panetta, ‘‘Transform coefﬁcient histogram-based image enhancement algorithms using contrast entropy,’’ IEEE Trans. Image Process., vol. 16, no. 3, pp. 741–758, Mar. 2007.
[298] Z. Chen, B. R. Abidi, D. L. Page, and M. A. Abidi, ‘‘Gray-level grouping (GLG): An automatic method for optimized image contrast enhancement—Part I: The basic method,’’ IEEE Trans. Image Process., vol. 15, no. 8, pp. 2290–2302, Aug. 2006.
[299] K. Gu, G. Zhai, X. Yang, and W. Zhang, ‘‘Using free energy principle for blind image quality assessment,’’ IEEE Trans. Multimedia, vol. 17, no. 1, pp. 50–63, Jan. 2015.
[300] C. E. Shannon, ‘‘A mathematical theory of communication,’’ Bell Syst. Tech. J., vol. 27, no. 3, pp. 379–423, Jul./Oct. 1948.
[301] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‘‘Image quality assessment: From error visibility to structural similarity,’’ IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, Apr. 2004.
[302] S. Wang, K. Ma, H. Yeganeh, Z. Wang, and W. Lin, ‘‘A patch-structure representation method for quality assessment of contrast changed images,’’ IEEE Signal Process. Lett., vol. 22, no. 12, pp. 2387–2390, Dec. 2015.
[303] K. Gu, D. Tao, J.-F. Qiao, and W. Lin, ‘‘Learning a no-reference quality assessment model of enhanced images with big data,’’ IEEE Trans. Neural Netw. Learn. Syst., vol. 29, no. 4, pp. 1301–1313, Apr. 2018.
[304] W. Xue, L. Zhang, X. Mou, and A. C. Bovik, ‘‘Gradient magnitude similarity deviation: A highly efﬁcient perceptual image quality index,’’ IEEE Trans. Image Process., vol. 23, no. 2, pp. 684–695, Feb. 2014.
[305] H. R. Sheikh and A. C. Bovik, ‘‘Image information and visual quality,’’ IEEE Trans. Image Process., vol. 15, no. 2, pp. 430–444, Feb. 2006.
[306] L. Zhang, Y. Shen, and H. Li, ‘‘VSI: A visual saliency-induced index for perceptual image quality assessment,’’ IEEE Trans. Image Process., vol. 23, no. 10, pp. 4270–4281, Oct. 2014.
[307] H. Yeganeh and Z. Wang, ‘‘Objective quality assessment of tone-mapped images,’’ IEEE Trans. Image Process., vol. 22, no. 2, pp. 657–667, Feb. 2013.
[308] L. Zhang, L. Zhang, X. Mou, and D. Zhang, ‘‘FSIM: A feature similarity index for image quality assessment,’’ IEEE Trans. Image Process., vol. 20, no. 8, pp. 2378–2386, Aug. 2011.
[309] J. Yang, X. Jiang, C. Pan, and C.-L. Liu, ‘‘Enhancement of low light level images with coupled dictionary learning,’’ in Proc. Int. Conf. Pattern Recognit., Dec. 2016, pp. 751–756.
[310] S. Yu, S. Ko, W. Kang, and J. Paik, ‘‘Low-light image enhancement using fast adaptive binning for mobile phone cameras,’’ in Proc. IEEE 5th Int. Conf. Consum. Electron.-Berlin (ICCE-Berlin), Sep. 2015, pp. 170–171.
[311] J. Yan, J. Li, and X. Fu, ‘‘No-reference quality assessment of contrastdistorted images using contrast enhancement,’’ 2019, arXiv:1904.08879. [Online]. Available: http://arxiv.org/abs/1904.08879
[312] L. Liu, B. Liu, H. Huang, and A. C. Bovik, ‘‘No-reference image quality assessment based on spatial and spectral entropies,’’ Signal Process., Image Commun., vol. 29, no. 8, pp. 856–863, Sep. 2014.
[313] G. Yang, D. Li, F. Lu, Y. Liao, and W. Yang, ‘‘RVSIM: A feature similarity method for full-reference image quality assessment,’’ EURASIP J. Image Video Process., vol. 2018, no. 1, p. 6, Dec. 2018.
[314] H. R. Sheikh, A. C. Bovik, and G. de Veciana, ‘‘An information ﬁdelity criterion for image quality assessment using natural scene statistics,’’ IEEE Trans. Image Process., vol. 14, no. 12, pp. 2117–2128, Dec. 2005.

XIAOJIN WU received the Ph.D. degree in trafﬁc information engineering control from Beijing Jiaotong University, in 2011. He is currently with the College of Information and Control Engineering, Weifang University. He is also a Visiting Scholar with the University of North Texas, engaged in the research of image enhancement technology. He has published and authored more than ten articles on academic journals and conferences. His main research interests include intelligent systems and image processing
XIAOHUI YUAN (Senior Member, IEEE) received the B.S. degree in electrical engineering from the Hefei University of Technology, China, in 1996, and the Ph.D. degree in computer science from Tulane University, in 2004. He is currently an Associate Professor with the University of North Texas (UNT). His research ﬁndings are reported in over 150 peer-reviewed articles. His research interests include computer vision, data mining, machine learning, and artiﬁcial intelligence. He was a recipient of Ralph E. Powe Junior Faculty Enhancement Award, in 2008, and the Air Force Summer Faculty Fellowship, in 2011, 2012, and 2013. He served as the session chairs with many conferences. He also served as a Panel Reviewer for funding agencies, including NSF, NIH, and the Louisiana Board of Regent’s Research Competitiveness Program, and the editorial board of several international journals.

WENCHENG WANG (Member, IEEE) received the Ph.D. degree in pattern recognition and intelligent system from Shandong University. From 2015 to 2016, he was a Visiting Scholar with the University of North Texas, engaged in the research of image dehazing and enhancement technology. He is currently a Professor with Weifang University. He is also a Principal Investigator with the Weifang City’s Key Laboratory and the Innovation Team of Shandong Provincial Education Department on robot’s vision perception and control. He has participated in more than ten scientiﬁc research projects. He has published more than 60 articles on academic journals and conferences. He holds over 14 patents. His main research interests include computer vision, pattern recognition, and intelligent computing.

ZAIRUI GAO received the Ph.D. degree in control theory and control engineering from the Ocean University of China, in 2012. Since 2012, he has been a Lecturer with the College of Information and Control Engineering, Weifang University, China. He has published and authored more than 20 articles on academic journals and conferences. His research interests include variable structure control, intelligent control, singular systems, and information processing.

VOLUME 8, 2020

87917

