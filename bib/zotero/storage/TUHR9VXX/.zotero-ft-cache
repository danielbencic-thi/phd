IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Typesetting math: 4%

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Journals & Magazines > IEEE Access > Volume: 9
Motion Planning for Mobile Robots—Focusing on Deep Reinforcement Learning: A Systematic Review
Publisher: IEEE
Cite This
PDF
Huihui Sun ; Weijie Zhang ; Runxiang Yu ; Yujie Zhang
All Authors
2
Paper
Citations
1803
Full
Text Views
Open Access
Comment(s)

    Alerts

Under a Creative Commons License
Abstract
Document Sections

    I.
    Introduction
    II.
    Related Research
    III.
    Motion Planning Based on DRL
    IV.
    Multi-Robot Cooperative Planning
    V.
    Difficulty for DRL Motion Planning

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
Abstract:
Mobile robots contributed significantly to the intelligent development of human society, and the motion-planning policy is critical for mobile robots. This paper reviews the methods based on motion-planning policy, especially the ones involving Deep Reinforcement Learning (DRL) in the unstructured environment. The conventional methods of DRL are categorized to value-based, policy-based and actor-critic-based algorithms, and the corresponding theories and applications are surveyed. Furthermore, the recently-emerged methods of DRL are also surveyed, especially the ones involving the imitation learning, meta-learning and multi-robot systems. According to the surveys, the potential research directions of motion-planning algorithms serving for mobile robots are enlightened.
Published in: IEEE Access ( Volume: 9 )
Page(s): 69061 - 69081
Date of Publication: 29 April 2021
Electronic ISSN: 2169-3536
INSPEC Accession Number: 20986391
DOI: 10.1109/ACCESS.2021.3076530
Publisher: IEEE
Funding Agency:
Mobile robots contributed significantly to the intelligent development of human society, and the motion-planning policy is critical for mobile robots. This paper reviews the methods based on motion-planning policy, especially the ones involving Deep Reinforcement Learning (DRL) in the unstructured environment. The conventional methods of DRL are categorized to value-based, policy-based and actor-critic-based algorithms, and the corresponding theories and applications are surveyed. Furthermore, the recently-emerged methods of DRL are also surveyed, especially the ones involving the imitation learning, meta-learning and multi-robot systems. According to the surveys, the potential research directions of motion-planning algorithms serving for mobile robots are enlightened.
Mobile robots contributed significantly to the intelligent development of human society, and the motion-planning policy is critical for mobile robots. This paper reviews ... View more
Hide Full Abstract
CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https :// creativecommons . org / licenses / by / 4 . 0 / to obtain full-text articles and stipulations in the API documentation.
SECTION I.
Introduction

Mobile robots are commonly applied in the dangerous and complex environments to complete various tasks [1] , including exploring unknown and chaotic environments, searching trapped victims and carrying important materials, etc. In the urgent and dangerous scenes, mobile robots may even be irreplaceable [2] .

In recent decades, the research on mobile robot technology may reflect the trend of the advanced technology [3] . Many countries regard mobile robots as a key industry in the robotics field. With the rapid development of Artificial Intelligence (AI), sensors, networks and communication technology, mobile robots become more and more intelligent [4] . Intelligent mobile robot should have the high autonomy, and thus it could quickly and accurately complete tasks without the guidance of human, ignoring any environmental restrictions [5] . The motion planning is to search for a non-collision optimal path from the current position to the destination with the external environment information obtained by the sensors [6] . It is one of the most important capabilities of intelligent robots. Traditional motion planning methods is commonly incapable to complete the tasks in the dynamically-changing scenes lacking of the prior knowledge and maps, such as areas where fire, earthquake, explosion and other emergency cases occur [7] .

In this context, motion planning methods with self-learning ability becomes an important research direction [8] , [9] . Today, Robot AI technology has been deeply applied to all aspects of society, and Deep Learning (DL) and Reinforcement Learning(RL) technology is considered as the most appropriate methods to solve the motion planning problems in an unknown dynamic environment [10] .

Deep learning, as an important part of machine learning, has made remarkable achievements in image processing [11] , face recognition [12] , video detection [13] and natural language processing [14] field. Through multi-layer network structure and nonlinear transformation, deep learning combines low-level features to form an abstract and easily distinguishable high-level representation, so as to discover distributed feature representation of data and enable mobile robots to have strong environmental awareness.

RL is mainly used to solve sequential decision making problems. Inspired by the trial-and-error method in animal learning, RL uses the reward value obtained from the interaction between agents and the environment as feedback signals to train agents, which does not need auxiliary means such as artificial markers. Reinforcement signals is provided by the environment, which are used to evaluate the effectiveness of the executive actions, rather than to instruct the agent how to perform the correct actions. Different from the tutor signal in supervised learning, the reinforcement signal in RL is easier to obtain for some decision control problems, and RL has become an effective method to solve the decision problem for mobile robot.

Further, DeepMind team combined DL with RL to propose a deep reinforcement learning(DRL) method. DRL was applied to Alphago and defeated human champions in 2016 [15] . Then, OpenAI team proposed an OpenAI-Five method based on multi-agent DRL (MADRL), which beat human players in the 5v5 mode of Dota2 game [16] . In 2019, DeepMind proposed AlphaStar based on MADRL, reaching the level of human masters in the Starcraft game [17] . In closed, static and deterministic environments, DRL can reach or even surpass human decision-making levels.

The motion planning method based on DRL can enable the robot to have autonomous learning ability and thus answer the question “how to reach the target position” [18] . DRL [19] enables robots to gain the perceptual and decision-making abilities. The input data are processed to yield the output in an end-to-end manner [20] . Sensor data (color/depth image, laser scanning, etc.) serve as the input state, and robot motion parameters (coordinates, displacement, direction, etc.) are referred to as outputs. The training depends on the interactions between the robot and the environment [21] . The optimization process does not depend on the environment model and any prior knowledge, which can guide the robot to navigate to the target position without collision in real time [22] . Compared with other algorithms, DRL has the following advantages: First, since it does not need sample labeling, it can solve special cases in the environment more effectively; Secondly, the end-to-end motion planning method can treat the system as a whole, which makes the system more robust. Third, reinforcement learning can better explore and learn some more excellent movements. These advantages play an important role in the optimal decision control of mobile robots. The motion planning method based on DRL has made substantial breakthroughs in many tasks involving high-dimensional raw input data and complex decision control [23] . Many famous research institutions and companies have devoted plentiful resources to study the motion-planning methods based on DRL, and a great deal has been achieved [24] .

This paper summarizes the research status and of motion planning for mobile robots [25] . Aiming at the defect of conventional motion planning algorithm in complex environment, the mobile robot motion planning method based on DRL technology is emphatically summarized and compared. The main structure of this paper is as follows:

    Related research

    Motion planning based on DRL

    Multi-robot Cooperative planning

    Difficulty for DRL motion planning

    Future Research

    Conclusion

SECTION II.
Related Research
A. Conventional Algorithm

The optimization goal for mobile robot motion planning [26] is to take the shortest planning path, the minimum system cost and the minimum training time [27] . Conventional motion planning algorithms of mobile robots can be divided into two categories according to the prior knowledge of environmental: the global motion planning algorithm and the local motion planning algorithm, which is shown as FIGURE 1 .
FIGURE 1.

Schematic diagram of mobile robot motion planning.

Show All

Global motion planning algorithms [28] belongs to static and off-line motion planning, which is generally applied when the obstacle information in the robot operating environment is fully understood. However, the motion planning depends on the accuracy of environment acquisition. Although the planning results are global and superior, they have poor robustness to the error of environment model and noise interference. According to different search methods, global motion planning algorithm include graph search algorithm, random sampling algorithm and intelligent algorithm.

The commonly used graph search algorithms include Dijkstra [29] , A* [30] , D* algorithm [31] etc. Compared with Dijkstra algorithm, A* algorithm has higher efficiency by increasing heuristic estimation, reducing search volume. But the efficiency of A* algorithm still cannot be guaranteed when the environment is complex and large. Random sampling algorithms mainly include probability graph method (PRM) [32] and rapid Exploration random Tree method (RRT) [33] , and these algorithms search for the optimal path in the whole space by randomly selecting scatter points. When the scatters cover the starting point and the end point, the shortest line between the scatters is the calculated path. There are still some problems, such as high cost, poor real-time performance, and the planned path may not be the optimal path.

Intelligent algorithm simulates biological evolution and insect foraging behaviors in nature, mainly including genetic algorithm(GA) [34] , ant colony algorithm (ACO) [35] , particle swarm algorithm [36] , etc. GA has the characteristics of potential parallelism and is suitable for solving and optimizing complex problems. However, there are problems of slow operation speed and immature solutions. Particle swarm algorithm overcomes this disadvantage and has the advantage of fast convergence. The comparison of global motion planning for global motion planning is shown in Table 1 .
TABLE 1 Comparison of Global Motion Planning

Local motion planning is to enable the robot to autonomously obtain an optimal path without collision in an unknown environment based on the surrounding environment information [37] . Its advantage is that it is more adaptive to the unknown and real-time environment. Local motion planning belongs to dynamic planning and focuses on considering the local environment information of the robot, which enables the robot to have good obstacle avoidance ability [38] . Robots need to be robust enough to environmental errors and noise, and they can provide real-time feedback and correction for planning results through efficient information processing capability [39] . The algorithms used in local motion planning mainly include artificial potential field method [40] , simulated annealing method [41] , fuzzy logic method [42] , neural network method(NN) [43] , dynamic window Approach(DWA) [44] , etc.

There is no essential difference between global motion planning and local motion planning. Many methods applicable to global motion planning can also be applied to local motion planning after being improved. The comparison of typical local planning for global motion planning is shown in Table 2 .
TABLE 2 Comparison of Local Motion Planning

In order to show the relationship between different algorithms better, the algorithm mechanism diagram is shown in FIGURE 2 . Traditional motion planning algorithms include graph-based, sampling - based and data-based methods. Intelligent bionic algorithm includes genetic algorithm, ant colony algorithm, etc. Learn-based algorithm includes deep learning algorithm, reinforcement learning algorithm, imitation learning algorithm [45] and meta learning [46] . Among them, DRL motion planning algorithm will be reviewed as a key point in this paper.
FIGURE 2.

Classification diagram of motion planning algorithm.

Show All

B. Sensors

Sensors equipped with mobile robots mainly include three categories: self sensors, Positioning sensors and surrounding sensors [47] .

The self sensors uses proprioceptive sensors to measure the current state of the robot using preinstalled measurement units, such as odometer, IMU and CAN bus [48] . Positioning sensors use GPS or inertial measurement unit dead reckoning to determine the robot’s global and local positions. Surrounding sensors use external sensors to sense road markings, weather conditions, obstacle status, which includes lidar sensors, vision sensors, infrared sensors and ultrasonic sensors, etc [49] .
C. Environmental Perception

Environment perception refers to the process of obtaining information from objects around it, road surface and its own state data. It relies on a variety of sensors to help the robot understand the shapes and the positions of obstacles [50] . The perceptual algorithms includes two categories: Mediated Behavior Perception and Reflex Perception. Mediated Behavior Perception develops a detailed map of the robot’s surroundings by analyzing the distance, pedestrians, trees, road markers, etc [51] . Behavior Reflex Perception uses artificial intelligence techniques to apply sensor data directly to the control system. They include vision, point cloud and multi-sensor fusion based algorithms.
D. Location Mode

Most of the general positioning systems of mobile robots are based on GPS. However, they are not suitable for harsh environments for GPS cannot guarantee a signal in enclosed areas. So mobile robots are often equipped with positioning systems that do not rely on GPS [52] , which includes visual-based positioning, light reflection positioning, ultrasonic positioning, aircraft calculation, Space beacon positioning, and SLAM(Simultaneous Localization and Mapping) [53] .
SECTION III.
Motion Planning Based on DRL

In the face of the dynamic unknown task environment, the conventional motion planning algorithm still show many shortcomings for lacking of prior knowledge. DRL combines the decision ability of reinforcement learning (RL) and deep learning (DL) [10] . DL uses the perceptual ability of the neural network to extract the features from the input of the unknown environmental state, and realizes the fitting of environmental state to the action value function. On the other hand, RL completes the decision based on the output of deep neural network and its own exploration, and realizes the mapping from state to action. The DRL algorithm has been applied to solve the problem of motion planning in the high-dimensional environment of robots, and substantial breakthroughs have been made in such aspects as unmanned vehicle driving, mobile robot navigation, and bionic robot skill learning. Mirowski et al. [54] used Google street view as as the video input in an interactive navigation environment, and realizes the unmanned vehicle navigation ( FIGURE 3(a) ) across the city. Everett et al. [55] used on-board sensors to judge the state of pedestrians near the robot, so as to select reasonable movements and speeds and realize the obstacle avoidance in the crowded pedestrians ( FIGURE 3(b) ). Through the deep reinforcement learning, Abhik et al. . [56] trained the quadruped robot “STOCH” to move in a simulation environment, including all the basic walking modes of the quadruped robot, and gained trot, walk, gallop, and standstill skills in a real environment ( FIGURE 3(c) ). Chen et al. . [57] . propose a Takagi-Sugeno (T-S) fuzzy control system based RL, and use UAVs to pursue UAVs ( FIGURE 3(d) ).
FIGURE 3.

Application of DRL in the field of robotics.

Show All

In additon, DRL algorithms are often combined with conventional motion planning algorithms. Conventional algorithm is preferred for global motion planning in static environment, and then deep reinforcement learning algorithm is used for dynamic obstacle avoidance. This hybrid DRL method can also be suitable for motion planning problems in dynamic unstructured unknown environments.
A. MDP

The model of RL is a standard Markov decision process (MDP). Different from supervised learning and unsupervised learning method, RL uses a feedback signal(reward) instead of a large number of labeled samples. Classical motion planning strategies (such as artificial potential field method, A* algorithm, etc.) are based on the environment with certain information, and usually have certain limitations in the application process. MDP provides a unified model description for the decision-making and solving process in the environment with incomplete information, which can solve the motion planning problem in the unstructured environment well.

As shown in FIGURE 4 , MDP includes the environment, Agent, Action, Reward and State [58] . The Agent represents the mobile robot; The Environment refers to the map information of the task. The State is the state space of the mobile robot. And the action is a set of actions of a mobile robot. The goal of reinforcement learning is to find a strategy that maximizes cumulative rewards of the robot. Different from the immediate reward, the cumulative reward evaluates the long-term impact of a certain strategy on the performance of agents in the Environment. The commonly used calculation methods include discount cumulative reward and step cumulative reward. When the reward value is not easy to set, the Inverse Reinforcement Learning can be used to learn the reward value function.
FIGURE 4.

MDP for motion planning.

Show All

In contrast to conventional motion planning methods, the mobile robot is not told which action to choose, but instead tries to find the one that will get the most reward. Motion planning can be viewed as a process of trial evaluation. Firstly, the Agent will select an action for the environment and changes the state after the environment accepts the action. The action selection of the current state s is determined by the strategy \pi (a\vert s) : \pi (a\vert s)=P(A_{t} =a\vert S_{t} =s) , and it represents the conditional probability distribution of action selection, and the effect is evaluated by the value function V{ }_{^{\pi }}(s) [59] : \begin{equation*} V_{\pi } (s)={\mathbb {E}}_{\pi } (R_{t+1} +\gamma R_{t+2} +\gamma ^{2}R_{t+3} +\ldots \vert S_{t} =s)\tag{1}\end{equation*}
View Source \begin{equation*} V_{\pi } (s)={\mathbb {E}}_{\pi } (R_{t+1} +\gamma R_{t+2} +\gamma ^{2}R_{t+3} +\ldots \vert S_{t} =s)\tag{1}\end{equation*} where \gamma \in \left [{ {0,1} }\right] is the discount factor, between [0,1].

Considering the influence of action A on the value function, the action value function Q_{\pi } (s,a) is defined as [60] : \begin{align*}&Q_{\pi } (s,a)={\mathbb {E}}_{\pi } (R_{t+1} +\gamma R_{t+2} +\gamma ^{2}R_{t+3} \\&\qquad \qquad \qquad \qquad \qquad \qquad \quad \!\!+\,\ldots \vert S_{t} =s,A_{t} =a)\tag{2}\end{align*}
View Source \begin{align*}&Q_{\pi } (s,a)={\mathbb {E}}_{\pi } (R_{t+1} +\gamma R_{t+2} +\gamma ^{2}R_{t+3} \\&\qquad \qquad \qquad \qquad \qquad \qquad \quad \!\!+\,\ldots \vert S_{t} =s,A_{t} =a)\tag{2}\end{align*}

According to the definition of the action-value function Q_{\pi } (s,a) and the state value function V_{\pi } (s) , transformation relationship between them is: \begin{equation*} v_{\pi } (s)=\sum \limits _{a\in A} \pi (a\vert s)q_{\pi } (s,a)\tag{3}\end{equation*}
View Source \begin{equation*} v_{\pi } (s)=\sum \limits _{a\in A} \pi (a\vert s)q_{\pi } (s,a)\tag{3}\end{equation*}

The purpose of robot motion planning is to find an optimal strategy for the mobile robot to consistently get more rewards during movement than any other strategy, and this optimal strategy can be represented by \pi ^{\ast }(a\vert s) : \begin{align*} \pi ^{_{\ast }}(a\vert s)=\begin{cases} \displaystyle 1 & {if\;a={\text {argmax}}_{a\in A} Q_{\ast } (s,a)} \\ \displaystyle 0 & {else} \\ \displaystyle \end{cases}\tag{4}\end{align*}
View Source \begin{align*} \pi ^{_{\ast }}(a\vert s)=\begin{cases} \displaystyle 1 & {if\;a={\text {argmax}}_{a\in A} Q_{\ast } (s,a)} \\ \displaystyle 0 & {else} \\ \displaystyle \end{cases}\tag{4}\end{align*} where: Q_{\ast } (s,a)=R_{s}^{a} +\gamma \sum \limits _{s'\in S} {P_{s{s}'}^{a}} \max _{a'} Q_{\ast } ({s}',{a}') .

In general, it is difficult to find an optimal strategy for motion planning, but a better strategy can be determined by comparing the advantages of several different strategies.
B. Model-Free RL

RL can be divided into model-based RL and model-free RL according to whether the environment model needs to be learned and understood [61] . The model-free RL motion planning process for the unknown environment model do not need to estimate the MDP model, and the value function or policy function can be evaluated directly by sampling to approximate the solution of reinforcement learning tasks.

The main methods include value-based (Value function approximation), policy-based (strategy gradient method), and Actor-Critic algorithm based on the combination of value and policy, which is shown as FIGURE 5 .
FIGURE 5.

Classification of RL algorithm.

Show All

1) Value-Based RL

Value-based reinforcement learning optimizes strategy by maximizing value function. The value function of each state of robot can be obtained by estimation. The commonly used estimation methods include Monte Carlo(MC) [62] , TD ( \lambda ) [63] , Q-learning [64] , SARSA [65] and so on. Among them, the most widely used algorithm is Q-learning. Q-learning algorithm continuously optimizes the strategy through the information perceived by the robot in the environment, so as to achieve the optimal performance. It is one of the most effective reinforcement learning algorithms, which uses greedy strategy to select actions and can guarantee the convergence of the algorithm without knowing the model.

The state-action value function Q ( s , a ) is used as the evaluation function, and the \varepsilon -greedy strategy is used to select actions. The convergence of the algorithm can be guaranteed without knowing the model. It is one of the most effective reinforcement learning algorithms at present. Value function Q ( s , a ) is the estimated value rather than the actual value. The algorithm selects the maximum Q value function from the estimated values of different actions to update. The update method of evaluation function is as follows: \begin{align*} Q^{\ast }(s_{t},a_{t})\!=\!Q(s_{t},a_{t})+\alpha (r_{t} +\gamma \max \limits _{a} Q(s_{t+1},a)\!-\!Q(s_{t},a)) \\\tag{5}\end{align*}
View Source \begin{align*} Q^{\ast }(s_{t},a_{t})\!=\!Q(s_{t},a_{t})+\alpha (r_{t} +\gamma \max \limits _{a} Q(s_{t+1},a)\!-\!Q(s_{t},a)) \\\tag{5}\end{align*} where, Q^{\ast } is the optimal value function, r_{t} is the current reward, \alpha is the step factor, and \gamma is the discount factor.

The motion planning algorithm of mobile robot based on Q-Learning generally uses lidar data or depth camera data as the state input, stores the discrete state-action value function Q (s, a) in the Q table, and updates the Q value in the Q table through iteration. In the complex dynamic environment, improving the learning efficiency of Q-learning algorithm is an important problem to be solved in robot motion planning. Jaradat M [66] established a new state space to limit the number of state inputs, so as to greatly reduce the size of the Q table, thus improving the speed of the planning algorithm; Song [67] based on the known environmental information and Q table, a mapping is created between the initial values of the value table. The dynamic wave expansion neural network is used to specify the initial Q value properly. The prior knowledge in the environment is integrated into the learning system to speed up the learning efficiency and obtain better strategies

In recent years, scholars have been trying to improve Q-learning algorithm for robot motion planning. Chakraborty et al. [68] used RL method to drive the robot to navigation in an unknown map and perform the forecasting task. At the same time, the fuzzy logic control method and Q-learning are combined to carry out self reinforcement learning of robot motion strategy. Based on the known topological map, Romero [69] and others use Q-learning method to define state, action and reward, and provide navigation map related to environment map to realize online navigation algorithm. However, this method only uses odometer measurement to navigate, and can only obtain less and inaccurate sensor information.

As shown in FIGURE 6(a) , in order to enhance the fitting ability of Q-learning, Yi et al. [70] proposed a type-2 fuzzy neural network to realize the robot path planning in a complex environment. Reinforcement learning based on value function is also applied to the research of UAV flight control system. Behzad et al. . [71] studied the problem of aircraft path planning in a single environment by using a double-Q learning reinforcement learning method. As shown in FIGURE 6(b) , the problem of overestimation of the Q value of Q-learning has been successfully solved.
FIGURE 6.

Robot motion planning based on Q-learning.

Show All

As shown in FIGURE 6(c) , Song [72] combined Q-learning with Boltzmann strategy to improve the efficiency of multi robot system and reduce the number of explorations. Yuan et al. [73] used a Q-learning approach for obstacle avoidance by imitating human behavior, which is shown in FIGURE 6(d) .

DQN: When dealing with the problems of motion control, navigation and obstacle avoidance, the robot needs to make correct and efficient decisions for the next action according to the information it feels, such as visual information, LIDAR point cloud and so on. Although the basic reinforcement learning method has strong decision-making ability, its perception of the environment is weak. Especially facing high-dimensional problems with large state space, there is still the danger of dimensional disaster. In recent years, deep Q network (DQN) algorithm and its related improved algorithms are widely used in robot motion planning tasks. In order to deal with the task of robot motion planning based on visual perception, Mnih [74] first combined CNN with traditional reinforcement learning Q-learning algorithm and proposed a DQN model. FIGURE 7 has shown the network diagram of mobile robot based on vision navigation.
FIGURE 7.

Architecture diagram of mobile robot motion planning.

Show All

DQN uses image or video as input information, constructs value network with memory ability through convolution neural network and long-term and short-term memory model (LSTM), and estimates value function. The number of neurons in the output layer of neural network is the number of actions that the agent can perform, such as forward, backward, left turn, right turn and so on.

There are still two problems in the process of robot motion planning based on DQN reinforcement learning algorithm, one is that the Q value is easy to be highly estimated, the other is that the sample correlation is too high. To solve these two problems, Van et al. [75] proposed a double DQN algorithm based on the double neural network. One network selects the optimal action, and the other network estimates the value function, which can better solve the problem. In order to break the correlation between the collected samples.

FIGURE 8 shows the network structure of the most widely used dqn algorithm, including two neural networks and a memory playback unit.
FIGURE 8.

DQN algorithm network architecture.

Show All

Schaul et al. [76] proposed prioritized replay DQN algorithm by maximizing the absolute value of TD error on the Q -network, which makes the neural network update more efficient. With the increase of the training data scale, the neural network structure needs to be further optimized to obtain more effective information. Dueling DQN [77] algorithm decomposes the Q value into State-value and Advantage function, which improves the learning efficiency of the network.

Furthermore, Noisy DQN [78] improves the exploration ability of the algorithm, Distributed DQN enhances the stability of the algorithm [79] , and Asynchronous DQN makes the estimation of target value more accurate [80] . “Deepmind” finally integrated the advantages of the improved DQN algorithm, and proposed a comprehensive algorithm, named “Rainbow” [81] . Tai et al. [82] use the depth image as the input of the Q-network, and the action and speed of the mobile robot are used as the output to train the robot turnlebot to move (forward, right and left) in the indoor corridor while avoiding the wall. The navigation effect is verified by using a variety of different 3D simulation environments. The motion planning of the mobile robot in the case of a variety of static obstacles is realized,which has been shown in FIGURE 9 (a) .
FIGURE 9.

Motion planning using DQN algorithm.

Show All

DQN algorithm also uses other types of images as input. Based on the depth image information, Zhang et al. [83] migrated the learned navigation strategy to the unknown environment by means of inheriting features, and realized the migration of robot navigation strategy from simulation to reality by DQN, as shown in FIGURE 9(b) . In the 3D simulation environment, Barron T et al. [84] used RGB image as the input of DQN and used deeper neural network to train the robot in complex tasks, which achieved better performance than the depth image and video. As shown in FIGURE 9(c) , their theory was verified in the virtual natural environment.

As shown in FIGURE 9(d) , Haora et al. [85] trained special robots to make automatic navigation exploration in unknown environments, and proposed a decision algorithm named AFCQN. The algorithm used deep neural network to learn exploration strategies from local maps and constructed a full convolutional Q network (FCQN). Compared with the basic DQN method, AFCQN can effectively reduce the probability of navigation failure. Similarly, Hussein et al. [86] proposed an improved DQN method combining demonstrations learning and experiential learning, which can shape a potential reward function by training a supervised convolutional neural network, and the algorithm is proved to be efficient in the navigation task of learning from raw pixels.

Kang et al. [87] applied DQN method to control four-rotor UAV. This method uses real-world data to learn system dynamics, and uses simulated data to learn a scalable sensing system, so that the robot can avoid collision with only one monocular camera, which is shown in FIGURE 10 .
FIGURE 10.

Motion planning for four rotor UAV.

Show All

The Value-based reinforcement learning motion planning method has the advantages of simplicity and efficiency, but it also has some disadvantages:

    Deterministic Strategy: Value-based RL is an indirect method to get the optimal strategy. According to value function, the action with the best value is selected by greedy. Facing the same state every time, the selected action is the same.

    Policy Degradation: The method based on value function is to use approximators to fit the real value function. There must be some errors, which may lead to poor strategy.

    Discrete Control: DQN is not suitable for scenes with continuous control or large action space, because the output action value of DQN is discrete and finite

    Slow Convergence: Value-based RL optimizes iteratively through value function and strategy, which has the disadvantage of low efficiency

2) Policy Gradient Algorithm

The output motion parameters of value-based RL is not suitable for the continuous action space system of mobile robot. Policy-based reinforcement learning makes it possible to solve the above problems based on policy gradient [88] , [89] . Rather than learning a value function, the policy gradient directly attempts to optimize the policy function \pi . The strategy is expressed as a function \pi _{\theta } with parameter \theta , and the optimization of the strategy is indirectly transformed into the optimization of parameter \theta . The given policy evaluation function is: \begin{equation*} \eta (\theta)=\textrm {E}\left [{ {\sum \limits _{t=0}^{H} r \left ({{s_{t},a_{t} } }\right)\vert \pi _{\theta }} }\right]\tag{6}\end{equation*}
View Source \begin{equation*} \eta (\theta)=\textrm {E}\left [{ {\sum \limits _{t=0}^{H} r \left ({{s_{t},a_{t} } }\right)\vert \pi _{\theta }} }\right]\tag{6}\end{equation*}

According to the policy gradient method, by solving the derivative of the policy evaluation function with respect to parameter \theta , the search direction of the strategy parameter \theta is \nabla _{\theta }\eta (\theta) : \begin{equation*} \nabla _{\theta } \eta (\theta)=\sum \limits _\tau p (\tau;\theta)\nabla _{\theta } \log p(\tau;\theta)R(\tau)\tag{7}\end{equation*}
View Source \begin{equation*} \nabla _{\theta } \eta (\theta)=\sum \limits _\tau p (\tau;\theta)\nabla _{\theta } \log p(\tau;\theta)R(\tau)\tag{7}\end{equation*} where, p ( \tau;\theta ) represents the probability distribution of trajectory \tau obtained by executing strategy \pi _{\theta } .

Then, the updated strategy parameter \theta _{\mathrm {i+1}} is: \begin{equation*} \theta _{i+1} =\theta _{i} +\alpha \nabla _{\theta } \eta (\theta)\tag{8}\end{equation*}
View Source \begin{equation*} \theta _{i+1} =\theta _{i} +\alpha \nabla _{\theta } \eta (\theta)\tag{8}\end{equation*} where, \alpha is the updating step size.

In the aspect of robot motion planning [90] , as shown in FIGURE 11 (a) , Andrew [91] used the appropriate “prompt” to shape the optimal return function, and proposed a PEGASUS search strategy to automatically design a stable UAV controller, which got the flight test though the remote control helicopter.
FIGURE 11.

Policy-based RL Motion Planning.

Show All

In order to reduce the training time of mobile robot motion planning based on visual navigation, as shown in FIGURE 11 (b) , Kulhanek et al. [92] added auxiliary tasks of visual data processing to pre train part of the network based on the extended strategy search algorithm, thus significantly reducing the training time and increasing the learning efficiency. Li et al. [93] based on the learning termination function interruption mechanism, added human experience into the algorithm framework, derived a hybrid hierarchical reinforcement learning structure, and realized the motion planning and target exploration tasks of mobile robot in indoor environment in simulation environment.

The policy-based RL uses the rewards as the estimation of state value. Although it is unbiased, the noise is relatively large. The policy gradient needs complete sequence samples to iterate, and the variability is always too high. In addition, the direction of updating the policy parameters is probably not the optimal direction of the policy gradient. Therefore, the policy-based RL needs to be further improved. So the most widely used improvement method is actor-critic RL algorithm.
3) Actor-Critic

The motion planning method based on Actor-Critic (AC) combines policy gradient and value function, which includes two parts: Actor and Critic [93] . Among them, Actor is the policy function equivalent to the policy gradient, which is responsible for generating actions and interacting with the environment. Critic uses the value function similar to DQN instead of Monte Carlo method to calculate the value of each step and get the value function. Then, the value function is used to evaluate the performance of the Actor and guide the action of the Actor in the next stage. In the Actor-Critic algorithm, two groups of approximations are needed \begin{equation*} \pi _{\theta } (s,a)=P(a\vert s,\theta)\approx \pi (a\vert s)\tag{9}\end{equation*}
View Source \begin{equation*} \pi _{\theta } (s,a)=P(a\vert s,\theta)\approx \pi (a\vert s)\tag{9}\end{equation*}

The second group is the approximation of value function \begin{equation*} {\hat {v}}(s,w)\approx v_{\pi }(s);\quad \hat {q}(s,a,w)\approx q_{\pi }(s,a)\tag{10}\end{equation*}
View Source \begin{equation*} {\hat {v}}(s,w)\approx v_{\pi }(s);\quad \hat {q}(s,a,w)\approx q_{\pi }(s,a)\tag{10}\end{equation*}

The parameter update formula of the strategy is as follows: \begin{equation*} \theta =\theta +\alpha \nabla _{\theta } log\pi _{\theta } (s_{t},a_{t})v_{t}\tag{11}\end{equation*}
View Source \begin{equation*} \theta =\theta +\alpha \nabla _{\theta } log\pi _{\theta } (s_{t},a_{t})v_{t}\tag{11}\end{equation*} where, v_{t} is the optimal state value calculated by Critic through Q-network.

Actor uses v_{t} to update the parameter \theta of policy function, and then select actions to get feedback and new state. Critical uses feedback and new state to update Q-network parameter w . Finally, Critic uses new network parameter w to help Actor calculate the optimal value of state v_{t} . The mobile robot motion planning with continuous action space can obtain better performance based on Actor-Critic.

Jaderberg M et al. [95] added auxiliary control and reward prediction into Actor-Critic algorithm, and Let the robot learn to navigate in the 3D Labyrinth environment. The method improved the data efficiency and parameter robustness successfully; Brunner G et al. [96] based on Actor-Critic and relocation unit, used 2DMaze global map and first-person perspective image as input to train the robot. In the DeepMind Lab virtual environment, the robot can quickly locate itself and estimate its orientation angle.

Compared with the traditional strategy iteration method, the Actor-Critic method can be updated in every step. But its disadvantage is that the network convergence depends on the evaluation of the Critic. However, the Critic itself is difficult to converge.
a: DDPG

One of the main reasons for Actor-Critic motion planning algorithm difficult to converge is that the data correlation is too strong. Based on the experience replay technology of DQN, Lillicrap et al. [97] from the DeepMind team fused deep neural networks with Actor-Critic, and proposed a model-free off-line Actor-Critic algorithm: Depth Deterministic Policy Gradient algorithm (DDPG). DDPG is also divided into Actor part and Critical part. Actor is responsible for updating the network, and Critic part is responsible for updating the action-value function.

DDPG uses deterministic strategy \mu to select actions a_{t} =\mu (s_{t} \vert \theta ^{\mu }) , \theta ^{\mu } are the parameters of the policy network that generates deterministic actions. The strategy network \mu is the actor, and the value network is the critic part for Q ( s , a ) function. In the process of motion planning, the objective function of DDPG is as follows: \begin{equation*} J(\theta ^{\mu })=E_{\theta ^{\mu }} [r_{1} +\gamma r_{2} +\gamma ^{2}r_{3} +\cdots]\tag{12}\end{equation*}
View Source \begin{equation*} J(\theta ^{\mu })=E_{\theta ^{\mu }} [r_{1} +\gamma r_{2} +\gamma ^{2}r_{3} +\cdots]\tag{12}\end{equation*}

The goal of DDPG is to maximize the objective function and minimize the loss function of value-network Q . The optimal deterministic behavior policy \mu ^{\ast } could be found through the objective function, which is equivalent to maximizing the gradient of the objective function on the parameters \theta ^{\mu } of the policy network,

Similar to the structure of using DQN, DDPG uses Q networks to fit Q functions. In other words, policy \mu is used to select robot actions in state s, and the expected return could be obtained: \begin{equation*} Q^{\mu }(s_{t},a_{t})=E[r(s_{t},a_{t})+\gamma Q^{\mu }(s_{t+1},\mu (s_{t+1}))]\tag{13}\end{equation*}
View Source \begin{equation*} Q^{\mu }(s_{t},a_{t})=E[r(s_{t},a_{t})+\gamma Q^{\mu }(s_{t+1},\mu (s_{t+1}))]\tag{13}\end{equation*} The gradient of Actor network is: \begin{equation*} \nabla _{\theta } J_{\beta } (\mu _{\theta })=E_{s\sim \rho ^{\beta }} [\nabla _{\theta } \mu _{\theta } (s)Q^{\mu }(s,a)\vert _{a=\mu _{\theta }}]\tag{14}\end{equation*}
View Source \begin{equation*} \nabla _{\theta } J_{\beta } (\mu _{\theta })=E_{s\sim \rho ^{\beta }} [\nabla _{\theta } \mu _{\theta } (s)Q^{\mu }(s,a)\vert _{a=\mu _{\theta }}]\tag{14}\end{equation*}

On the other hand, the value gradient of Critic network is \begin{align*} \frac {\partial L(\theta ^{Q})}{\partial \theta ^{Q}}=E_{s,a,r,s\prime \sim D} \left[{(TargetQ-Q(s,a\vert \theta ^{Q})\frac {\partial Q(s,a\vert \theta ^{Q})}{\partial \theta ^{Q}}}\right] \\\tag{15}\end{align*}
View Source \begin{align*} \frac {\partial L(\theta ^{Q})}{\partial \theta ^{Q}}=E_{s,a,r,s\prime \sim D} \left[{(TargetQ-Q(s,a\vert \theta ^{Q})\frac {\partial Q(s,a\vert \theta ^{Q})}{\partial \theta ^{Q}}}\right] \\\tag{15}\end{align*} where, TargetQ=r+\gamma Q\prime (s\prime,\pi (s\prime \vert \theta ^{\mu \prime })\vert \theta ^{Q\prime })

Using the gradient formulas of Eq .(14,15), the network can be updated with gradient descent algorithm. The motion planning method of mobile robot based on DDPG is shown in FIGURE 12 .
FIGURE 12.

Motion Planning principle of DDPG.

Show All

Heess et al. [98] further extended the DDPG method to POMDP by using two recursive neural networks to approximate Actor and Critic, and developed an algorithm called repeated deterministic policy gradient (RDPG). In terms of robot motion planning, Alejandro et al. [99] applied the new DDPG algorithm to design a general reinforcement learning framework, which solved the landing operation of UAV on a mobile platform. Better results were achieved in both simulation and actual flight experiments, which is shown in FIGURE 13 (a) . Tai L et al. [100] proposed a learning-based Mapless motion planner: asynchronous DDPG (ADDPG) using the improved DDPG.
FIGURE 13.

Motion Planning Method Based on DDPG.

Show All

As shown in FIGURE 13(b) , with sparse 10-dimensional Lidar information and target position coordinates as inputs and continuous steering instructions as outputs, the mapless motion planner can be trained end-to-end without any prior human intervention. The trained policy can be applied directly to both virtual and real environments. Wang et al. [101] used the deterministic policy gradient to control the flight of the quadrotor helicopter. DDPG directly maps the system state to the control command, and introduces the integral compensator into the criticism structure, so that the tracking accuracy and robustness of the quadrotor helicopter are greatly improved.

DDPG deals with the problem of continuous control of mobile robots well, and solves the problem of Actor-Critic convergence through dual network structure and priority experience replay mechanism, which is widely used in the field of robot motion planning. However, in the process of updating policy parameters, parameter replication needs to choose a appropriate step, which will directly affect the training effect of the algorithm, and even lead to the collapse of the motion planning system.
b: TRPO

In order to find an appropriate step in the policy gradient and guarantee the return function monotonically increased, Schulmam et al. [102] from Berkeley proposed a Trust region policy optimizatio(TRPO) method. TRPO decompose the return function of the new policy into the old policy and other items. If the other terms of the new strategy are greater than or equal to zero, the new policy could ensure that the return function remains monotonous. The objective function of TRPO method is: \begin{align*}&\mathop {\textrm {maximize}}\limits _{\theta } \hat {{{\rm \text {E}}}}_{t} \left [{ {\frac {\pi _{\theta } \left ({{a_{t} \left |{ {s_{t}} }\right.} }\right)}{\pi _{\theta _{old}} \left ({{a_{t} \left |{ {s_{t}} }\right.} }\right)}\mathord {\stackrel {{\lower 3pt\hbox {$$\scriptscriptstyle \frown $$}}} {A}}_{t}} }\right] \\&\textrm {subject to}~\hat {{{\rm \text {E}}}}_{t} \left [{ {KL\left [{ {\pi _{\theta _{old}} \left ({{\bullet \left |{ {s_{t}} }\right.} }\right),\pi _{\theta } \left ({{\bullet \left |{ {s_{t}} }\right.} }\right)} }\right]} }\right]\le \delta\tag{16}\end{align*}
View Source \begin{align*}&\mathop {\textrm {maximize}}\limits _{\theta } \hat {{{\rm \text {E}}}}_{t} \left [{ {\frac {\pi _{\theta } \left ({{a_{t} \left |{ {s_{t}} }\right.} }\right)}{\pi _{\theta _{old}} \left ({{a_{t} \left |{ {s_{t}} }\right.} }\right)}\mathord {\stackrel {{\lower 3pt\hbox {$\scriptscriptstyle \frown $}}} {A}}_{t}} }\right] \\&\textrm {subject to}~\hat {{{\rm \text {E}}}}_{t} \left [{ {KL\left [{ {\pi _{\theta _{old}} \left ({{\bullet \left |{ {s_{t}} }\right.} }\right),\pi _{\theta } \left ({{\bullet \left |{ {s_{t}} }\right.} }\right)} }\right]} }\right]\le \delta\tag{16}\end{align*} where, \hat {A}_{t} is the estimated value of the Advantage function; \pi _{\theta } and \pi _{\theta old} respectively represent the new and old policy on the same batch of training data. \delta is a small value, which is used to limit the differences in KL divergence between the old and new policy. The motion planning principle of mobile robot based on TRPO is shown in FIGURE 14 .

FIGURE 14.

Motion Planning principle of TRPO.

Show All

Finally, the data samples are obtained by sampling to solve the optimization problem. TRPO algorithm has been successfully applied to robot operation skill learning in virtual and real scene. Li et al. [103] adopted TRPO for end-to-end optimization to solve the problem of social navigation in living environment. William et al. [104] used the TRPO algorithm to realize the autonomous flight of the quadrotor UAV with a stable posture in the high-fidelity simulation environment.

TRPO motion planning algorithm has solved the problem of choosing appropriate update step. However, Thre too many approximations used in the solution process, which not only complicate the process, but also bring large errors. At the same time, it is a difficult process to solve a constrained optimization problem. Because of these problems, TRPO has not been widely used in the field of robot motion planning.
c: PPO

In order to simplify the solution process of TRPO, OpenAI [105] proposed an algorithm that is simpler than TRPO theory and simpler in specific operation: Proximal Policy Optimization (PPO). PPO not only has a good performance in continuous action space of mobile robot, but also is easier to implement compared to TRPO. The objective function of PPO method is: \begin{align*}&L_{t}^{CLIP+VF+S} \left ({\theta }\right)=\hat {{{\rm \text {E}}}}_{t} \left [{ L_{t}^{CLIP} \left ({\theta }\right)-c_{1} L_{t}^{VF} \left ({\theta }\right)}\right. \\&\qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \left.{+\,c_{2} S\left [{ {\pi {}_{\theta }} }\right]\left ({{s_{t}} }\right) }\right]\tag{17}\end{align*}
View Source \begin{align*}&L_{t}^{CLIP+VF+S} \left ({\theta }\right)=\hat {{{\rm \text {E}}}}_{t} \left [{ L_{t}^{CLIP} \left ({\theta }\right)-c_{1} L_{t}^{VF} \left ({\theta }\right)}\right. \\&\qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \left.{+\,c_{2} S\left [{ {\pi {}_{\theta }} }\right]\left ({{s_{t}} }\right) }\right]\tag{17}\end{align*} where, L_{t}^{CLIP} \left ({\theta }\right) is the main objective function of strategy optimization;The loss between network output value and TD error target value is required to be minimum by L_{t}^{VF} \left ({\theta }\right) ; S\left [{ {\pi {}_{\theta }} }\right]\left ({{s_{t}} }\right) is the information entropy of measuring the latest strategy gradient function.

Under the condition of satisfying the sampling to be the maximum likelihood probability, the policy is random and has the least factors assumes. The motion planning principle of mobile robot based on PPO is shown in FIGURE 15 .
FIGURE 15.

Motion Planning principle of PPO.

Show All

PPO policy can realize some very complex motion planning task, and the policy can even help mobile robot to make rotation, roll and other random behaviors when arriving at its destination. Chen et al. [106] trained the motor skills of wheel-leg robots based on an improved PPO algorithm. This algorithm directly maps high-dimensional images to motor commands and decomposes complex navigation tasks into manageable navigation behaviors.

In order to solve the problem of indoor maze navigation, Marchesini et al. [107] proposed a PPO algorithm based on double deep Q-network, which greatly reduces the training time though multi-batch priority to experience replay, which is shown in FIGURE 16 . In addition, a domain randomization technique is proposed in PPO motion planning algorithm, which enables the robot to navigate to the target position while avoiding obstacles. It overcomes the problems of low data efficiency and sparse reward, and improves the efficiency of robot motion planning.
FIGURE 16.

Navigate through the PPO algorithm.

Show All

d: A3C

An important problem in robot motion planning is the fast convergence. To achieve this goal, the correlations between data must be broken. Both DQN and DDPG make use of the principle of empirical replay. However, experience replay is not the only method. Another method is the principle of asynchronous training. The asynchronous approach is that the data are not generated simultaneously. Based on this idea, DeepMind team Mnih et al. [108] developed Asynchronous Advantage Actor-critic(A3C) for reinforcement learning. The basic framework of A3C is also the Actor-Critic framework. Instead of using a single thread, it uses multiple threads to train simultaneously. Each thread trains a robot in a random exploration environment, multiple robots explore and compute policy gradients in parallel. These policy gradients are used together to update the weights of the shared model \theta .

For the loss function part of Actor and Critic network, the entropy term of policy \pi is added, and the coefficient is c. That is, the gradient of the strategy parameter is updated as: \begin{equation*} \theta =\theta +\alpha \nabla _{\theta } log\pi _{\theta } (s_{t},a_{t})A(S,t)+c\nabla _{\theta } H(\pi (S_{t},\theta))\tag{18}\end{equation*}
View Source \begin{equation*} \theta =\theta +\alpha \nabla _{\theta } log\pi _{\theta } (s_{t},a_{t})A(S,t)+c\nabla _{\theta } H(\pi (S_{t},\theta))\tag{18}\end{equation*}

Using asynchronous advantage reinforcement learning algorithm, Zhang et al. [109] trained four-wheel robot navigation in USAR environment on rugged terrain. As shown in FIGURE 17 (a) . The training process provides the target position for the task, and the robot autonomously learns how to move forward, backward and turn. The whole navigation task consists of the robot performing a series of these basic actions to reach a predefined target position. This method has been successfully validated in different 3D simulated USAR environments.
FIGURE 17.

Navigating in complex environments based on A3C.

Show All

Zhu et al. [110] input both the first-angle image and the target object image into the A3C model and approximate the target function based on the general value function. After 100 million frames of training, the strategy obtained by A3C algorithm can not only be applied to the simulation environment, but also be able to navigate the robot in the real environment, which is shown in FIGURE 17(b) .

Niroui et al. [111] applied the deep reinforcement learning algorithm to the urban search and rescue robot. The method combines A3C network with frontier exploration to enable the robot to explore the unknown messy environment autonomously. This exploration method can effectively navigate to the appropriate boundary position in the unknown clutter environment of different sizes and layouts. It is also robust to different environment layouts and layouts.

Zeng et al. [112] proposed an Actor-Critic method (MK-A3C) based on asynchronous advantages of memory and knowledge for the navigation problem of nonholonomic robots with continuous control. MK-A3C constructs a memory neural network based on GRU to enhance the temporal reasoning ability of the robot. At the same time, the robot is given a certain memory ability. Through the estimation of the environment model, the local minimum trap is avoided and the non-convergence strategy problem caused by reward sparse is solved. Mk-A3C can effectively navigate in unknown dynamic environments, and the robot can successfully navigate in unknown and challenging environments through the output of continuous acceleration instructions.
e: SAC

In order to improve the exploratory and robust of mobile robots motion planning, Tuomas et al. [113] from the University of California, Berkeley proposed an off-policy algorithm Soft Actor-Critic (SAC) in 2018. SAC has achieved good performance in robot motion control and can be directly applied to robots in real environment. In contrast to DDPG, SAC uses a random strategy. At the same time, the problems of high sensitivity to over parameter and weak convergence are improved.

Compared with TRPO, A3C and PPO, SAC can reuse the past experience and improve the utilization efficiency of training samples. It is based on maximum entropy reinforcement learning framework, in which the entropy increase objective function is: \begin{equation*} J(\pi)={\mathbb {E}}_{\pi } \left [{ {\sum \limits _{t} r \left ({{{\boldsymbol{{s}}}_{t},{\boldsymbol{ {a}}}_{t}} }\right)-\alpha \log \left ({{\pi \left ({{{\boldsymbol{ {a}}}_{t} \vert {\boldsymbol{ {s}}}_{t}} }\right)} }\right)} }\right]\tag{19}\end{equation*}
View Source \begin{equation*} J(\pi)={\mathbb {E}}_{\pi } \left [{ {\sum \limits _{t} r \left ({{{\boldsymbol{{s}}}_{t},{\boldsymbol{ {a}}}_{t}} }\right)-\alpha \log \left ({{\pi \left ({{{\boldsymbol{ {a}}}_{t} \vert {\boldsymbol{ {s}}}_{t}} }\right)} }\right)} }\right]\tag{19}\end{equation*} where, s and a represent states and actions respectively, and the E_{\pi } expectation contains dynamic parameters from the real system.

The goal of strategy optimization not only needs to maximize the expectation, but also needs to maximize the entropy of expectation, and the parameter \alpha balances the influence of these two parts on the result.

When \alpha is 0, the Eq (19) degenerates to the traditional objective function. The objective function can be regarded as the maximum expected return of entropy constraint, and the \alpha parameter is automatically learned instead of the hyperparameter. The robot motion planning principle based on SAC algorithm is shown in FIGURE 18 .
FIGURE 18.

Robot motion planning principle based on SAC.

Show All

In order to verify the ability of SAC in the real world, Haarnoja et al. [114] made the robot learn from zero without relying on simulation or manual teaching, and realized the autonomous planning in the complex walking system of the foot robot. The algorithm directly mapped the sensor input to low-level actions. The learning process requires only a small task adjustment and a moderate number of experiments to learn better neural network strategies. As shown in FIGURE 19 , the robot learns a stable gait in the real world within two hours without relying on any model or simulation. The strategy obtained is robust enough to moderate changes in the environment.
FIGURE 19.

Quadruped robot in real and simulation environment.

Show All

The gait control training data of robot in simulated environment were transferred to reality, and the stable walking of a medium-sized dog-sized quadruped robot ANYmal was realized.

AnyMal can run faster than ever before, and can even self-recover from falls in complex environments with a high level speed commands, which is shown in FIGURE 20 .
FIGURE 20.

Train the quadruped robot ANYmal to walk.

Show All

DRL algorithms have been widely used in motion planning of mobile robots. Value-based DQN algorithm is the most widely used and simplest method in reinforcement learning field, and has a good performance in most discrete action Spaces. The algorithms based on Actor-Critic framework, such as DDPG, TRPO, PPO, A3C and SAC, have great advantages compared with DQN in continuous action space. The comparison of these deep reinforcement learning algorithms is shown in Table 3 .
TABLE 3 Comparison of DRL Algorithms

C. Model Based RL

Motion-planning method based on Model-Free Reinforcement Learning requires a lot of sample training, which limits its wide application in real environment. For example, we can train a control algorithm for a UAV in a simulated environment by constantly letting the UAV explore and learn. But in the real world, it is very difficult. We have to consider the risk of collision, the cost of repairs, the cost of exporting data to the GPU for training. All these problems indicate that the cost of data acquisition in the actual physical scene is far greater than that in the simulation environment.

In real physics scenarios, especially robotics projects, the more data-efficient approach is quite popular. Model-based reinforcement learning is one of them. It obtains an environment model through supervised training, and then expands to learn this value function implicitly in order to expect maximum return. Compared with model-free RL (MFRL), model-based RL (MBRL) has higher sample efficiency, faster convergence speed and lower data requirement.

Xi et al. [116] combined MBRL with MFRL and proposed a Gaussian process (GP) model with two independent levels. The algorithm overcomes the problem of over-fitting in the complexity model, and reduceds the amount of training data. The biped robot can stabilize itself on the rotating platform, and can adapt to the platform with different angular velocity ( FIGURE 21 ).
FIGURE 21.

Biped Robot on a Rotating Platform.

Show All

Munk et al. [117] proposed an model learning Deep Deterministic Policy Gradient(ML-DDPG) for continuous control policies of robot based on model-state representation learning. ML-DDPG includes three deep neural networks: Model network, Critic network and Actor network. As shown in FIGURE 22 , this method can learn a series of different challenging continuous control strategies, which improves the robustness and efficiency of the algorithm. Nursultan et al. [118] applied model-based algorithm to solve the flight control problem of UAV in the execution of practical tasks.
FIGURE 22.

ML-DDPG Algorithm architecture.

Show All

Model-based RL algorithm can realize rapid and targeted exploration of uncertain effective strategies and fast learning with few samples.
D. Imitation Learning

Imitation learning refers to an mobile robot’s ability to mimic a desired behavior by learning from observations, which aims to learn reward functions from a expert-controlled agent. The mobile robot will be trained to perform a task from demonstrations by learning a mapping between observations and actions [119] . The mothin planning methods based on Imitation learning will use prior knowledge and recordedvexpert experiences to generate reward functions and movement trajectories, which can rapidly improve the performance of the initially learned policy and significantly reduce the quantity demand of samples.

Many researchers apply imitation learning to 3D motion planning for mobile robots. Based on DL and Imitation learning, as shown in FIGURE 23 , Hussein et al. [120] proposed a deep imitative learning method to perform navigation tasks in 3D simulation environment. Combining demonstrations and experience, the algorithm employs deep CNN and raw visual input, which significantly improves the training efficiency and generalization ability.
FIGURE 23.

Imitation learning in a 3D simulation environment.

Show All

On the other hand, Wu et al. [121] present a motion planning method for UAV, which is based on generative imitation learning, and realizes mapless navigation in indoor scene. As shown in FIGURE 24(a) , this method can automatically learn a navigation strategy, imitate data by experts, and apply to multi-robot system. As shown in FIGURE 24(b) , Zhang et al. [122] proposed an imitation learning model (IADRL) to drive UAV to cooperate with each other to complete more complex tasks. The model provides a strategy for multi-UAV system, which makes multi-UAV complete the task with the minimum system cost.
FIGURE 24.

Navigation in a complex environment.

Show All

In order to solve the obstacle avoidance problem of UAV visual navigation, Park et al. [123] proposed an imitation learning strategy based on human experts sharing flight data. As shown in FIGURE 25 , the strategy takes UAV visual image as input, uses neural network to extract features, and trains UAV obstacle avoidance strategy.
FIGURE 25.

Human data-based imitation learning framework using sequential neural networks.

Show All

E. Meta-Learning

A certain amount of training data is required whether for reinforcement learning or imitation learning. Using a small amount of training data to learn new operational skills has become more important for the application of robots.

Meta learning is an improved reinforcement learning method based on a small amount of training data, which trains policy through a large number of related tasks. Each task contains a small amount of tagged data in the task set, which can automatically learn the common knowledge of the training task set. Many scholars have applied this method to the fast motion planning learning of mobile robots. Gaudet et al. [124] proposed an adaptive guidance model based on meta learning, which combines loop strategy and value function approximator. As shown in FIGURE 26 , the model solves the problem of landing on Mars and asteroids, and realizes the combination of guidance and motion planning.
FIGURE 26.

Adaptive guidance system.

Show All

In order to improve the adaptability and robustness of robot motion planning, Wortsman et al. [125] proposed an adaptive visual motion planning method (SAVN) based on meta reinforcement learning. As shown in FIGURE 27 (a) , this method does not need any explicit supervision, but optimizes its own navigation strategy by learning a self supervised interaction loss. In the low resource training environment with only a few labeled objects, Liu et al. [126] proposed a new unsupervised transferable meta skill based on visual navigation meta reinforcement learning. As shown in FIGURE 27 (b) , as long as the reward information is provided to the robot, the robot can use these meta skills to quickly adapt to the environment with few resources.
FIGURE 27.

Indoor navigation based on meta-learning.

Show All

SECTION IV.
Multi-Robot Cooperative Planning

As the working environment becomes more and more complex, the working mode of a single robot shows disadvantages, such as limited perception ability, low task reliability and low execution efficiency. In the past 10 years, it has become more advantageous to carry out multi-robot cooperation to perform tasks together [127] , and it has gradually become a research consensus and hotspot in the field of robotics. The working mode of multi-robot cooperation brings more challenges to the inter-individual motion planning in the group. How to carry out the cooperative motion planning effectively becomes the unique feature of this field, which is different from the single robot motion planning. The architecture of reinforcement learning motion planning system for multi-mobile robots can be mainly divided into two categories: centralized [128] and distributed [129] .

Centralized reinforcement learning takes the common task of multiple robots as the training goal, and there is a centralized computing unit that can obtain the state and sensor information of all robots, and the centralized computing unit is responsible for the centralized strategy training and distribution. The advantage of centralized reinforcement learning is that real-time position, speed and target information of any robot can be obtained directly. Moreover, it can direct all individual robots to complete tasks such as task objective assignment, cooperation mechanism and path collaboration under optimal conditions. While ensuring the completion of the task, energy, time and other personal losses can be reduced as much as possible [130] .

Distributed reinforcement learning is different from centralized reinforcement learning. Firstly, the whole task needs to be segmented in real time, and then the segmented sub task is sent to a single robot. After receiving the task, the robot, as a separate individual, participates in the training of the sub task independently, just like the movement plan of single robot reinforcement learning. Taking the maximum reward as the goal and considering the overall total return, the overall task and planning can be realized through cooperation. The advantage of distributed reinforcement learning is that it can fully mobilize the resources of each robot and improve the utilization of hardware. Each robot is an independent individual with strong fault tolerance and redundancy.

Although the centralized reinforcement learning motion planning algorithm can get the global information, but the computational consumption will become huge when facing the large-scale mobile robot system, and the accuracy and real-time performance cannot be guaranteed. In order to solve this problem, Chen et al. [131] estimated the reach-time to the target by predicting value function, and degraded the centralized online calculation into a distributed offline calculation process. By learning a value function of “implicitly encoding cooperative behavior”, a more real-time and efficient multi-robot motion planning system is obtained. As shown in FIGURE 28 (a) , the cooperative obstacle avoidance in a crowded environment for multi-robot system is realized. To solve the efficient logistics transportation problem in dynamic production environment, Malus et al. [132] used reinforcement learning method to train multiple mobile robots for logistics scheduling. The modified algorithm learns to bid on orders based on their observations and their own positions and current plans to maximize the benefits of the system. Compared with the common transportation scheduling rules, the cooperative planning based on reinforcement learning is more efficient.
FIGURE 28.

Motion planning for RL of multi-robots.

Show All

On the other hand, Long et al. [133] proposed a distributed RL model based on multi-sensor in order to solve the motion planning problem of multiple robots. As shown in FIGURE 28 (b) , the algorithm maps the original sensor data containing speed information to the steering command, and uses the multi-scene and multi-stage framework for training to obtain the optimal strategy.

In order to ensure that each driverless vehicle learns to cooperate with each other, Shi et al. [134] uses interactive feature attention mechanism to capture interactive information between autopilot cars, and promotes information sharing strategy of multiple autonomous driving vehicles. As shown in FIGURE 29 , the cooperation performance of the whole transportation system is improved by adjusting the human driven traffic flow.
FIGURE 29.

A highly collaborative autonomous driving.

Show All

The number of mobile robots directly affects the motion planning effect of multi robot system. In order to solve the problem that the DRL multi robot motion planning system deviates from the real goal with the increase of the number of robots, Michael [55] used an A3C framework to simulate the complex interaction and cooperation between robots. As shown in FIGURE 30 , with LSTM recurrent neural network, it breaks the limitation that the conventional collision avoidance network needs to input fixed length in the convolution layer and pooling layer feed-forward network, and realizes the collision avoidance algorithm only through learning without preset behavior norms of other dynamic robots.
FIGURE 30.

[122] RL network framework for multiple mobile robots.

Show All

In addition, Samaneh et al. [135] designed a hybrid reinforcement learning motion planning method based on the mechanical motion planning method. A new mixed reward function reduces the chance of collisions in crowded environments. As shown in FIGURE 31 , the algorithm can switch automatically according to the complexity of different environments, and achieves good results in both 3D virtual environment and real environment.
FIGURE 31.

Hybrid control framework for DRL.

Show All

In conclusion, the centralized reinforcement learning motion planning system has strong coordination ability, and does not need to consider the cooperation between robots. However, the system needs huge amount of computation and has poor scalability. The distributed reinforcement learning motion planning system is more flexible and convenient, and the processing ability is stronger. However, the cooperation between individual robots and collision avoidance are important problems to be studied in motion planning of multi-robot in complex dynamic environment.
SECTION V.
Difficulty for DRL Motion Planning
A. Difficulty for Designing Reward Function

In the motion planning task of DRL, mobile robots learn optimal policies by estimating cumulative rewards, and it can be performed well in a conventional task environment. However, this training type based on cumulative rewards has a huge search space and needs to make multi-step decisions in the whloe process. when facing a complex working environment, the robots will not get rewards unless they reach the predetermined goals, and less effective information will be obtained from the intermediate proces. Such “sparse rewards” will make it difficult for robots to learn effective strategies and fail in motion planning. How to design an efficient reward function to improve search efficiency is one of the problems in DRL movement planning field.
B. Insufficient Data Sample

DRL motion planning requires a large number of data samples to train the strategy, and the data samples are mainly acquired in the process of “trial and error”. However, the collection process in the real environment is difficult due to mechanical reliability and time cost,, which will lead to the problem of insufficient training data sample. So the advantages of data-driven DRL method in feature extraction and parameter optimization will not be fully exerted. How to overcome the problem of insufficient data sample in the real environment is one of the problems to be solved urgently at present.
C. Poor Understanding Ability

DRL can solve problems of modeling with conventional algorithms, but it still relies on a large amount of data training to fit the motion model. The intelligence level of DRL is low and still in the stage of computational intelligence for lacking of knowledge and understanding of environmental models. The time consumption and sample data requirements are enormous even in the face of simple models. How to enhance the cognitive and understanding ability of the DRL and learn from integrate the experience of human experts, is one of the important problems
D. Insufficient Generalization Ability

At present, DRL motion planning is mainly used in static, closed and deterministic environments, such as mazes, factories and indoor scenarios. However, many task environments of mobile robots are dynamic, opening and uncertain. The performance of the training model has certain deviation when the simulation is transferred to the real environment, and it is difficult to apply the strategy of the same task to different physical robots. Therefore, how to improve the generalization ability of DRL motion planning algorithm in practical application scenarios, so that mobile robots can obtain better adaptability, will be a huge challenge.
SECTION VI.
Future Research
A. Representation of Complex Environment

Environment perception of mobile robots is one of the key abilities of motion planning in DRL. At present, the main way to obtain environmental state is to rely on optical sensors, such as ordinary cameras, depth cameras, three-dimensional Lidar, etc., but these sensors generally have common problems such as insufficient sampling efficiency, delay in real-time feedback, lack of environmental information, and insufficient accuracy [3] .

In addition, the real environment of the robot has the changes of light, season and weather, which further restricts the effect of mobile robot on environment observation and recognition, and then poses a huge challenge to the robot motion planning. How to ensure DRL accurately perceive the characteristics of time-varying real environment in complex environment, and obtain the ability for semantic description and abstract reasoning representation, will be an important research direction in the field of motion planning for deep reinforcement learning.
B. Collaborative Planning for Heterogeneous Systems

With the demand of three-dimensional task, the working environment becomes collaborative working mode. At present, the DRL motion planning is mostly used to solve the problem of navigation and collision avoidance in the low dimensional plane ground environment, while the research on the comprehensive planning in the three-dimensional space is less, such as the land marine amphibious robot and land air cooperative bionic robot system [136] . Compared with two-dimensional land space, high-dimensional motion planning will face more complex dynamic constraints and more uncertainties. This will put forward more stringent requirements for data collection and perception, deep network model construction, policy training and migration. In the future, large-scale deep neural networks can be combined to construct deeper Actor-Critic networks. At the same time, data preprocessing should be enhanced to further improve the perception and extraction ability for input features, and different data priorities should be used to cope with the surging data information in heterogeneous.
C. Multi-Source Domain Migration

The training process of motion planning is time-consuming. A model with good generality and mobility will greatly improve the efficiency of robot motion planning. However, in the practical application, especially in the face of multi-robot motion planning system, model migration in the actual environment is faced with huge problems due to the difference of original data and target domain and the gap between robot systems. At present, most of the multi-source domain transfer learning algorithms for reinforcement learning are still in their infancy, and the effect of task transfer depends on the correlation between the source domain and the target domain. The source and target domains need to be in the same data space. In the future, we will study how to measure the difference of characteristics of multiple source domains and target domains adaptively according to environmental samples of different source domain tasks of mobile robots. How to extract common features or parameters and automatically align them to a specific state space. How to express features in a unified standard form, obtain invariant feature parameters between multi-source domains, and realize task, parameter and feature migration of multi-source domains in DRL. All these are important guarantees for mobile robots to perform tasks in a more diversified environment in the future.
D. Data-Model Hybrid RL

The model-free DRL motion planning algorithm is based on data and does not depend on the system model, which requires less prior knowledge of the environment. However, it faces the problems of very low sample efficiency and difficult design of reward function, so that the algorithm is easy to fall into local optimality and can not find the optimal programming model. Model-based motion planning algorithm is based on robot ontology and environment model, and even shows more efficient performance than model-free DRL in static environment, especially when the map is known. However, in the face of complex tasks, the modeling is too complex, and the dynamic characteristics cannot be represented.

One of the solutions is to combine the model-based RL motion planning algorithm with the model-free RL motion planning algorithm. The hybrid algorithm can learn the environment model from the data, then optimize the policy based on the learned model, and reverse update and improve the model. It can make full use of environmental samples to approach the model, greatly improve the use efficiency of training sample data and shorten the learning process of the robot. In addition, when the mobile robot encounters a new environment, it can combine with multi-source domains, quickly adapt to the new model and greatly improve its own generalization ability based on the models. This Data-model hybrid reinforcement learning algorithm will be an important direction of motion planning in the future.
SECTION VII.
Conclusion

The robot-motion-planning method based on DRL promotes the policy improvement of the robot through its interactions with the environment. Robots based on the method may obtain the robust ability of automatic learning and decision-making. This ability is critical for the unstructured environment, e.g., partial mapped and dynamically-changing environments. The method based on DRL lowers the programming complexity, and removes the dependency of the prior knowledge about environments. The method not only enables the robot to analyze the environment, but also boosts its ability of planning motion, avoiding obstacles in real time, searching object, and making decisions. Therefore, the methods based on DRL promote the development of intelligent robots.

However, the robot-motion-planning method based on DRL is rarely found in practice mainly due to its limited theoretical development and the low interpretability. Accordingly, the positioning accuracy, motion dexterity and stability required by the real-time application cannot be guaranteed. Additionally, laboratory environment differs drastically from the real world, e.g., ground disorganization, data delay and the mechanical structure unreliability, etc. These facts challenge the trial-and-error way of training the robots based on DRL. Thus, the solution remains to be developed in the further.

Authors
Figures
References
Citations
Keywords
Metrics
  < Previous    |    Back to Results    |    Next >  
More Like This
An Improved Real-Time Path Planning Method Based on Dragonfly Algorithm for Heterogeneous Multi-Robot System

IEEE Access

Published: 2020
Path Planning Based on Ant Colony Algorithm and Distributed Local Navigation for Multi-Robot Systems

2006 International Conference on Mechatronics and Automation

Published: 2006
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
