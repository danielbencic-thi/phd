IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading web-font TeX/Caligraphic/Regular

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

        Cart 
        Create Account
        Personal Sign In 

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Journals & Magazines > IEEE Transactions on Circuits... > Volume: 31 Issue: 4
DesnowGAN: An Efficient Single Image Snow Removal Framework Using Cross-Resolution Lateral Connection and GANs
Publisher: IEEE
Cite This
PDF
Da-Wei Jaw ; Shih-Chia Huang ; Sy-Yen Kuo
All Authors
View Document
11
Paper
Citations
806
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Related Works
    III.
    Proposed Method
    IV.
    Experiments
    V.
    Conclusion

Authors
Figures
References
Citations
Keywords
Metrics
More Like This

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: In this paper, we present a simple, efficient, and highly modularized network architecture for single-image snow-removal. To address the challenging snow-removal problem ... View more
Metadata
Abstract:
In this paper, we present a simple, efficient, and highly modularized network architecture for single-image snow-removal. To address the challenging snow-removal problem in terms of network interpretability and computational complexity, we employ a pyramidal hierarchical design with lateral connections across different resolutions. This design enables us to incorporate high-level semantic features with other feature maps at different scales to enrich location information and reduce computational time. In addition, a refinement stage based on recently introduced generative adversarial networks (GANs) is proposed to further improve the visual quality of the resulting snow-removed images and make a refined image and a clean image indistinguishable by a computer vision algorithm to avoid the potential perturbations of machine interpretation. Finally, atrous spatial pyramid pooling (ASPP) is adopted to probe features at multiple scales and further boost the performance. The proposed DesnowGAN (DS-GAN) performs significantly better than state-of-the-art methods quantitatively and qualitatively on the Snow100K dataset.
Published in: IEEE Transactions on Circuits and Systems for Video Technology ( Volume: 31 , Issue: 4 , April 2021 )
Page(s): 1342 - 1350
Date of Publication: 17 June 2020
ISSN Information:
INSPEC Accession Number: 20850719
DOI: 10.1109/TCSVT.2020.3003025
Publisher: IEEE
Funding Agency:
Contents
SECTION I.
Introduction

Atmospheric phenomena removal has attracted much attention nowadays, but the interpretation of computer vision systems can be adversely affected by the unpredictable impairment of images under bad weather conditions. As shown in Figure 1 , the falling snow considerably affects state-of-the-art object detection techniques [1] .
Fig. 1. - A real-world winter photograph (top) and the corresponding snow-removed result (bottom) obtained by the proposed method. Labels and confidences are supported by YOLO v3 [1].
Fig. 1.

A real-world winter photograph (top) and the corresponding snow-removed result (bottom) obtained by the proposed method. Labels and confidences are supported by YOLO v3 [1] .

Show All

Numerous studies have been performed to address the challenging snow-removal problem. Specifically, learning-based methods have utilized synthetic data to simulate images under different weather conditions and train neural networks to eliminate atmospheric particles that appear in images. In particular, convolutional neural networks (CNNs) that are used to remove haze [2] and rain [3] , [4] have considerably improved performance in terms of the resultant image quality and generalization ability as compared with handcrafted methods.

In [5] , a fundamental snow-removal framework consisting of a synthesized snowy image dataset ( S n o w 100 K ) and a multistage network architecture is proposed. The framework outperforms state-of-the-art haze-removal and rain-removal CNNs quantitatively and qualitatively on the Snow100K dataset. In [6] , Liu e t   a l . proposed a dual residual connection mechanism and various network architectures to correspond with different image regression tasks, obtaining fine results in these tasks.

Despite their good results on synthetic data, learning-based methods still suffer from artifacts or failed detection of particles when it comes to real-world applications. Two major factors can explain this finding. The first factor concerns the use of synthetic datasets in CNN-based frameworks, where the dataset is manually made according to general human perceptions or handcrafted rules. The second factor concerns the use of pixel-level loss function to train the neural networks, which neither reproduces the human visual system nor detects unnatural texture in structures.

To manage this problem, we consider four major challenges in designing CNNs for the snow removal task: (1) computational complexity, (2) network interpretability, (3) image quality, and (4) generalization ability.

The major constraint of challenges (1) and (2) is that the resolution of the input snowy image and the resulting output snow-free image must be identical. Therefore, the computational complexity and interpretability of the snow variance are critical for the network design.

In Liu e t   a l .’s work [5] , Inception-v4 [7] and D i l a t i o n   P y r a m i d are employed as the backbone feature extractors while removing all pooling and striding operations to maintain the same resolutions. However, despite the good results, this type of full-resolution network are time-consuming, and the model size of Inception-v4 is too large for lightweight applications.

Therefore, in this study, we develop a bottom-up pyramidal hierarchical pathway to extract high-level semantic features and a top-down pathway with lateral connections to comprehensively aggregate the feature from different resolutions and to further benefit the computational complexity and interpretability of the system. In addition, a pyramidal context feature extractor and a lightweight s p l i t - t r a n s f o r m - m e r g e topology are employed to further enhance the context awareness and feature diversity of the proposed network.

Regarding challenges (3) and (4) , to further improve the resultant image quality and generalization ability, we develop a two-stage network architecture. The first snow removal stage involves removing all falling snow that appears in an image, and the second refinement stage generates a refinement complement to enrich the details and recover unnatural areas. To constrain the network to focus on structural details and generate a natural image that corresponds to human perceptions, we use the structural similarity (SSIM) index loss and a Discriminator module to further boost the performance and push the solution space to the ground-truth manifold, respectively. Specifically, a generative adversarial network (GAN) framework is used to refine the resultant image quality through the approximation of manifolds and to enhance the generalization ability through its unsupervised discriminability.

The contributions of this study are described as follows:

    A novel loss function is proposed to facilitate the neural network on jointly learning the awareness of sharpness, structure, and realisticness. In addition, the generalization capacity to identify snow in the real-world image has significantly improved through the usage of the proposed unsupervised learning procedures.

    A novel deep network architecture with a top-down pathway and lateral cross-resolution connections is proposed to exploit high-level semantic features and low-level spatial features to increase overall efficiency and computing speed. The use of the split - transform - merge topology greatly reduces the model size and computational costs, and the use of atrous spatial pyramid pooling (ASPP) to explore the multiscale and global receptive field further improves efficiency. The proposed method outperforms state-of-the-art methods qualitatively and quantitatively as shown in the experimental results. Specifically, the proposed method achieved efficient computational speeds and light model sizes up to 1736% faster and 28.96% smaller than the state-of-the-art single-image snow removal approach [5] .

    We performed detailed qualitative and quantitative analyses to evaluate the respective abilities, especially the visual and quantitative generalization capability on real-world snowy images.

The remainder of this paper is organized as follows. Section II discusses existing learning-based atmospheric-particle-removal techniques and other related studies. Section III provides details on the proposed DS-GAN framework. Section IV describes qualitative and quantitative analyses of the DS-GAN framework. Finally, Section V presents our conclusion.
SECTION II.
Related Works

In this section, we briefly review studies on atmospheric-particle-removal approaches and GANs.
A. Atmospheric Particle Removal

As discussed in Section I , removing atmospheric degradation from images is challenging owing to the lack of ground-truth information and temporal information. Thus, numerous priors have been used to automatically detect and remove the particles.

For rain removal, sparsity prior [8] – [10] , Gaussian Maxture Model [11] , patch-rank prior [12] and HOG prior [13] have been utilized as the mainstream handcrafted features. By contrast, haze-removal techniques embrace dark channel priors [14] – [16] and an atmospheric scattering model [17] to estimate medium transmission and remove haze particles.

Most recently, to overcome the limited generalization ability of handcrafted priors, approaches based on the same-resolution CNNs and synthetic datasets have been proposed to learn the mapping between input degraded images and the corresponding clean ground truth. DehazeNet [2] and DerainNet [3] combine handcrafted priors and CNNs to accomplish the particle removal process. Conversely, JORDER [4] directly learns the mapping between the residual complement and input degraded image without using handcrafted priors with its well-designed network architecture. In [5] , a two-stage cascaded full-resolution network architecture to estimate the different attributes of snows was proposed. The work achieved state-of-the-art quantitative and qualitative results on the Snow100K dataset, but the process is time-consuming and has bad generalization ability. In [6] , a modular block named Dual Residual Block was introduced, along with five types of network architectures to deal with five different image regression tasks.
B. Generative Adversarial Networks

The GAN framework was first proposed by Goodfellow e t   a l . [18] . The purpose of GANs is to train a generative model G to produce samples from a given distribution P z into a target distribution P t . To this end, a min-max optimization framework with a generator model G and discriminator D is developed, which is described as follows: min G max D E x ∼ P t [ log ( D ( x ) ) ] + E x ~ ∼ P G [ log ( 1 − D ( x ~ ) ) ] , (1) View Source Right-click on figure for MathML and additional features. \begin{equation*} \min _{G}\max _{D} \underset {{x\sim \mathbb {P}_{t}}}{\mathbb {E}}[\log (D(x))] + \underset {\tilde {x}\sim \mathbb {P}_{G}}{\mathbb {E}}[\log (1-D(\tilde {x}))],\tag{1}\end{equation*} where P t is the target distribution, P G is the model distribution and x ~ = G ( z ) .

However, the optimization framework suffers from the vanishing gradient problem on the generator G when D is trained and becomes an optimal discriminator that attempts to minimize the Jensen-Shannon divergence of P d a t a and P G , which will be a constant value when the two distributions do not overlap. To deal with this problem, the Wasserstein GAN (WGAN) of Arjovsky e t   a l . [19] utilized the Wasserstein distance between the real and generated data, where this distance is continuous and differentiable when D is under the 1- L i p s c h i t z constraint.

An improved version named WGAN-GP [20] was proposed to accelerate the convergence through a gradient penalty term. The loss function of the discriminator in the WGAN-GP framework is defined as L = E x ~ ∼ P G [ D ( x ~ ) ] − E x ∼ P t [ D ( x ) ] + λ E x ^ ∼ P x ^ [ ( ∥ ∇ x ^ D ( x ^ ) ∥ 2 − 1 ) 2 ] , (2) View Source Right-click on figure for MathML and additional features. \begin{align*}&\hspace {-.5pc} L = \underset {\tilde {x}\sim \mathbb {P}_{G}}{\mathbb {E}}[D(\tilde {x})] - \underset {x\sim \mathbb {P}_{t}}{\mathbb {E}}[D(x)] \\&\qquad\qquad\qquad\qquad\qquad\displaystyle { + \lambda \mathop{\mathbb {E}}\limits_{\hat {x}\sim \mathbb {P}_{\hat {x}}}[(\Vert \nabla _{\hat {x}}D(\hat {x}) \Vert _{2} - 1)^{2}], } \tag{2}\end{align*} where the first two terms estimate Wasserstein’s distance between two sample spaces and the last term is the gradient penalty term that enforces the 1- L i p s c h i t z constraint. In addition, x ^ is the uniform sample along a straight line of x and x ~ , and λ is the weighting of the gradient penalty term.
SECTION III.
Proposed Method

Our DS-GAN framework, as shown in Fig. 2 , consists of three modules: (1) the Snow Removal (SR) module as depicted in Fig. 3 , which removes the appearance of falling snow in a given snowy image x ; (2) the Refinement module which generates a refinement complement r f ∈ [ − 1 , 1 ] to further improve the visual quality and enrich the details of the estimated snow-free result y ′ ; and (3) the Discriminator module, which is trained to distinguish the generated clean image y ^ and the ground truth image y .
Fig. 2. - Overview of the proposed DS-GAN.
Fig. 2.

Overview of the proposed DS-GAN.

Show All
Fig. 3. - The proposed Snow Removal module of our DS-GAN.
Fig. 3.

The proposed Snow Removal module of our DS-GAN.

Show All

In the SR and refinement modules, two modularized descriptors D m and D r are designed to extract multiscale features f m and f r , two interpreters S R and R G interprets the snow-free image y ′ and refinement complement r f to yield the generated clean image y ^ , respectively. The derivation of y ′ and r f is described in the following subsection, and Fig. 4 visualizes the variables from Figs. 2 and 3 .
Fig. 4. - Visualization of variables in DS-GAN. (a) Natural snowy image $\mathbf {x}$ . (b) Heatmap of the estimated snow mask $\hat {\mathbf {z}}$ . (c) Snow-removed image $\mathbf {y}'$ . (d) Residual complement $\mathbf {r}_{m}$ (normalized). (e) Refined image $\hat {\mathbf {y}}$ . (f) Refinement complement $\mathbf {r}_{f}$ (normalized).
Fig. 4.

Visualization of variables in DS-GAN. (a) Natural snowy image x . (b) Heatmap of the estimated snow mask z ^ . (c) Snow-removed image y ′ . (d) Residual complement r m (normalized). (e) Refined image \hat {\mathbf {y}} . (f) Refinement complement \mathbf {r}_{f} (normalized).

Show All

A. Descriptor

In this study, the design of our descriptor focuses on two major purposes: network interpretability and computational complexity. The detailed architecture of the proposed descriptor, as shown in Fig. 5 , consists of three major components: (1) pyramidal lateral connections, (2) ResNeXt blocks, and (3) atrous spatial pyramid pooling (ASPP). Notably, the network architectures of the two descriptors, D_{m} and D_{r} are identical.
Fig. 5. - The proposed descriptor of our DS-GAN.
Fig. 5.

The proposed descriptor of our DS-GAN.

Show All

1) Pyramidal Lateral Connection:

Top-down networks with fused cross-resolution features have been proven efficient in various computer vision tasks [21] – [24] . This type of architecture not only reduces computational complexity through its bottom-up feature hierarchical pathway but also combines high-level semantic features with low-level ones to enrich localization information.

Fig. 5 shows the building blocks of the proposed descriptor, where the orange blocks denote the downsampling operation either by strided convolution or pooling, the purple blocks refer to the upsampling operation with transposed convolution (known as “deconvolution” in [25] ), and the blue blocks indicate operations that have identical resolutions.
2) ResNeXt Block:

In [5] , Inception-v4 [7] is employed as the backbone of the descriptor. However, despite its good feature interpretability, it has too many hyperparameters to adjust when it comes to optimizing the network architecture. Therefore, we choose the ResNeXt block [26] as our fundamental feature extractor. Its split - transform - merge strategy corresponds to the complex variation of snow, and its cardinality dimension design can further reduce the number of parameters and computational complexity while increasing the feature diversity, which are all consistent with our demands.
3) Atrous Spatial Pyramid Pooling:

The Dilation~Pyramid described in [5] is a concatenation of multiple dilated convolution layers with various dilation factors. However, inspired by PSPNet [27] , the latest ASPP module [28] not only concatenates features from multiple dilated convolution layers but also concatenates image-level features and a convolutional 1\times 1 layer, which represents an improved version of the Dilation~Pyramid .

Therefore, the latest ASPP module is utilized in a high-level feature space, as shown in Fig. 5 , to explore context and global information. The output features are further upsampled and concatenated with those from lower levels. Then, a convolutional 3\times 3 layer is employed prior to output.
B. Snow Removal Module

As shown in Fig. 3 , after the feature extraction process of the descriptor is completed, the SR module uses the extracted feature \mathbf {f_{m}} as the input to infer the snow mask \hat {\mathbf {z}} and residual complement \mathbf {r}_{m} . The estimated residual complement \mathbf {r}_{m} then conducts an element-wise addition with input image \mathbf {x} to remove the falling snow from the image. Notably, instead of using an additional cascaded full-resolution network to generate a residual complement \mathbf {r}_{m} from the estimated snow mask \hat {\mathbf {z}} as in [5] , we simultaneously estimate the snow mask \hat {\mathbf {z}} and residual complement \mathbf {r}_{m} . This multitask learning strategy reduces computational complexity and aids in optimization, as proven in [29] .

After the snow removal process, the generated snow-free image \mathbf {y}' , residual complement \mathbf {r}_{m} and the snow mask \hat {\mathbf {z}} are concatenated into a semantic prior \mathbf {f}_{c} as the input of the refinement module.
C. Refinement Module and GANs

The purpose of Refinement module is to generate a refinement complement \mathbf {r}_{f} from the estimated priors \mathbf {f}_{c} to yield a visually realistic snow-removed image \hat {\mathbf {y}} . To this end, we utilize the discriminator module that was used in WGAN-GP [20] to differentiate the input image derived from either the ground-truth manifold or the generated manifolds. By optimizing the discriminator and generator , the solution space of \hat {\mathbf {y}} is favored to the natural image manifold.
D. Loss Functions

In this study, we used the pyramidal Euclidean loss function: \begin{equation*} \mathfrak {L}(\mathbf {m}, \hat {\mathbf {m}}) = \sum _{i=0}^{\tau }\Vert P_{2^{i}}(\mathbf {m}) - P_{2^{i}}(\hat {\mathbf {m}}) \Vert ^{2}_{2},\tag{3}\end{equation*} L ( m , m ^ ) = ∑ i = 0 τ ∥ P 2 i ( m ) − P 2 i ( m ^ ) ∥ 2 2 , (3) View Source Right-click on figure for MathML and additional features. \begin{equation*} \mathfrak {L}(\mathbf {m}, \hat {\mathbf {m}}) = \sum _{i=0}^{\tau }\Vert P_{2^{i}}(\mathbf {m}) - P_{2^{i}}(\hat {\mathbf {m}}) \Vert ^{2}_{2},\tag{3}\end{equation*} where \mathbf {m} and \hat {\mathbf {m}} denotes two matrices with identity size, \tau \in \mathbb {R} denotes the level of pyramidal loss, P_{n} denotes the nonoverlapping pooling operation with kernel size and stride n\times n .

A multiscaled pixel-level loss is then conducted between (\mathbf {z}, \hat {\mathbf {z}}) , (\mathbf {y}, \mathbf {y}') , and (\mathbf {y}, \hat {\mathbf {y}}) defined as follows: \begin{equation*} \mathcal {L}_{Euclidean} = \mathfrak {L}(\mathbf {z}, \hat {\mathbf {z}}) + \mathfrak {L}(\mathbf {y}, \mathbf {y}') + \mathfrak {L}(\mathbf {y}, \hat {\mathbf {y}}),\tag{4}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \mathcal {L}_{Euclidean} = \mathfrak {L}(\mathbf {z}, \hat {\mathbf {z}}) + \mathfrak {L}(\mathbf {y}, \mathbf {y}') + \mathfrak {L}(\mathbf {y}, \hat {\mathbf {y}}),\tag{4}\end{equation*} where the \mathfrak {L}(\cdot) is defined in Eq. (3) .

SSIM loss as defined in [30] is utilized to further enhance the structure-awareness ability of the refinement module: \begin{equation*} \mathcal {L}_{SSIM} = 1 - \text {SSIM}(\mathbf {y}, \hat {\mathbf {y}}).\tag{5}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \mathcal {L}_{SSIM} = 1 - \text {SSIM}(\mathbf {y}, \hat {\mathbf {y}}).\tag{5}\end{equation*}

Therefore, the overall Generator loss in our GAN framework is defined as \begin{align*} \mathcal {L}_{GAN}=&-D(\hat {\mathbf {y}}), \\ \mathcal {L}_{Generator}=&\mathcal {L}_{GAN} + \lambda _{p}(\mathcal {L}_{SSIM} + \mathcal {L}_{Euclidean}), \tag{6}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} \mathcal {L}_{GAN}=&-D(\hat {\mathbf {y}}), \\ \mathcal {L}_{Generator}=&\mathcal {L}_{GAN} + \lambda _{p}(\mathcal {L}_{SSIM} + \mathcal {L}_{Euclidean}), \tag{6}\end{align*} where the D denotes the Discriminator module, \mathcal {L}_{GAN} is the Generator loss term defined in the WGAN-GP framework [20] , and \lambda _{p} \in \mathbb {R} denotes the weighting of pixel-level loss.

On the other hand, the Discriminator loss is then followed by the WGAN-GP framework: \begin{align*} \mathcal {L}_{critic}=&\mathbb {E}_{\hat {\mathbf {y}}\sim \mathbb {P}_{G}}[D(\hat {\mathbf {y}})] - \mathbb {E}_{\mathbf {y}\sim \mathbb {P}_{r}}[D(\mathbf {y})], \\ \mathcal {L}_{gp}=&\mathbb {E}_{\tilde {\mathbf {y}}\sim \mathbb {P}_{\tilde {\mathbf {y}}}}[(\Vert \nabla _{\tilde {\mathbf {y}}}D(\tilde {\mathbf {y}}) \Vert _{2} - 1)^{2}], \\ \mathcal {L}_{Discriminator}=&\mathcal {L}_{critic} + \lambda _{gp} \mathcal {L}_{gp},\tag{7}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} \mathcal {L}_{critic}=&\mathbb {E}_{\hat {\mathbf {y}}\sim \mathbb {P}_{G}}[D(\hat {\mathbf {y}})] - \mathbb {E}_{\mathbf {y}\sim \mathbb {P}_{r}}[D(\mathbf {y})], \\ \mathcal {L}_{gp}=&\mathbb {E}_{\tilde {\mathbf {y}}\sim \mathbb {P}_{\tilde {\mathbf {y}}}}[(\Vert \nabla _{\tilde {\mathbf {y}}}D(\tilde {\mathbf {y}}) \Vert _{2} - 1)^{2}], \\ \mathcal {L}_{Discriminator}=&\mathcal {L}_{critic} + \lambda _{gp} \mathcal {L}_{gp},\tag{7}\end{align*} where the \mathbb {P}_{G} denotes the Generator distribution, \mathbb {P}_{r} denotes the natural data distribution, \mathbb {P}_{\tilde {\mathbf {y}}} denotes the data uniformly sampled along straight lines between pairs of points sampled from \mathbb {P}_{G} and \mathbb {P}_{r} .
SECTION IV.
Experiments
A. Implementation Details
1) ResNeXt Block:

The implementation of the ResNeXt block was based on [32] , and the settings of the hyperparameters are listed in Table I . The cardinality was set to four for all ResNeXt blocks as in the original paper [26] .
TABLE I Hyperparameters of Proposed Descriptor
Table I- Hyperparameters of Proposed Descriptor

2) ASPP:

The dilation factor was set at [2 1 , 2 2 , 2 3 ], the number of kernels in each branch was set to 64, and all convolution operations in ASPP were followed by a batch normalization [33] .
3) Descriptor:

The convolution layer after \mathbf {f}_{in} was Conv_{7} with a kernel number of 64, where the subscript n in Conv_{n} denotes the size of the kernel n\times n . The convolution layer between Conv_{\mathbf {f}_{in}} and the Concat layer is a Conv_{1} with a kernel number of 64. All the Conv_{t} layers have a kernel number of 64, the kernel size and corresponding stride depending on the upsampling rate. The convolution layer before \mathbf {f_{out}} was Conv_{3} with a kernel number of 64.
4) Others:

The implementation of the discriminator network was followed by the WGAN-GP [20] framework containing five convolution layers and one dense layer. \lambda _{p} in Eq. (6) was set to 3 and \lambda _{gp} in Eq. (7) was set to 10.
5) Data Augmentation:

In our experiment, we used the following random augmentation techniques: (1) horizontal flip, (2) brightness adjustment, (3) hue adjustment, (4) contrast adjustment, (5) random scale from 0.3 to 1.75, and (6) random crop with a size of 128\times 128 as the training patch.
B. Dataset

In our experiments, the Snow100K dataset [5] is utilized to train/test our framework. The dataset consists of 50k training and testing synthesized snowy/snow-free image pairs, and its corresponding snow mask. The test set of the Snow100K dataset is further separated into three subsets: Snow100K-S, Snow100K-M and Snow100K-L, which refer to the number of snow masks overlapping on the snow-free image. Specifically, images in the Snow100K-S subset have one snow mask, images in the Snow100K-M subset have two snow masks, and images in the Snow100K-L subset have three snow masks.
C. Ablation Study

We conducted a comprehensive ablation study, as described in Table II , to illustrate the effects of different network architectures in our DS-GAN. The peak signal-to-noise ratio (PSNR) and SSIM are used as the evaluation metrics, and all performances are evaluated using the Snow100K test set. Notably, the baseline network is an identical-resolution network that directly outputted \mathbf {y}' and \hat {\mathbf {z}} from the given snowy image \mathbf {x} without performing downsampling, lateral connections, refinement stage, ASPP, SSIM loss and GAN loss.
TABLE II Ablation Study of the Proposed DS-GAN
Table II- Ablation Study of the Proposed DS-GAN

1) Descriptor:

As previously mentioned, all pooling, strided convolution layers, transposed convolution layers, and ASPP are removed to evaluate the performance of the DS-GAN-baseline model. The resultant PSNR and SSIM are 27.1738 and 0.9008, respectively, as shown in Table II .

The frames per second (FPS) and number of parameters for each architecture are listed in Table III . We also list the FPS and number of parameters from [5] . The table clearly reveals that the proposed DS-GAN significantly improves the model size and computational complexity. The FPS was tested on a 480\times 480 image using a GTX 1080TI.
TABLE III Comparison of Different Design in the Proposed DS-GAN
Table III- Comparison of Different Design in the Proposed DS-GAN

2) Loss Functions:

We compare the quantitative and qualitative performances with and without \mathcal {L}_{SSIM} and \mathcal {L}_{GAN} . The quantitative evaluation results are listed in Table II , and the qualitative comparison is shown in Fig. 6 . The top row of Fig. 6 shows that DS-GAN- SSIM has a better image quality than DS-GAN- ASPP , whereas DS-GAN- GAN not only produced better image quality but also removed most snow.
Fig. 6. - Comparison of DS-GAN using different loss functions: (a) natural snowy image, (b) DS-GAN- $ASPP$ , (c) DS-GAN- $SSIM$ , and (d) DS-GAN- $GAN$ .
Fig. 6.

Comparison of DS-GAN using different loss functions: (a) natural snowy image, (b) DS-GAN- ASPP , (c) DS-GAN- SSIM , and (d) DS-GAN- GAN .

Show All

In the bottom row, although DS-GAN- ASPP and DS-GAN- SSIM removed the appearance of falling snow, the image details in the background were blurry and unnatural, whereas GAN preserved high-frequency components.
D. Comparison

The quantitative comparison results with state-of-the-art methods are listed in Table IV . Specifically, we improve the performance by 1.0027 and 0.0177 on the PSNR and SSIM metrics with the Snow100K test set, achieving a 1736% faster computational speed and 71.04% reduced model size, respectively, as shown in Table III . In addition, the Snow100K-S, -M, -L subsets were introduced in Section IV-B . The proposed method not only outperforms state-of-the-art snow removal algorithm [5] in every subset on PSNR and SSIM metric, it also significantly improves 1.0027 PSNR and 0.228 SSIM value on the most difficult Snow100K-L subset.
TABLE IV Performances of Various Methods on Snow100K’s Test Set. The Results in Synthesized Data Row Denote the Similarities Between the Synthesized Snowy Image \mathbf{x} and the Snow-Free Ground Truth \mathbf{y} ; the Overall Column Presents the Averages Over the Entire Test Set
Table IV- Performances of Various Methods on Snow100K’s Test Set. The Results in Synthesized Data Row Denote the Similarities Between the Synthesized Snowy Image $\mathbf{x}$ and the Snow-Free Ground Truth $\mathbf{y}$ ; the Overall Column Presents the Averages Over the Entire Test Set

The qualitative comparison results shown in Fig. 7 clearly show that the proposed DS-GAN has a significantly better generalization ability on real-world snow removal task in terms of the resultant image quality and the detection of real-world snow particles. The reason behind such visually appealing results is because of the proposed unsupervised training policy, where Discriminator networks enforces the Refinement networks to convert snowy images from the snow-remain manifold ( \mathbf {y}' ) into the snow-free manifold ( \mathbf {y} ).
Fig. 7. - Comparison of DS-GAN and other methods. (a) Natural snowy image. (b) Zheng $et~al$ . [9]. (c) DehazeNet [2]. (d) JORDER [4]. (e) DuRN-S-P [6]. (f) DesnowNet [5]. (g) DS-GAN (purposed).
Fig. 7.

Comparison of DS-GAN and other methods. (a) Natural snowy image. (b) Zheng et~al . [9] . (c) DehazeNet [2] . (d) JORDER [4] . (e) DuRN-S-P [6] . (f) DesnowNet [5] . (g) DS-GAN (purposed).

Show All

In addition, we conduct a user study to objectively evaluate the image quality of the snow-removed images. We additionally collect 100 realistic snowy images from Internet and categorize them in two categories and two attributes: (1) the Simple-BG class, which refers to images with relatively simple background, and (2) the Complex-BG class, which refers to images with relatively complex background. The two attributes are: (1) light snow, which are images with normal size of snowflakes, and (2) heavy snow, which refers to images that contain large size of snowflakes. Notably, each image is categorized into one class and with one attribute, examples for each class and attributes can be found in Fig. 8 .
Fig. 8. - Examples of the classes and attributes of real-world snowy images (top row) and our corresponding snow-removed results (bottom row).
Fig. 8.

Examples of the classes and attributes of real-world snowy images (top row) and our corresponding snow-removed results (bottom row).

Show All

The top four algorithms, JORDER [4] , DuRN-S-P [6] , DesnowNet [5] and the proposed DesnowGAN, which are evaluated in Table IV , are chosen for the comparison. The detailed settings of the experiment included 10 natural snowy images with 40 estimated snow-free results shown to each participant in a random order. Each participant was asked to rank from 1 to 5 to the resultant image using two criteria, \kappa _{c} and \kappa _{q} , where \kappa _{c} evaluates the algorithm’s ability of detecting and removing snowflakes, and \kappa _{q} evaluates the overall resultant image quality and realisticness. In total, 15 people participated in our experiment and 100 natural snowy images were employed, whereas 63 and 37 images are in the Simple-BG and Complex-BG classes and 73 and 27 images have light and heavy attributes, respectively. The scores for the methods are listed in Table V . As the JORDER [4] , DuRN-S-P [6] and DesnowNet [5] reached similar subject scores among the categories, the proposed method outperforms other methods in terms of its ability to detect and remove snow ( \kappa _{c} ) and the quality of the resultant snow-free image ( \kappa _{q} ).
TABLE V Subjective Evaluation Results. The Clearness and Quality Scores From 1 to 5, While Higher Refers to Better Snow Removing Ability and Image Quality, Respectively
Table V- Subjective Evaluation Results. The Clearness and Quality Scores From 1 to 5, While Higher Refers to Better Snow Removing Ability and Image Quality, Respectively

SECTION V.
Conclusion

In this paper, we propose a novel snow-removal framework that combines the functions of the pixel-, structural-, and perceptual-level losses to mimic the visual system of humans. A significant breakthrough that reduced the computational complexity and improved the overall snow-removing efficiency for synthesized and real-world data is the proposed descriptor with pyramidal lateral connections across multiple resolutions. Compared with the state-of-the-art single-image snow-removal approach [5] , the proposed network architecture has been 1736% faster in processing, and the model size was reduced by 71.04%. Ultimately, the application of the GAN framework is a major change in the framework’s generalization ability, which provides visually realistic snow-removed results as illustrated in the qualitative and subjective evaluations.

Authors
Figures
References
Citations
Keywords
Metrics
More Like This
Research on fuzzy image enhancement and restoration effect

2017 First International Conference on Electronics Instrumentation & Information Systems (EIIS)

Published: 2017
An Experimental-Based Review of Image Enhancement and Image Restoration Methods for Underwater Imaging

IEEE Access

Published: 2019
Show More
References
1.
J. Redmon and A. Farhadi, "YOLOv3: An incremental improvement", arXiv:1804.02767 , 2018, [online] Available: http://arxiv.org/abs/1804.02767.
Show in Context Google Scholar
2.
B. Cai, X. Xu, K. Jia, C. Qing and D. Tao, "DehazeNet: An end-to-end system for single image haze removal", IEEE Trans. Image Process. , vol. 25, no. 11, pp. 5187-5198, Nov. 2016.
Show in Context View Article
Google Scholar
3.
X. Fu, J. Huang, X. Ding, Y. Liao and J. Paisley, "Clearing the skies: A deep network architecture for single-image rain removal", IEEE Trans. Image Process. , vol. 26, no. 6, pp. 2944-2956, Jun. 2017.
Show in Context View Article
Google Scholar
4.
W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo and S. Yan, "Deep joint rain detection and removal from a single image", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 1357-1366, Jul. 2017.
Show in Context View Article
Google Scholar
5.
Y.-F. Liu, D.-W. Jaw, S.-C. Huang and J.-N. Hwang, "DesnowNet: Context-aware deep network for snow removal", IEEE Trans. Image Process. , vol. 27, no. 6, pp. 3064-3073, Jun. 2018.
Show in Context View Article
Google Scholar
6.
X. Liu, M. Suganuma, Z. Sun and T. Okatani, "Dual residual networks leveraging the potential of paired operations for image restoration", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 7007-7016, Jun. 2019.
Show in Context View Article
Google Scholar
7.
C. Szegedy, S. Ioffe, V. Vanhoucke and A. A. Alemi, "Inception-v4 inception-resnet and the impact of residual connections on learning", Proc. AAAI , vol. 4, pp. 12, 2017.
Show in Context Google Scholar
8.
L.-W. Kang, C.-W. Lin and Y.-H. Fu, "Automatic single-image-based rain streaks removal via image decomposition", IEEE Trans. Image Process. , vol. 21, no. 4, pp. 1742-1755, Apr. 2012.
Show in Context View Article
Google Scholar
9.
X. Zheng, Y. Liao, W. Guo, X. Fu and X. Ding, "Single-image-based rain and snow removal using multi-guided filter", Proc. Int. Conf. Neural Inf. Process. , pp. 258-265, 2013.
Show in Context CrossRef Google Scholar
10.
Y. Luo, Y. Xu and H. Ji, "Removing rain from a single image via discriminative sparse coding", Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , pp. 3397-3405, Dec. 2015.
Show in Context View Article
Google Scholar
11.
Y. Li, R. T. Tan, X. Guo, J. Lu and M. S. Brown, "Rain streak removal using layer priors", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 2736-2744, Jun. 2016.
Show in Context View Article
Google Scholar
12.
Y.-L. Chen and C.-T. Hsu, "A generalized low-rank appearance model for spatio-temporally correlated rain streaks", Proc. IEEE Int. Conf. Comput. Vis. , pp. 1968-1975, Dec. 2013.
Show in Context View Article
Google Scholar
13.
D.-Y. Chen, C.-C. Chen and L.-W. Kang, "Visual depth guided color image rain streaks removal using sparse coding", IEEE Trans. Circuits Syst. Video Technol. , vol. 24, no. 8, pp. 1430-1455, Aug. 2014.
Show in Context View Article
Google Scholar
14.
K. He, J. Sun and X. Tang, "Single image haze removal using dark channel prior", IEEE Trans. Pattern Anal. Mach. Intell. , vol. 33, no. 12, pp. 2341-2353, Dec. 2011.
Show in Context View Article
Google Scholar
15.
S.-C. Huang, J.-H. Ye and B.-H. Chen, "An advanced single-image visibility restoration algorithm for real-world hazy scenes", IEEE Trans. Ind. Electron. , vol. 62, no. 5, pp. 2962-2972, May 2015.
Show in Context View Article
Google Scholar
16.
B.-H. Chen, S.-C. Huang and F.-C. Cheng, "A high-efficiency and high-speed gain intervention refinement filter for haze removal", J. Display Technol. , vol. 12, no. 7, pp. 753-759, Jul. 2016.
Show in Context View Article
Google Scholar
17.
E. J. McCartney, Optics of the Atmosphere: Scattering by Molecules and Particles, New York, NY, USA:Wiley, pp. 421, 1976.
Show in Context Google Scholar
18.
I. Goodfellow et al., "Generative adversarial nets", Proc. Adv. Neural Inf. Process. Syst. , pp. 2672-2680, 2014.
Show in Context Google Scholar
19.
M. Arjovsky, S. Chintala and L. Bottou, "Wasserstein GAN", arXiv:1701.07875 , 2017, [online] Available: http://arxiv.org/abs/1701.07875.
Show in Context Google Scholar
20.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin and A. C. Courville, "Improved training of Wasserstein GANs", Proc. Adv. Neural Inf. Process. Syst. , pp. 5769-5779, 2017.
Show in Context Google Scholar
21.
J. Long, E. Shelhamer and T. Darrell, "Fully convolutional networks for semantic segmentation", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 3431-3440, Jun. 2015.
Show in Context View Article
Google Scholar
22.
T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan and S. Belongie, "Feature pyramid networks for object detection", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 2117-2125, Jul. 2017.
Show in Context View Article
Google Scholar
23.
O. Ronneberger, P. Fischer and T. Brox, "U-net: Convolutional networks for biomedical image segmentation", Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent. , pp. 234-241, 2015.
Show in Context Google Scholar
24.
C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi and A. C. Berg, "DSSD: Deconvolutional single shot detector", arXiv:1701.06659 , 2017, [online] Available: http://arxiv.org/abs/1701.06659.
Show in Context Google Scholar
25.
M. D. Zeiler, G. W. Taylor and R. Fergus, "Adaptive deconvolutional networks for mid and high level feature learning", Proc. Int. Conf. Comput. Vis. , pp. 2018-2025, Nov. 2011.
Show in Context View Article
Google Scholar
26.
S. Xie, R. Girshick, P. Dollar, Z. Tu and K. He, "Aggregated residual transformations for deep neural networks", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 5987-5995, Jul. 2017.
Show in Context View Article
Google Scholar
27.
H. Zhao, J. Shi, X. Qi, X. Wang and J. Jia, "Pyramid scene parsing network", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 2881-2890, Jul. 2017.
Show in Context View Article
Google Scholar
28.
L.-C. Chen, G. Papandreou, F. Schroff and H. Adam, "Rethinking atrous convolution for semantic image segmentation", arXiv:1706.05587 , 2017, [online] Available: http://arxiv.org/abs/1706.05587.
Show in Context Google Scholar
29.
K. He, G. Gkioxari, P. Dollár and R. Girshick, "Mask R-CNN", Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , pp. 2980-2988, Oct. 2017.
Show in Context View Article
Google Scholar
30.
H. Zhao, O. Gallo, I. Frosio and J. Kautz, "Loss functions for image restoration with neural networks", IEEE Trans. Comput. Imag. , vol. 3, no. 1, pp. 47-57, Mar. 2017.
Show in Context View Article
Google Scholar
31.
L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy and A. L. Yuille, "DeepLab: Semantic image segmentation with deep convolutional nets atrous convolution and fully connected CRFs", IEEE Trans. Pattern Anal. Mach. Intell. , vol. 40, no. 4, pp. 834-848, Apr. 2018.
View Article
Google Scholar
32.
K. He, X. Zhang, S. Ren and J. Sun, "Identity mappings in deep residual networks", Proc. Eur. Conf. Comput. Vis. , pp. 630-645, 2016.
Show in Context Google Scholar
33.
S. Ioffe and C. Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift", arXiv:1502.03167 , 2015, [online] Available: http://arxiv.org/abs/1502.03167.
Show in Context Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
