Undergraduate Texts in Mathematics
Sheldon Axler
Linear Algebra Done Right
Third Edition

Undergraduate Texts in Mathematics

Undergraduate Texts in Mathematics
Series Editors: Sheldon Axler San Francisco State University, San Francisco, CA, USA Kenneth Ribet University of California, Berkeley, CA, USA
Advisory Board: Colin Adams, Williams College, Williamstown, MA, USA Alejandro Adem, University of British Columbia, Vancouver, BC, Canada Ruth Charney, Brandeis University, Waltham, MA, USA Irene M. Gamba, The University of Texas at Austin, Austin, TX, USA Roger E. Howe, Yale University, New Haven, CT, USA David Jerison, Massachusetts Institute of Technology, Cambridge, MA, USA Jeffrey C. Lagarias, University of Michigan, Ann Arbor, MI, USA Jill Pipher, Brown University, Providence, RI, USA Fadil Santosa, University of Minnesota, Minneapolis, MN, USA Amie Wilkinson, University of Chicago, Chicago, IL, USA
Undergraduate Texts in Mathematics are generally aimed at third- and fourthyear undergraduate mathematics students at North American universities. These texts strive to provide students and teachers with new perspectives and novel approaches. The books include motivation that guides the reader to an appreciation of interrelations among different aspects of the subject. They feature examples that illustrate key concepts as well as exercises that strengthen understanding.
For further volumes: http://www.springer.com/series/666

Sheldon Axler
Linear Algebra Done Right
Third edition
123

Sheldon Axler Department of Mathematics San Francisco State University San Francisco, CA, USA

ISSN 0172-6056

ISSN 2197-5604 (electronic)

ISBN 978-3-319-11079-0

ISBN 978-3-319-11080-6 (eBook)

DOI 10.1007/978-3-319-11080-6

Springer Cham Heidelberg New York Dordrecht London

Library of Congress Control Number: 2014954079

Mathematics Subject ClassiÔ¨Åcation (2010): 15-01, 15A03, 15A04, 15A15, 15A18, 15A21

c Springer International Publishing 2015 This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, speciÔ¨Åcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microÔ¨Ålms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection with reviews or scholarly analysis or material supplied speciÔ¨Åcally for the purpose of being entered and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of this publication or parts thereof is permitted only under the provisions of the Copyright Law of the Publisher‚Äôs location, in its current version, and permission for use must always be obtained from Springer. Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a speciÔ¨Åc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. While the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.

Typeset by the author in LaTeX

Cover Ô¨Ågure: For a statement of Apollonius‚Äôs Identity and its connection to linear algebra, see the last exercise in Section 6.A.

Printed on acid-free paper

Springer is part of Springer Science+Business Media (www.springer.com)

Contents
Preface for the Instructor xi
Preface for the Student xv
Acknowledgments xvii
1 Vector Spaces 1 1.A Rn and Cn 2
Complex Numbers 2 Lists 5 Fn 6 Digression on Fields 10 Exercises 1.A 11
1.B DeÔ¨Ånition of Vector Space 12
Exercises 1.B 17
1.C Subspaces 18
Sums of Subspaces 20 Direct Sums 21 Exercises 1.C 24
2 Finite-Dimensional Vector Spaces 27 2.A Span and Linear Independence 28
Linear Combinations and Span 28 Linear Independence 32 Exercises 2.A 37
v

vi Contents
2.B Bases 39
Exercises 2.B 43
2.C Dimension 44
Exercises 2.C 48
3 Linear Maps 51
3.A The Vector Space of Linear Maps 52
DeÔ¨Ånition and Examples of Linear Maps 52 Algebraic Operations on L.V; W / 55 Exercises 3.A 57
3.B Null Spaces and Ranges 59
Null Space and Injectivity 59 Range and Surjectivity 61 Fundamental Theorem of Linear Maps 63 Exercises 3.B 67
3.C Matrices 70
Representing a Linear Map by a Matrix 70 Addition and Scalar Multiplication of Matrices 72 Matrix Multiplication 74 Exercises 3.C 78
3.D Invertibility and Isomorphic Vector Spaces 80
Invertible Linear Maps 80 Isomorphic Vector Spaces 82 Linear Maps Thought of as Matrix Multiplication 84 Operators 86 Exercises 3.D 88
3.E Products and Quotients of Vector Spaces 91
Products of Vector Spaces 91 Products and Direct Sums 93 Quotients of Vector Spaces 94 Exercises 3.E 98

Contents vii
3.F Duality 101
The Dual Space and the Dual Map 101 The Null Space and Range of the Dual of a Linear Map 104 The Matrix of the Dual of a Linear Map 109 The Rank of a Matrix 111 Exercises 3.F 113
4 Polynomials 117
Complex Conjugate and Absolute Value 118 Uniqueness of CoefÔ¨Åcients for Polynomials 120 The Division Algorithm for Polynomials 121 Zeros of Polynomials 122 Factorization of Polynomials over C 123 Factorization of Polynomials over R 126 Exercises 4 129
5 Eigenvalues, Eigenvectors, and Invariant Subspaces 131
5.A Invariant Subspaces 132
Eigenvalues and Eigenvectors 133 Restriction and Quotient Operators 137 Exercises 5.A 138
5.B Eigenvectors and Upper-Triangular Matrices 143
Polynomials Applied to Operators 143 Existence of Eigenvalues 145 Upper-Triangular Matrices 146 Exercises 5.B 153
5.C Eigenspaces and Diagonal Matrices 155
Exercises 5.C 160
6 Inner Product Spaces 163
6.A Inner Products and Norms 164
Inner Products 164 Norms 168 Exercises 6.A 175

viii Contents
6.B Orthonormal Bases 180
Linear Functionals on Inner Product Spaces 187 Exercises 6.B 189
6.C Orthogonal Complements and Minimization Problems 193
Orthogonal Complements 193 Minimization Problems 198 Exercises 6.C 201
7 Operators on Inner Product Spaces 203
7.A Self-Adjoint and Normal Operators 204
Adjoints 204 Self-Adjoint Operators 209 Normal Operators 212 Exercises 7.A 214
7.B The Spectral Theorem 217
The Complex Spectral Theorem 217 The Real Spectral Theorem 219 Exercises 7.B 223
7.C Positive Operators and Isometries 225
Positive Operators 225 Isometries 228 Exercises 7.C 231
7.D Polar Decomposition and Singular Value Decomposition 233
Polar Decomposition 233 Singular Value Decomposition 236 Exercises 7.D 238
8 Operators on Complex Vector Spaces 241
8.A Generalized Eigenvectors and Nilpotent Operators 242
Null Spaces of Powers of an Operator 242 Generalized Eigenvectors 244 Nilpotent Operators 248 Exercises 8.A 249

Contents ix
8.B Decomposition of an Operator 252
Description of Operators on Complex Vector Spaces 252 Multiplicity of an Eigenvalue 254 Block Diagonal Matrices 255 Square Roots 258 Exercises 8.B 259
8.C Characteristic and Minimal Polynomials 261
The Cayley‚ÄìHamilton Theorem 261 The Minimal Polynomial 262 Exercises 8.C 267
8.D Jordan Form 270
Exercises 8.D 274
9 Operators on Real Vector Spaces 275
9.A ComplexiÔ¨Åcation 276
ComplexiÔ¨Åcation of a Vector Space 276 ComplexiÔ¨Åcation of an Operator 277 The Minimal Polynomial of the ComplexiÔ¨Åcation 279 Eigenvalues of the ComplexiÔ¨Åcation 280 Characteristic Polynomial of the ComplexiÔ¨Åcation 283 Exercises 9.A 285
9.B Operators on Real Inner Product Spaces 287
Normal Operators on Real Inner Product Spaces 287 Isometries on Real Inner Product Spaces 292 Exercises 9.B 294
10 Trace and Determinant 295
10.A Trace 296
Change of Basis 296 Trace: A Connection Between Operators and Matrices 299 Exercises 10.A 304

x Contents
10.B Determinant 307
Determinant of an Operator 307 Determinant of a Matrix 309 The Sign of the Determinant 320 Volume 323 Exercises 10.B 330
Photo Credits 333
Symbol Index 335
Index 337

Preface for the Instructor
You are about to teach a course that will probably give students their second exposure to linear algebra. During their Ô¨Årst brush with the subject, your students probably worked with Euclidean spaces and matrices. In contrast, this course will emphasize abstract vector spaces and linear maps.
The audacious title of this book deserves an explanation. Almost all linear algebra books use determinants to prove that every linear operator on a Ô¨Ånite-dimensional complex vector space has an eigenvalue. Determinants are difÔ¨Åcult, nonintuitive, and often deÔ¨Åned without motivation. To prove the theorem about existence of eigenvalues on complex vector spaces, most books must deÔ¨Åne determinants, prove that a linear map is not invertible if and only if its determinant equals 0, and then deÔ¨Åne the characteristic polynomial. This tortuous (torturous?) path gives students little feeling for why eigenvalues exist.
In contrast, the simple determinant-free proofs presented here (for example, see 5.21) offer more insight. Once determinants have been banished to the end of the book, a new route opens to the main goal of linear algebra‚Äî understanding the structure of linear operators.
This book starts at the beginning of the subject, with no prerequisites other than the usual demand for suitable mathematical maturity. Even if your students have already seen some of the material in the Ô¨Årst few chapters, they may be unaccustomed to working exercises of the type presented here, most of which require an understanding of proofs.
Here is a chapter-by-chapter summary of the highlights of the book:
 Chapter 1: Vector spaces are deÔ¨Åned in this chapter, and their basic properties are developed.
 Chapter 2: Linear independence, span, basis, and dimension are deÔ¨Åned in this chapter, which presents the basic theory of Ô¨Ånite-dimensional vector spaces.
xi

xii Preface for the Instructor
 Chapter 3: Linear maps are introduced in this chapter. The key result here is the Fundamental Theorem of Linear Maps (3.22): if T is a linear map on V, then dim V D dim null T C dim range T. Quotient spaces and duality are topics in this chapter at a higher level of abstraction than other parts of the book; these topics can be skipped without running into problems elsewhere in the book.
 Chapter 4: The part of the theory of polynomials that will be needed to understand linear operators is presented in this chapter. This chapter contains no linear algebra. It can be covered quickly, especially if your students are already familiar with these results.
 Chapter 5: The idea of studying a linear operator by restricting it to small subspaces leads to eigenvectors in the early part of this chapter. The highlight of this chapter is a simple proof that on complex vector spaces, eigenvalues always exist. This result is then used to show that each linear operator on a complex vector space has an upper-triangular matrix with respect to some basis. All this is done without deÔ¨Åning determinants or characteristic polynomials!
 Chapter 6: Inner product spaces are deÔ¨Åned in this chapter, and their basic properties are developed along with standard tools such as orthonormal bases and the Gram‚ÄìSchmidt Procedure. This chapter also shows how orthogonal projections can be used to solve certain minimization problems.
 Chapter 7: The Spectral Theorem, which characterizes the linear operators for which there exists an orthonormal basis consisting of eigenvectors, is the highlight of this chapter. The work in earlier chapters pays off here with especially simple proofs. This chapter also deals with positive operators, isometries, the Polar Decomposition, and the Singular Value Decomposition.
 Chapter 8: Minimal polynomials, characteristic polynomials, and generalized eigenvectors are introduced in this chapter. The main achievement of this chapter is the description of a linear operator on a complex vector space in terms of its generalized eigenvectors. This description enables one to prove many of the results usually proved using Jordan Form. For example, these tools are used to prove that every invertible linear operator on a complex vector space has a square root. The chapter concludes with a proof that every linear operator on a complex vector space can be put into Jordan Form.

Preface for the Instructor xiii
 Chapter 9: Linear operators on real vector spaces occupy center stage in this chapter. Here the main technique is complexiÔ¨Åcation, which is a natural extension of an operator on a real vector space to an operator on a complex vector space. ComplexiÔ¨Åcation allows our results about complex vector spaces to be transferred easily to real vector spaces. For example, this technique is used to show that every linear operator on a real vector space has an invariant subspace of dimension 1 or 2. As another example, we show that that every linear operator on an odd-dimensional real vector space has an eigenvalue.
 Chapter 10: The trace and determinant (on complex vector spaces) are deÔ¨Åned in this chapter as the sum of the eigenvalues and the product of the eigenvalues, both counting multiplicity. These easy-to-remember deÔ¨Ånitions would not be possible with the traditional approach to eigenvalues, because the traditional method uses determinants to prove that sufÔ¨Åcient eigenvalues exist. The standard theorems about determinants now become much clearer. The Polar Decomposition and the Real Spectral Theorem are used to derive the change of variables formula for multivariable integrals in a fashion that makes the appearance of the determinant there seem natural.
This book usually develops linear algebra simultaneously for real and complex vector spaces by letting F denote either the real or the complex numbers. If you and your students prefer to think of F as an arbitrary Ô¨Åeld, then see the comments at the end of Section 1.A. I prefer avoiding arbitrary Ô¨Åelds at this level because they introduce extra abstraction without leading to any new linear algebra. Also, students are more comfortable thinking of polynomials as functions instead of the more formal objects needed for polynomials with coefÔ¨Åcients in Ô¨Ånite Ô¨Åelds. Finally, even if the beginning part of the theory were developed with arbitrary Ô¨Åelds, inner product spaces would push consideration back to just real and complex vector spaces.
You probably cannot cover everything in this book in one semester. Going through the Ô¨Årst eight chapters is a good goal for a one-semester course. If you must reach Chapter 10, then consider covering Chapters 4 and 9 in Ô¨Åfteen minutes each, as well as skipping the material on quotient spaces and duality in Chapter 3.
A goal more important than teaching any particular theorem is to develop in students the ability to understand and manipulate the objects of linear algebra. Mathematics can be learned only by doing. Fortunately, linear algebra has many good homework exercises. When teaching this course, during each class I usually assign as homework several of the exercises, due the next class. Going over the homework might take up a third or even half of a typical class.

xiv Preface for the Instructor
Major changes from the previous edition:
 This edition contains 561 exercises, including 337 new exercises that were not in the previous edition. Exercises now appear at the end of each section, rather than at the end of each chapter.
 Many new examples have been added to illustrate the key ideas of linear algebra.
 Beautiful new formatting, including the use of color, creates pages with an unusually pleasant appearance in both print and electronic versions. As a visual aid, deÔ¨Ånitions are in beige boxes and theorems are in blue boxes (in color versions of the book).
 Each theorem now has a descriptive name.
 New topics covered in the book include product spaces, quotient spaces, and duality.
 Chapter 9 (Operators on Real Vector Spaces) has been completely rewritten to take advantage of simpliÔ¨Åcations via complexiÔ¨Åcation. This approach allows for more streamlined presentations in Chapters 5 and 7 because those chapters now focus mostly on complex vector spaces.
 Hundreds of improvements have been made throughout the book. For example, the proof of Jordan Form (Section 8.D) has been simpliÔ¨Åed.
Please check the website below for additional information about the book. I may occasionally write new sections on additional topics. These new sections will be posted on the website. Your suggestions, comments, and corrections are most welcome.
Best wishes for teaching a successful linear algebra class!
Sheldon Axler Mathematics Department San Francisco State University San Francisco, CA 94132, USA
website: linear.axler.net e-mail: linear@axler.net Twitter: @AxlerLinear

Preface for the Student
You are probably about to begin your second exposure to linear algebra. Unlike your Ô¨Årst brush with the subject, which probably emphasized Euclidean spaces and matrices, this encounter will focus on abstract vector spaces and linear maps. These terms will be deÔ¨Åned later, so don‚Äôt worry if you do not know what they mean. This book starts from the beginning of the subject, assuming no knowledge of linear algebra. The key point is that you are about to immerse yourself in serious mathematics, with an emphasis on attaining a deep understanding of the deÔ¨Ånitions, theorems, and proofs.
You cannot read mathematics the way you read a novel. If you zip through a page in less than an hour, you are probably going too fast. When you encounter the phrase ‚Äúas you should verify‚Äù, you should indeed do the veriÔ¨Åcation, which will usually require some writing on your part. When steps are left out, you need to supply the missing pieces. You should ponder and internalize each deÔ¨Ånition. For each theorem, you should seek examples to show why each hypothesis is necessary. Discussions with other students should help.
As a visual aid, deÔ¨Ånitions are in beige boxes and theorems are in blue boxes (in color versions of the book). Each theorem has a descriptive name.
Please check the website below for additional information about the book. I may occasionally write new sections on additional topics. These new sections will be posted on the website. Your suggestions, comments, and corrections are most welcome.
Best wishes for success and enjoyment in learning linear algebra!
Sheldon Axler Mathematics Department San Francisco State University San Francisco, CA 94132, USA website: linear.axler.net e-mail: linear@axler.net Twitter: @AxlerLinear
xv

Acknowledgments
I owe a huge intellectual debt to the many mathematicians who created linear algebra over the past two centuries. The results in this book belong to the common heritage of mathematics. A special case of a theorem may Ô¨Årst have been proved in the nineteenth century, then slowly sharpened and improved by many mathematicians. Bestowing proper credit on all the contributors would be a difÔ¨Åcult task that I have not undertaken. In no case should the reader assume that any theorem presented here represents my original contribution. However, in writing this book I tried to think about the best way to present linear algebra and to prove its theorems, without regard to the standard methods and proofs used in most textbooks.
Many people helped make this a better book. The two previous editions of this book were used as a textbook at about 300 universities and colleges. I received thousands of suggestions and comments from faculty and students who used the second edition. I looked carefully at all those suggestions as I was working on this edition. At Ô¨Årst, I tried keeping track of whose suggestions I used so that those people could be thanked here. But as changes were made and then replaced with better suggestions, and as the list grew longer, keeping track of the sources of each suggestion became too complicated. And lists are boring to read anyway. Thus in lieu of a long list of people who contributed good ideas, I will just say how truly grateful I am to everyone who sent me suggestions and comments. Many many thanks!
Special thanks to Ken Ribet and his giant (220 students) linear algebra class at Berkeley that class-tested a preliminary version of this third edition and that sent me more suggestions and corrections than any other group.
Finally, I thank Springer for providing me with help when I needed it and for allowing me the freedom to make the Ô¨Ånal decisions about the content and appearance of this book. Special thanks to Elizabeth Loew for her wonderful work as editor and David Kramer for unusually skillful copyediting.
Sheldon Axler
xvii

CHAPTER
1

Ren√© Descartes explaining his work to Queen Christina of Sweden. Vector spaces are a generalization of the description of a plane using two coordinates, as published by Descartes in 1637.

Vector Spaces

Linear algebra is the study of linear maps on Ô¨Ånite-dimensional vector spaces. Eventually we will learn what all these terms mean. In this chapter we will deÔ¨Åne vector spaces and discuss their elementary properties.
In linear algebra, better theorems and more insight emerge if complex numbers are investigated along with real numbers. Thus we will begin by introducing the complex numbers and their basic properties.
We will generalize the examples of a plane and ordinary space to Rn and Cn, which we then will generalize to the notion of a vector space. The elementary properties of a vector space will already seem familiar to you.
Then our next topic will be subspaces, which play a role for vector spaces analogous to the role played by subsets for sets. Finally, we will look at sums of subspaces (analogous to unions of subsets) and direct sums of subspaces (analogous to unions of disjoint sets).

LEARNING OBJECTIVES FOR THIS CHAPTER
basic properties of the complex numbers Rn and Cn vector spaces subspaces sums and direct sums of subspaces

¬© Springer International Publishing 2015

1

S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics,

DOI 10.1007/978-3-319-11080-6__1

2 CHAPTER 1 Vector Spaces
1.A Rn and Cn
Complex Numbers
You should already be familiar with basic properties of the set R of real numbers. Complex numbers were invented so that we can take square roots of negative numbers. The idea is to assume we have a square root of 1, denoted i, that obeys the usual rules of arithmetic. Here are the formal deÔ¨Ånitions:

1.1 DeÔ¨Ånition complex numbers
 A complex number is an ordered pair .a; b/, where a; b 2 R, but we will write this as a C bi.
 The set of all complex numbers is denoted by C: C D fa C bi W a; b 2 Rg:
 Addition and multiplication on C are deÔ¨Åned by .a C bi/ C .c C d i/ D .a C c/ C .b C d /i; .a C bi/.c C d i/ D .ac bd / C .ad C bc/i I
here a; b; c; d 2 R.

If a 2 R, we identify a C 0i with the real number a. Thus we can think

of R as a subset of C. We also usually write 0 C bi as just bi, and we usually

write 0 C 1i as just i.

The spymbol i was Ô¨Årst used to denote 1 by Swiss mathematician

Using multiplication as deÔ¨Åned above, you should verify that i 2 D 1.

Leonhard Euler in 1777.

Do not memorize the formula for the

product of two complex numbers; you

can always rederive it by recalling that i 2 D 1 and then using the usual rules

of arithmetic (as given by 1.3).

1.2 Example Evaluate .2 C 3i /.4 C 5i /.

Solution

.2 C 3i /.4 C 5i/ D 2  4 C 2  .5i/ C .3i /  4 C .3i /.5i/ D 8 C 10i C 12i 15 D 7 C 22i

SECTION 1.A Rn and Cn

3

1.3 Properties of complex arithmetic
commutativity Àõ C Àá D Àá C Àõ and ÀõÀá D ÀáÀõ for all Àõ; Àá 2 C;
associativity .Àõ CÀá/C D Àõ C.Àá C/ and .ÀõÀá/ D Àõ.Àá/ for all Àõ; Àá;  2 C;
identities  C 0 D  and 1 D  for all  2 C;
additive inverse for every Àõ 2 C, there exists a unique Àá 2 C such that Àõ C Àá D 0;
multiplicative inverse for every Àõ 2 C with Àõ ¬§ 0, there exists a unique Àá 2 C such that ÀõÀá D 1;
distributive property .Àõ C Àá/ D Àõ C Àá for all ; Àõ; Àá 2 C.

The properties above are proved using the familiar properties of real numbers and the deÔ¨Ånitions of complex addition and multiplication. The next example shows how commutativity of complex multiplication is proved. Proofs of the other properties above are left as exercises.
1.4 Example Show that ÀõÀá D ÀáÀõ for all Àõ; Àá;  2 C.
Solution Suppose Àõ D a C bi and Àá D c C d i , where a; b; c; d 2 R. Then the deÔ¨Ånition of multiplication of complex numbers shows that
ÀõÀá D .a C bi/.c C d i/ D .ac bd / C .ad C bc/i
and
ÀáÀõ D .c C d i/.a C bi/ D .ca db/ C .cb C da/i:
The equations above and the commutativity of multiplication and addition of real numbers show that ÀõÀá D ÀáÀõ.

4 CHAPTER 1 Vector Spaces

1.5 DeÔ¨Ånition Let Àõ; Àá 2 C.

Àõ, subtraction, 1=Àõ, division

 Let Àõ denote the additive inverse of Àõ. Thus Àõ is the unique complex number such that
Àõ C . Àõ/ D 0:

 Subtraction on C is deÔ¨Åned by Àá Àõ D Àá C . Àõ/:

 For Àõ ¬§ 0, let 1=Àõ denote the multiplicative inverse of Àõ. Thus 1=Àõ is the unique complex number such that
Àõ.1=Àõ/ D 1:

 Division on C is deÔ¨Åned by Àá=Àõ D Àá.1=Àõ/:

So that we can conveniently make deÔ¨Ånitions and prove theorems that apply to both real and complex numbers, we adopt the following notation:

1.6 Notation F Throughout this book, F stands for either R or C.

The letter F is used because R and C are examples of what are called Ô¨Åelds.

Thus if we prove a theorem involving F, we will know that it holds when F is replaced with R and when F is replaced

with C.

Elements of F are called scalars. The word ‚Äúscalar‚Äù, a fancy word for

‚Äúnumber‚Äù, is often used when we want to emphasize that an object is a number,

as opposed to a vector (vectors will be deÔ¨Åned soon). For Àõ 2 F and m a positive integer, we deÔ¨Åne Àõm to denote the product of

Àõ with itself m times:

Àõm D ‚ÄûÀõ∆í ‚Äö ‚Ä¶Àõ :

m times

Clearly .Àõm/n D Àõmn and .ÀõÀá/m D ÀõmÀám for all Àõ; Àá 2 F and all positive

integers m; n.

SECTION 1.A Rn and Cn

5

Lists
Before deÔ¨Åning Rn and Cn, we look at two important examples.

1.7 Example R2 and R3
 The set R2, which you can think of as a plane, is the set of all ordered pairs of real numbers: R2 D f.x; y/ W x; y 2 Rg:
 The set R3, which you can think of as ordinary space, is the set of all ordered triples of real numbers: R3 D f.x; y; z/ W x; y; z 2 Rg:
To generalize R2 and R3 to higher dimensions, we Ô¨Årst need to discuss the concept of lists.

1.8 DeÔ¨Ånition list, length
Suppose n is a nonnegative integer. A list of length n is an ordered collection of n elements (which might be numbers, other lists, or more abstract entities) separated by commas and surrounded by parentheses. A list of length n looks like this:
.x1; : : : ; xn/:
Two lists are equal if and only if they have the same length and the same elements in the same order.
Thus a list of length 2 is an ordered Many mathematicians call a list of pair, and a list of length 3 is an ordered length n an n-tuple. triple.
Sometimes we will use the word list without specifying its length. Remember, however, that by deÔ¨Ånition each list has a Ô¨Ånite length that is a nonnegative integer. Thus an object that looks like
.x1; x2; : : : /;
which might be said to have inÔ¨Ånite length, is not a list. A list of length 0 looks like this: . /. We consider such an object to be a
list so that some of our theorems will not have trivial exceptions. Lists differ from sets in two ways: in lists, order matters and repetitions
have meaning; in sets, order and repetitions are irrelevant.

6 CHAPTER 1 Vector Spaces

1.9 Example lists versus sets
 The lists .3; 5/ and .5; 3/ are not equal, but the sets f3; 5g and f5; 3g are equal.
 The lists .4; 4/ and .4; 4; 4/ are not equal (they do not have the same length), although the sets f4; 4g and f4; 4; 4g both equal the set f4g.

Fn
To deÔ¨Åne the higher-dimensional analogues of R2 and R3, we will simply replace R with F (which equals R or C) and replace theFana 2 or 3 with an arbitrary positive integer. SpeciÔ¨Åcally, Ô¨Åx a positive integer n for the rest of this section.

1.10 DeÔ¨Ånition Fn Fn is the set of all lists of length n of elements of F:
Fn D f.x1; : : : ; xn/ W xj 2 F for j D 1; : : : ; ng: For .x1; : : : ; xn/ 2 Fn and j 2 f1; : : : ; ng, we say that xj is the j th coordinate of .x1; : : : ; xn/.

If F D R and n equals 2 or 3, then this deÔ¨Ånition of Fn agrees with our previous notions of R2 and R3.

1.11 Example C4 is the set of all lists of four complex numbers: C4 D f.z1; z2; z3; z4/ W z1; z2; z3; z4 2 Cg:

For an amusing account of how R3 would be perceived by creatures living in R2, read Flatland: A Romance of Many Dimensions,
by Edwin A. Abbott. This novel,
published in 1884, may help you
imagine a physical space of four or
more dimensions.

If n  4, we cannot visualize Rn as a physical object. Similarly, C1 can
be thought of as a plane, but for n  2,
the human brain cannot provide a full image of Cn. However, even if n is
large, we can perform algebraic manipulations in Fn as easily as in R2 or R3. For example, addition in Fn is deÔ¨Åned
as follows:

SECTION 1.A Rn and Cn

7

1.12 DeÔ¨Ånition addition in Fn Addition in Fn is deÔ¨Åned by adding corresponding coordinates:
.x1; : : : ; xn/ C .y1; : : : ; yn/ D .x1 C y1; : : : ; xn C yn/:
Often the mathematics of Fn becomes cleaner if we use a single letter to denote a list of n numbers, without explicitly writing the coordinates. For example, the result below is stated with x and y in Fn even though the proof requires the more cumbersome notation of .x1; : : : ; xn/ and .y1; : : : ; yn/.
1.13 Commutativity of addition in Fn If x; y 2 Fn, then x C y D y C x.

Proof Suppose x D .x1; : : : ; xn/ and y D .y1; : : : ; yn/. Then
x C y D .x1; : : : ; xn/ C .y1; : : : ; yn/ D .x1 C y1; : : : ; xn C yn/ D .y1 C x1; : : : ; yn C xn/ D .y1; : : : ; yn/ C .x1; : : : ; xn/ D y C x;
where the second and fourth equalities above hold because of the deÔ¨Ånition of addition in Fn and the third equality holds because of the usual commutativity of addition in F.
If a single letter is used to denote The symbol means ‚Äúend of the an element of Fn, then the same letter proof‚Äù. with appropriate subscripts is often used when coordinates must be displayed. For example, if x 2 Fn, then letting x equal .x1; : : : ; xn/ is good notation, as shown in the proof above. Even better, work with just x and avoid explicit coordinates when possible.

1.14 DeÔ¨Ånition 0 Let 0 denote the list of length n whose coordinates are all 0:
0 D .0; : : : ; 0/:

8 CHAPTER 1 Vector Spaces

Here we are using the symbol 0 in two different ways‚Äîon the left side of the equation in 1.14, the symbol 0 denotes a list of length n, whereas on the right side, each 0 denotes a number. This potentially confusing practice actually causes no problems because the context always makes clear what is intended.
1.15 Example Consider the statement that 0 is an additive identity for Fn: x C 0 D x for all x 2 Fn:
Is the 0 above the number 0 or the list 0?
Solution Here 0 is a list, because we have not deÔ¨Åned the sum of an element of Fn (namely, x) and the number 0.

x1 , x2 x
Elements of R2 can be thought of as points or as vectors.
x x
A vector.
Mathematical models of the economy can have thousands of variables, say x1; : : : ; x5000, which means that we must operate in R5000. Such a space cannot be dealt with geometrically. However, the algebraic approach works well. Thus our subject is called linear algebra.

A picture can aid our intuition. We will draw pictures in R2 because we can sketch this space on 2-dimensional surfaces such as paper and blackboards. A typical element of R2 is a point x D .x1; x2/. Sometimes we think of x not as a point but as an arrow starting at the origin and ending at .x1; x2/, as shown here. When we think of x as an arrow, we refer to it as a vector.
When we think of vectors in R2 as arrows, we can move an arrow parallel to itself (not changing its length or direction) and still think of it as the same vector. With that viewpoint, you will often gain better understanding by dispensing with the coordinate axes and the explicit coordinates and just thinking of the vector, as shown here.
Whenever we use pictures in R2 or use the somewhat vague language of points and vectors, remember that these are just aids to our understanding, not substitutes for the actual mathematics that we will develop. Although we cannot draw good pictures in highdimensional spaces, the elements of these spaces are as rigorously deÔ¨Åned as elements of R2.

SECTION 1.A Rn and Cn

9

For example, .2;

3;

17;

;

p 2/

is

an

element

of

R5

,

and

we

may

casually

refer to it as a point in R5 or a vector in R5 without worrying about whether

the geometry of R5 has any physical meaning.

Recall that we deÔ¨Åned the sum of two elements of Fn to be the element of

Fn obtained by adding corresponding coordinates; see 1.12. As we will now

see, addition has a simple geometric interpretation in the special case of R2.

Suppose we have two vectors x and

y in R2 that we want to add. Move

y

the vector y parallel to itself so that its

initial point coincides with the end point x y

x

of the vector x, as shown here. The

sum x C y then equals the vector whose

initial point equals the initial point of

x and whose end point equals the end

The sum of two vectors.

point of the vector y, as shown here.

In the next deÔ¨Ånition, the 0 on the right side of the displayed equation below is the list 0 2 Fn.

1.16 DeÔ¨Ånition additive inverse in Fn
For x 2 Fn, the additive inverse of x, denoted such that
x C . x/ D 0:

x, is the vector

x 2 Fn

In other words, if x D .x1; : : : ; xn/, then x D . x1; : : : ; xn/.

For a vector x 2 R2, the additive in-

verse x is the vector parallel to x and

x

with the same length as x but pointing in

x

the opposite direction. The Ô¨Ågure here

illustrates this way of thinking about the additive inverse in R2.
Having dealt with addition in Fn, we A vector and its additive inverse.

now turn to multiplication. We could deÔ¨Åne a multiplication in Fn in a similar fashion, starting with two elements of Fn and getting another element of Fn by multiplying corresponding coor-

dinates. Experience shows that this deÔ¨Ånition is not useful for our purposes.

Another type of multiplication, called scalar multiplication, will be central

to our subject. SpeciÔ¨Åcally, we need to deÔ¨Åne what it means to multiply an element of Fn by an element of F.

10 CHAPTER 1 Vector Spaces

1.17 DeÔ¨Ånition scalar multiplication in Fn The product of a number  and a vector in Fn is computed by multiplying each coordinate of the vector by :
.x1; : : : ; xn/ D .x1; : : : ; xn/I here  2 F and .x1; : : : ; xn/ 2 Fn.

In scalar multiplication, we multiply together a scalar and a vector, getting a vector. You may be familiar with the dot product in R2 or R3, in which we multiply together two vectors and get a scalar. Generalizations of the dot product will become important when we study inner products in Chapter 6.
32 x x
12 x
Scalar multiplication.

Scalar multiplication has a nice geometric interpretation in R2. If  is a positive number and x is a vector in R2, then x is the vector that points in the same direction as x and whose length is  times the length of x. In other words, to get x, we shrink or stretch x by a factor of , depending on whether  < 1 or  > 1.
If  is a negative number and x is a vector in R2, then x is the vector that
points in the direction opposite to that of x and whose length is jj times the length of x, as shown here.

Digression on Fields
A Ô¨Åeld is a set containing at least two distinct elements called 0 and 1, along with operations of addition and multiplication satisfying all the properties listed in 1.3. Thus R and C are Ô¨Åelds, as is the set of rational numbers along with the usual operations of addition and multiplication. Another example of a Ô¨Åeld is the set f0; 1g with the usual operations of addition and multiplication except that 1 C 1 is deÔ¨Åned to equal 0.
In this book we will not need to deal with Ô¨Åelds other than R and C. However, many of the deÔ¨Ånitions, theorems, and proofs in linear algebra that work for both R and C also work without change for arbitrary Ô¨Åelds. If you prefer to do so, throughout Chapters 1, 2, and 3 you can think of F as denoting an arbitrary Ô¨Åeld instead of R or C, except that some of the examples and exercises require that for each positive integer n we have ‚Äû1 C 1 C∆í‚Äö   C‚Ä¶1 ¬§ 0.
n times

EXERCISES 1.A

SECTION 1.A Rn and Cn

11

1 Suppose a and b are real numbers, not both 0. Find real numbers c and d such that 1=.a C bi/ D c C d i:

2 Show that

p 1 C 3i

2

is a cube root of 1 (meaning that its cube equals 1).

3 Find two distinct square roots of i .

4 Show that Àõ C Àá D Àá C Àõ for all Àõ; Àá 2 C.

5 Show that .Àõ C Àá/ C  D Àõ C .Àá C / for all Àõ; Àá;  2 C. 6 Show that .ÀõÀá/ D Àõ.Àá/ for all Àõ; Àá;  2 C.

7 Show that for every Àõ 2 C, there exists a unique Àá 2 C such that Àõ C Àá D 0.

8 Show that for every Àõ 2 C with Àõ ¬§ 0, there exists a unique Àá 2 C such that ÀõÀá D 1.
9 Show that .Àõ C Àá/ D Àõ C Àá for all ; Àõ; Àá 2 C. 10 Find x 2 R4 such that

.4; 3; 1; 7/ C 2x D .5; 9; 6; 8/:

11 Explain why there does not exist  2 C such that .2 3i; 5 C 4i; 6 C 7i/ D .12 5i; 7 C 22i; 32 9i /:

12 Show that .x C y/ C z D x C .y C z/ for all x; y; z 2 Fn. 13 Show that .ab/x D a.bx/ for all x 2 Fn and all a; b 2 F. 14 Show that 1x D x for all x 2 Fn. 15 Show that .x C y/ D x C y for all  2 F and all x; y 2 Fn. 16 Show that .a C b/x D ax C bx for all a; b 2 F and all x 2 Fn.

12 CHAPTER 1 Vector Spaces
1.B DeÔ¨Ånition of Vector Space
The motivation for the deÔ¨Ånition of a vector space comes from properties of addition and scalar multiplication in Fn: Addition is commutative, associative, and has an identity. Every element has an additive inverse. Scalar multiplication is associative. Scalar multiplication by 1 acts as expected. Addition and scalar multiplication are connected by distributive properties.
We will deÔ¨Åne a vector space to be a set V with an addition and a scalar multiplication on V that satisfy the properties in the paragraph above.
1.18 DeÔ¨Ånition addition, scalar multiplication
 An addition on a set V is a function that assigns an element uCv 2 V to each pair of elements u; v 2 V.
 A scalar multiplication on a set V is a function that assigns an element v 2 V to each  2 F and each v 2 V.
Now we are ready to give the formal deÔ¨Ånition of a vector space.
1.19 DeÔ¨Ånition vector space A vector space is a set V along with an addition on V and a scalar multiplication on V such that the following properties hold:
commutativity u C v D v C u for all u; v 2 V ;
associativity .u C v/ C w D u C .v C w/ and .ab/v D a.bv/ for all u; v; w 2 V and all a; b 2 F;
additive identity there exists an element 0 2 V such that v C 0 D v for all v 2 V ;
additive inverse for every v 2 V, there exists w 2 V such that v C w D 0;
multiplicative identity 1v D v for all v 2 V ;
distributive properties a.u C v/ D au C av and .a C b/v D av C bv for all a; b 2 F and all u; v 2 V.

SECTION 1.B DeÔ¨Ånition of Vector Space 13 The following geometric language sometimes aids our intuition.

1.20 DeÔ¨Ånition vector, point Elements of a vector space are called vectors or points.

The scalar multiplication in a vector space depends on F. Thus when we
need to be precise, we will say that V is a vector space over F instead of saying simply that V is a vector space. For example, Rn is a vector space over R, and Cn is a vector space over C.

1.21 DeÔ¨Ånition real vector space, complex vector space  A vector space over R is called a real vector space.  A vector space over C is called a complex vector space.

Usually the choice of F is either obvious from the context or irrelevant. Thus we often assume that F is lurking in the background without speciÔ¨Åcally
mentioning it.
With the usual operations of addition The simplest vector space contains and scalar multiplication, Fn is a vector only one point. In other words, f0g space over F, as you should verify. The is a vector space. example of Fn motivated our deÔ¨Ånition
of vector space.

1.22 Example F1 is deÔ¨Åned to be the set of all sequences of elements

of F:

F1 D f.x1; x2; : : : / W xj 2 F for j D 1; 2; : : : g:

Addition and scalar multiplication on F1 are deÔ¨Åned as expected:

.x1; x2; : : : / C .y1; y2; : : : / D .x1 C y1; x2 C y2; : : : /; .x1; x2; : : : / D .x1; x2; : : : /:
With these deÔ¨Ånitions, F1 becomes a vector space over F, as you should verify. The additive identity in this vector space is the sequence of all 0‚Äôs.

Our next example of a vector space involves a set of functions.

14 CHAPTER 1 Vector Spaces

1.23 Notation FS
 If S is a set, then FS denotes the set of functions from S to F.  For f; g 2 FS , the sum f C g 2 FS is the function deÔ¨Åned by
.f C g/.x/ D f .x/ C g.x/ for all x 2 S .  For  2 F and f 2 FS , the product f 2 FS is the function deÔ¨Åned by
.f /.x/ D f .x/ for all x 2 S.

As an example of the notation above, if S is the interval ≈í0; 1¬ç and F D R, then R≈í0;1¬ç is the set of real-valued functions on the interval ≈í0; 1¬ç.
You should verify all three bullet points in the next example.

1.24 Example FS is a vector space  If S is a nonempty set, then FS (with the operations of addition and scalar multiplication as deÔ¨Åned above) is a vector space over F.
 The additive identity of FS is the function 0 W S ! F deÔ¨Åned by 0.x/ D 0
for all x 2 S.
 For f 2 FS , the additive inverse of f is the function f W S ! F deÔ¨Åned by . f /.x/ D f .x/ for all x 2 S.

The elements of the vector space R≈í0;1¬ç are real-valued functions on
≈í0; 1¬ç, not lists. In general, a vector

Our previous examples of vector spaces, Fn and F1, are special cases of the vector space FS because a list of

space is an abstract entity whose elements might be lists, functions, or weird objects.

length n of numbers in F can be thought of as a function from f1; 2; : : : ; ng to F and a sequence of numbers in F can be

thought of as a function from the set of positive integers to F. In other words, we can think of Fn as Ff1;2;:::;ng and we can think of F1 as Ff1;2;::: g.

SECTION 1.B DeÔ¨Ånition of Vector Space 15

Soon we will see further examples of vector spaces, but Ô¨Årst we need to develop some of the elementary properties of vector spaces.
The deÔ¨Ånition of a vector space requires that it have an additive identity. The result below states that this identity is unique.

1.25 Unique additive identity A vector space has a unique additive identity.

Proof Suppose 0 and 00 are both additive identities for some vector space V.

Then

00 D 00 C 0 D 0 C 00 D 0;

where the Ô¨Årst equality holds because 0 is an additive identity, the second equality comes from commutativity, and the third equality holds because 00 is an additive identity. Thus 00 D 0, proving that V has only one additive
identity.

Each element v in a vector space has an additive inverse, an element w in the vector space such that v C w D 0. The next result shows that each element in a vector space has only one additive inverse.

1.26 Unique additive inverse Every element in a vector space has a unique additive inverse.

Proof Suppose V is a vector space. Let v 2 V. Suppose w and w0 are additive inverses of v. Then
w D w C 0 D w C .v C w0/ D .w C v/ C w0 D 0 C w0 D w0: Thus w D w0, as desired.
Because additive inverses are unique, the following notation now makes sense.
1.27 Notation v, w v Let v; w 2 V. Then
 v denotes the additive inverse of v;  w v is deÔ¨Åned to be w C . v/.

16 CHAPTER 1 Vector Spaces

Almost all the results in this book involve some vector space. To avoid having to restate frequently that V is a vector space, we now make the necessary declaration once and for all:

1.28 Notation V For the rest of the book, V denotes a vector space over F.

In the next result, 0 denotes a scalar (the number 0 2 F) on the left side of the equation and a vector (the additive identity of V ) on the right side of the equation.

1.29 The number 0 times a vector 0v D 0 for every v 2 V.

Note that 1.29 asserts something Proof For v 2 V, we have

about scalar multiplication and the additive identity of V. The only

0v D .0 C 0/v D 0v C 0v:

part of the deÔ¨Ånition of a vector space that connects scalar multiplication and vector addition is the distributive property. Thus the dis-

Adding the additive inverse of 0v to both sides of the equation above gives 0 D 0v, as desired.

tributive property must be used in the proof of 1.29.

In the next result, 0 denotes the additive identity of V. Although their proofs

are similar, 1.29 and 1.30 are not identical. More precisely, 1.29 states that

the product of the scalar 0 and any vector equals the vector 0, whereas 1.30

states that the product of any scalar and the vector 0 equals the vector 0.

1.30 A number times the vector 0 a0 D 0 for every a 2 F.

Proof For a 2 F, we have
a0 D a.0 C 0/ D a0 C a0:
Adding the additive inverse of a0 to both sides of the equation above gives 0 D a0, as desired.
Now we show that if an element of V is multiplied by the scalar 1, then the result is the additive inverse of the element of V.

SECTION 1.B DeÔ¨Ånition of Vector Space 17
1.31 The number 1 times a vector . 1/v D v for every v 2 V. Proof For v 2 V, we have
 v C . 1/v D 1v C . 1/v D 1 C . 1/ v D 0v D 0: This equation says that . 1/v, when added to v, gives 0. Thus . 1/v is the additive inverse of v, as desired.

EXERCISES 1.B

1 Prove that . v/ D v for every v 2 V.

2 Suppose a 2 F, v 2 V, and av D 0. Prove that a D 0 or v D 0.

3 Suppose v; w 2 V. Explain why there exists a unique x 2 V such that v C 3x D w.

4 The empty set is not a vector space. The empty set fails to satisfy only one of the requirements listed in 1.19. Which one?

5 Show that in the deÔ¨Ånition of a vector space (1.19), the additive inverse condition can be replaced with the condition that
0v D 0 for all v 2 V:
Here the 0 on the left side is the number 0, and the 0 on the right side is the additive identity of V. (The phrase ‚Äúa condition can be replaced‚Äù in a deÔ¨Ånition means that the collection of objects satisfying the deÔ¨Ånition is unchanged if the original condition is replaced with the new condition.)

6 Let 1 and 1 denote two distinct objects, neither of which is in R.

DeÔ¨Åne an addition and scalar multiplication on R [ f1g [ f 1g as you

could guess from the notation. SpeciÔ¨Åcally, the sum and product of two

real numbers is as usual, and for t 2 R deÔ¨Åne

8

8

ÀÜ< 1 if t < 0;

ÀÜ<1

if t < 0;

t1 D ÀÜ:1 0

if t D 0; if t > 0;

t . 1/ D ÀÜ:0 1

if t D 0; if t > 0;

t C 1 D 1 C t D 1;

t C . 1/ D . 1/ C t D 1;

1 C 1 D 1; . 1/ C . 1/ D 1; 1 C . 1/ D 0:

Is R [ f1g [ f 1g a vector space over R? Explain.

18 CHAPTER 1 Vector Spaces
1.C Subspaces
By considering subspaces, we can greatly expand our examples of vector spaces.
1.32 DeÔ¨Ånition subspace A subset U of V is called a subspace of V if U is also a vector space (using the same addition and scalar multiplication as on V ).

1.33 Example f.x1; x2; 0/ W x1; x2 2 Fg is a subspace of F3.

Some mathematicians use the term linear subspace, which means the same as subspace.

The next result gives the easiest way to check whether a subset of a vector space is a subspace.

1.34 Conditions for a subspace A subset U of V is a subspace of V if and only if U satisÔ¨Åes the following three conditions:
additive identity 02U
closed under addition u; w 2 U implies u C w 2 U ;
closed under scalar multiplication a 2 F and u 2 U implies au 2 U.

The additive identity condition above could be replaced with the condition that U is nonempty (then taking u 2 U, multiplying it by 0, and using the condition that U is closed under scalar multiplication would imply that 0 2 U ). However, if U is indeed a subspace of V, then the easiest way to show that U is nonempty is to show that 0 2 U.

Proof If U is a subspace of V, then U satisÔ¨Åes the three conditions above by the deÔ¨Ånition of vector space.
Conversely, suppose U satisÔ¨Åes the three conditions above. The Ô¨Årst condition above ensures that the additive identity of V is in U.
The second condition above ensures that addition makes sense on U. The third condition ensures that scalar multiplication makes sense on U.

SECTION 1.C Subspaces 19

If u 2 U, then u [which equals . 1/u by 1.31] is also in U by the third condition above. Hence every element of U has an additive inverse in U.
The other parts of the deÔ¨Ånition of a vector space, such as associativity and commutativity, are automatically satisÔ¨Åed for U because they hold on the larger space V. Thus U is a vector space and hence is a subspace of V.
The three conditions in the result above usually enable us to determine quickly whether a given subset of V is a subspace of V. You should verify all the assertions in the next example.

1.35 Example subspaces
(a) If b 2 F, then f.x1; x2; x3; x4/ 2 F4 W x3 D 5x4 C bg
is a subspace of F4 if and only if b D 0.
(b) The set of continuous real-valued functions on the interval ≈í0; 1¬ç is a subspace of R≈í0;1¬ç.
(c) The set of differentiable real-valued functions on R is a subspace of RR.
(d) The set of differentiable real-valued functions f on the interval .0; 3/ such that f 0.2/ D b is a subspace of R.0;3/ if and only if b D 0.
(e) The set of all sequences of complex numbers with limit 0 is a subspace of C1.

Verifying some of the items above Clearly f0g is the smallest subshows the linear structure underlying space of V and V itself is the

parts of calculus. For example, the sec- largest subspace of V. The empty

ond item above requires the result that set is not a subspace of V because

the sum of two continuous functions is continuous. As another example, the fourth item above requires the result that for a constant c, the derivative of

a subspace must be a vector space and hence must contain at least one element, namely, an additive identity.

cf equals c times the derivative of f .
The subspaces of R2 are precisely f0g, R2, and all lines in R2 through the origin. The subspaces of R3 are precisely f0g, R3, all lines in R3 through the origin, and all planes in R3 through the origin. To prove that all these objects

are indeed subspaces is easy‚Äîthe hard part is to show that they are the only subspaces of R2 and R3. That task will be easier after we introduce some

additional tools in the next chapter.

20 CHAPTER 1 Vector Spaces

Sums of Subspaces
The union of subspaces is rarely a subspace (see Exercise 12), which is why we usually work with sums rather than unions.

When dealing with vector spaces, we are usually interested only in subspaces, as opposed to arbitrary subsets. The notion of the sum of subspaces will be useful.

1.36 DeÔ¨Ånition sum of subsets
Suppose U1; : : : ; Um are subsets of V. The sum of U1; : : : ; Um, denoted U1 C    C Um, is the set of all possible sums of elements of U1; : : : ; Um. More precisely,
U1 C    C Um D fu1 C    C um W u1 2 U1; : : : ; um 2 Umg:

Let‚Äôs look at some examples of sums of subspaces.

1.37 Example Suppose U is the set of all elements of F3 whose second and third coordinates equal 0, and W is the set of all elements of F3 whose Ô¨Årst and third coordinates equal 0:
U D f.x; 0; 0/ 2 F3 W x 2 Fg and W D f.0; y; 0/ 2 F3 W y 2 Fg:

Then

U C W D f.x; y; 0/ W x; y 2 Fg;

as you should verify.

1.38 Example Suppose that U D f.x; x; y; y/ 2 F4 W x; y 2 Fg and W D f.x; x; x; y/ 2 F4 W x; y 2 Fg. Then
U C W D f.x; x; y; z/ 2 F4 W x; y; z 2 Fg; as you should verify.
The next result states that the sum of subspaces is a subspace, and is in fact the smallest subspace containing all the summands.
1.39 Sum of subspaces is the smallest containing subspace Suppose U1; : : : ; Um are subspaces of V. Then U1 C    C Um is the smallest subspace of V containing U1; : : : ; Um.

SECTION 1.C Subspaces 21

Proof It is easy to see that 0 2 U1 C    C Um and that U1 C    C Um

is closed under addition and scalar multiplication. Thus 1.34 implies that

U1 C    C Um is a subspace of V.

Clearly U1; : : : ; Um are all con- Sums of subspaces in the theory tained in U1 C    C Um (to see this, of vector spaces are analogous

consider sums u1 C    C um where to unions of subsets in set theory.

all except one of the u‚Äôs are 0). Con- Given two subspaces of a vector

versely, every subspace of V containing U1; : : : ; Um contains U1 C  CUm (because subspaces must contain all Ô¨Ånite sums of their elements). Thus U1 C    C Um is the smallest subspace

space, the smallest subspace containing them is their sum. Analogously, given two subsets of a set, the smallest subset containing them is their union.

of V containing U1; : : : ; Um.

Direct Sums
Suppose U1; : : : ; Um are subspaces of V. Every element of U1 C    C Um can be written in the form
u1 C    C um; where each uj is in Uj . We will be especially interested in cases where each vector in U1 C    C Um can be represented in the form above in only one way. This situation is so important that we give it a special name: direct sum.
1.40 DeÔ¨Ånition direct sum
Suppose U1; : : : ; Um are subspaces of V.
 The sum U1 C    C Um is called a direct sum if each element of U1 C    C Um can be written in only one way as a sum u1 C    C um, where each uj is in Uj .
 If U1 C    C Um is a direct sum, then U1 Àö    Àö Um denotes U1 C    C Um, with the Àö notation serving as an indication that this is a direct sum.

1.41 Example Suppose U is the subspace of F3 of those vectors whose last coordinate equals 0, and W is the subspace of F3 of those vectors whose
Ô¨Årst two coordinates equal 0: U D f.x; y; 0/ 2 F3 W x; y 2 Fg and W D f.0; 0; z/ 2 F3 W z 2 Fg:
Then F3 D U Àö W, as you should verify.

 7 CHAPTER 1 Vector Spaces

1.42 Example Suppose Uj is the subspace of Fn of those vectors whose coordinates are all 0, except possibly in the j th slot (thus, for example, U2 D f.0; x; 0; : : : ; 0/ 2 Fn W x 2 Fg). Then
Fn D U1 Àö    Àö Un;
as you should verify.

Sometimes nonexamples add to our understanding as much as examples.

1.43 Example Let U1 D f.x; y; 0/ 2 F3 W x; y 2 Fg; U2 D f.0; 0; z/ 2 F3 W z 2 Fg; U3 D f.0; y; y/ 2 F3 W y 2 Fg:
Show that U1 C U2 C U3 is not a direct sum.
Solution Clearly F3 D U1 C U2 C U3, because every vector .x; y; z/ 2 F3 can be written as
.x; y; z/ D .x; y; 0/ C .0; 0; z/ C .0; 0; 0/;
where the Ô¨Årst vector on the right side is in U1, the second vector is in U2, and the third vector is in U3.
However, F3 does not equal the direct sum of U1; U2; U3, because the vector .0; 0; 0/ can be written in two different ways as a sum u1 C u2 C u3, with each uj in Uj . SpeciÔ¨Åcally, we have
.0; 0; 0/ D .0; 1; 0/ C .0; 0; 1/ C .0; 1; 1/
and, of course,
.0; 0; 0/ D .0; 0; 0/ C .0; 0; 0/ C .0; 0; 0/;
where the Ô¨Årst vector on the right side of each equation above is in U1, the second vector is in U2, and the third vector is in U3.

The symbol Àö, which is a plus sign inside a circle, serves as a reminder that we are dealing with a special type of sum of subspaces‚Äî each element in the direct sum can be represented only one way as a sum of elements from the speciÔ¨Åed subspaces.

The deÔ¨Ånition of direct sum requires that every vector in the sum have a unique representation as an appropriate sum. The next result shows that when deciding whether a sum of subspaces is a direct sum, we need only consider whether 0 can be uniquely written as an appropriate sum.

SECTION 1.C Subspaces 23
1.44 Condition for a direct sum
Suppose U1; : : : ; Um are subspaces of V. Then U1 C    C Um is a direct sum if and only if the only way to write 0 as a sum u1 C    C um, where each uj is in Uj , is by taking each uj equal to 0.
Proof First suppose U1 C    C Um is a direct sum. Then the deÔ¨Ånition of direct sum implies that the only way to write 0 as a sum u1 C    C um, where each uj is in Uj , is by taking each uj equal to 0.
Now suppose that the only way to write 0 as a sum u1 C    C um, where each uj is in Uj , is by taking each uj equal to 0. To show that U1 C    C Um is a direct sum, let v 2 U1 C    C Um. We can write
v D u1 C    C um for some u1 2 U1; : : : ; um 2 Um. To show that this representation is unique, suppose we also have
v D v1 C    C vm; where v1 2 U1; : : : ; vm 2 Um. Subtracting these two equations, we have
0 D .u1 v1/ C    C .um vm/: Because u1 v1 2 U1; : : : ; um vm 2 Um, the equation above implies that each uj vj equals 0. Thus u1 D v1; : : : ; um D vm, as desired.
The next result gives a simple condition for testing which pairs of subspaces give a direct sum.
1.45 Direct sum of two subspaces
Suppose U and W are subspaces of V. Then U C W is a direct sum if and only if U \ W D f0g.
Proof First suppose that U C W is a direct sum. If v 2 U \ W, then 0 D v C . v/, where v 2 U and v 2 W. By the unique representation of 0 as the sum of a vector in U and a vector in W, we have v D 0. Thus U \ W D f0g, completing the proof in one direction.
To prove the other direction, now suppose U \ W D f0g. To prove that U C W is a direct sum, suppose u 2 U, w 2 W, and
0 D u C w:
To complete the proof, we need only show that u D w D 0 (by 1.44). The equation above implies that u D w 2 W. Thus u 2 U \ W. Hence u D 0, which by the equation above implies that w D 0, completing the proof.

24 CHAPTER 1 Vector Spaces

Sums of subspaces are analogous to unions of subsets. Similarly, direct sums of subspaces are analogous to disjoint unions of subsets. No two subspaces of a vector space can be disjoint, because both contain 0. So disjointness is replaced, at least in the case of two subspaces, with the requirement that the intersection equals f0g.

The result above deals only with the case of two subspaces. When asking about a possible direct sum with more than two subspaces, it is not enough to test that each pair of the subspaces intersect only at 0. To see this, consider Example 1.43. In that nonexample of a direct sum, we have U1 \ U2 D U1 \ U3 D U2 \ U3 D f0g.

EXERCISES 1.C

1 For each of the following subsets of F3, determine whether it is a subspace of F3:
(a) f.x1; x2; x3/ 2 F3 W x1 C 2x2 C 3x3 D 0g; (b) f.x1; x2; x3/ 2 F3 W x1 C 2x2 C 3x3 D 4g; (c) f.x1; x2; x3/ 2 F3 W x1x2x3 D 0g; (d) f.x1; x2; x3/ 2 F3 W x1 D 5x3g.

2 Verify all the assertions in Example 1.35.

3 Show that the set of differentiable real-valued functions f on the interval . 4; 4/ such that f 0. 1/ D 3f .2/ is a subspace of R. 4;4/.

4

Suppose b 2 R. Show that the on the interval ≈í0; 1¬ç such that

sRe1t
0

of f

continuous real-valued functions f D b is a subspace of R≈í0;1¬ç if and

only if b D 0.

5 Is R2 a subspace of the complex vector space C2?

6 (a) Is f.a; b; c/ 2 R3 W a3 D b3g a subspace of R3? (b) Is f.a; b; c/ 2 C3 W a3 D b3g a subspace of C3?

7 Give an example of a nonempty subset U of R2 such that U is closed
under addition and under taking additive inverses (meaning u 2 U whenever u 2 U ), but U is not a subspace of R2.

8 Give an example of a nonempty subset U of R2 such that U is closed under scalar multiplication, but U is not a subspace of R2.

SECTION 1.C Subspaces 25

9 A function f W R ! R is called periodic if there exists a positive number p such that f .x/ D f .x C p/ for all x 2 R. Is the set of periodic functions from R to R a subspace of RR? Explain.
10 Suppose U1 and U2 are subspaces of V. Prove that the intersection U1 \ U2 is a subspace of V.
11 Prove that the intersection of every collection of subspaces of V is a subspace of V.
12 Prove that the union of two subspaces of V is a subspace of V if and only if one of the subspaces is contained in the other.
13 Prove that the union of three subspaces of V is a subspace of V if and only if one of the subspaces contains the other two. [This exercise is surprisingly harder than the previous exercise, possibly because this exercise is not true if we replace F with a Ô¨Åeld containing only two elements.]
14 Verify the assertion in Example 1.38.
15 Suppose U is a subspace of V. What is U C U ?
16 Is the operation of addition on the subspaces of V commutative? In other words, if U and W are subspaces of V, is U C W D W C U ?
17 Is the operation of addition on the subspaces of V associative? In other words, if U1; U2; U3 are subspaces of V, is
.U1 C U2/ C U3 D U1 C .U2 C U3/‚Äπ

18 Does the operation of addition on the subspaces of V have an additive identity? Which subspaces have additive inverses?

19 Prove or give a counterexample: if U1; U2; W are subspaces of V such that U1 C W D U2 C W;
then U1 D U2.

20 Suppose

U D f.x; x; y; y/ 2 F4 W x; y 2 Fg:

Find a subspace W of F4 such that F4 D U Àö W.

26 CHAPTER 1 Vector Spaces
21 Suppose U D f.x; y; x C y; x y; 2x/ 2 F5 W x; y 2 Fg:
Find a subspace W of F5 such that F5 D U Àö W.
22 Suppose U D f.x; y; x C y; x y; 2x/ 2 F5 W x; y 2 Fg:
Find three subspaces W1; W2; W3 of F5, none of which equals f0g, such that F5 D U Àö W1 Àö W2 Àö W3. 23 Prove or give a counterexample: if U1; U2; W are subspaces of V such that
V D U1 Àö W and V D U2 Àö W; then U1 D U2. 24 A function f W R ! R is called even if
f . x/ D f .x/
for all x 2 R. A function f W R ! R is called odd if
f . x/ D f .x/
for all x 2 R. Let Ue denote the set of real-valued even functions on R and let Uo denote the set of real-valued odd functions on R. Show that RR D Ue Àö Uo.

CHAPTER
2

American mathematician Paul Halmos (1916‚Äì2006), who in 1942 published the Ô¨Årst modern linear algebra book. The title of Halmos‚Äôs book was the same as the title of this chapter.

Finite-Dimensional Vector Spaces

Let‚Äôs review our standing assumptions:
2.1 Notation F, V
 F denotes R or C.  V denotes a vector space over F.
In the last chapter we learned about vector spaces. Linear algebra focuses not on arbitrary vector spaces, but on Ô¨Ånite-dimensional vector spaces, which we introduce in this chapter.
LEARNING OBJECTIVES FOR THIS CHAPTER
span linear independence bases dimension

¬© Springer International Publishing 2015

27

S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics,

DOI 10.1007/978-3-319-11080-6__2

28 CHAPTER 2 Finite-Dimensional Vector Spaces
2.A Span and Linear Independence
We have been writing lists of numbers surrounded by parentheses, and we will continue to do so for elements of Fn; for example, .2; 7; 8/ 2 F3. However, now we need to consider lists of vectors (which may be elements of Fn or of other vector spaces). To avoid confusion, we will usually write lists of vectors without surrounding parentheses. For example, .4; 1; 6/; .9; 5; 7/ is a list of length 2 of vectors in R3.
2.2 Notation list of vectors We will usually write lists of vectors without surrounding parentheses.
Linear Combinations and Span
Adding up scalar multiples of vectors in a list gives what is called a linear combination of the list. Here is the formal deÔ¨Ånition:
2.3 DeÔ¨Ånition linear combination A linear combination of a list v1; : : : ; vm of vectors in V is a vector of the form
a1v1 C    C amvm; where a1; : : : ; am 2 F.
2.4 Example In F3,  .17; 4; 2/ is a linear combination of .2; 1; 3/; .1; 2; 4/ because .17; 4; 2/ D 6.2; 1; 3/ C 5.1; 2; 4/:
 .17; 4; 5/ is not a linear combination of .2; 1; 3/; .1; 2; 4/ because there do not exist numbers a1; a2 2 F such that .17; 4; 5/ D a1.2; 1; 3/ C a2.1; 2; 4/: In other words, the system of equations 17 D 2a1 C a2 4 D a1 2a2 5 D 3a1 C 4a2 has no solutions (as you should verify).

SECTION 2.A Span and Linear Independence 29
2.5 DeÔ¨Ånition span The set of all linear combinations of a list of vectors v1; : : : ; vm in V is called the span of v1; : : : ; vm, denoted span.v1; : : : ; vm/. In other words,
span.v1; : : : ; vm/ D fa1v1 C    C amvm W a1; : : : ; am 2 Fg: The span of the empty list . / is deÔ¨Åned to be f0g.
2.6 Example The previous example shows that in F3, 
 .17; 4; 2/ 2 span .2; 1; 3/; .1; 2; 4/ ; 
 .17; 4; 5/ ‚Ä¶ span .2; 1; 3/; .1; 2; 4/ .
Some mathematicians use the term linear span, which means the same as span.
2.7 Span is the smallest containing subspace The span of a list of vectors in V is the smallest subspace of V containing all the vectors in the list.
Proof Suppose v1; : : : ; vm is a list of vectors in V. First we show that span.v1; : : : ; vm/ is a subspace of V. The additive
identity is in span.v1; : : : ; vm/, because
0 D 0v1 C    C 0vm:
Also, span.v1; : : : ; vm/ is closed under addition, because
.a1v1C  Camvm/C.c1v1C  Ccmvm/ D .a1Cc1/v1C  C.amCcm/vm:
Furthermore, span.v1; : : : ; vm/ is closed under scalar multiplication, because
.a1v1 C    C amvm/ D a1v1 C    C amvm:
Thus span.v1; : : : ; vm/ is a subspace of V (by 1.34). Each vj is a linear combination of v1; : : : ; vm (to show this, set aj D 1
and let the other a‚Äôs in 2.3 equal 0). Thus span.v1; : : : ; vm/ contains each vj . Conversely, because subspaces are closed under scalar multiplication and addition, every subspace of V containing each vj contains span.v1; : : : ; vm/. Thus span.v1; : : : ; vm/ is the smallest subspace of V containing all the vectors v1; : : : ; vm.

30 CHAPTER 2 Finite-Dimensional Vector Spaces
2.8 DeÔ¨Ånition spans If span.v1; : : : ; vm/ equals V, we say that v1; : : : ; vm spans V.

2.9 Example Suppose n is a positive integer. Show that
.1; 0; : : : ; 0/; .0; 1; 0; : : : ; 0/; : : : ; .0; : : : ; 0; 1/ spans Fn. Here the j th vector in the list above is the n-tuple with 1 in the j th slot and 0 in all other slots. Solution Suppose .x1; : : : ; xn/ 2 Fn. Then
.x1; : : : ; xn/ D x1.1; 0; : : : ; 0/ C x2.0; 1; 0; : : : ; 0/ C    C xn.0; : : : ; 0; 1/: 
Thus .x1; : : : ; xn/ 2 span .1; 0; : : : ; 0/; .0; 1; 0; : : : ; 0/; : : : ; .0; : : : ; 0; 1/ , as desired.
Now we can make one of the key deÔ¨Ånitions in linear algebra.

2.10 DeÔ¨Ånition Ô¨Ånite-dimensional vector space
A vector space is called Ô¨Ånite-dimensional if some list of vectors in it spans the space.

Recall that by deÔ¨Ånition every list has Ô¨Ånite length.

Example 2.9 above shows that Fn is a Ô¨Ånite-dimensional vector space for every positive integer n.

The deÔ¨Ånition of a polynomial is no doubt already familiar to you.

2.11 DeÔ¨Ånition polynomial, P.F/
 A function p W F ! F is called a polynomial with coefÔ¨Åcients in F if there exist a0; : : : ; am 2 F such that p.z/ D a0 C a1z C a2z2 C    C amzm for all z 2 F.
 P.F/ is the set of all polynomials with coefÔ¨Åcients in F.

SECTION 2.A Span and Linear Independence 31
With the usual operations of addition and scalar multiplication, P.F/ is a vector space over F, as you should verify. In other words, P.F/ is a subspace of FF, the vector space of functions from F to F.
If a polynomial (thought of as a function from F to F) is represented by two sets of coefÔ¨Åcients, then subtracting one representation of the polynomial from the other produces a polynomial that is identically zero as a function on F and hence has all zero coefÔ¨Åcients (if you are unfamiliar with this fact, just believe it for now; we will prove it later‚Äîsee 4.7). Conclusion: the coefÔ¨Åcients of a polynomial are uniquely determined by the polynomial. Thus the next deÔ¨Ånition uniquely deÔ¨Ånes the degree of a polynomial.
2.12 DeÔ¨Ånition degree of a polynomial, deg p
 A polynomial p 2 P.F/ is said to have degree m if there exist scalars a0; a1; : : : ; am 2 F with am ¬§ 0 such that p.z/ D a0 C a1z C    C amzm for all z 2 F. If p has degree m, we write deg p D m.
 The polynomial that is identically 0 is said to have degree 1.
In the next deÔ¨Ånition, we use the convention that 1 < m, which means that the polynomial 0 is in Pm.F/.
2.13 DeÔ¨Ånition Pm.F/ For m a nonnegative integer, Pm.F/ denotes the set of all polynomials with coefÔ¨Åcients in F and degree at most m.
To verify the next example, note that Pm.F/ D span.1; z; : : : ; zm/; here we are slightly abusing notation by letting zk denote a function.
2.14 Example Pm.F/ is a Ô¨Ånite-dimensional vector space for each nonnegative integer m.
2.15 DeÔ¨Ånition inÔ¨Ånite-dimensional vector space A vector space is called inÔ¨Ånite-dimensional if it is not Ô¨Ånite-dimensional.

32 CHAPTER 2 Finite-Dimensional Vector Spaces
2.16 Example Show that P.F/ is inÔ¨Ånite-dimensional.
Solution Consider any list of elements of P.F/. Let m denote the highest degree of the polynomials in this list. Then every polynomial in the span of this list has degree at most m. Thus zmC1 is not in the span of our list. Hence no list spans P.F/. Thus P.F/ is inÔ¨Ånite-dimensional.
Linear Independence
Suppose v1; : : : ; vm 2 V and v 2 span.v1; : : : ; vm/. By the deÔ¨Ånition of span, there exist a1; : : : ; am 2 F such that
v D a1v1 C    C amvm:
Consider the question of whether the choice of scalars in the equation above is unique. Suppose c1; : : : ; cm is another set of scalars such that
v D c1v1 C    C cmvm:
Subtracting the last two equations, we have
0 D .a1 c1/v1 C    C .am cm/vm:
Thus we have written 0 as a linear combination of .v1; : : : ; vm/. If the only way to do this is the obvious way (using 0 for all scalars), then each aj cj equals 0, which means that each aj equals cj (and thus the choice of scalars was indeed unique). This situation is so important that we give it a special name‚Äîlinear independence‚Äîwhich we now deÔ¨Åne.
2.17 DeÔ¨Ånition linearly independent
 A list v1; : : : ; vm of vectors in V is called linearly independent if the only choice of a1; : : : ; am 2 F that makes a1v1 C    C amvm equal 0 is a1 D    D am D 0.
 The empty list . / is also declared to be linearly independent.
The reasoning above shows that v1; : : : ; vm is linearly independent if and only if each vector in span.v1; : : : ; vm/ has only one representation as a linear combination of v1; : : : ; vm.

SECTION 2.A Span and Linear Independence 33
2.18 Example linearly independent lists
(a) A list v of one vector v 2 V is linearly independent if and only if v ¬§ 0. (b) A list of two vectors in V is linearly independent if and only if neither
vector is a scalar multiple of the other. (c) .1; 0; 0; 0/; .0; 1; 0; 0/; .0; 0; 1; 0/ is linearly independent in F4. (d) The list 1; z; : : : ; zm is linearly independent in P.F/ for each nonnega-
tive integer m.
If some vectors are removed from a linearly independent list, the remaining list is also linearly independent, as you should verify.
2.19 DeÔ¨Ånition linearly dependent
 A list of vectors in V is called linearly dependent if it is not linearly independent.
 In other words, a list v1; : : : ; vm of vectors in V is linearly dependent if there exist a1; : : : ; am 2 F, not all 0, such that a1v1 C    C amvm D 0.
2.20 Example linearly dependent lists  .2; 3; 1/; .1; 1; 2/; .7; 3; 8/ is linearly dependent in F3 because 2.2; 3; 1/ C 3.1; 1; 2/ C . 1/.7; 3; 8/ D .0; 0; 0/:  The list .2; 3; 1/; .1; 1; 2/; .7; 3; c/ is linearly dependent in F3 if and only if c D 8, as you should verify.  If some vector in a list of vectors in V is a linear combination of the other vectors, then the list is linearly dependent. (Proof: After writing one vector in the list as equal to a linear combination of the other vectors, move that vector to the other side of the equation, where it will be multiplied by 1.)  Every list of vectors in V containing the 0 vector is linearly dependent. (This is a special case of the previous bullet point.)

34 CHAPTER 2 Finite-Dimensional Vector Spaces

The lemma below will often be useful. It states that given a linearly dependent list of vectors, one of the vectors is in the span of the previous ones and furthermore we can throw out that vector without changing the span of the original list.

2.21 Linear Dependence Lemma
Suppose v1; : : : ; vm is a linearly dependent list in V. Then there exists j 2 f1; 2; : : : ; mg such that the following hold:

(a) vj 2 span.v1; : : : ; vj 1/;
(b) if the j th term is removed from v1; : : : ; vm, the span of the remaining list equals span.v1; : : : ; vm/.

Proof Because the list v1; : : : ; vm is linearly dependent, there exist numbers a1; : : : ; am 2 F, not all 0, such that

a1v1 C    C amvm D 0:

Let j be the largest element of f1; : : : ; mg such that aj ¬§ 0. Then

2.22

vj D

a1 aj

v1



aj aj

1

vj

1;

proving (a). To prove (b), suppose u 2 span.v1; : : : ; vm/. Then there exist numbers
c1; : : : ; cm 2 F such that

u D c1v1 C    C cmvm:

In the equation above, we can replace vj with the right side of 2.22, which shows that u is in the span of the list obtained by removing the j th term from
v1; : : : ; vm. Thus (b) holds.

Choosing j D 1 in the Linear Dependence Lemma above means that v1 D 0, because if j D 1 then condition (a) above is interpreted to mean that v1 2 span. /; recall that span. / D f0g. Note also that the proof of part (b) above needs to be modiÔ¨Åed in an obvious way if v1 D 0 and j D 1.
In general, the proofs in the rest of the book will not call attention to special cases that must be considered involving empty lists, lists of length 1, the subspace f0g, or other trivial cases for which the result is clearly true but needs a slightly different proof. Be sure to check these special cases yourself.
Now we come to a key result. It says that no linearly independent list in V is longer than a spanning list in V.

SECTION 2.A Span and Linear Independence 35
2.23 Length of linearly independent list  length of spanning list
In a Ô¨Ånite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors.
Proof Suppose u1; : : : ; um is linearly independent in V. Suppose also that w1; : : : ; wn spans V. We need to prove that m  n. We do so through the multi-step process described below; note that in each step we add one of the u‚Äôs and remove one of the w‚Äôs.
Step 1 Let B be the list w1; : : : ; wn, which spans V. Thus adjoining any vector in V to this list produces a linearly dependent list (because the newly adjoined vector can be written as a linear combination of the other vectors). In particular, the list
u1; w1; : : : ; wn
is linearly dependent. Thus by the Linear Dependence Lemma (2.21), we can remove one of the w‚Äôs so that the new list B (of length n) consisting of u1 and the remaining w‚Äôs spans V.
Step j The list B (of length n) from step j 1 spans V. Thus adjoining any vector to this list produces a linearly dependent list. In particular, the list of length .n C 1/ obtained by adjoining uj to B, placing it just after u1; : : : ; uj 1, is linearly dependent. By the Linear Dependence Lemma (2.21), one of the vectors in this list is in the span of the previous ones, and because u1; : : : ; uj is linearly independent, this vector is one of the w‚Äôs, not one of the u‚Äôs. We can remove that w from B so that the new list B (of length n) consisting of u1; : : : ; uj and the remaining w‚Äôs spans V.
After step m, we have added all the u‚Äôs and the process stops. At each step as we add a u to B, the Linear Dependence Lemma implies that there is some w to remove. Thus there are at least as many w‚Äôs as u‚Äôs.
The next two examples show how the result above can be used to show, without any computations, that certain lists are not linearly independent and that certain lists do not span a given vector space.

36 CHAPTER 2 Finite-Dimensional Vector Spaces
2.24 Example Show that the list .1; 2; 3/; .4; 5; 8/; .9; 6; 7/; . 3; 2; 8/ is not linearly independent in R3.
Solution The list .1; 0; 0/; .0; 1; 0/; .0; 0; 1/ spans R3. Thus no list of length larger than 3 is linearly independent in R3.
2.25 Example Show that the list .1; 2; 3; 5/; .4; 5; 8; 3/; .9; 6; 7; 1/ does not span R4.
Solution The list .1; 0; 0; 0/; .0; 1; 0; 0/; .0; 0; 1; 0/; .0; 0; 0; 1/ is linearly independent in R4. Thus no list of length less than 4 spans R4.
Our intuition suggests that every subspace of a Ô¨Ånite-dimensional vector space should also be Ô¨Ånite-dimensional. We now prove that this intuition is correct.
2.26 Finite-dimensional subspaces Every subspace of a Ô¨Ånite-dimensional vector space is Ô¨Ånite-dimensional.
Proof Suppose V is Ô¨Ånite-dimensional and U is a subspace of V. We need to prove that U is Ô¨Ånite-dimensional. We do this through the following multi-step construction.
Step 1 If U D f0g, then U is Ô¨Ånite-dimensional and we are done. If U ¬§ f0g, then choose a nonzero vector v1 2 U.
Step j If U D span.v1; : : : ; vj 1/, then U is Ô¨Ånite-dimensional and we are done. If U ¬§ span.v1; : : : ; vj 1/, then choose a vector vj 2 U such that vj ‚Ä¶ span.v1; : : : ; vj 1/:
After each step, as long as the process continues, we have constructed a list of vectors such that no vector in this list is in the span of the previous vectors. Thus after each step we have constructed a linearly independent list, by the Linear Dependence Lemma (2.21). This linearly independent list cannot be longer than any spanning list of V (by 2.23). Thus the process eventually terminates, which means that U is Ô¨Ånite-dimensional.

SECTION 2.A Span and Linear Independence 37
EXERCISES 2.A

1 Suppose v1; v2; v3; v4 spans V. Prove that the list

v1 v2; v2 v3; v3 v4; v4

also spans V.

2 Verify the assertions in Example 2.18.

3 Find a number t such that

.3; 1; 4/; .2; 3; 5/; .5; 9; t/ is not linearly independent in R3.

4 Verify the assertion in the second bullet point in Example 2.20.

5 (a) (b)

Show that if we think of C as a vector space over R, then the list .1 C i; 1 i / is linearly independent.
Show that if we think of C as a vector space over C, then the list .1 C i; 1 i / is linearly dependent.

6 Suppose v1; v2; v3; v4 is linearly independent in V. Prove that the list

v1 v2; v2 v3; v3 v4; v4

is also linearly independent.

7 Prove or give a counterexample: If v1; v2; : : : ; vm is a linearly independent list of vectors in V, then

5v1 4v2; v2; v3; : : : ; vm

is linearly independent.

8 Prove or give a counterexample: If v1; v2; : : : ; vm is a linearly independent list of vectors in V and  2 F with  ¬§ 0, then v1; v2; : : : ; vm is linearly independent.

9 Prove or give a counterexample: If v1; : : : ; vm and w1; : : : ; wm are linearly independent lists of vectors in V, then v1 C w1; : : : ; vm C wm is linearly independent.

10 Suppose v1; : : : ; vm is linearly independent in V and w 2 V. Prove that if v1 C w; : : : ; vm C w is linearly dependent, then w 2 span.v1; : : : ; vm/.

38 CHAPTER 2 Finite-Dimensional Vector Spaces
11 Suppose v1; : : : ; vm is linearly independent in V and w 2 V. Show that v1; : : : ; vm; w is linearly independent if and only if
w ‚Ä¶ span.v1; : : : ; vm/:
12 Explain why there does not exist a list of six polynomials that is linearly independent in P4.F/.
13 Explain why no list of four polynomials spans P4.F/.
14 Prove that V is inÔ¨Ånite-dimensional if and only if there is a sequence v1; v2; : : : of vectors in V such that v1; : : : ; vm is linearly independent for every positive integer m.
15 Prove that F1 is inÔ¨Ånite-dimensional.
16 Prove that the real vector space of all continuous real-valued functions on the interval ≈í0; 1¬ç is inÔ¨Ånite-dimensional.
17 Suppose p0; p1; : : : ; pm are polynomials in Pm.F/ such that pj .2/ D 0 for each j . Prove that p0; p1; : : : ; pm is not linearly independent in Pm.F/.

SECTION 2.B Bases 39
2.B Bases
In the last section, we discussed linearly independent lists and spanning lists. Now we bring these concepts together.
2.27 DeÔ¨Ånition basis A basis of V is a list of vectors in V that is linearly independent and spans V.

2.28 Example bases
(a) The list .1; 0; : : : ; 0/; .0; 1; 0; : : : ; 0/; : : : ; .0; : : : ; 0; 1/ is a basis of Fn, called the standard basis of Fn.
(b) The list .1; 2/; .3; 5/ is a basis of F2.
(c) The list .1; 2; 4/; .7; 5; 6/ is linearly independent in F3 but is not a basis of F3 because it does not span F3.
(d) The list .1; 2/; .3; 5/; .4; 13/ spans F2 but is not a basis of F2 because it is not linearly independent.
(e) The list .1; 1; 0/; .0; 0; 1/ is a basis of f.x; x; y/ 2 F3 W x; y 2 Fg.
(f) The list .1; 1; 0/; .1; 0; 1/ is a basis of f.x; y; z/ 2 F3 W x C y C z D 0g:
(g) The list 1; z; : : : ; zm is a basis of Pm.F/.
In addition to the standard basis, Fn has many other bases. For example, .7; 5/; . 4; 9/ and .1; 2/; .3; 5/ are both bases of F2.
The next result helps explain why bases are useful. Recall that ‚Äúuniquely‚Äù means ‚Äúin only one way‚Äù.

2.29 Criterion for basis
A list v1; : : : ; vn of vectors in V is a basis of V if and only if every v 2 V can be written uniquely in the form

2.30

v D a1v1 C    C anvn;

where a1; : : : ; an 2 F.

40 CHAPTER 2 Finite-Dimensional Vector Spaces
Proof First suppose that v1; : : : ; vn is a basis of V. Let v 2 V. Because v1; : : : ; vn spans V, there exist a1; : : : ; an 2 F such that 2.30 holds. To This proof is essentially a repeti- show that the representation in 2.30 is tion of the ideas that led us to the unique, suppose c1; : : : ; cn are scalars deÔ¨Ånition of linear independence. such that we also have
v D c1v1 C    C cnvn:
Subtracting the last equation from 2.30, we get
0 D .a1 c1/v1 C    C .an cn/vn:
This implies that each aj cj equals 0 (because v1; : : : ; vn is linearly independent). Hence a1 D c1; : : : ; an D cn. We have the desired uniqueness, completing the proof in one direction.
For the other direction, suppose every v 2 V can be written uniquely in the form given by 2.30. Clearly this implies that v1; : : : ; vn spans V. To show that v1; : : : ; vn is linearly independent, suppose a1; : : : ; an 2 F are such that
0 D a1v1 C    C anvn:
The uniqueness of the representation 2.30 (taking v D 0) now implies that a1 D    D an D 0. Thus v1; : : : ; vn is linearly independent and hence is a basis of V.
A spanning list in a vector space may not be a basis because it is not linearly independent. Our next result says that given any spanning list, some (possibly none) of the vectors in it can be discarded so that the remaining list is linearly independent and still spans the vector space.
As an example in the vector space F2, if the procedure in the proof below is applied to the list .1; 2/; .3; 6/; .4; 7/; .5; 9/, then the second and fourth vectors will be removed. This leaves .1; 2/; .4; 7/, which is a basis of F2.
2.31 Spanning list contains a basis
Every spanning list in a vector space can be reduced to a basis of the vector space.
Proof Suppose v1; : : : ; vn spans V. We want to remove some of the vectors from v1; : : : ; vn so that the remaining vectors form a basis of V. We do this through the multi-step process described below.
Start with B equal to the list v1; : : : ; vn.

SECTION 2.B Bases 41
Step 1 If v1 D 0, delete v1 from B. If v1 ¬§ 0, leave B unchanged.
Step j If vj is in span.v1; : : : ; vj 1/, delete vj from B. If vj is not in span.v1; : : : ; vj 1/, leave B unchanged.
Stop the process after step n, getting a list B. This list B spans V because our original list spanned V and we have discarded only vectors that were already in the span of the previous vectors. The process ensures that no vector in B is in the span of the previous ones. Thus B is linearly independent, by the Linear Dependence Lemma (2.21). Hence B is a basis of V.
Our next result, an easy corollary of the previous result, tells us that every Ô¨Ånite-dimensional vector space has a basis.
2.32 Basis of Ô¨Ånite-dimensional vector space Every Ô¨Ånite-dimensional vector space has a basis.
Proof By deÔ¨Ånition, a Ô¨Ånite-dimensional vector space has a spanning list. The previous result tells us that each spanning list can be reduced to a basis.
Our next result is in some sense a dual of 2.31, which said that every spanning list can be reduced to a basis. Now we show that given any linearly independent list, we can adjoin some additional vectors (this includes the possibility of adjoining no additional vectors) so that the extended list is still linearly independent but also spans the space.
2.33 Linearly independent list extends to a basis Every linearly independent list of vectors in a Ô¨Ånite-dimensional vector space can be extended to a basis of the vector space.
Proof Suppose u1; : : : ; um is linearly independent in a Ô¨Ånite-dimensional vector space V. Let w1; : : : ; wn be a basis of V. Thus the list
u1; : : : ; um; w1; : : : ; wn
spans V. Applying the procedure of the proof of 2.31 to reduce this list to a basis of V produces a basis consisting of the vectors u1; : : : ; um (none of the u‚Äôs get deleted in this procedure because u1; : : : ; um is linearly independent) and some of the w‚Äôs.

42 CHAPTER 2 Finite-Dimensional Vector Spaces

As an example in F3, suppose we start with the linearly independent

list .2; 3; 4/; .9; 6; 8/. If we take w1; w2; w3 in the proof above to be the standard basis of F3, then the procedure in the proof above produces the list
.2; 3; 4/; .9; 6; 8/; .0; 1; 0/, which is a basis of F3.

Using the same basic ideas but considerably more advanced tools,

As an application of the result above, we now show that every subspace of a

the next result can be proved with- Ô¨Ånite-dimensional vector space can be

out the hypothesis that V is Ô¨Ånitedimensional.

paired with another subspace to form a direct sum of the whole space.

2.34 Every subspace of V is part of a direct sum equal to V
Suppose V is Ô¨Ånite-dimensional and U is a subspace of V. Then there is a subspace W of V such that V D U Àö W.

Proof Because V is Ô¨Ånite-dimensional, so is U (see 2.26). Thus there is a basis u1; : : : ; um of U (see 2.32). Of course u1; : : : ; um is a linearly independent list of vectors in V. Hence this list can be extended to a basis u1; : : : ; um; w1; : : : ; wn of V (see 2.33). Let W D span.w1; : : : ; wn/.
To prove that V D U Àö W, by 1.45 we need only show that

V D U C W and U \ W D f0g:

To prove the Ô¨Årst equation above, suppose v 2 V. Then, because the list

u1; : : : ; um; w1; : : : ; wn spans V, there exist a1; : : : ; am; b1; : : : ; bn 2 F such

that v D ‚Äûa1u1 C ∆í ‚Äö C amum‚Ä¶ C ‚Äûb1w1 C ∆í ‚Äö C bnw‚Ä¶n :

u

w

In other words, we have v D u C w, where u 2 U and w 2 W are deÔ¨Åned as above. Thus v 2 U C W, completing the proof that V D U C W.
To show that U \ W D f0g, suppose v 2 U \ W. Then there exist scalars a1; : : : ; am; b1; : : : ; bn 2 F such that

v D a1u1 C    C amum D b1w1 C    C bnwn:

Thus

a1u1 C    C amum b1w1    bnwn D 0:

Because u1; : : : ; um; w1; : : : ; wn is linearly independent, this implies that a1 D    D am D b1 D    D bn D 0. Thus v D 0, completing the proof that U \ W D f0g.

EXERCISES 2.B

SECTION 2.B Bases 43

1 Find all vector spaces that have exactly one basis.

2 Verify all the assertions in Example 2.28. 3 (a) Let U be the subspace of R5 deÔ¨Åned by

U D f.x1; x2; x3; x4; x5/ 2 R5 W x1 D 3x2 and x3 D 7x4g:

(b) (c) 4 (a)

Find a basis of U. Extend the basis in part (a) to a basis of R5. Find a subspace W of R5 such that R5 D U Àö W.
Let U be the subspace of C5 deÔ¨Åned by

U D f.z1; z2; z3; z4; z5/ 2 C5 W 6z1 D z2 and z3 C2z4 C3z5 D 0g:

Find a basis of U. (b) Extend the basis in part (a) to a basis of C5. (c) Find a subspace W of C5 such that C5 D U Àö W.

5 Prove or disprove: there exists a basis p0; p1; p2; p3 of P3.F/ such that none of the polynomials p0; p1; p2; p3 has degree 2.

6 Suppose v1; v2; v3; v4 is a basis of V. Prove that

v1 C v2; v2 C v3; v3 C v4; v4

is also a basis of V.

7 Prove or give a counterexample: If v1; v2; v3; v4 is a basis of V and U is a subspace of V such that v1; v2 2 U and v3 ‚Ä¶ U and v4 ‚Ä¶ U, then v1; v2 is a basis of U.
8 Suppose U and W are subspaces of V such that V D U Àö W. Suppose also that u1; : : : ; um is a basis of U and w1; : : : ; wn is a basis of W. Prove that u1; : : : ; um; w1; : : : ; wn
is a basis of V.

44 CHAPTER 2 Finite-Dimensional Vector Spaces
2.C Dimension
Although we have been discussing Ô¨Ånite-dimensional vector spaces, we have not yet deÔ¨Åned the dimension of such an object. How should dimension be deÔ¨Åned? A reasonable deÔ¨Ånition should force the dimension of Fn to equal n. Notice that the standard basis
.1; 0; : : : ; 0/; .0; 1; 0; : : : ; 0/; : : : ; .0; : : : ; 0; 1/ of Fn has length n. Thus we are tempted to deÔ¨Åne the dimension as the length of a basis. However, a Ô¨Ånite-dimensional vector space in general has many different bases, and our attempted deÔ¨Ånition makes sense only if all bases in a given vector space have the same length. Fortunately that turns out to be the case, as we now show.
2.35 Basis length does not depend on basis Any two bases of a Ô¨Ånite-dimensional vector space have the same length.
Proof Suppose V is Ô¨Ånite-dimensional. Let B1 and B2 be two bases of V. Then B1 is linearly independent in V and B2 spans V, so the length of B1 is at most the length of B2 (by 2.23). Interchanging the roles of B1 and B2, we also see that the length of B2 is at most the length of B1. Thus the length of B1 equals the length of B2, as desired.
Now that we know that any two bases of a Ô¨Ånite-dimensional vector space have the same length, we can formally deÔ¨Åne the dimension of such spaces.
2.36 DeÔ¨Ånition dimension, dim V
 The dimension of a Ô¨Ånite-dimensional vector space is the length of any basis of the vector space.
 The dimension of V (if V is Ô¨Ånite-dimensional) is denoted by dim V.
2.37 Example dimensions  dim Fn D n because the standard basis of Fn has length n.  dim Pm.F/ D m C 1 because the basis 1; z; : : : ; zm of Pm.F/ has length m C 1.

SECTION 2.C Dimension 45

Every subspace of a Ô¨Ånite-dimensional vector space is Ô¨Ånite-dimensional (by 2.26) and so has a dimension. The next result gives the expected inequality about the dimension of a subspace.

2.38 Dimension of a subspace If V is Ô¨Ånite-dimensional and U is a subspace of V, then dim U  dim V.

Proof Suppose V is Ô¨Ånite-dimensional and U is a subspace of V. Think of a basis of U as a linearly independent list in V, and think of a basis of V as a spanning list in V. Now use 2.23 to conclude that dim U  dim V.

To check that a list of vectors in V is a basis of V, we must, according to the deÔ¨Ånition, show that the list in question satisÔ¨Åes two properties: it must be linearly independent and it must span V. The next two results show that if the list in question has the right length, then we need only check that it satisÔ¨Åes one of the two required properties. First we prove that every linearly independent list with the right length is a basis.

The real vector space R2 has dimension 2; the complex vector space C has dimension 1. As sets, R2 can be identiÔ¨Åed with C (and addition is the same on both spaces, as is scalar multiplication by real numbers). Thus when we talk about the dimension of a vector space, the role played by the choice of F cannot be neglected.

2.39 Linearly independent list of the right length is a basis
Suppose V is Ô¨Ånite-dimensional. Then every linearly independent list of vectors in V with length dim V is a basis of V.

Proof Suppose dim V D n and v1; : : : ; vn is linearly independent in V. The list v1; : : : ; vn can be extended to a basis of V (by 2.33). However, every basis of V has length n, so in this case the extension is the trivial one, meaning that no elements are adjoined to v1; : : : ; vn. In other words, v1; : : : ; vn is a basis of V, as desired.

2.40 Example Show that the list .5; 7/; .4; 3/ is a basis of F2.
Solution This list of two vectors in F2 is obviously linearly independent (because neither vector is a scalar multiple of the other). Note that F2 has dimension 2. Thus 2.39 implies that the linearly independent list .5; 7/; .4; 3/ of length 2 is a basis of F2 (we do not need to bother checking that it spans F2).

46 CHAPTER 2 Finite-Dimensional Vector Spaces
2.41 Example Show that 1; .x 5/2; .x 5/3 is a basis of the subspace U of P3.R/ deÔ¨Åned by
U D fp 2 P3.R/ W p0.5/ D 0g:
Solution Clearly each of the polynomials 1, .x 5/2, and .x 5/3 is in U. Suppose a; b; c 2 R and
a C b.x 5/2 C c.x 5/3 D 0
for every x 2 R. Without explicitly expanding the left side of the equation above, we can see that the left side has a cx3 term. Because the right side has no x3 term, this implies that c D 0. Because c D 0, we see that the left side has a bx2 term, which implies that b D 0. Because b D c D 0, we can also conclude that a D 0.
Thus the equation above implies that a D b D c D 0. Hence the list 1; .x 5/2; .x 5/3 is linearly independent in U.
Thus dim U  3. Because U is a subspace of P3.R/, we know that dim U  dim P3.R/ D 4 (by 2.38). However, dim U cannot equal 4, because otherwise when we extend a basis of U to a basis of P3.R/ we would get a list with length greater than 4. Hence dim U D 3. Thus 2.39 implies that the linearly independent list 1; .x 5/2; .x 5/3 is a basis of U.
Now we prove that a spanning list with the right length is a basis.
2.42 Spanning list of the right length is a basis
Suppose V is Ô¨Ånite-dimensional. Then every spanning list of vectors in V with length dim V is a basis of V.
Proof Suppose dim V D n and v1; : : : ; vn spans V. The list v1; : : : ; vn can be reduced to a basis of V (by 2.31). However, every basis of V has length n, so in this case the reduction is the trivial one, meaning that no elements are deleted from v1; : : : ; vn. In other words, v1; : : : ; vn is a basis of V, as desired.
The next result gives a formula for the dimension of the sum of two subspaces of a Ô¨Ånite-dimensional vector space. This formula is analogous to a familiar counting formula: the number of elements in the union of two Ô¨Ånite sets equals the number of elements in the Ô¨Årst set, plus the number of elements in the second set, minus the number of elements in the intersection of the two sets.

SECTION 2.C Dimension 47
2.43 Dimension of a sum
If U1 and U2 are subspaces of a Ô¨Ånite-dimensional vector space, then
dim.U1 C U2/ D dim U1 C dim U2 dim.U1 \ U2/:
Proof Let u1; : : : ; um be a basis of U1 \ U2; thus dim.U1 \ U2/ D m. Because u1; : : : ; um is a basis of U1 \ U2, it is linearly independent in U1. Hence this list can be extended to a basis u1; : : : ; um; v1; : : : ; vj of U1 (by 2.33). Thus dim U1 D m C j . Also extend u1; : : : ; um to a basis u1; : : : ; um; w1; : : : ; wk of U2; thus dim U2 D m C k.
We will show that
u1; : : : ; um; v1; : : : ; vj ; w1; : : : ; wk is a basis of U1 C U2. This will complete the proof, because then we will have
dim.U1 C U2/ D m C j C k D .m C j / C .m C k/ m D dim U1 C dim U2 dim.U1 \ U2/:
Clearly span.u1; : : : ; um; v1; : : : ; vj ; w1; : : : ; wk/ contains U1 and U2 and hence equals U1 C U2. So to show that this list is a basis of U1 C U2 we need only show that it is linearly independent. To prove this, suppose
a1u1 C    C amum C b1v1 C    C bj vj C c1w1 C    C ckwk D 0; where all the a‚Äôs, b‚Äôs, and c‚Äôs are scalars. We need to prove that all the a‚Äôs, b‚Äôs, and c‚Äôs equal 0. The equation above can be rewritten as
c1w1 C    C ckwk D a1u1    amum b1v1    bj vj ; which shows that c1w1 C    C ckwk 2 U1. All the w‚Äôs are in U2, so this implies that c1w1 C    C ckwk 2 U1 \ U2. Because u1; : : : ; um is a basis of U1 \ U2, we can write
c1w1 C    C ckwk D d1u1 C    C dmum for some choice of scalars d1; : : : ; dm. But u1; : : : ; um; w1; : : : ; wk is linearly independent, so the last equation implies that all the c‚Äôs (and d ‚Äôs) equal 0. Thus our original equation involving the a‚Äôs, b‚Äôs, and c‚Äôs becomes
a1u1 C    C amum C b1v1 C    C bj vj D 0:
Because the list u1; : : : ; um; v1; : : : ; vj is linearly independent, this equation implies that all the a‚Äôs and b‚Äôs are 0. We now know that all the a‚Äôs, b‚Äôs, and c‚Äôs equal 0, as desired.

48 CHAPTER 2 Finite-Dimensional Vector Spaces
EXERCISES 2.C

1 Suppose V is Ô¨Ånite-dimensional and U is a subspace of V such that dim U D dim V. Prove that U D V.
2 Show that the subspaces of R2 are precisely f0g, R2, and all lines in R2 through the origin.
3 Show that the subspaces of R3 are precisely f0g, R3, all lines in R3 through the origin, and all planes in R3 through the origin.

4 (a) (b) (c)
5 (a) (b) (c)

Let U D fp 2 P4.F/ W p.6/ D 0g. Find a basis of U. Extend the basis in part (a) to a basis of P4.F/. Find a subspace W of P4.F/ such that P4.F/ D U Àö W.
Let U D fp 2 P4.R/ W p00.6/ D 0g. Find a basis of U. Extend the basis in part (a) to a basis of P4.R/. Find a subspace W of P4.R/ such that P4.R/ D U Àö W.

6 (a) (b) (c)

Let U D fp 2 P4.F/ W p.2/ D p.5/g. Find a basis of U. Extend the basis in part (a) to a basis of P4.F/. Find a subspace W of P4.F/ such that P4.F/ D U Àö W.

7 (a) (b) (c)
8 (a) (b) (c)

Let U D fp 2 P4.F/ W p.2/ D p.5/ D p.6/g. Find a basis of U.

Extend the basis in part (a) to a basis of P4.F/.

Find a subspace W of P4.F/ such that P4.F/ D U Àö W.

Let

U

D

fp

2

P4.R/

W

R

1 1

p

D

0g.

Find

a

basis

of

U.

Extend the basis in part (a) to a basis of P4.R/.

Find a subspace W of P4.R/ such that P4.R/ D U Àö W.

9 Suppose v1; : : : ; vm is linearly independent in V and w 2 V. Prove that

dim span.v1 C w; : : : ; vm C w/  m 1:

10 Suppose p0; p1; : : : ; pm 2 P.F/ are such that each pj has degree j . Prove that p0; p1; : : : ; pm is a basis of Pm.F/.
11 Suppose that U and W are subspaces of R8 such that dim U D 3, dim W D 5, and U C W D R8. Prove that R8 D U Àö W.

SECTION 2.C Dimension 49

12 Suppose U and W are both Ô¨Åve-dimensional subspaces of R9. Prove that U \ W ¬§ f0g.
13 Suppose U and W are both 4-dimensional subspaces of C6. Prove that there exist two vectors in U \ W such that neither of these vectors is a scalar multiple of the other.
14 Suppose U1; : : : ; Um are Ô¨Ånite-dimensional subspaces of V. Prove that U1 C    C Um is Ô¨Ånite-dimensional and
dim.U1 C    C Um/  dim U1 C    C dim Um:

15 Suppose V is Ô¨Ånite-dimensional, with dim V D n  1. Prove that there exist 1-dimensional subspaces U1; : : : ; Un of V such that
V D U1 Àö    Àö Un:

16 Suppose U1; : : : ; Um are Ô¨Ånite-dimensional subspaces of V such that U1 C    C Um is a direct sum. Prove that U1 Àö    Àö Um is Ô¨Ånitedimensional and

dim U1 Àö    Àö Um D dim U1 C    C dim Um:
[The exercise above deepens the analogy between direct sums of subspaces and disjoint unions of subsets. SpeciÔ¨Åcally, compare this exercise to the following obvious statement: if a set is written as a disjoint union of Ô¨Ånite subsets, then the number of elements in the set equals the sum of the numbers of elements in the disjoint subsets.]
17 You might guess, by analogy with the formula for the number of elements in the union of three subsets of a Ô¨Ånite set, that if U1; U2; U3 are subspaces of a Ô¨Ånite-dimensional vector space, then

dim.U1 C U2 C U3/ D dim U1 C dim U2 C dim U3 dim.U1 \ U2/ dim.U1 \ U3/ C dim.U1 \ U2 \ U3/:

dim.U2 \ U3/

Prove this or give a counterexample.

CHAPTER
3

German mathematician Carl Friedrich Gauss (1777‚Äì1855), who in 1809 published a method for solving systems of linear equations. This method, now called Gaussian elimination, was also used in a Chinese book published over 1600 years earlier.

Linear Maps

So far our attention has focused on vector spaces. No one gets excited about vector spaces. The interesting part of linear algebra is the subject to which we now turn‚Äîlinear maps.
In this chapter we will frequently need another vector space, which we will call W, in addition to V. Thus our standing assumptions are now as follows:
3.1 Notation F, V, W
 F denotes R or C.
 V and W denote vector spaces over F.

LEARNING OBJECTIVES FOR THIS CHAPTER
Fundamental Theorem of Linear Maps the matrix of a linear map with respect to given bases isomorphic vector spaces product spaces quotient spaces the dual space of a vector space and the dual of a linear map

¬© Springer International Publishing 2015

51

S. Axler, Linear Algebra Done Right, Undergraduate Texts in Mathematics,

DOI 10.1007/978-3-319-11080-6__3

52 CHAPTER 3 Linear Maps

3.A The Vector Space of Linear Maps

DeÔ¨Ånition and Examples of Linear Maps
Now we are ready for one of the key deÔ¨Ånitions in linear algebra.

3.2 DeÔ¨Ånition linear map
A linear map from V to W is a function T W V ! W with the following properties:
additivity T .u C v/ D T u C T v for all u; v 2 V ;
homogeneity T .v/ D .T v/ for all  2 F and all v 2 V.

Some mathematicians use the term linear transformation, which means the same as linear map.

Note that for linear maps we often use the notation T v as well as the more standard functional notation T .v/.

3.3 Notation L.V; W /
The set of all linear maps from V to W is denoted L.V; W /.
Let‚Äôs look at some examples of linear maps. Make sure you verify that each of the functions deÔ¨Åned below is indeed a linear map:
3.4 Example linear maps zero In addition to its other uses, we let the symbol 0 denote the function that takes each element of some vector space to the additive identity of another vector space. To be speciÔ¨Åc, 0 2 L.V; W / is deÔ¨Åned by
0v D 0: The 0 on the left side of the equation above is a function from V to W, whereas the 0 on the right side is the additive identity in W. As usual, the context should allow you to distinguish between the many uses of the symbol 0.
identity The identity map, denoted I, is the function on some vector space that takes each element to itself. To be speciÔ¨Åc, I 2 L.V; V / is deÔ¨Åned by
I v D v:

SECTION 3.A The Vector Space of Linear Maps 53

differentiation



DeÔ¨Åne D 2 L P.R/; P.R/ by

Dp D p0:

The assertion that this function is a linear map is another way of stating a basic result about differentiation: .f C g/0 D f 0 C g0 and .f /0 D f 0 whenever
f; g are differentiable and  is a constant.

integration



DeÔ¨Åne T 2 L P.R/; R by

Z1

Tp D p.x/ dx:
0

The assertion that this function is linear is another way of stating a basic result

about integration: the integral of the sum of two functions equals the sum

of the integrals, and the integral of a constant times a function equals the

constant times the integral of the function.

multiplication by x2



DeÔ¨Åne T 2 L P.R/; P.R/ by

.Tp/.x/ D x2p.x/

for x 2 R.

backward shift Recall that F1 denotes the vector space of all sequences of elements of F. DeÔ¨Åne T 2 L.F1; F1/ by
T .x1; x2; x3; : : : / D .x2; x3; : : : /:
from R3 to R2 DeÔ¨Åne T 2 L.R3; R2/ by
T .x; y; z/ D .2x y C 3z; 7x C 5y 6z/:
from Fn to Fm Generalizing the previous example, let m and n be positive integers, let Aj;k 2 F for j D 1; : : : ; m and k D 1; : : : ; n, and deÔ¨Åne T 2 L.Fn; Fm/ by
T .x1; : : : ; xn/ D .A1;1x1 C    C A1;nxn; : : : ; Am;1x1 C    C Am;nxn/: Actually every linear map from Fn to Fm is of this form.

The existence part of the next result means that we can Ô¨Ånd a linear map that takes on whatever values we wish on the vectors in a basis. The uniqueness part of the next result means that a linear map is completely determined by its values on a basis.

54 CHAPTER 3 Linear Maps

3.5 Linear maps and basis of domain
Suppose v1; : : : ; vn is a basis of V and w1; : : : ; wn 2 W. Then there exists a unique linear map T W V ! W such that

for each j D 1; : : : ; n.

T vj D wj

Proof First we show the existence of a linear map T with the desired property. DeÔ¨Åne T W V ! W by
T .c1v1 C    C cnvn/ D c1w1 C    C cnwn;
where c1; : : : ; cn are arbitrary elements of F. The list v1; : : : ; vn is a basis of V, and thus the equation above does indeed deÔ¨Åne a function T from V to W (because each element of V can be uniquely written in the form c1v1 C    C cnvn).
For each j , taking cj D 1 and the other c‚Äôs equal to 0 in the equation above shows that T vj D wj .
If u; v 2 V with u D a1v1 C    C anvn and v D c1v1 C    C cnvn, then 
T .u C v/ D T .a1 C c1/v1 C    C .an C cn/vn D .a1 C c1/w1 C    C .an C cn/wn D .a1w1 C    C anwn/ C .c1w1 C    C cnwn/ D T u C T v:
Similarly, if  2 F and v D c1v1 C    C cnvn, then
T .v/ D T .c1v1 C    C cnvn/ D c1w1 C    C cnwn D .c1w1 C    C cnwn/ D T v:
Thus T is a linear map from V to W. To prove uniqueness, now suppose that T 2 L.V; W / and that T vj D wj
for j D 1; : : : ; n. Let c1; : : : ; cn 2 F. The homogeneity of T implies that T .cj vj / D cj wj for j D 1; : : : ; n. The additivity of T now implies that
T .c1v1 C    C cnvn/ D c1w1 C    C cnwn:
Thus T is uniquely determined on span.v1; : : : ; vn/ by the equation above. Because v1; : : : ; vn is a basis of V, this implies that T is uniquely determined on V.

SECTION 3.A The Vector Space of Linear Maps 55
Algebraic Operations on L.V; W /
We begin by deÔ¨Åning addition and scalar multiplication on L.V; W /.

3.6 DeÔ¨Ånition addition and scalar multiplication on L.V; W / Suppose S; T 2 L.V; W / and  2 F. The sum S C T and the product T are the linear maps from V to W deÔ¨Åned by
.S C T /.v/ D Sv C T v and .T /.v/ D .T v/ for all v 2 V.

You should verify that S CT and T as deÔ¨Åned above are indeed linear maps. In other words, if S; T 2 L.V; W / and  2 F, then S C T 2 L.V; W / and T 2 L.V; W /.
Because we took the trouble to deÔ¨Åne addition and scalar multiplication on L.V; W /, the next result should not be a surprise.

Although linear maps are pervasive throughout mathematics, they are not as ubiquitous as imagined by some confused students who seem to think that cos is a linear map from R to R when they write that cos 2x equals 2 cos x and that cos.x C y/ equals cos x C cos y.

3.7 L.V; W / is a vector space
With the operations of addition and scalar multiplication as deÔ¨Åned above, L.V; W / is a vector space.

The routine proof of the result above is left to the reader. Note that the additive identity of L.V; W / is the zero linear map deÔ¨Åned earlier in this section.
Usually it makes no sense to multiply together two elements of a vector space, but for some pairs of linear maps a useful product exists. We will need a third vector space, so for the rest of this section suppose U is a vector space over F.
3.8 DeÔ¨Ånition Product of Linear Maps
If T 2 L.U; V / and S 2 L.V; W /, then the product ST 2 L.U; W / is deÔ¨Åned by
.S T /.u/ D S.T u/
for u 2 U.

56 CHAPTER 3 Linear Maps
In other words, ST is just the usual composition S ƒ± T of two functions, but when both functions are linear, most mathematicians write ST instead of S ƒ± T. You should verify that S T is indeed a linear map from U to W whenever T 2 L.U; V / and S 2 L.V; W /.
Note that ST is deÔ¨Åned only when T maps into the domain of S.

3.9 Algebraic properties of products of linear maps

associativity

.T1T2/T3 D T1.T2T3/

whenever T1, T2, and T3 are linear maps such that the products make sense (meaning that T3 maps into the domain of T2, and T2 maps into the domain of T1).

identity

TI D IT D T

whenever T 2 L.V; W / (the Ô¨Årst I is the identity map on V, and the second I is the identity map on W ).

distributive properties

.S1 C S2/T D S1T C S2T and S.T1 C T2/ D S T1 C S T2

whenever T; T1; T2 2 L.U; V / and S; S1; S2 2 L.V; W /.

The routine proof of the result above is left to the reader. Multiplication of linear maps is not commutative. In other words, it is not necessarily true that ST D T S, even if both sides of the equation make sense.



3.10 Example Suppose D 2 L deÔ¨Åned in Example 3.4 and T 2 L

P.R/; P.R/ P.R/; P.R/

is the differentiation map is the multiplication by x2

map deÔ¨Åned earlier in this section. Show that TD ¬§ DT.

Solution We have

 .TD/p .x/

D

x2p0.x/

but

.DT

 /p .x/

D

x2p0.x/

C

2xp.x/:

In other words, differentiating and then multiplying by x2 is not the same as multiplying by x2 and then differentiating.

SECTION 3.A The Vector Space of Linear Maps 57
3.11 Linear maps take 0 to 0 Suppose T is a linear map from V to W. Then T .0/ D 0.
Proof By additivity, we have T .0/ D T .0 C 0/ D T .0/ C T .0/:
Add the additive inverse of T .0/ to each side of the equation above to conclude that T .0/ D 0.

EXERCISES 3.A

1 Suppose b; c 2 R. DeÔ¨Åne T W R3 ! R2 by

T .x; y; z/ D .2x 4y C 3z C b; 6x C cxyz/:

Show that T is linear if and only if b D c D 0.

2 Suppose b; c 2 R. DeÔ¨Åne T W P.R/ ! R2 by



Z2



Tp D 3p.4/ C 5p0.6/ C bp.1/p.2/; x3p.x/ dx C c sin p.0/ :

1

Show that T is linear if and only if b D c D 0.

3 Suppose T 2 L.Fn; Fm/. Show that there exist scalars Aj;k 2 F for j D 1; : : : ; m and k D 1; : : : ; n such that

T .x1; : : : ; xn/ D .A1;1x1 C    C A1;nxn; : : : ; Am;1x1 C    C Am;nxn/
for every .x1; : : : ; xn/ 2 Fn. [The exercise above shows that T has the form promised in the last item of Example 3.4.]

4 Suppose T 2 L.V; W / and v1; : : : ; vm is a list of vectors in V such that T v1; : : : ; T vm is a linearly independent list in W. Prove that v1; : : : ; vm is linearly independent.

5 Prove the assertion in 3.7.

6 Prove the assertions in 3.9.

58 CHAPTER 3 Linear Maps

7 Show that every linear map from a 1-dimensional vector space to itself is multiplication by some scalar. More precisely, prove that if dim V D 1 and T 2 L.V; V /, then there exists  2 F such that T v D v for all v 2 V.
8 Give an example of a function ' W R2 ! R such that

'.av/ D a'.v/
for all a 2 R and all v 2 R2 but ' is not linear. [The exercise above and the next exercise show that neither homogeneity nor additivity alone is enough to imply that a function is a linear map.]

9 Give an example of a function ' W C ! C such that

'.w C z/ D '.w/ C '.z/

for all w; z 2 C but ' is not linear. (Here C is thought of as a complex vector space.) [There also exists a function ' W R ! R such that ' satisÔ¨Åes the additivity condition above but ' is not linear. However, showing the existence of such a function involves considerably more advanced tools.]

10 Suppose U is a subspace of V with U ¬§ V. Suppose S 2 L.U; W / and

S ¬§ 0 (which means that S u ¬§ 0 for some u 2 U ). DeÔ¨Åne T W V ! W

by

(

T v D S v if v 2 U; 0 if v 2 V and v ‚Ä¶ U:

Prove that T is not a linear map on V.

11 Suppose V is Ô¨Ånite-dimensional. Prove that every linear map on a subspace of V can be extended to a linear map on V. In other words, show that if U is a subspace of V and S 2 L.U; W /, then there exists T 2 L.V; W / such that T u D S u for all u 2 U.

12 Suppose V is Ô¨Ånite-dimensional with dim V > 0, and suppose W is inÔ¨Ånite-dimensional. Prove that L.V; W / is inÔ¨Ånite-dimensional.

13 Suppose v1; : : : ; vm is a linearly dependent list of vectors in V. Suppose also that W ¬§ f0g. Prove that there exist w1; : : : ; wm 2 W such that no T 2 L.V; W / satisÔ¨Åes T vk D wk for each k D 1; : : : ; m.
14 Suppose V is Ô¨Ånite-dimensional with dim V  2. Prove that there exist S; T 2 L.V; V / such that S T ¬§ T S.

SECTION 3.B Null Spaces and Ranges 59
3.B Null Spaces and Ranges
Null Space and Injectivity
In this section we will learn about two subspaces that are intimately connected with each linear map. We begin with the set of vectors that get mapped to 0.
3.12 DeÔ¨Ånition null space, null T For T 2 L.V; W /, the null space of T, denoted null T, is the subset of V consisting of those vectors that T maps to 0:
null T D fv 2 V W T v D 0g:

3.13 Example null space

 If T is the zero map from V to W, in other words if T v D 0 for every v 2 V, then null T D V.

 Suppose ' 2 L.C3; F/ is deÔ¨Åned by '.z1; z2; z3/ D z1 C 2z2 C 3z3. Then null ' D f.z1; z2; z3/ 2 C3 W z1 C 2z2 C 3z3 D 0g. A basis of

null ' is . 2; 1; 0/; . 3; 0; 1/.

  Suppose D 2 L P.R/; P.R/ is the differentiation map deÔ¨Åned by
Dp D p0. The only functions whose derivative equals the zero function

are the constant functions. Thus the null space of D equals the set of

constant functions.



Suppose T

2L

P

.R/;

P

 .R/

is

the

multiplication

by

x2

map

deÔ¨Åned

by .Tp/.x/ D x2p.x/. The only polynomial p such that x2p.x/ D 0

for all x 2 R is the 0 polynomial. Thus null T D f0g.

 Suppose T 2 L.F1; F1/ is the backward shift deÔ¨Åned by
T .x1; x2; x3; : : : / D .x2; x3; : : : /:
Clearly T .x1; x2; x3; : : : / equals 0 if and only if x2; x3; : : : are all 0. Thus in this case we have null T D f.a; 0; 0; : : : / W a 2 Fg.

The next result shows that the null space of each linear map is a subspace of the domain. In particular, 0 is in the null space of every linear map.

Some mathematicians use the term kernel instead of null space. The word ‚Äúnull‚Äù means zero. Thus the term ‚Äúnull space‚Äùshould remind you of the connection to 0.

60 CHAPTER 3 Linear Maps

3.14 The null space is a subspace Suppose T 2 L.V; W /. Then null T is a subspace of V.

Proof Because T is a linear map, we know that T .0/ D 0 (by 3.11). Thus 0 2 null T.
Suppose u; v 2 null T. Then

T .u C v/ D T u C T v D 0 C 0 D 0:

Hence u C v 2 null T. Thus null T is closed under addition. Suppose u 2 null T and  2 F. Then

T .u/ D T u D 0 D 0:

Hence u 2 null T. Thus null T is closed under scalar multiplication.

Take another look at the null spaces that were computed in Example

We have shown that null T contains 0 and is closed under addition and scalar

3.13 and note that all of them are multiplication. Thus null T is a sub-

subspaces.

space of V (by 1.34).

As we will soon see, for a linear map the next deÔ¨Ånition is closely connected to the null space.

3.15 DeÔ¨Ånition injective A function T W V ! W is called injective if T u D T v implies u D v.

Many mathematicians use the term one-to-one, which means the same as injective.

The deÔ¨Ånition above could be rephrased to say that T is injective if u ¬§ v implies that T u ¬§ T v. In other

words, T is injective if it maps distinct

inputs to distinct outputs.

The next result says that we can check whether a linear map is injective

by checking whether 0 is the only vector that gets mapped to 0. As a simple

application of this result, we see that of the linear maps whose null spaces we computed in 3.13, only multiplication by x2 is injective (except that the zero

map is injective in the special case V D f0g).

SECTION 3.B Null Spaces and Ranges 61
3.16 Injectivity is equivalent to null space equals f0g Let T 2 L.V; W /. Then T is injective if and only if null T D f0g.
Proof First suppose T is injective. We want to prove that null T D f0g. We already know that f0g  null T (by 3.11). To prove the inclusion in the other direction, suppose v 2 null T. Then
T .v/ D 0 D T .0/:
Because T is injective, the equation above implies that v D 0. Thus we can conclude that null T D f0g, as desired.
To prove the implication in the other direction, now suppose null T D f0g. We want to prove that T is injective. To do this, suppose u; v 2 V and T u D T v. Then
0 D T u T v D T .u v/: Thus u v is in null T, which equals f0g. Hence u v D 0, which implies that u D v. Hence T is injective, as desired.
Range and Surjectivity
Now we give a name to the set of outputs of a function.
3.17 DeÔ¨Ånition range For T a function from V to W, the range of T is the subset of W consisting of those vectors that are of the form T v for some v 2 V :
range T D fT v W v 2 V g:
3.18 Example range
 If T is the zero map from V to W, in other words if T v D 0 for every v 2 V, then range T D f0g.
 Suppose T 2 L.R2; R3/ is deÔ¨Åned by T .x; y/ D .2x; 5y; x C y/, then range T D f.2x; 5y; x C y/ W x; y 2 Rg. A basis of range T is .2; 0; 1/; .0; 5; 1/. 
 Suppose D 2 L P.R/; P.R/ is the differentiation map deÔ¨Åned by Dp D p0. Because for every polynomial q 2 P.R/ there exists a polynomial p 2 P.R/ such that p0 D q, the range of D is P.R/.

62 CHAPTER 3 Linear Maps

Some mathematicians use the word image, which means the same as range.

The next result shows that the range of each linear map is a subspace of the vector space into which it is being mapped.

3.19 The range is a subspace If T 2 L.V; W /, then range T is a subspace of W.

Proof Suppose T 2 L.V; W /. Then T .0/ D 0 (by 3.11), which implies that 0 2 range T.
If w1; w2 2 range T, then there exist v1; v2 2 V such that T v1 D w1 and T v2 D w2. Thus
T .v1 C v2/ D T v1 C T v2 D w1 C w2:
Hence w1 C w2 2 range T. Thus range T is closed under addition. If w 2 range T and  2 F, then there exists v 2 V such that T v D w.
Thus T .v/ D T v D w:
Hence w 2 range T. Thus range T is closed under scalar multiplication. We have shown that range T contains 0 and is closed under addition and
scalar multiplication. Thus range T is a subspace of W (by 1.34).

3.20 DeÔ¨Ånition surjective A function T W V ! W is called surjective if its range equals W.

To illustrate the deÔ¨Ånition above, note that of the ranges we computed in

3.18, only the differentiation map is surjective (except that the zero map is

surjective in the special case W D f0g.

Many mathematicians use the term onto, which means the same as sur-

Whether a linear map is surjective depends on what we are thinking of as

jective.

the vector space into which it maps.

 3.21 Example The differentiation map D 2 L P5.R/; P5.R/ deÔ¨Åned by Dp D p0 is not surjective, because the polynomial x5 is notin the range of D. However, the differentiation map S 2 L P5.R/; P4.R/ deÔ¨Åned by Sp D p0 is surjective, because its range equals P4.R/, which is now the
vector space into which S maps.

SECTION 3.B Null Spaces and Ranges 63

Fundamental Theorem of Linear Maps
The next result is so important that it gets a dramatic name.

3.22 Fundamental Theorem of Linear Maps Suppose V is Ô¨Ånite-dimensional and T 2 L.V; W /. Then range T is Ô¨Ånite-dimensional and
dim V D dim null T C dim range T:

Proof Let u1; : : : ; um be a basis of null T ; thus dim null T D m. The linearly independent list u1; : : : ; um can be extended to a basis

u1; : : : ; um; v1; : : : ; vn

of V (by 2.33). Thus dim V D m C n. To complete the proof, we need only show that range T is Ô¨Ånite-dimensional and dim range T D n. We will do this by proving that T v1; : : : ; T vn is a basis of range T.
Let v 2 V. Because u1; : : : ; um; v1; : : : ; vn spans V, we can write

v D a1u1 C    C amum C b1v1 C    C bnvn;

where the a‚Äôs and b‚Äôs are in F. Applying T to both sides of this equation, we get
T v D b1T v1 C    C bnT vn;
where the terms of the form T uj disappeared because each uj is in null T. The last equation implies that T v1; : : : ; T vn spans range T. In particular, range T is Ô¨Ånite-dimensional.
To show T v1; : : : ; T vn is linearly independent, suppose c1; : : : ; cn 2 F and
c1T v1 C    C cnT vn D 0:

Then

T .c1v1 C    C cnvn/ D 0:

Hence

c1v1 C    C cnvn 2 null T:

Because u1; : : : ; um spans null T, we can write

c1v1 C    C cnvn D d1u1 C    C dmum;

where the d ‚Äôs are in F. This equation implies that all the c‚Äôs (and d ‚Äôs) are 0 (because u1; : : : ; um; v1; : : : ; vn is linearly independent). Thus T v1; : : : ; T vn is linearly independent and hence is a basis of range T, as desired.

64 CHAPTER 3 Linear Maps
Now we can show that no linear map from a Ô¨Ånite-dimensional vector space to a ‚Äúsmaller‚Äù vector space can be injective, where ‚Äúsmaller‚Äù is measured by dimension.

3.23 A map to a smaller dimensional space is not injective
Suppose V and W are Ô¨Ånite-dimensional vector spaces such that dim V > dim W. Then no linear map from V to W is injective.

Proof Let T 2 L.V; W /. Then

dim null T D dim V  dim V > 0;

dim range T dim W

where the equality above comes from the Fundamental Theorem of Linear Maps (3.22). The inequality above states that dim null T > 0. This means that null T contains vectors other than 0. Thus T is not injective (by 3.16).

The next result shows that no linear map from a Ô¨Ånite-dimensional vector space to a ‚Äúbigger‚Äù vector space can be surjective, where ‚Äúbigger‚Äù is measured by dimension.

3.24 A map to a larger dimensional space is not surjective
Suppose V and W are Ô¨Ånite-dimensional vector spaces such that dim V < dim W. Then no linear map from V to W is surjective.

Proof Let T 2 L.V; W /. Then
dim range T D dim V dim null T  dim V < dim W;
where the equality above comes from the Fundamental Theorem of Linear Maps (3.22). The inequality above states that dim range T < dim W. This means that range T cannot equal W. Thus T is not surjective.
As we will now see, 3.23 and 3.24 have important consequences in the theory of linear equations. The idea here is to express questions about systems of linear equations in terms of linear maps.

SECTION 3.B Null Spaces and Ranges 65

3.25 Example Rephrase in terms of a linear map the question of whether a homogeneous system of linear equations has a nonzero solution.

Solution

Fix positive integers m and n, and let Aj;k 2 F for j D 1; : : : ; m and k D 1; : : : ; n. Consider the homogeneous system of linear equations

Homogeneous, in this context, means that the constant term on the right side of each equation below is 0.

Xn A1;kxk D 0

kD1
::: Xn
Am;kxk D 0:

kD1

Obviously x1 D    D xn D 0 is a solution of the system of equations above;
the question here is whether any other solutions exist. DeÔ¨Åne T W Fn ! Fm by

Xn

Xn



T .x1; : : : ; xn/ D

A1;kxk; : : : ; Am;kxk :

kD1

kD1

The equation T .x1; : : : ; xn/ D 0 (the 0 here is the additive identity in Fm,

namely, the list of length m of all 0‚Äôs) is the same as the homogeneous system

of linear equations above.

Thus we want to know if null T is strictly bigger than f0g. In other words,

we can rephrase our question about nonzero solutions as follows (by 3.16):

What condition ensures that T is not injective?

3.26 Homogeneous system of linear equations
A homogeneous system of linear equations with more variables than equations has nonzero solutions.
Proof Use the notation and result from the example above. Thus T is a linear map from Fn to Fm, and we have a homogeneous system of m linear equations with n variables x1; : : : ; xn. From 3.23 we see that T is not injective if n > m.
Example of the result above: a homogeneous system of four linear equations with Ô¨Åve variables has nonzero solutions.

66 CHAPTER 3 Linear Maps

3.27 Example Consider the question of whether an inhomogeneous system of linear equations has no solutions for some choice of the constant terms. Rephrase this question in terms of a linear map.

Solution Fix positive integers m and n, and let Aj;k 2 F for j D 1; : : : ; m and k D 1; : : : ; n. For c1; : : : ; cm 2 F, consider the system of linear equations

Xn A1;kxk D c1

3.28

kD1
::: Xn
Am;kxk D cm:

kD1

The question here is whether there is some choice of c1; : : : ; cm 2 F such that

no solution exists to the system above.

DeÔ¨Åne T W Fn ! Fm by

Xn

Xn



T .x1; : : : ; xn/ D

A1;kxk; : : : ; Am;kxk :

kD1

kD1

The equation T .x1; : : : ; xn/ D .c1; : : : ; cm/ is the same as the system of equations 3.28. Thus we want to know if range T ¬§ Fm. Hence we can rephrase
our question about not having a solution for some choice of c1; : : : ; cm 2 F
as follows: What condition ensures that T is not surjective?

3.29 Inhomogeneous system of linear equations
An inhomogeneous system of linear equations with more equations than variables has no solution for some choice of the constant terms.

Our results about homogeneous systems with more variables than equations and inhomogeneous sys-

Proof Use the notation and result from
the example above. Thus T is a linear map from Fn to Fm, and we have a

tems with more equations than variables (3.26 and 3.29) are often proved using Gaussian elimination. The abstract approach taken here

system of m equations with n variables x1; : : : ; xn. From 3.24 we see that T is not surjective if n < m.

leads to cleaner proofs.

Example of the result above: an

inhomogeneous system of Ô¨Åve linear

equations with four variables has no solution for some choice of the con-

stant terms.

EXERCISES 3.B

SECTION 3.B Null Spaces and Ranges 67

1 Give an example of a linear map T such that dim null T D 3 and dim range T D 2.
2 Suppose V is a vector space and S; T 2 L.V; V / are such that
range S  null T: Prove that .S T /2 D 0. 3 Suppose v1; : : : ; vm is a list of vectors in V. DeÔ¨Åne T 2 L.Fm; V / by
T .z1; : : : ; zm/ D z1v1 C    C zmvm:
(a) What property of T corresponds to v1; : : : ; vm spanning V ? (b) What property of T corresponds to v1; : : : ; vm being linearly
independent?

4 Show that

fT 2 L.R5; R4/ W dim null T > 2g

is not a subspace of L.R5; R4/.

5 Give an example of a linear map T W R4 ! R4 such that

range T D null T:

6 Prove that there does not exist a linear map T W R5 ! R5 such that range T D null T:

7 Suppose V and W are Ô¨Ånite-dimensional with 2  dim V  dim W. Show that fT 2 L.V; W / W T is not injectiveg is not a subspace of L.V; W /.
8 Suppose V and W are Ô¨Ånite-dimensional with dim V  dim W  2. Show that fT 2 L.V; W / W T is not surjectiveg is not a subspace of L.V; W /.
9 Suppose T 2 L.V; W / is injective and v1; : : : ; vn is linearly independent in V. Prove that T v1; : : : ; T vn is linearly independent in W.

68 CHAPTER 3 Linear Maps
10 Suppose v1; : : : ; vn spans V and T 2 L.V; W /. Prove that the list T v1; : : : ; T vn spans range T.
11 Suppose S1; : : : ; Sn are injective linear maps such that S1S2    Sn makes sense. Prove that S1S2    Sn is injective.
12 Suppose that V is Ô¨Ånite-dimensional and that T 2 L.V; W /. Prove that there exists a subspace U of V such that U \ null T D f0g and range T D fT u W u 2 U g.
13 Suppose T is a linear map from F4 to F2 such that
null T D f.x1; x2; x3; x4/ 2 F4 W x1 D 5x2 and x3 D 7x4g:
Prove that T is surjective.
14 Suppose U is a 3-dimensional subspace of R8 and that T is a linear map from R8 to R5 such that null T D U. Prove that T is surjective.
15 Prove that there does not exist a linear map from F5 to F2 whose null space equals
f.x1; x2; x3; x4; x5/ 2 F5 W x1 D 3x2 and x3 D x4 D x5g:
16 Suppose there exists a linear map on V whose null space and range are both Ô¨Ånite-dimensional. Prove that V is Ô¨Ånite-dimensional.
17 Suppose V and W are both Ô¨Ånite-dimensional. Prove that there exists an injective linear map from V to W if and only if dim V  dim W.
18 Suppose V and W are both Ô¨Ånite-dimensional. Prove that there exists a surjective linear map from V onto W if and only if dim V  dim W.
19 Suppose V and W are Ô¨Ånite-dimensional and that U is a subspace of V. Prove that there exists T 2 L.V; W / such that null T D U if and only if dim U  dim V dim W.
20 Suppose W is Ô¨Ånite-dimensional and T 2 L.V; W /. Prove that T is injective if and only if there exists S 2 L.W; V / such that ST is the identity map on V.
21 Suppose V is Ô¨Ånite-dimensional and T 2 L.V; W /. Prove that T is surjective if and only if there exists S 2 L.W; V / such that T S is the identity map on W.

SECTION 3.B Null Spaces and Ranges 69
22 Suppose U and V are Ô¨Ånite-dimensional vector spaces and S 2 L.V; W / and T 2 L.U; V /. Prove that
dim null S T  dim null S C dim null T:
23 Suppose U and V are Ô¨Ånite-dimensional vector spaces and S 2 L.V; W / and T 2 L.U; V /. Prove that
dim range S T  minfdim range S; dim range T g:
24 Suppose W is Ô¨Ånite-dimensional and T1; T2 2 L.V; W /. Prove that null T1  null T2 if and only if there exists S 2 L.W; W / such that T2 D S T1.
25 Suppose V is Ô¨Ånite-dimensional and T1; T2 2 L.V; W /. Prove that range T1  range T2 if and only if there exists S 2 L.V; V / such that T1 D T2S . 
26 Suppose D 2 L P.R/; P.R/ is such that deg Dp D .deg p/ 1 for every nonconstant polynomial p 2 P.R/. Prove that D is surjective. [The notation D is used above to remind you of the differentiation map that sends a polynomial p to p0. Without knowing the formula for the derivative of a polynomial (except that it reduces the degree by 1), you can use the exercise above to show that for every polynomial q 2 P.R/, there exists a polynomial p 2 P.R/ such that p0 D q.]
27 Suppose p 2 P.R/. Prove that there exists a polynomial q 2 P.R/ such that 5q00 C 3q0 D p. [This exercise can be done without linear algebra, but it‚Äôs more fun to do it using linear algebra.]
28 Suppose T 2 L.V; W /, and w1; : : : ; wm is a basis of range T. Prove that there exist '1; : : : ; 'm 2 L.V; F/ such that
T v D '1.v/w1 C    C 'm.v/wm
for every v 2 V.
29 Suppose ' 2 L.V; F/. Suppose u 2 V is not in null '. Prove that
V D null ' Àö fau W a 2 Fg:
30 Suppose '1 and '2 are linear maps from V to F that have the same null space. Show that there exists a constant c 2 F such that '1 D c'2.
31 Give an example of two linear maps T1 and T2 from R5 to R2 that have the same null space but are such that T1 is not a scalar multiple of T2.

70 CHAPTER 3 Linear Maps

3.C Matrices

Representing a Linear Map by a Matrix
We know that if v1; : : : ; vn is a basis of V and T W V ! W is linear, then the values of T v1; : : : ; T vn determine the values of T on arbitrary vectors in V (see 3.5). As we will soon see, matrices are used as an efÔ¨Åcient method of recording the values of the T vj ‚Äôs in terms of a basis of W.

3.30 DeÔ¨Ånition matrix, Aj;k

Let m and n denote positive integers. An m-by-n matrix A is a rectangular

array of elements of F with m rows and n columns:

0

1

A1;1 : : : A1;n

A D B@ :::

::: CA :

Am;1 : : : Am;n

The notation Aj;k denotes the entry in row j , column k of A. In other words, the Ô¨Årst index refers to the row number and the second index refers to the column number.

Thus A2;3 refers to the entry in the second row, third column of a matrix A.





3.31 Example

If A D

8 4 5 3i 19 7

, then A2;3 D 7.

Now we come to the key deÔ¨Ånition in this section.

3.32 DeÔ¨Ånition matrix of a linear map, M.T /
Suppose T 2 L.V; W / and v1; : : : ; vn is a basis of V and w1; : : : ; wm is a basis of W. The matrix of T with respect to these bases is the m-by-n matrix M.T / whose entries Aj;k are deÔ¨Åned by
T vk D A1;kw1 C    C Am;kwm:
If the bases are not clear from the context, then the notation M T; .v1; : : : ; vn/; .w1; : : : ; wm/ is used.

The matrix M.T / of a linear map T 2 L.V; W / depends on the basis v1; : : : ; vn of V and the basis w1; : : : ; wm of W, as well as on T. However, the bases should be clear from the context, and thus they are often not included in the notation.

SECTION 3.C Matrices 71

To remember how M.T / is constructed from T, you might write across the top of the matrix the basis vectors v1; : : : ; vn for the domain and along the left the basis vectors w1; : : : ; wm for the vector space into which T maps, as follows:

0 v1 : : : vk : : : vn 1

w1

A1;k

M.T / D ::: B@

:::

CA :

wm

Am;k

In the matrix above only the kth col-
umn is shown. Thus the second index
of each displayed entry of the matrix
above is k. The picture above should
remind you that T vk can be computed from M.T / by multiplying each entry in the kth column by the correspond-
ing wj from the left column, and then adding up the resulting vectors.
If T is a linear map from Fn to Fm,
then unless stated otherwise, assume the
bases in question are the standard ones (where the kth basis vector is 1 in the kth slot and 0 in all the other slots). If you think of elements of Fm as columns
of m numbers, then you can think of the kth column of M.T / as T applied to the kth standard basis vector.

The kth column of M.T / consists of the scalars needed to write T vk as a linear combination of .w1; : : : ; wm/:
X m T vk D Aj;kwj .
j D1
If T maps an n-dimensional vector space to an m-dimensional vector space, then M.T / is an m-by-n matrix.

3.33 Example Suppose T 2 L.F2; F3/ is deÔ¨Åned by T .x; y/ D .x C 3y; 2x C 5y; 7x C 9y/:
Find the matrix of T with respect to the standard bases of F2 and F3.

Solution Because T .1; 0/ D .1; 2; 7/ and T .0; 1/ D .3; 5; 9/, the matrix of

T with respect to the standard bases is the 3-by-2 matrix below:

0

1

13

M.T / D @ 2 5 A :

79

72 CHAPTER 3 Linear Maps

When working with Pm.F/, use the standard basis 1; x; x2; : : : ; xm unless the context indicates otherwise.

 3.34 Example Suppose D 2 L P3.R/; P2.R/ is the differentiation map deÔ¨Åned by Dp D p0. Find the matrix of D with respect to the standard bases
of P3.R/ and P2.R/.

Solution Because .xn/0 D nxn 1, the matrix of T with respect to the

standard bases is the 3-by-4 matrix below:

0

1

0100

M.D/ D @ 0 0 2 0 A :

0003

Addition and Scalar Multiplication of Matrices
For the rest of this section, assume that V and W are Ô¨Ånite-dimensional and that a basis has been chosen for each of these vector spaces. Thus for each linear map from V to W, we can talk about its matrix (with respect to the chosen bases, of course). Is the matrix of the sum of two linear maps equal to the sum of the matrices of the two maps?
Right now this question does not make sense, because although we have deÔ¨Åned the sum of two linear maps, we have not deÔ¨Åned the sum of two matrices. Fortunately, the obvious deÔ¨Ånition of the sum of two matrices has the right properties. SpeciÔ¨Åcally, we make the following deÔ¨Ånition.

3.35 DeÔ¨Ånition matrix addition

The sum of two matrices of the same size is the matrix obtained by adding

corresponding entries in the matrices:

0

10

1

A1;1 : : : A1;n

C1;1 : : : C1;n

B@ :::

::: CA C B@ :::

::: CA

Am;1 : : : Am;n

Cm;1 : : : Cm;n

0

1

A1;1 C C1;1 : : : A1;n C C1;n

D B@

:::

:::

CA :

Am;1 C Cm;1 : : : Am;n C Cm;n

In other words, .A C C /j;k D Aj;k C Cj;k.

SECTION 3.C Matrices 73

In the following result, the assumption is that the same bases are used for all three linear maps S C T, S, and T.

3.36 The matrix of the sum of linear maps Suppose S; T 2 L.V; W /. Then M.S C T / D M.S / C M.T /.

The veriÔ¨Åcation of the result above is left to the reader. Still assuming that we have some bases in mind, is the matrix of a scalar times a linear map equal to the scalar times the matrix of the linear map? Again the question does not make sense, because we have not deÔ¨Åned scalar multiplication on matrices. Fortunately, the obvious deÔ¨Ånition again has the right properties.

3.37 DeÔ¨Ånition scalar multiplication of a matrix

The product of a scalar and a matrix is the matrix obtained by multiplying

each entry in the matrix by the scalar:

0

10

1

A1;1 : : : A1;n

A1;1 : : : A1;n

 B@ :::

::: CA D B@ :::

::: CA :

Am;1 : : : Am;n

Am;1 : : : Am;n

In other words, .A/j;k D Aj;k.

In the following result, the assumption is that the same bases are used for both linear maps T and T.

3.38 The matrix of a scalar times a linear map Suppose  2 F and T 2 L.V; W /. Then M.T / D M.T /.

The veriÔ¨Åcation of the result above is also left to the reader. Because addition and scalar multiplication have now been deÔ¨Åned for matrices, you should not be surprised that a vector space is about to appear. We need only a bit of notation so that this new vector space has a name.
3.39 Notation Fm;n
For m and n positive integers, the set of all m-by-n matrices with entries in F is denoted by Fm;n.

74 CHAPTER 3 Linear Maps

3.40 dim Fm;n D mn
Suppose m and n are positive integers. With addition and scalar multiplication deÔ¨Åned as above, Fm;n is a vector space with dimension mn.

Proof The veriÔ¨Åcation that Fm;n is a vector space is left to the reader. Note that the additive identity of Fm;n is the m-by-n matrix whose entries all
equal 0.
The reader should also verify that the list of m-by-n matrices that have 0 in all entries except for a 1 in one entry is a basis of Fm;n. There are mn such matrices, so the dimension of Fm;n equals mn.

Matrix Multiplication

Suppose, as previously, that v1; : : : ; vn is a basis of V and w1; : : : ; wm is a basis of W. Suppose also that we have another vector space U and that u1; : : : ; up is a basis of U.
Consider linear maps T W U ! V and S W V ! W. The composition S T is a linear map from U to W. Does M.S T / equal M.S /M.T /? This question does not yet make sense, because we have not deÔ¨Åned the product of two matrices. We will choose a deÔ¨Ånition of matrix multiplication that forces this question to have a positive answer. Let‚Äôs see how to do this.
Suppose M.S / D A and M.T / D C . For 1  k  p, we have

Xn



.S T /uk D S Cr;kvr

r D1

Xn D Cr;kS vr

r D1

Xn

X m

D Cr;k Aj;r wj

r D1

j D1

X m Xn



D

Aj;r Cr;k wj :

j D1 rD1

Thus M.S T / is the m-by-p matrix whose entry in row j , column k, equals

Xn Aj;r Cr;k :
r D1

SECTION 3.C Matrices 75

Now we see how to deÔ¨Åne matrix multiplication so that the desired equation M.S T / D M.S /M.T / holds.

3.41 DeÔ¨Ånition matrix multiplication
Suppose A is an m-by-n matrix and C is an n-by-p matrix. Then AC is deÔ¨Åned to be the m-by-p matrix whose entry in row j , column k, is given by the following equation:
Xn .AC /j;k D Aj;r Cr;k:
r D1
In other words, the entry in row j , column k, of AC is computed by taking row j of A and column k of C , multiplying together corresponding entries, and then summing.

Note that we deÔ¨Åne the product of two matrices only when the number of columns of the Ô¨Årst matrix equals the number of rows of the second matrix.

You may have learned this deÔ¨Ånition of matrix multiplication in an earlier course, although you may not have seen the motivation for it.

3.42 Example Here we multiply together a 3-by-2 matrix and a 2-by-4

matrix, obtaining a 3-by-4 matrix:

0 1
@3
5

2 4 6

1 A

6 2

5 1

4 0

3 1



D

0 @

10 26 42

7 19 31

4 12 20

1 1 5 A:
9

Matrix multiplication is not commutative. In other words, AC is not necessarily equal to CA even if both products are deÔ¨Åned (see Exercise 12). Matrix multiplication is distributive and associative (see Exercises 13 and 14).
In the following result, the assumption is that the same basis of V is used in considering T 2 L.U; V / and S 2 L.V; W /, the same basis of W is used in considering S 2 L.V; W / and S T 2 L.U; W /, and the same basis of U is used in considering T 2 L.U; V / and S T 2 L.U; W /.

3.43 The matrix of the product of linear maps If T 2 L.U; V / and S 2 L.V; W /, then M.S T / D M.S /M.T /.

The proof of the result above is the calculation that was done as motivation before the deÔ¨Ånition of matrix multiplication.

76 CHAPTER 3 Linear Maps

In the next piece of notation, note that as usual the Ô¨Årst index refers to a row and the second index refers to a column, with a vertically centered dot used as a placeholder.

3.44 Notation Aj; , A;k Suppose A is an m-by-n matrix.

 If 1  j  m, then Aj; denotes the 1-by-n matrix consisting of row j of A.

 If 1  k  n, then A;k denotes the m-by-1 matrix consisting of column k of A.





3.45 Example

If A D

845 197

, then A2; is row 2 of A and A;2 is

column 2 of A. In other words, 
A2; D 1 9 7

and



A;2 D

4 9

:

The product of a 1-by-n matrix and an n-by-1 matrix is a 1-by-1 matrix.

However, we will frequently identify a 1-by-1 matrix with its entry.

3.46 Example However, we can

3 identify

4



6 2



D

 26 with 26,

 26 because 3  6
 writing 3 4

C4 
6
2

2 D

D 26. 26.

Our next result gives another way to think of matrix multiplication: the entry in row j , column k, of AC equals (row j of A) times (column k of C ).

3.47 Entry of matrix product equals row times column Suppose A is an m-by-n matrix and C is an n-by-p matrix. Then

.AC /j;k D Aj; C;k for 1  j  m and 1  k  p.

The proof of the result above follows immediately from the deÔ¨Ånitions.

3.48 Example The result above and Example 3.46 show why the entry in row 2, column 1, of the product in Example 3.42 equals 26.

SECTION 3.C Matrices 77

The next result gives yet another way to think of matrix multiplication. It states that column k of AC equals A times column k of C .

3.49 Column of matrix product equals matrix times column

Suppose A is an m-by-n matrix and C is an n-by-p matrix. Then

for 1  k  p.

.AC /;k D AC;k

Again, the proof of the result above follows immediately from the deÔ¨Ånitions and is left to the reader.

3.50 Example

Fro0m1the2re1sult @3 4A
56

above 

an0d

5 1

D@

the 7 19 31

e1quation A;

we see why column 2 in the matrix product in Example 3.42 is the right side

of the equation above.

We give one more way of thinking about the product of an m-by-n matrix and an n-by-1 matrix. The following example illustrates this approach.

3.51 Example In the example above, the product of a 3-by-2 matrix and

a 2-by-1 matrix is a linear combination of the columns of the 3-by-2 matrix,

with the scalars that multiply the columns coming from the 2-by-1 matrix.

SpeciÔ¨Åcally,

0 1 01 01

7

1

2

@ 19 A D 5 @ 3 A C 1 @ 4 A :

31

5

6

The next result generalizes the example above. Again, the proof follows easily from the deÔ¨Ånitions and is left to the reader.

3.52 Linear combination of columns 0 1 c1
Suppose A is an m-by-n matrix and c D B@ ::: CA is an n-by-1 matrix. cn
Then Ac D c1A;1 C    C cnA;n:
In other words, Ac is a linear combination of the columns of A, with the scalars that multiply the columns coming from c.

78 CHAPTER 3 Linear Maps

Two more ways to think about matrix multiplication are given by Exercises 10 and 11.
EXERCISES 3.C

1 Suppose V and W are Ô¨Ånite-dimensional and T 2 L.V; W /. Show that

with respect to each choice of bases of V and W, the matrix of T has at

least dim range T nonzero entries.

 2 Suppose D 2 L P3.R/; P2.R/ is the differentiation map deÔ¨Åned by
Dp D p0. Find a basis of P3.R/ and a basis of P2.R/ such that the
matrix of D with respect to these bases is

0

1

1000

@ 0 1 0 0 A:

0010

[Compare the exercise above to Example 3.34. The next exercise generalizes the exercise above.]

3 Suppose V and W are Ô¨Ånite-dimensional and T 2 L.V; W /. Prove that there exist a basis of V and a basis of W such that with respect to these bases, all entries of M.T / are 0 except that the entries in row j , column j , equal 1 for 1  j  dim range T.

4 Suppose v1; : : : ; vm is a basis of V and W is Ô¨Ånite-dimensional. Suppose T 2 L.V; W /. Prove that there exists a basis w1; : : : ; wn of W such that all the entries in the Ô¨Årst column of M.T / (with respect to the bases v1; : : : ; vm and w1; : : : ; wn) are 0 except for possibly a 1 in the Ô¨Årst row, Ô¨Årst column. [In this exercise, unlike Exercise 3, you are given the basis of V instead of being able to choose a basis of V.]

5 Suppose w1; : : : ; wn is a basis of W and V is Ô¨Ånite-dimensional. Suppose T 2 L.V; W /. Prove that there exists a basis v1; : : : ; vm of V such that all the entries in the Ô¨Årst row of M.T / (with respect to the bases v1; : : : ; vm and w1; : : : ; wn) are 0 except for possibly a 1 in the Ô¨Årst row, Ô¨Årst column. [In this exercise, unlike Exercise 3, you are given the basis of W instead of being able to choose a basis of W.]

SECTION 3.C Matrices 79
6 Suppose V and W are Ô¨Ånite-dimensional and T 2 L.V; W /. Prove that dim range T D 1 if and only if there exist a basis of V and a basis of W such that with respect to these bases, all entries of M.T / equal 1.
7 Verify 3.36.
8 Verify 3.38.
9 Prove 3.52.
10 Suppose A is an m-by-n matrix and C is an n-by-p matrix. Prove that
.AC /j; D Aj; C
for 1  j  m. In other words, show that row j of AC equals (row j of A) times C .
 11 Suppose a D a1    an is a 1-by-n matrix and C is an n-by-p
matrix. Prove that
aC D a1C1; C    C anCn; :
In other words, show that aC is a linear combination of the rows of C , with the scalars that multiply the rows coming from a.
12 Give an example with 2-by-2 matrices to show that matrix multiplication is not commutative. In other words, Ô¨Ånd 2-by-2 matrices A and C such that AC ¬§ CA.
13 Prove that the distributive property holds for matrix addition and matrix multiplication. In other words, suppose A, B, C , D, E, and F are matrices whose sizes are such that A.B C C / and .D C E/F make sense. Prove that AB C AC and DF C EF both make sense and that A.B C C / D AB C AC and .D C E/F D DF C EF .
14 Prove that matrix multiplication is associative. In other words, suppose A, B, and C are matrices whose sizes are such that .AB/C makes sense. Prove that A.BC / makes sense and that .AB/C D A.BC /.
15 Suppose A is an n-by-n matrix and 1  j; k  n. Show that the entry in row j , column k, of A3 (which is deÔ¨Åned to mean AAA) is Xn Xn Aj;pAp;r Ar;k :
pD1 rD1

80 CHAPTER 3 Linear Maps
3.D Invertibility and Isomorphic Vector Spaces
Invertible Linear Maps
We begin this section by deÔ¨Åning the notions of invertible and inverse in the context of linear maps.
3.53 DeÔ¨Ånition invertible, inverse
 A linear map T 2 L.V; W / is called invertible if there exists a linear map S 2 L.W; V / such that S T equals the identity map on V and T S equals the identity map on W.
 A linear map S 2 L.W; V / satisfying S T D I and T S D I is called an inverse of T (note that the Ô¨Årst I is the identity map on V and the second I is the identity map on W ).
3.54 Inverse is unique An invertible linear map has a unique inverse.
Proof Suppose T 2 L.V; W / is invertible and S1 and S2 are inverses of T. Then
S1 D S1I D S1.T S2/ D .S1T /S2 D I S2 D S2: Thus S1 D S2.
Now that we know that the inverse is unique, we can give it a notation.
3.55 Notation T 1 If T is invertible, then its inverse is denoted by T 1. In other words, if T 2 L.V; W / is invertible, then T 1 is the unique element of L.W; V / such that T 1T D I and T T 1 D I.
The following result characterizes the invertible linear maps.
3.56 Invertibility is equivalent to injectivity and surjectivity A linear map is invertible if and only if it is injective and surjective.

SECTION 3.D Invertibility and Isomorphic Vector Spaces 81
Proof Suppose T 2 L.V; W /. We need to show that T is invertible if and only if it is injective and surjective.
First suppose T is invertible. To show that T is injective, suppose u; v 2 V and T u D T v. Then
u D T 1.T u/ D T 1.T v/ D v;
so u D v. Hence T is injective. We are still assuming that T is invertible. Now we want to prove that T is
surjective. To do this, let w 2 W. Then w D T .T 1w/, which shows that w is in the range of T. Thus range T D W. Hence T is surjective, completing this direction of the proof.
Now suppose T is injective and surjective. We want to prove that T is invertible. For each w 2 W, deÔ¨Åne S w to be the unique element of V such that T .S w/ D w (the existence and uniqueness of such an element follow from the surjectivity and injectivity of T ). Clearly T ƒ± S equals the identity map on W.
To prove that S ƒ± T equals the identity map on V, let v 2 V. Then 
T .S ƒ± T /v D .T ƒ± S/.T v/ D I.T v/ D T v:
This equation implies that .S ƒ± T /v D v (because T is injective). Thus S ƒ± T equals the identity map on V.
To complete the proof, we need to show that S is linear. To do this, suppose w1, w2 2 W. Then
T .S w1 C S w2/ D T .S w1/ C T .S w2/ D w1 C w2:
Thus S w1 C S w2 is the unique element of V that T maps to w1 C w2. By the deÔ¨Ånition of S , this implies that S.w1 C w2/ D S w1 C S w2. Hence S satisÔ¨Åes the additive property required for linearity.
The proof of homogeneity is similar. SpeciÔ¨Åcally, if w 2 W and  2 F, then
T .Sw/ D T .Sw/ D w:
Thus S w is the unique element of V that T maps to w. By the deÔ¨Ånition of S, this implies that S.w/ D S w. Hence S is linear, as desired.
3.57 Example linear maps that are not invertible  The multiplication by x2 linear map from P.R/ to P.R/ (see 3.4) is not invertible because it is not surjective (1 is not in the range).  The backward shift linear map from F1 to F1 (see 3.4) is not invertible because it is not injective [.1; 0; 0; 0; : : : / is in the null space].

82 CHAPTER 3 Linear Maps

Isomorphic Vector Spaces
The next deÔ¨Ånition captures the idea of two vector spaces that are essentially the same, except for the names of the elements of the vector spaces.

3.58 DeÔ¨Ånition isomorphism, isomorphic

 An isomorphism is an invertible linear map.
 Two vector spaces are called isomorphic if there is an isomorphism from one vector space onto the other one.

Think of an isomorphism T W V ! W as relabeling v 2 V as T v 2 W. This

viewpoint explains why two isomorphic vector spaces have the same vector

space properties. The terms ‚Äúisomorphism‚Äù and ‚Äúinvertible linear map‚Äù mean

The Greek word isos means equal; the Greek word morph means shape. Thus isomorphic literally

the same thing. Use ‚Äúisomorphism" when you want to emphasize that the two spaces are essentially the same.

means equal shape.

3.59 Dimension shows whether vector spaces are isomorphic
Two Ô¨Ånite-dimensional vector spaces over F are isomorphic if and only if they have the same dimension.
Proof First suppose V and W are isomorphic Ô¨Ånite-dimensional vector spaces. Thus there exists an isomorphism T from V onto W. Because T is invertible, we have null T D f0g and range T D W. Thus dim null T D 0 and dim range T D dim W. The formula
dim V D dim null T C dim range T
(the Fundamental Theorem of Linear Maps, which is 3.22) thus becomes the equation dim V D dim W, completing the proof in one direction.
To prove the other direction, suppose V and W are Ô¨Ånite-dimensional vector spaces with the same dimension. Let v1; : : : ; vn be a basis of V and w1; : : : ; wn be a basis of W. Let T 2 L.V; W / be deÔ¨Åned by
T .c1v1 C    C cnvn/ D c1w1 C    C cnwn:
Then T is a well-deÔ¨Åned linear map because v1; : : : ; vn is a basis of V (see 3.5). Also, T is surjective because w1; : : : ; wn spans W. Furthermore, null T D f0g because w1; : : : ; wn is linearly independent; thus T is injective. Because T is injective and surjective, it is an isomorphism (see 3.56). Hence V and W are isomorphic, as desired.

SECTION 3.D Invertibility and Isomorphic Vector Spaces 83

The previous result implies that each
Ô¨Ånite-dimensional vector space V is isomorphic to Fn, where n D dim V.
If v1; : : : ; vn is a basis of V and w1; : : : ; wm is a basis of W, then for each T 2 L.V; W /, we have a matrix M.T / 2 Fm;n. In other words, once bases have been Ô¨Åxed for V and W,
M becomes a function from L.V; W / to Fm;n. Notice that 3.36 and 3.38 show
that M is a linear map. This linear map
is actually invertible, as we now show.

Because every Ô¨Ånite-dimensional vector space is isomorphic to some Fn, why not just study Fn instead of more general vector spaces? To answer this question, note that an investigation of Fn would soon lead to other vector spaces. For example, we would encounter the null space and range of linear maps. Although each of these vector spaces is isomorphic to some Fn, thinking of them that way often adds complexity but no new insight.

3.60 L.V; W / and Fm;n are isomorphic
Suppose v1; : : : ; vn is a basis of V and w1; : : : ; wm is a basis of W. Then M is an isomorphism between L.V; W / and Fm;n.

Proof We already noted that M is linear. We need to prove that M is injec-

tive and surjective. Both are easy. We begin with injectivity. If T 2 L.V; W /

and M.T / D 0, then T vk D 0 for k D 1; : : : ; n. Because v1; : : : ; vn is a
basis of V, this implies T D 0. Thus M is injective (by 3.16). To prove that M is surjective, suppose A 2 Fm;n. Let T be the linear map

from V to W such that

X m T vk D Aj;kwj

j D1

for k D 1; : : : ; n (see 3.5). Obviously M.T / equals A, and thus the range of M equals Fm;n, as desired.

Now we can determine the dimension of the vector space of linear maps from one Ô¨Ånite-dimensional vector space to another.

3.61 dim L.V; W / D .dim V /.dim W / Suppose V and W are Ô¨Ånite-dimensional. Then L.V; W / is Ô¨Ånitedimensional and
dim L.V; W / D .dim V /.dim W /:

Proof This follows from 3.60, 3.59, and 3.40.

84 CHAPTER 3 Linear Maps

Linear Maps Thought of as Matrix Multiplication
Previously we deÔ¨Åned the matrix of a linear map. Now we deÔ¨Åne the matrix of a vector.

3.62 DeÔ¨Ånition matrix of a vector, M.v/
Suppose v 2 V and v1; : : : ; vn is a basis of V. The matrix of v with respect to this basis is the n-by-1 matrix
01 c1
M.v/ D B@ ::: CA ; cn
where c1; : : : ; cn are the scalars such that
v D c1v1 C    C cnvn:

The matrix M.v/ of a vector v 2 V depends on the basis v1; : : : ; vn of V, as well as on v. However, the basis should be clear from the context and thus it is not included in the notation.

3.63 Example matrix of a vector

 The matrix of 2 is

7x C 5x3 with respect to the standard basis of P3.R/

01

2

BB@

7 0

CCA :

5

 The matrix of a vector x 2 Fn with respect to the standard basis is obtained by writing the coordinates of x as the entries in an n-by-1 matrix. In other words, if x D .x1; : : : ; xn/ 2 Fn, then 01 x1 M.x/ D B@ ::: CA :
xn

Occasionally we want to think of elements of V as relabeled to be n-by-1
matrices. Once a basis v1; : : : ; vn is chosen, the function M that takes v 2 V to M.v/ is an isomorphism of V onto Fn;1 that implements this relabeling.

