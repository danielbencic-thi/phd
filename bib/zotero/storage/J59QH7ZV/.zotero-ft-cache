4256

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 22, NO. 7, JULY 2021

ReViewNet: A Fast and Resource Optimized Network for Enabling Safe Autonomous Driving in Hazy Weather Conditions

Aryan Mehra , Murari Mandal , Member, IEEE, Pratik Narang , and Vinay Chamola , Senior Member, IEEE

Abstract— Adverse weather conditions such as fog, haze, snow, mist and glare create visibility problems for applications of autonomous vehicles. To ensure safe and smooth operations in frequent bad weather scenarios, image dehazing is crucial to any vehicular motion and navigation task on road or air. Moreover, the commonly deployed mobile systems are resource constrained in nature. Therefore, it is important to ensure memory, compute and run-time efﬁciency of dehazing algorithms. In this manuscript we propose ReViewNet, a fast, lightweight and robust dehazing system suitable for autonomous vehicles. The network uses components like spatial feature pooling, quadruple color-cue, multi-look architecture and multi-weighted loss to effectively dehaze images captured by cameras of autonomous vehicles. The effectiveness of the proposed model is analyzed by exhaustive quantitative evaluation on ﬁve benchmark datasets demonstrating its supremacy over other existing state-of-the-art methods. Further, a component-wise ablation and loss weight ratio analysis demonstrates the contribution of each and every component of the network. We also show the qualitative analysis with special use cases and visual responses on distinctive vehicular vision instances, establishing the effectiveness of the proposed method in numerous hazy weather conditions for autonomous vehicular applications.
Index Terms— Vehicular vision, dehazing, adverse weather, deep learning, resource-efﬁcient, lightweight.
I. INTRODUCTION
V ISIBILITY for autonomous vehicular systems powered with perception based sensors for navigation and surveillance is severely hindered by adverse weather condition. For autonomous vehicles, the low-level visual perception functions such as object detection [1]–[3], segmentation [4]–[6], object tracking [7], [8] require clear image representation of the street scenes. Similarly, for accurate analysis of the surveillance videos, good quality image/frames are desired. As some of the most common bad-weather conditions, fog,
Manuscript received March 15, 2020; revised May 21, 2020 and June 29, 2020; accepted July 20, 2020. Date of publication August 14, 2020; date of current version July 12, 2021. This work was supported by the BITS Additional Competitive Research Grant through the Project titled “Disaster Monitoring from Aerial Imagery using Deep Learning” under Grant PLN/AD/2018-19/5. The Associate Editor for this article was A. Jolfaei. (Corresponding author: Vinay Chamola.)
Aryan Mehra and Pratik Narang are with the Department of Computer Science and Information Systems, BITS Pilani, Pilani 333031, India (e-mail: f20170077@pilani.bits-pilani.ac.in; pratik.narang@pilani.bits-pilani.ac.in).
Murari Mandal is with the Department of Computer Science and Engineering, Malaviya National Institute of Technology (MNIT), Jaipur 302017, India (e-mail: murarimandal.cv@gmail.com).
Vinay Chamola is with the Department of Electrical and Electronics Engineering, BITS Pilani, Pilani 333031, India (e-mail: vinay.chamola@ pilani.bits-pilani.ac.in).
Digital Object Identiﬁer 10.1109/TITS.2020.3013099

mist, and haze drastically degrade the visual quality of images. Such visibility degradation has negative impacts on the performance of other vision-based systems as mentioned above. More importantly, effective dehazing is one of the primary tasks to avoid accidents in driver-less vehicular applications [9] on land [10], water or air [11]. The dehazing methods also need to be fast and close to real-time since they have to extended to real-time video applications, which necessitates the need for resource efﬁciency. While deep learning has been used for image processing in general [12], [13], the onset of deep learning in vehicular technologies is ever increasing and efﬁcient as well [14].
The haze in the atmosphere creates whitening effect which occludes and deforms both the foreground and background. Distant haze further reduces the visibility by the accumulated veiling effect. In addition to the deterioration in color, contrast and texture features, the degradation in hazy images also increases non-linearly with change in the distance between the camera lens and the scene, making accurate dehazing a very challenging task. The effect of fog/haze is signiﬁcant in street scenes resulting in degradation of high-level perception functions of autonomous vehicles and surveillance systems [15].
In most of the dehazing algorithms, the physical scattering model is frequently used to represent image formation. In this model, the image is formulated based on the properties of light transmission through the air. The earlier learning-based dehazing methods in the literature have used the physical scattering model for dehazing. The network usually learns one or more of the components of the scattering model. However, the accuracy of the estimated atmospheric light and transmission map greatly inﬂuence the quality of the dehazed image. The disjoint optimization of transmission map or atmospheric light may hamper the overall dehazing performance.
In this paper, we formulate the image dehazing problem as an end-to-end image-to-image mapping task, free from the intermediate computation of transmission map without relying on the physical scattering model. We propose a fast, lightweight network, ReViewNet, for dehazing in autonomous vehicles. To the best of our knowledge, only few papers [16], [17] have adopted such intermediate-computation free approach using the Pix-to-Pix and Cycle-GAN architectures respectively. However, re-purposing image-to-image translation GANs for dehazing can be very difﬁcult to optimize and not necessarily produce the optimal results, as can be veriﬁed in the experimental results comparison in Section IV.

1558-0016 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

MEHRA et al.: ReViewNeT: A FAST AND RESOURCE OPTIMIZED NETWORK FOR ENABLING SAFE AUTONOMOUS DRIVING

4257

Moreover, GANs tend to be heavier in computation at train and test time, having indirectly encompassed two networks within them. The proposed ReViewNet generates more realistic hazefree images in terms of color and details in comparison to existing state-of-the-art approaches, with lesser trainable parameters and faster runtime. The main contributions of this work can be summarized as follows:
1) We propose a fast and lightweight end-to-end model ReViewNet for image dehazing. The model is free from any intermediate component computation for the physical scattering model and, thus, learns the most optimal mapping between the hazy and haze-free image while being easily extendable to real time applications that process multiple frames per second.
2) The ReViewNet is designed to have multiple looks at different stages in the network for dehazing. We use different loss weights ratio for the ﬁrst and second look. We introduce a new bottleneck parallel spatial cleaning module. Moreover, we feed the multi-cue color space (RGB, HSV, YCrCb and LAB) to the network for robust haze removal.
3) ReViewNet is a fast and highly resource-efﬁcient (5 MB model size, 399,670 trainable parameters) network and can perform image dehazing in a highly resourceconstrained environment with high speed (CPU speed – 0.28 seconds per frame, GPU speed – 0.025 seconds per frame) for real-time applications.
4) ReViewNet signiﬁcantly outperforms the existing stateof-the-art methods in terms of PSNR and SSIM in HazeRD, D-Hazy and the more recent RESIDE-Standard (SOTS), RESIDE-β (SOTS) and RESIDE-β (HSTS) datasets.
II. RELATED WORK
A. Prior Based Approaches
The Dark Channel Prior (DCP) [18] approach by He et al. estimates the transmission map and soft matts the response, while Berman et al. [19] employed Non local priors (NLD) to model the hazy image with lines in the RGB space. Further, Ancuti et al. [20] used a multi-fusion algorithm to propose a night-time dehazing approach. Authors such as [19], [21] have also estimated the atmospheric light to remove the haze. These approaches are limited by their dependence on the priors, and this causes the dehazing approach to fail in case of complicated image structures. While the results produced by such approaches often have unrealistic color distortion and contrast, their performance is also limited by the accuracy of the assumptions they make.
B. Learning Based Approaches
Learning based approaches for dehazing are generally trained to directly learn the atmospheric light, transmission map, or both. Li et al. [22] proposed AOD-Net which learns a CNN-based mapping function for the reformulated physical scattering model, while Cai et al. [23] proposed DehazeNet which estimates the intermediate transmission map which is used to generate the haze-free image. The approach of

Liu et al. [24] learned haze-relevant priors with an iteration algorithm using deep CNNs. The estimation of transmission maps or atmospheric light (or both) for image dehazing has been performed by several CNN-based architectures proposed in literature [25]–[29].
The success of GANs in image-to-image translation tasks has also attracted its use in image dehazing. Zhu et al. [30] proposed DehazeGAN which utilizes differential programming to re-formulate the atmospheric scattering model. More recently, CD-Net [31] and RI-GAN by Dudhane et al. [32] re-purposed the Cycle-GAN architecture to learn the transmission map. Some researchers [16], [17], [33] have proposed GAN based architectures which argue in favor of direct image-to-image mapping over intermediate transmission map estimation. The estimation of intermediate transmission map or atmosphere light through CNN or GAN based methods increases the training cost and result generation time. Furthermore, such architecture often fail on dense haze conditions.
III. PROPOSED METHOD
ReViewNet is unique in design and functioning, and has four pivotal contributions which make it the lightest learningbased dehazing solution – the use of quadruple color space, the double look architecture, the different loss weights ratio for the ﬁrst and second look, and the use of bottleneck parallel spatial cleaning. The use of multiple color spaces provides the network a wholesome input feature vector, which enables a faster convergence. Fig. 1 shows the network architecture in greater detail.
The following sections explain each component of the network and their contribution towards the performance achieved.

A. Quadruple-Color Space

Most of the current work in the ﬁeld of dehazing uses a single color space, mostly restricted to RGB or HSV. While RGB is the most common, YCrCb is also an absolute color space lying in the family of 3 dimensional vector color spaces. HSV aligns more closely with how the humans perceive the colors around them. The change in the amount of color perceived in Lab color space is same as the numerical change in the values, which is the inspiration behind it’s inception. The proposed method leverages information from above mentioned 4 color spaces, namely RGB, HSV, YCrCb and Lab, thus mapping the 12 channeled input to an RGB output. The interconversion between these color spaces is simply mathematical, requiring no additional storage for learnt parameters while preprocessing. Let an image matrix of height m and width n, consisting of 3 channels be represented as I3. Then the quadruple-color space matrix I12 consisting of 12 channels be represented as in Eq. 1.

I12 = [ Rm,n,1, Gm,n,1, Bm,n,1, Hm,n,1,

Sm,n,1, Vm,n,1, Ym,n,1, Cbm,n,1,

Crm,n,1, Lm,n,1, am,n,1, bm,n,1]

(1)

where R, G, B, H, S, V, Y, Cb, Cr, L, a, b, denote the red, green, blue, hue, saturation, value, luma, blue-difference, reddifference, lightness, chromaticity coordinates between red to

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

4258

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 22, NO. 7, JULY 2021

Fig. 1. ReViewNet Network Architecture: Inter and intra-carry connections with 4 path spatial pooling bottleneck.

green axis, chromaticity coordinates between yellow to blue axis, respectively. The RGB space is the most universal image depiction color space and has special importance because the output of the network is also RGB. Research has conclusively shown that the HSV color space is beneﬁcial for dehazing [34], [35]. The YCrCb color space contributes to better luminance and color contrast to the dehazed output, as analyzed by Tufail et al. [36] and Bianco et al. [37]. The YCrCb channel contributes to a lesser mean squared error as compared to the RGB channel, making it a suitable choice speciﬁcally for image dehazing and enhancement tasks. It is intuitive that the L channel of the Lab color space is pivotal to the dehazing task because haze primarily affects the lightness of the pixels, thus affecting clarity. Wang et al. [38] and [39] use the CIELAB color space modelling and remove the haze further by processing the image using simple linear iterative clustering to generate super-pixels of the image. All these advantages are clear from the performance boost that we witness by incorporating quadruple-color-space into the model, as explained in the ablation section and Table XI.
B. Multi-Look Architecture
Most prior work on image dehazing incorporate a single encoder-decoder based networks or GANs that are heavy in terms of the number of parameters and require deepening of the networks for an increase in accuracy. Moreover, a single look does not give an opportunity to create customised losses and quantitatively perceive the improvement taking place in the image as it passes through the network. It is imperative

to understand that the the multi-look architecture used by ReViewNet does not add on to the number of parameters because the second look or pass is not added after a static ﬁrst look. Instead, keeping the number of parameters and size of the architecture ﬁxed, the entire network is split into two looks – making the entire structure more efﬁcient for image enhancement and light enough to be incorporated in autonomous vehicles.
The proposed ReViewNet consist of two set of encoderdecoder modules. We denote them as the ﬁrst look (RV Net f ) and second look (RV Nets ) modules, respectively. For an input tensor Ix , the RV Net f is computed using Eq.2.

RV Net f = DeM f (En M f (Ix ))

(2)

where the ﬁrst look encoder (En M f ) and decoder (DeM f ) modules are computed as shown in Fig. 1. For the tensor Ix of size P × P with the number of channels denoted as ch,
the response of the convolutional layer (conv) with a kernel function f (·) and size h × h is computed by Eq. 3.

ch

conv(Ix ) =

f

k (h)

×

I

n j

|dk=1

(3)

j =1

where n ∈ [1, P] and d is the ﬁlter depth. Eq. 4 shows the constituent operations in the En M f (Ix ) module.

En M f (Ix ) = C f3(C f2(C f1(Ix )))

(4)

Each C fi for i ∈ [1, 2, 3] is composed of two convolution (conv) operations and one maxpool (mp) operation.

C fi = mp(conv(conv(F)))

(5)

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

MEHRA et al.: ReViewNeT: A FAST AND RESOURCE OPTIMIZED NETWORK FOR ENABLING SAFE AUTONOMOUS DRIVING

4259

where F denotes the input feature maps. Similarly, the DeM f (·) module is composed of the following set of operations.

TABLE I
DESCRIPTION OF THE TRAINING AND TESTING DATASETS (NUMBER OF IMAGES CONTRIBUTED IN EACH CATEGORY)

DeM f (En M f ) = up(conv(D f3(D f2(D f1(En M f ))))) (6)

The D fi for i ∈ [1, 2, 3] is composed of a transpose convolution (convT ) and an upsample (up) operation.

D fi = up(convT (F))

(7)

The convolutional layers in the encoder and decoder modules have internal skip connections to accelerate the learning process and enable the low-level features to propagate without progressively degrading through the network and even circumvent the bottleneck. It also helps in improving the colour and edge feature extraction process in the network.
While a primary glance is essential for learning the dehazing task, the second pass acts as a revision with certain insights of its own. The second pass also has over six carry connections from the ﬁrst look. They act as an inter-network feed forward system, ensuring that the second look deﬁnitely learns something more than the ﬁrst. The features thus circumvent the bottleneck and take the context straight to the output [40]. The second look module RV Nets is computed using Eq. 8.

RV Nets = DeMs (S P B(En Ms ([Ix , RV Net f ]))) (8)

The second look encoder (En Ms ) is composed of convolutional blocks along with the carry responses from the ﬁrst look encoder (En M f ) as shown in Eq. 9.

En Ms = Cs3(Cs2(Cs1([Ix , RV Net f ])))

(9)

where the Csi is computed as

otherwise build up in deeper layers. The SPB is computed as given in Eq. 13 and Eq. 14.

S P B = [Q, Q1, Q2, Q3, Q4]

(13)

Q = [En Ms , Cs3]

Q1 = up(conv(mp(Q)))

Q2 = up(conv(ap4(Q)))

Q3 = up(conv(ap8(Q)))

Q4 = up(conv(gp(Q)))

(14)

where ap, gp, mp denote the average, global and max pooling respectively. The ap4 and ap8 downsample the input feature maps by the factor of 4 and 8, respectively. The Q1, Q2, Q3, Q4 are encoded in parallel and concatenated with Q before passing on to the next layer.
The spatial pooling aspect is added in the second and not the ﬁrst look because the second look builds upon the output of the ﬁrst, and as demonstrated in the ablation section and Table XI, giving spatial advantage to the second look yields better results.

Csi =

mp(conv(conv([F, C fi−1]))),

if i ∈ [2, 3] (10)

mp(conv(conv([Ix, RV Net f ]))), if i = 1

The encoded features are cleansed with a spatial pooling block (SPB). More details about the SPB is discussed in the next subsection. The response of SPB block is decoded through DeMs module as given in Eq. 11.

DeM f = convt (conv(conv([Ds3(Ds2(Ds1(S P B))), D f3])) (11)

The Dsi is computed as in Eq. 12. The carry branches from the ﬁrst look decoder and second look encoder are also fed to these decoder blocks.

Dsi =

up(convT ([F, D fi−1, Cs4−i ])), if i ∈ [2, 3]

up(convT (S P B)),

if i = 1

(12)

Spatial Pooling Block: The second look has an additional parallel pooling layer that leverages the spatial dimensions of the feature vector after the encoding process as shown in Fig. 1. The feature vector is pooled via global pooling layers to extract coarse features, average pooling to extract medium level features and maxpooling to extract ﬁne features. This bottleneck also uses different spatial strides in these pooling layers to facilitate a similar intuition. Such spatial pooling helps shallow networks to extract features that would

C. Different Loss Weights
The network is capable of producing two outputs, referred as the auxiliary and the main output, produced at the end of the ﬁrst and second look respectively. The network is thus trained like a multi-output image-to-image mapping architecture and the two MSE losses are combined by multiplying them with suitable scaling factors. These scaling factors or loss weights can be treated as the importance given to the two looks. Thus if L1 is the MSE loss for the auxiliary output and L2 is the MSE loss for the main output, the net loss or principle target function for the network is given as

Lossnet = W1 L1 + W2 L2

(15)

where W1 is the scaling factor of the ﬁrst look and W2 is the scaling factor for the second look. In order to analyse the principle behind introducing a second look and understand the importance of the two looks, the scaling factors are varied across ﬁve different combinations, as discussed further in Section IV-D.

IV. EXPERIMENTS AND ANALYSIS
The model is trained and tested on a total of 5 different datasets. The details of the train-test split are highlighted in Table I along with descriptions in the datasets section

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

4260

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 22, NO. 7, JULY 2021

TABLE II
COMPARATIVE RESULTS OVER RESIDE-STANDARD SOTS [41] INDOOR DATASET

TABLE III
COMPARATIVE RESULTS OVER RESIDE-β SOTS [41] OUTDOOR DATASET

TABLE IV COMPARATIVE RESULTS OVER HAZERD [52] DATASET

below. For quantitative evaluation, structural similarity index (SSIM) and peak signal to noise ratio (PSNR) metrics are used, which are the most frequently adopted metrics for image dehazing algorithms. These metrics enable comparison of the proposed method to a wide range of existing solutions. Training takes place separately for indoor and outdoor models, primarily because autonomous vehicles for the outdoors differ signiﬁcantly from indoor applications of the same.

TABLE V COMPARATIVE RESULTS OVER HSTS DATASET [41]

A. Datasets
We use 5 datasets in this work – RESIDE-standard indoor (13,990 image pairs), RESIDE-β outdoor (72,135 image pairs), RESIDE HSTS (10 image pairs) [41], HazeRD (75 image pairs) [52] and D-Hazy (1,499 image pairs) [53]. We choose RESIDE dataset for the training because, apart from being one of the largest publicly available datasets for dehazing, it benchmarks nine representative state of the art dehazing methods by providing full reference evaluation metrics like PSNR and SSIM for the synthetic objective testing set (SOTS). For robustness, the indoor models are also trained on the D-Hazy [53] dataset. Further, the train-test split is highlighted in the Table I.

TABLE VI COMPARATIVE RESULTS OVER D-HAZY [53] DATASET

B. Quantitative Analysis
We report the average PSNR and SSIM of all stated networks and the proposed method. Since the proposed method outperforms all the existing state-of-the-art on all mentioned datasets, we also report the percentage increase it brings on every existing method. Table II to Table VI clearly demonstrate the supremacy of the proposed method on PSNR and SSIM as compared to other benchmarks that exist on these datasets. As an example, Table III demonstrates how ReViewNet outperforms the existing methods with improvements ranging from 8.08 to 33.56 percent in PSNR on MADN and DCP methods respectively. There is a 2.70 percent improvement over Deep DCP method on the HSTS dataset in Table V on SSIM and a remarkable 29.80 percent over BCCR method.

C. Qualitative Analysis for Safe Autonomous Driving
We present a detailed qualitative analysis which visually depicts the efﬁcacy of ReViewNet for enabling safe autonomous driving in hazy weather conditions. A visual comparison with existing approaches also establishes the superior performance of ReViewNet. We delineate several use-cases

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

MEHRA et al.: ReViewNeT: A FAST AND RESOURCE OPTIMIZED NETWORK FOR ENABLING SAFE AUTONOMOUS DRIVING

4261

Fig. 2. Qualitative comparison: Multiple objects in a scene. Magnify for minor details.

Fig. 3. Qualitative comparison: Roadside signs and distant T-Point dead-end enhancement. Magnify for minor details.

related to the autonomous vehicular context in hazy weather conditions. ReViewNet is able to perform effective dehazing in a large variety of driving scenarios and consistently produces haze-free images with clear color and contrast details. These results further strengthen the application of dehazed output of ReViewNet for Vision-based vehicular applications like object detection or semantic segmentation.
1) Multiple Objects and Contextual Clarity: Real life situations will require the autonomous vehicles to occasionally enter busy streets full of two-wheelers, pedestrians and dynamic road surfaces as depicted in Fig. 2. The highlighted ares of the image depict how ReViewNet elegantly dehazes the image with clarity in context of the pedestrian, vehicles and distant objects. While the other deep learning based solutions like AODNet and DCPDN fail to provide complete dehazing, mathematical models like DCP, NLD and FVR introduce an unusual color contrast, hindering object detection or road segmentation tasks performed by an autonomous vehicle.

2) Road Sign and Structural Navigation Integrity: Road signs and intersections like T-points are extremely important in the context of autonomous vision based driving applications. Fig. 3 shows how the parking sign is most clearly visible in the dehazed image produced by ReViewNet, the output being closest to the available ground truth as well. Similarly the distant T-point is most clearly dehazed by the proposed method as compared to other methods demonstrated in Fig. 3. For networks with extremely low runtime for real time applications, it is rare to witness such structural integrity and nuance.
3) Short Distance Obstacles and Urban Dynamic Trafﬁc Situations: It is important for the image to maintain a color contrast similar to the actual truth and have clearly visible road surfaces for navigation. ReViewNet effectively uses its spatial feature extraction and skip connections to obtain the dehazed output as close to the ground truth as possible, as depicted in Fig. 4. Similarly Fig. 5 shows the same effect on a real dynamic trafﬁc image taken from a trafﬁc camera.

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

4262

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 22, NO. 7, JULY 2021

Fig. 4. Qualitative comparison: Short distance and color contrast quality in dehazing outputs. Magnify for minor details.

Fig. 5. Qualitative comparison: Urban trafﬁc and dynamic scenarios. Magnify for minor details.

Fig. 6. Qualitative comparison: Comparison of dehazing in atmospheric dispersion. Magnify for minor details. TABLE VII
LOSS WEIGHT ANALYSIS FOR DIFFERENT WEIGHT RATIOS FOR THE FIRST AND SECOND LOOK
4) Atmospheric Light and Dispersion: Fig. 6 depicts how instead form a circumvented image about the source of light non-learning based methods like DCP, BCCR, NLD and in haze. While the learning based methods like AODNet FVR cannot sometimes deal with atmospheric dispersion and and DCPDN do overcome that issue, they fail to dehaze the
Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

MEHRA et al.: ReViewNeT: A FAST AND RESOURCE OPTIMIZED NETWORK FOR ENABLING SAFE AUTONOMOUS DRIVING

4263

Fig. 7. Vehicular vision speciﬁc qualitative use cases. Magnify for minor details.

image completely. ReViewNet, on the other hand, distinctively dehazes the aerial view with minimal distortion in structure and color contrast.
5) Speciﬁc Use Cases for Autonomous Transportation Safety and Surveillance: In this section, we discuss several use cases for the utility of the proposed ReViewNet. Fig. 7 highlights over 32 examples of hazy and our dehazed outputs, divided into 5 categories – trafﬁc camera view, autonomous

driving road navigation, road navigation and pedestrian prone crossings, railroad and waterway use cases and aerial (dronebased) building and rooftop applications.
The proposed system is a useful preprocessor for several vision based trafﬁc analysis tasks such as object detection, lane detection, and segmentation. The ﬁrst ﬁve rows in Fig. 7 highlight the effectiveness of dehazing in the prominence of zebra crossings and turnings of the road that are essential

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

4264

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 22, NO. 7, JULY 2021

Fig. 8. Graphs depicting loss weight ratio to decide W 1 and W 2. (a) PSNR values for all the datasets at different W 1/ W 2 ratios, (b) SSIM values for all the datasets at different W 1/ W 2 ratios.

for automated decisions required in autonomous vehicles, such road lane segmentation and lane changes. The photos of pedestrian-prone areas clearly show how, in some cases, object which were not clearly visible in the hazy image are made visible in the dehazed output, which in turn improves object detection. The dehazed objects do not merge with the haze anymore, and they become more prominent and natural, despite the lightweight and fast architecture of the model. These objects range from parking lot signs to number plates. The sixth row in Fig. 7 extend to applications beyond roads to railroad and water bodies. Sample images in the last two rows in Fig. 7 demonstrate that aerial imagery, that is simulated from the perspective of delivery drones, is effectively dehazed despite the adverse solar glare present in the images. Detecting rooftops better can act as a big advantage to aerial vehicular technology for adverse weather conditions. During our experimentation, the only limitation that was observed was on images with artiﬁcial dense haze combined with mushy or fuzzy green backgrounds. ReViewNet could not fully dehaze them. As a future prospect, it is planned to overcome this difﬁculty.
D. Loss Weight Ratio Analysis and Explainable Revision Improvement
We now answer the mathematical basis for varying the loss ratio of the ﬁrst and second look. As stated before, the main impact of the multi-look architecture is seen in the reduction of running time and lightweight architecture of the model. We now experimentally demonstrate that given the same weight (50-50 ratio) for both the looks, the second look does in fact learn more than the ﬁrst through Table VIII. It depicts how the main output is consistently better than the ﬁrst on all four datasets in the table on both PSNR and SSIM, thus reiterating the fact and assumption that the second look improves upon the ﬁrst like a “revision” or “review”.
To decide the weights of the ﬁrst and second look, we create ﬁve trials that have ratio of losses for the ﬁrst and second

TABLE VIII COMPARISON BETWEEN EQUALLY WEIGHTED FIRST
(AUXILIARY) AND SECOND (MAIN) OUTPUTS
look as depicted by the Table VII. The table depicts the ﬁnal output (second look) of the ﬁve trials. The Fig. 8 shows the graphical plots of these values for SSIM and PSNR. We can clearly see that though the second look is more important than the ﬁrst, the sharp distinctive peak at a weight ratio of 0.4 for the ﬁrst versus 0.6 for the second look shows an optimal conﬁguration. After this, the graphs takes a dip again. The values at 0.3/0.7 weight are still better than the 0.6/0.4 and 0.7/0.3, demonstrating that the second look is learning much more than the ﬁrst. Thus, context speciﬁc novelty of the experimentation is observed. We successfully establish the technique of dividing the network into two parts, boosting performance without increase in parameters with unique dehazing-speciﬁc yet computationally simple components for image enhancement.
E. Running Time Analysis For all vehicular vision applications, dehazing will pre-
dominantly be used as a pre-processing step and hence it necessitates that it be fast as well as computationally efﬁcient. We present a comparison of the state of the art learning based and mathematical methods in terms of their running time on CPU and GPU environments. We see from Table IX and X that the proposed ReViewNet is the fastest strategy among all methods. AODNet [22] is famous for being one of the fastest networks, and ReViewNet is almost three times faster in computation, without any compromise in PSNR or SSIM.

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

MEHRA et al.: ReViewNeT: A FAST AND RESOURCE OPTIMIZED NETWORK FOR ENABLING SAFE AUTONOMOUS DRIVING
TABLE IX AVERAGE PER IMAGE CPU RUNNING TIME (IN SECONDS) COMPARISON OF VARIOUS METHODS OVER RESIDE SOTS IMAGES

4265

TABLE X AVERAGE PER IMAGE GPU RUNNING TIME (IN SECONDS) COMPARISON OF VARIOUS METHODS

TABLE XI ABLATION ANALYSIS. T1 IS WITHOUT THE MULTI-COLOR SPACE, T2 HAS
SPATIAL POOLING IN THE FIRST LOOK AND T3 IS A SINGLE LOOK ARCHITECTURE. THE REST OF THE PARAMETERS ARE CONSTANT
The running time shown here includes the multi color-cue spacial modelling and inter-conversion as well. This will give a tremendous boost to the top level vision tasks on aerial drones and autonomous cars and boats, making the model most suitable for typical autonomous vehicle video frame rates upto the range of 40 frames per second.
F. Ablation and Novelty Analysis The ablation analysis in Table XI shows the rationale
behind several design choices by comparing performance on two of the largest datasets in the experiment. Comparison of ReViewNet with t1 shows the quantitative contribution of multi-color space, while comparison with t2 shows the increase in accuracy when spatial pooling is used in the second look rather than the ﬁrst. An explanation for this is also hinted in the fact that since the second look uses the output of the ﬁrst along with the original image as well, giving it the spatial component gives the network an upper hand. The performance increase by dividing the network into two parts is seen by comparison with t3, showing how ReViewNet is better than a simple encoder decoder with spatial pooling. Hence, the novelty of the network is observed in its use of multi-color spaces, multi-look architecture, loss weighting, spatial pooling, skip connections and computation efﬁcient dehazing without dependency on physical phenomenon. It is the harmonious synchronisation of all these aspects that makes the network unique and, more importantly, context-speciﬁc for dehazing in real-time autonomous driving applications. This harmony is also demonstrated in the fact that the network outperforms all state of the art in the ﬁeld, in both PSNR ans SSIM metrics. Moreover, we experimentally establish a precedent that image enhancement tasks will see an increase in performance by dividing the network into two parts, keeping the training parameters same.

V. CONCLUSION
This work presents a fast and resource-efﬁcient network, ReViewNet, for image dehazing which is suitable for real time applications in autonomous driving. The ReViewNet architecture looks twice over the hazy image and the network is optimized with a hybrid weighted loss. Ablation analysis of components and experimentation on ratios of the two different losses are conducted to determine the optimal weight ratio. We demonstrate sample qualitative results over 32 different scenarios for speciﬁc use-cases of vehicular vision. This work adds to the state-of-the-art by demonstrating conclusive proof that using context speciﬁc components and features of a deep learning network can result in faster (0.025 seconds per frame on a GPU) and more accurate image enhancement modules. This further leads better preprocessing and higher performance in Vision-based tasks for vehicular technologies such as object detection and road segmentation, thus enabling safer autonomous driving. An exhaustive experimental analysis on ﬁve benchmark haze datasets demonstrates that the proposed ReViewNet network signiﬁcantly outperforms all the existing state-of-the-art CNN and GAN based methods in quantitative, qualitative, and computation speed evaluations.
REFERENCES
[1] L. Zhou, W. Min, D. Lin, Q. Han, and R. Liu, “Detecting motion blurred vehicle logo in IoV using ﬁlter-DeblurGAN and VL-YOLO,” IEEE Trans. Veh. Technol., vol. 69, no. 4, pp. 3604–3614, Apr. 2020.
[2] M. Mandal, M. Shah, P. Meena, and S. K. Vipparthi, “SSSDET: Simple short and shallow network for resource efﬁcient vehicle detection in aerial scenes,” in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2019, pp. 3098–3102.
[3] M. Mandal, M. Shah, P. Meena, S. Devi, and S. K. Vipparthi, “AVDNet: A small-sized vehicle detection network for aerial visual data,” IEEE Geosci. Remote Sens. Lett., vol. 17, no. 3, pp. 494–498, Mar. 2020.
[4] M. Mandal, M. Chaudhary, S. K. Vipparthi, S. Murala, A. B. Gonde, and S. K. Nagar, “ANTIC: Antithetic isomeric cluster patterns for medical image retrieval and change detection,” IET Comput. Vis., vol. 13, no. 1, pp. 31–43, Feb. 2019.
[5] M. Mandal, P. Saxena, S. K. Vipparthi, and S. Murala, “CANDID: Robust change dynamics and deterministic update policy for dynamic background subtraction,” in Proc. 24th Int. Conf. Pattern Recognit. (ICPR), Aug. 2018, pp. 2468–2473.
[6] M. Mandal, V. Dhar, A. Mishra, and S. K. Vipparthi, “3DFR: A swift 3D feature reductionist framework for scene independent change detection,” IEEE Signal Process. Lett., vol. 26, no. 12, pp. 1882–1886, Dec. 2019.
[7] P.-H. Chiu, P.-H. Tseng, and K.-T. Feng, “Interactive mobile augmented reality system for image and hand motion tracking,” IEEE Trans. Veh. Technol., vol. 67, no. 10, pp. 9995–10009, Oct. 2018.
[8] M. Mandal, L. K. Kumar, M. S. Saran, and S. K. Vipparthi, “MotionRec: A uniﬁed deep framework for moving object recognition,” in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), Mar. 2020, pp. 2734–2743.

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

4266

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 22, NO. 7, JULY 2021

[9] A. Jolfaei and K. Kant, “Privacy and security of connected vehicles in intelligent transportation system,” in Proc. 49th Annu. IEEE/IFIP Int. Conf. Dependable Syst. Netw.-Supplemental Volume (DSN-S), Jun. 2019, pp. 9–10.
[10] M. Shaﬁq, Z. Tian, A. K. Bashir, A. Jolfaei, and X. Yu, “Data mining and machine learning methods for sustainable smart cities trafﬁc classiﬁcation: A survey,” Sustain. Cities Soc., vol. 60, Sep. 2020, Art. no. 102177.
[11] S. Kouroshnezhad, A. Peiravi, M. S. Haghighi, and A. Jolfaei, “An energy-aware drone trajectory planning scheme for terrestrial sensors localization,” Comput. Commun., vol. 154, pp. 542–550, Mar. 2020.
[12] G. Bansal, V. Chamola, P. Narang, S. Kumar, and S. Raman, “Deep3DSCan: Deep residual network and morphological descriptor based framework for lung cancer classiﬁcation and 3D segmentation,” IET Image Process., vol. 14, no. 7, pp. 1240–1247, May 2020.
[13] A. Mehra, N. Jain, and H. S. Srivastava, “A novel approach to use semantic segmentation based deep learning networks to classify multi-temporal SAR data,” Geocarto Int., pp. 1–16, 2020, doi: 10.1080/10106049.2019.1704072.
[14] V. Hassija, V. Gupta, S. Garg, and V. Chamola, “Trafﬁc jam probability estimation based on blockchain and deep neural networks,” IEEE Trans. Intell. Transp. Syst., early access, Jun. 3, 2020, doi: 10.1109/TITS.2020.2988040.
[15] S. Huang, B. Chen, and Y. Cheng, “An efﬁcient visibility enhancement algorithm for road scenes captured by intelligent transportation systems,” IEEE Trans. Intell. Transp. Syst., vol. 15, no. 5, pp. 2321–2332, Oct. 2014, doi: 10.1109/TITS.2014.2314696.
[16] Y. Qu, Y. Chen, J. Huang, and Y. Xie, “Enhanced Pix2pix dehazing network,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 8160–8168.
[17] D. Engin, A. Genc, and H. K. Ekenel, “Cycle-dehaze: Enhanced CycleGAN for single image dehazing,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2018, pp. 825–833.
[18] K. He, J. Sun, and X. Tang, “Single image haze removal using dark channel prior,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 12, pp. 2341–2353, Dec. 2011.
[19] D. Berman, T. Treibitz, and S. Avidan, “Non-local image dehazing,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 1674–1682.
[20] C. Ancuti, C. O. Ancuti, C. De Vleeschouwer, and A. C. Bovik, “Nighttime dehazing by fusion,” in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2016, pp. 2256–2260.
[21] M. Sulami, I. Glatzer, R. Fattal, and M. Werman, “Automatic recovery of the atmospheric light in hazy images,” in Proc. IEEE Int. Conf. Comput. Photogr. (ICCP), May 2014, pp. 1–11.
[22] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng, “AOD-Net: All-inone dehazing network,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 4770–4778.
[23] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, “DehazeNet: An end-to-end system for single image haze removal,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5187–5198, Nov. 2016.
[24] Y. Liu, J. Pan, J. Ren, and Z. Su, “Learning deep priors for image dehazing,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 2492–2500.
[25] Y.-T. Peng, Z. Lu, F.-C. Cheng, Y. Zheng, and S.-C. Huang, “Image haze removal using airlight white correction, local light ﬁlter, and aerial perspective prior,” IEEE Trans. Circuits Syst. Video Technol., vol. 30, no. 5, pp. 1385–1395, May 2020.
[26] S. Chen, Y. Chen, Y. Qu, J. Huang, and M. Hong, “Multi-scale adaptive dehazing network,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2019, pp. 1–9.
[27] W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao, and M.-H. Yang, “Single image dehazing via multi-scale convolutional neural networks,” in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2016, pp. 154–169.
[28] W. Ren et al., “Gated fusion network for single image dehazing,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 3253–3261.
[29] C. O. Ancuti et al., “NTIRE 2020 challenge on NonHomogeneous dehazing,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2020, pp. 490–491.
[30] H. Zhu, X. Peng, V. Chandrasekhar, L. Li, and J.-H. Lim, “DehazeGAN: When image dehazing meets differential programming,” in Proc. IJCAI, Jul. 2018, pp. 1234–1240.

[31] A. Dudhane and S. Murala, “CDNet: Single image de-hazing using unpaired adversarial training,” in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2019, pp. 1147–1155.
[32] A. Dudhane, H. S. Aulakh, and S. Murala, “RI-GAN: An end-toend network for single image haze removal,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2019, pp. 1–10.
[33] A. Mehta, H. Sinha, P. Narang, and M. Mandal, “HIDeGan: A hyperspectral-guided image dehazing GAN,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2020, pp. 212–213.
[34] Y. Wan and Q. Chen, “Joint image dehazing and contrast enhancement using the HSV color space,” in Proc. Vis. Commun. Image Process. (VCIP), Dec. 2015, pp. 1–4.
[35] T. Zhang, H.-M. Hu, and B. Li, “A naturalness preserved fast dehazing algorithm using HSV color space,” IEEE Access, vol. 6, pp. 10644–10649, 2018.
[36] Z. Tufail, K. Khurshid, A. Salman, I. F. Nizami, K. Khurshid, and B. Jeon, “Improved dark channel prior for image defogging using RGB and YCbCr color space,” IEEE Access, vol. 6, pp. 32576–32587, 2018.
[37] S. Bianco, L. Celona, F. Piccoli, and R. Schettini, “High-resolution single image dehazing using encoder-decoder architecture,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2019.
[38] P. Wang, Q. Fan, Y. Zhang, F. Bao, and C. Zhang, “A novel dehazing method for color ﬁdelity and contrast enhancement on mobile devices,” IEEE Trans. Consum. Electron., vol. 65, no. 1, pp. 47–56, Feb. 2019.
[39] M. Yang, J. Liu, and Z. Li, “Superpixel-based single nighttime image haze removal,” IEEE Trans. Multimedia, vol. 20, no. 11, pp. 3008–3018, Nov. 2018.
[40] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” 2016, arXiv:1611.07004. [Online]. Available: https://arxiv.org/abs/1611.07004
[41] B. Li et al., “Benchmarking single-image dehazing and beyond,” IEEE Trans. Image Process., vol. 28, no. 1, pp. 492–505, Jan. 2019.
[42] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 2223–2232.
[43] J.-P. Tarel and N. Hautiere, “Fast visibility restoration from a single color or gray level image,” in Proc. IEEE 12th Int. Conf. Comput. Vis., Sep. 2009, pp. 2201–2208.
[44] G. Meng, Y. Wang, J. Duan, S. Xiang, and C. Pan, “Efﬁcient image dehazing with boundary constraint and contextual regularization,” in Proc. IEEE Int. Conf. Comput. Vis., Dec. 2013, pp. 617–624.
[45] A. Dudhane and S. Murala, “C2MSNet: A novel approach for single image haze removal,” in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), Mar. 2018, pp. 1397–1404.
[46] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 1125–1134.
[47] X. Yang, Z. Xu, and J. Luo, “Towards perceptual image dehazing by physics-based disentanglement and adversarial training,” in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 1–8.
[48] Q. Zhu, J. Mai, and L. Shao, “A fast single image haze removal algorithm using color attenuation prior,” IEEE Trans. Image Process., vol. 24, no. 11, pp. 3522–3533, Nov. 2015.
[49] C. Chen, M. N. Do, and J. Wang, “Robust image and video dehazing with visual artifact suppression via gradient residual minimization,” in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2016, pp. 576–591.
[50] A. Dudhane and S. Murala, “RYF-Net: Deep fusion network for single image haze removal,” IEEE Trans. Image Process., vol. 29, pp. 628–640, 2020.
[51] H. Zhang and V. M. Patel, “Densely connected pyramid dehazing network,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 3194–3203.
[52] Y. Zhang, L. Ding, and G. Sharma, “HazeRD: An outdoor scene dataset and benchmark for single image dehazing,” in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2017, pp. 3205–3209.
[53] C. Ancuti, C. O. Ancuti, and C. De Vleeschouwer, “D-HAZY: A dataset to evaluate quantitatively dehazing algorithms,” in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2016, pp. 2226–2230.
[54] A. Golts, D. Freedman, and M. Elad, “Unsupervised single image dehazing using dark channel prior loss,” 2018, arXiv:1812.07051. [Online]. Available: http://arxiv.org/abs/1812.07051

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on May 13,2022 at 12:48:54 UTC from IEEE Xplore. Restrictions apply.

