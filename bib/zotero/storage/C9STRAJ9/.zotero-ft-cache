IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathMenu.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Journals & Magazines > IEEE Access > Volume: 7
Image Enhancement via Indented Frame Over Fusion
Publisher: IEEE
Cite This
PDF
  << Results   
Baoju Zhang ; Wenrui Yan ; Gang Li ; Jingqi Fei ; Cuiping Zhang ; Chuyi Chen
All Authors
View Document
1
Paper
Citation
388
Full
Text Views
Open Access
Comment(s)

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Under a Creative Commons License
Abstract
Document Sections

    I.
    Introduction
    II.
    Related Work
    III.
    Experimental Basis
    IV.
    Process and Results
    V.
    Conclusion

Authors
Figures
References
Citations
Keywords
Metrics
More Like This

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

The architecture of the method based on Indented Frame Over Fusion.
The architecture of the method based on Indented Frame Over Fusion.
Abstract: In this era of artificial intelligence (AI), the Internet of things (IoT) with the capability of connecting a great number of heterogeneous terminals and the popularity o... View more
Topic: Mission Critical Sensors and Sensor Networks (MC-SSN)
Metadata
Abstract:
In this era of artificial intelligence (AI), the Internet of things (IoT) with the capability of connecting a great number of heterogeneous terminals and the popularity of mobile devices, which makes more devices available as another pair of eyes for people, such as video surveillance and smart navigation. As image enhancement is assisting people to better work together and helping people live a smarter life, it is therefore becoming increasingly important. Nevertheless, many vision systems are sensitive to factors like the scattering of light or motion which may cause blur. This paper promotes an image enhancement method and is dedicated to reduce the adverse effects of blurred images on vision systems. We first researched the theory of indented frame over fusion (IFOF) and presented an automatic screening function. Then subtly combined color reversal, registration and transformation. Lastly, we tested two examples with this method and gained good results. Image processed by this method, facing strong scattering of light environment, has improved its quality and visual effects. With the development of this kind of technology, there will be more practical and intelligent applications in our lives, such as license plate numbers recognition in bad weather and important items search on fire scenes.
Topic: Mission Critical Sensors and Sensor Networks (MC-SSN)
Published in: IEEE Access ( Volume: 7 )
Page(s): 181092 - 181099
Date of Publication: 29 November 2019
Electronic ISSN: 2169-3536
INSPEC Accession Number: 19214533
DOI: 10.1109/ACCESS.2019.2956747
Publisher: IEEE
Funding Agency:
The architecture of the method based on Indented Frame Over Fusion.
The architecture of the method based on Indented Frame Over Fusion.
Hide Full Abstract
Contents
CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https :// creativecommons . org / licenses / by / 4 . 0 / to obtain full-text articles and stipulations in the API documentation.
SECTION I.
Introduction

Nowadays, as the life of humans becomes ever more digital, humans are beginning to enter the era of intelligence. Meanwhile, with the improvement of algorithms and the emergence of artificial intelligence (AI), internet of things (IoT) and wireless communication (e.g., 5th generation wireless systems [1] ), people get down to develop the architecture of smart cities [2] and new types electronic products are becoming popular [3] , such as smart watches and smart glasses. That means, only the smart devices (e.g., smart phone) and its relevant applications enable people to control every object and gain diversified information through the internet in the daily life [4] . For example, it can assist in supervising or predicting traffic conditions before commuting with the help of smart phones, and the house owner can adjust the light intensity and monitor indoor temperatures and humidity by connecting the smart phone to internet. Besides, it is an inevitable trend for enterprises and factories to bring in intelligent facilities as a competitive edge over the shortage of labor force which improves production efficiency and ensures their sustainable yield [5] . Applications of IoT and other technologies are also employed in other domains, such as in agriculture [6] , [7] , smart hospitals [8] , [9] , geological detection and topography and environmental change [10] . It cannot be denied that these new sorts of technologies such as IoT, AI have already become part of our lives and various work fields, which indeed bring about tremendous convenience to human.

With the advancement in technologies like IoT and service applications, there will be an increasing demand for image enhancement to keep up with the pace of advancement. Firstly, people need to gain information via certain equipment such as sensors and webcam to process and analyze data [11] . In some situations such as overexposure or low light [12] – [13] [14] [15] , depending on the hardware used, image enhancement becomes necessary. In the medical sector, image enhancement plays an important role in detecting diseases which are difficult for human eyes to directly diagnose, due to the limitations of the imaging mechanism of the equipment [16] , [17] from unprocessed images [18] , [19] . With the help of image enhancement, the valuable content will be highlighted [20] and the noise will be reduced [21] , [22] which certainly enhance the accuracy and reliability of the diagnosis results. Image processing is also applied in forest management [23] , facial recognition technology [24] , [25] , magnetic resonance imaging (MRI) [26] , surveillance immune influence of variations in weather conditions [27] and target tracking [28] – [29] [30] [31] . Image processing technology can help extract features which are required in classification [32] and discrimination. Furthermore, the demand of customers for the sharpness of images in smart phones is growing, and some Applications downloaded onto smart phones like filters and beauty cameras [33] , are supported by image processing technology. Hence, image processing technology including image enhancement has real significance in promoting the development of intelligent life and construction of smart cities.

This paper proposes a method to enhance the image quality. Firstly, we presented a theoretical study of image enhancement in which the indented frame over fusion (IFOF) and automatic screening function are expounded. The calculation method of the signal-to-noise ratio (SNR) multiples, which, in theory, can be improved by IFOF, was given in this section. Secondly, this article creatively combined color reversal, image matching, image transformation, IFOF with automatic screening function organically. Finally, the image enhancement method proposed by this paper verified its feasibility through two types of specific data collected by our team.

Section II studied and discussed theories used in this image enhancement method. In Section III , the method was validated through two specific cases. Lastly, Section IV discusses the conclusions reached in this paper.
SECTION II.
Related Work
A. Image Enhancement

The origin of image enhancement dates back to the early 1920s when the first picture was transmitted from London to New York by Bartlane method of simulating the centre hue to recover the picture [34] . In the initial period of image enhancement techniques often involved setting hardware parameters, such as the choice of printing process and the distribution of brightness levels. In the early 1960s, the first mainframe computer with the function of digital image processing was manufactured, which symbolized the advent of digital image processing era. The research of Jet Propulsion Laboratory (JPL) adopted geometric correction, gray transform, suppression of noise, Fourier transform and two-dimensional linear filtering to deal with the thousands of moon pictures from the space probe Prowler 7 in 1964, and repainted the image of the lunar surface [35] . In the end of 1960s and the early 1970s, the researchers started to combine image processing with remote sensing techniques, medical imaging and astronomy, and in 1895 X-ray which is used widely in medical image nowadays, was discovered by Wilhelm Roentgen [36] . With the development of electronic devices toward excellent performance, small size and high reliability [37] , in 1990s, the technology of image enhancement had embedded in every aspect of human life and social development. At the present, with the booming of cloud computing, big data, deep learning [38] and neural network algorithms [39] , [40] , image enhancement becomes increasingly generalized. There are brightness enhancement, gray enhancement [41] and sharpness enhancement [42] in the field of image enhancement, applied in specific circumstances such as foggy weather [43] .
B. Image Fusion

Image fusion which can integrate advanced information is a combination of multiple images in the same scene. Its processing level can be divided into three categories: 1) pixel level 2) feature level 3) decision level [44] . At the outset, frame accumulated can be deemed to a special image fusion. The SNR can be improved n times when n source images are almost equal [45] . Burt and Adelson [46] proposed Laplacian pyramid fusion scheme which applied the concept of pyramidal decomposition to the image fusion. The gradient pyramid adopted gradient operator of Gaussian pyramid’s every level. Wavelet fusion [47] was first put forward in 1990 as the better method. Rockinger and Fechner [48] used the undecimated discrete wavelet transform (UDWT) to solve artifacts. Compared with UDWT, this strategy which introduces the dual-tree complex wavelet transformation has less redundancy. The above methods are all pixel level image fusion techniques. As for the feature level, Golipour et al. [49] devised another method, under the Bayesian framework, which integrated hierarchical segmentation results with Markov random field prior, classified hyperspectral image in spectral space. Fauvel et al. [50] proposed a multiple classifiers fusion scheme which offered complementary services for information. Decision fusion is a cognitive-based approach. For example, the pansharpening method proposed by Luo et al. [51] and the object recognition method proposed by Mahmoudi et al. [52] belongs to the application of decision fusion level.
SECTION III.
Experimental Basis
A. Indented Frame Over Fusion

In some common strong scattering environments, such as fog or haze, it is hard for the naked eye or camera to have a clear view. At this point, the importance of improving the quality of the captured image is revealed. It can help people see things in such environment and get more information they need. We set out to come up with a method which would be an efficient way to address this kind of problems: IFOF, which enhances the image SNR and contrast.

In this scenario, the video will be divided into multiple frames. After that, we select the first frame and the next frame to match the feature points, and transform the next frame image according to the matching point, so that the correspondence of the pixel points between the two frames is more accurate, and the two images are respectively fused by the ratio of \$u\$ and \$v\$ to obtain the new image. The new image continues to match, transform, and blend with the next frame until the last frame merges. Because the time interval between the adjacent frames is too short to make great changes and easy for subsequent discussion, the image signal of each frame, approximate to equal, \$s_{1} =s_{2} =\ldots =\text {s}_{n} =\text {s}\$ ; the random noise of each frame, \$z_{1} \$ , z 2 , \$\ldots \$ , \$z_{n} \$ , can be considered as independent and identically distributed. Suppose random noise \$\text {z}_{i} \$ satisfies distribution \$G\$ , then we can use \$\sigma ^{2}\$ to represent the variance of \$\text {z}_{i} \$ : \begin{equation*} D\left ({{\text {z}_{\text {i}}} }\right)=\sigma ^{2}\tag{1}\end{equation*} View Source \begin{equation*} D\left ({{\text {z}_{\text {i}}} }\right)=\sigma ^{2}\tag{1}\end{equation*} where i = 1, 2, 3, \$\ldots\$ , n.

The SNR of the first frame could be expressed as \begin{equation*} SNR_{1} =\sqrt {\frac {s^{2}}{\sigma ^{2}}}\tag{2}\end{equation*} View Source \begin{equation*} SNR_{1} =\sqrt {\frac {s^{2}}{\sigma ^{2}}}\tag{2}\end{equation*}

Concentrate the first and second maps by assigning weights \$u\$ and \$v\$ respectively, and then a new image is obtained. Its SNR is as follows: \begin{align*} SNR_{2}=&\sqrt {\frac {(u\ast s_{1} +\text {v}\ast \text {s}_{2})^{2}}{D(u\ast z_{1} +v\ast z_{2})}} \\=&\sqrt {\frac {\left ({{u+v} }\right)^{2}}{u^{2}+v^{2}}\ast \frac {s^{2}}{\sigma ^{2}}} \tag{3}\end{align*} View Source \begin{align*} SNR_{2}=&\sqrt {\frac {(u\ast s_{1} +\text {v}\ast \text {s}_{2})^{2}}{D(u\ast z_{1} +v\ast z_{2})}} \\=&\sqrt {\frac {\left ({{u+v} }\right)^{2}}{u^{2}+v^{2}}\ast \frac {s^{2}}{\sigma ^{2}}} \tag{3}\end{align*}

It is worth mentioning that this method is intended to try to increase the contrast of the image by over fusion which is reflected in following inequalities: \$0< u< 1\$ , \$0< v< 1\$ , 1 < u + v.

Continue to fusion the newly obtained image with the subsequent frame to get the updated image, whose SNR is \begin{align*} SNR_{3}=&\sqrt {\frac {\left [{ {u\ast (u\ast s_{1} +\text {v}\ast \text {s}_{2})+\text {v}\ast \text {s}_{3}} }\right]^{2}}{D\left ({{u\ast (u\ast z_{1} +v\ast z_{2})+v\ast z_{3} } }\right)}} \\=&\sqrt {\frac {\left ({{u^{2}+u\ast v+v} }\right)^{2}}{u^{4}+u^{2}\ast v^{2}+v^{2}}\ast \frac {s^{2}}{\sigma ^{2}}}\tag{4}\end{align*} View Source \begin{align*} SNR_{3}=&\sqrt {\frac {\left [{ {u\ast (u\ast s_{1} +\text {v}\ast \text {s}_{2})+\text {v}\ast \text {s}_{3}} }\right]^{2}}{D\left ({{u\ast (u\ast z_{1} +v\ast z_{2})+v\ast z_{3} } }\right)}} \\=&\sqrt {\frac {\left ({{u^{2}+u\ast v+v} }\right)^{2}}{u^{4}+u^{2}\ast v^{2}+v^{2}}\ast \frac {s^{2}}{\sigma ^{2}}}\tag{4}\end{align*} \${_{\mathrm {\ldots }}}\$

The n th updated image SNR is \begin{align*}&\hspace {-0.6pc}SNR_{\text {n}} \\=&\sqrt {{\frac {(u^{\text {n}-1}+u^{n-2}\ast v+\ldots +u\ast v+v)^{2}}{(u^{\text {n}-1})^{2}+(u^{n-2}\ast v)^{2}+\ldots +(u\ast v)^{2}+v^{2}}\ast \frac {s^{2}}{\sigma ^{2}}}} \\ =&\sqrt {\frac {\left [(1-u^{2})\ast (1-v)\ast u^{n-1}+(1+u)\ast v\ast (1-u^{n})\right ]^{2}} {(1-u^{2})^{2}\ast (1-v^{2})\ast u^{2n-2}+(1-u^{2})\ast v^{2}\ast (1-u^{2n})}} \\&\ast \frac {s^{2}}{\sigma ^{2}}\\ =&\sqrt {{\frac {\left [{ {(1-u^{2})\ast (1-v)\ast u^{n-1}+(1+u)\ast v\ast (1-u^{n})} }\right]^{2}}{(1-u^{2})^{2}\ast (1-v^{2})\ast u^{2n-2}+(1-u^{2})\ast v^{2}\ast (1-u^{2n})}}} \\&SNR_{1}\tag{5}\end{align*} View Source \begin{align*}&\hspace {-0.6pc}SNR_{\text {n}} \\=&\sqrt {{\frac {(u^{\text {n}-1}+u^{n-2}\ast v+\ldots +u\ast v+v)^{2}}{(u^{\text {n}-1})^{2}+(u^{n-2}\ast v)^{2}+\ldots +(u\ast v)^{2}+v^{2}}\ast \frac {s^{2}}{\sigma ^{2}}}} \\ =&\sqrt {\frac {\left [(1-u^{2})\ast (1-v)\ast u^{n-1}+(1+u)\ast v\ast (1-u^{n})\right ]^{2}} {(1-u^{2})^{2}\ast (1-v^{2})\ast u^{2n-2}+(1-u^{2})\ast v^{2}\ast (1-u^{2n})}} \\&\ast \frac {s^{2}}{\sigma ^{2}}\\ =&\sqrt {{\frac {\left [{ {(1-u^{2})\ast (1-v)\ast u^{n-1}+(1+u)\ast v\ast (1-u^{n})} }\right]^{2}}{(1-u^{2})^{2}\ast (1-v^{2})\ast u^{2n-2}+(1-u^{2})\ast v^{2}\ast (1-u^{2n})}}} \\&SNR_{1}\tag{5}\end{align*} where \$n=2\$ , 3, 4 \$\ldots\$

Define m is the ratio of \$SNR_{\text {n}} \$ to \$SNR_{1}\$ , which means the times of \$SNR_{\text {n}} \$ is \$SNR_{1}\$ , then we get \begin{align*}&\hspace {-1.8pc}\text {m}=\frac {SNR_{\text {n}}}{SNR_{1}} \\=&\sqrt {\!\frac {\!\!\left [{\! {(1-u^{2})\ast (1-v)\ast u^{n-1}\!+\!(1+u)\ast v\ast (1-u^{n})}\! }\right]^{2}\!}{(1-u^{2})^{2}\ast (1\!-\!v^{2})\ast u^{2n\!-\!2}\!+\!(1\!-\!u^{2})\ast v^{2}\ast \! (1\!-\!u^{2n})}\!}\!\!\!\!\!\!\!\!\!\! \\ {}\tag{6}\end{align*} View Source \begin{align*}&\hspace {-1.8pc}\text {m}=\frac {SNR_{\text {n}}}{SNR_{1}} \\=&\sqrt {\!\frac {\!\!\left [{\! {(1-u^{2})\ast (1-v)\ast u^{n-1}\!+\!(1+u)\ast v\ast (1-u^{n})}\! }\right]^{2}\!}{(1-u^{2})^{2}\ast (1\!-\!v^{2})\ast u^{2n\!-\!2}\!+\!(1\!-\!u^{2})\ast v^{2}\ast \! (1\!-\!u^{2n})}\!}\!\!\!\!\!\!\!\!\!\! \\ {}\tag{6}\end{align*}

When n is greater than 1, obviously, m is constant greater than 1 and SNR is improved. For a more vivid description, we plot Figure 1 with n from 2 to 100 where u is 0.9 and v is 0.3:
FIGURE 1.

The multiple of the n th image compared to the SNR of the first image.

Show All

The m is from approximately 1.3 to close to 4.4. By calculation we get that the value of m when n tends to infinity is approximately 4.4. When the value of u, v is replaced under the above constraints, a similar scatter plot will be obtained, the value of m will also be changed.
B. Automatic Screening Function

In practice, the pixel depth for ordinary image is 8 bits, which means that the pixel value will overflow after being added to 255. Additionally, the actual transformed image sometimes does not accurately compensate for the difference between frames, which results in an unsatisfactory image obtained by the fusion. In order to get better results, how to screen out high-quality images is especially important. To this end, this paper proposes a screening function to help select better quality images.

First and foremost, we plan to filter the image from two perspectives: clarity and structural stability of the image. Hence the clearer the image is, the more popular it becomes. The first aspect is well understood. The second aspect is to avoid image distortion caused by various reasons, which will be described in details later. \$GS\$ and \$SS\$ represent the formulas that measure the clarity and structural stability of the image respectively. We assign GS and SS the corresponding weights, so we get the following formula, \begin{equation*} FS=\lambda ^\ast GS+(1-\lambda)^\ast SS\tag{7}\end{equation*} View Source \begin{equation*} FS=\lambda ^\ast GS+(1-\lambda)^\ast SS\tag{7}\end{equation*} where \$\lambda \$ is from 0 to 1.

As we all know, gradient can help us extract edge information, and clearer images have larger gradients due to their more distinct edge. In order to reduce the running time, the calculation of the gradient is approximated by the difference method, and only the lateral case is considered. Find the sum of squares of horizontal gradient of all points as the criterion for the image clarity. The evaluation formula is as follows, \begin{equation*} GS'=\sum \limits _{x=0}^{X-1} {\sum \limits _{y=0}^{Y-1} {(I\text {mg}(x+1,y)-\text {Im}g(x,y))^{2}}}\tag{8}\end{equation*} View Source \begin{equation*} GS'=\sum \limits _{x=0}^{X-1} {\sum \limits _{y=0}^{Y-1} {(I\text {mg}(x+1,y)-\text {Im}g(x,y))^{2}}}\tag{8}\end{equation*} where Img’s size is \$\text{X}\times \text{Y}\$ , Img (x, y) means the gray value of Img at point (x, y).

In addition, the inaccurate image transformation and the limitations of the gray value 255 of the general image contribute to the loss and distortion of image. Therefore, we filter out the image with less deformation by comparing the image structure with the original ones. Here is the first frame of the video as the original image, and the formula is as follows, \begin{equation*} SS'=\frac {\text {c}ov(\text {Im}g_{o},\text {Im}g)+K}{\sigma _{\text {Im}g_{\text {o}}} ^\ast \sigma _{\text {Im}g} +K}\tag{9}\end{equation*} View Source \begin{equation*} SS'=\frac {\text {c}ov(\text {Im}g_{o},\text {Im}g)+K}{\sigma _{\text {Im}g_{\text {o}}} ^\ast \sigma _{\text {Im}g} +K}\tag{9}\end{equation*} as \$\text {cov}(\text {Im}g_{o},\text {Im}g)\$ represents the covariance between \$I\text {mg}_{o} \$ and \$I\text {mg}\$ , \$\sigma _{\text {img}_{o}} \$ and \$\sigma _{\text {img}_{o}} \$ represent their respective variances, \$K\$ is a constant that prevents the denominator from being 0, its definition is as follows, \begin{equation*} K=(C^\ast R)^{2}\tag{10}\end{equation*} View Source \begin{equation*} K=(C^\ast R)^{2}\tag{10}\end{equation*} where \$C\$ is a constant much smaller than 1 and \$R\$ is the dynamic range of the pixel value, such as 255.

Last but not the least. This paper normalizes \$GS'\$ and \$SS'\$ to eliminate the impact of the dimensions in the final result. Combining the arc tangent and \$GS'\$ , we have \begin{equation*} GS=\arctan (\eta ^\ast GS')^\ast \frac {2}{\pi }\tag{11}\end{equation*} View Source \begin{equation*} GS=\arctan (\eta ^\ast GS')^\ast \frac {2}{\pi }\tag{11}\end{equation*} as \$\eta \$ is a constant which has an effect on adjusting the degree of data difference.

Similarly, in order to balance the degree of difference in data, this article also changed \$SS'\$ whose original value range is 0 to 1. \begin{equation*} SS=(SS')^{\theta }\tag{12}\end{equation*} View Source \begin{equation*} SS=(SS')^{\theta }\tag{12}\end{equation*} where \$\theta \$ is a constant that has a similar effect to \$\eta \$ .

Finally, we get the automatic screening function: \begin{align*} FS=&\lambda \! \ast \big(\arctan \!\left({\eta \!\ast \!\!\left({\sum \limits _{x=0}^{X\!-\!1} \!\!{\sum \limits _{y=0}^{Y-1} \!\!{(I\text {mg}(x\!+\!1,y)-\text {Im}g(x,y))^{2}}} }\right)\!\ast \! \frac {2}{\pi }}\right)\!\! \\&+\,(1-\lambda)\ast \left({\frac {\text {c}ov(\text {Im}g_{o},\text {Im}g)+(C\ast R)^{2}}{\sigma \text {Im}g_{\text {o}} \ast \sigma \text {Im}g+(C\ast R)^{2}}}\right)^{\theta }\tag{13}\end{align*} View Source \begin{align*} FS=&\lambda \! \ast \big(\arctan \!\left({\eta \!\ast \!\!\left({\sum \limits _{x=0}^{X\!-\!1} \!\!{\sum \limits _{y=0}^{Y-1} \!\!{(I\text {mg}(x\!+\!1,y)-\text {Im}g(x,y))^{2}}} }\right)\!\ast \! \frac {2}{\pi }}\right)\!\! \\&+\,(1-\lambda)\ast \left({\frac {\text {c}ov(\text {Im}g_{o},\text {Im}g)+(C\ast R)^{2}}{\sigma \text {Im}g_{\text {o}} \ast \sigma \text {Im}g+(C\ast R)^{2}}}\right)^{\theta }\tag{13}\end{align*}

It could be used to filter out the higher quality image.
C. Supplementary

In this section, we will briefly introduce and explain some of the processes in the whole method. These theories are generally known, and their specific combination is still important, which can help get a clearer image.

In order to better help build smart cities, promote the use of more popular low-cost devices to capture images. However, the image data collected by these devices, if not specially processed, generally has a pixel depth of 8, and each pixel of each channel ranges from 0 to 255. In the face of images obtained under strong light or strong scattering environment, we utilize color inversion to accommodate the fusion of more images, thus providing the possibility to utilize more image information. The color inversion formula is: \begin{equation*} \text {Im}g'(x,y\text {,t})=255-\text {Im}g(x,y,t)\tag{14}\end{equation*} View Source \begin{equation*} \text {Im}g'(x,y\text {,t})=255-\text {Im}g(x,y,t)\tag{14}\end{equation*} as \$\text {Im}g(x,y\text {,t})\$ represents the pixel before color inversion at \$(x,y)\$ position of channel t, and \$\text {Im}g'(x,y\text {,t})\$ represents the pixel after color inversion.

Furthermore, for more people to participate in the construction of intelligent life and the concept of easy operation, we allow the photographer to capture images without additional fixtures, which may cause differences between frames, such as differences in target position and size. Faced with differences between frames, we creatively performed matching feature points employing speed-up robust features (SURF) [53] and the rigid transformation (RT) before each fusion. This operation compensates for differences between frames, ensures the stability of IFOF, and also provides more information for image enhancement by taking advantages of differences between frames. The RT formula is \begin{equation*} \left ({{{\begin{array}{cccccccccccccccccccc} {\text {x'}} \\ {\text {y'}} \\ {1} \\ \end{array}}} }\right)=\left ({{{\begin{array}{cccccccccccccccccccc} {a_{11}} & {a_{12}} & {a_{13}} \\ {a_{21}} & {a_{22}} & {a_{23}} \\ 0 & 0 & 1 \\ \end{array}}} }\right)\left ({{{\begin{array}{cccccccccccccccccccc} x \\ y \\ 1 \\ \end{array}}} }\right)\tag{15}\end{equation*} View Source \begin{equation*} \left ({{{\begin{array}{cccccccccccccccccccc} {\text {x'}} \\ {\text {y'}} \\ {1} \\ \end{array}}} }\right)=\left ({{{\begin{array}{cccccccccccccccccccc} {a_{11}} & {a_{12}} & {a_{13}} \\ {a_{21}} & {a_{22}} & {a_{23}} \\ 0 & 0 & 1 \\ \end{array}}} }\right)\left ({{{\begin{array}{cccccccccccccccccccc} x \\ y \\ 1 \\ \end{array}}} }\right)\tag{15}\end{equation*} with a 11 to a 23 can be obtained through the relationship between feature points matched by SURF.
SECTION IV.
Process and Results
A. Experiment Process

This paper adopted two different examples to illustrate the method. One is a video of a construction site gate shot in real foggy weather, and the other is simulated mammary gland tissue video. The architecture of this method is shown in Figure 2 .
FIGURE 2.

The architecture of the method based on Indented Frame Over Fusion.

Show All

Based on the above architecture, we further explain the complete experimental process:

    Capture video and frame it.

    Reverse the color of each frame.

    Selecting two frames of images, obtaining matching feature points among the two frames by SURF and performing the rigid transformation according to the points. The two frames of images are: the image of the last fusion and the image to be merged in the next frame. Wherein at the initial stage, without the merged image, the first frame and the second frame image are chosen.

    Frame over fusion.

    Evaluate the new image through the screening function and update the record when there is a better image to get.

    Repeat step 3) through 5), until the last frame of complete fusion.

    At the end of the iteration, the image with the highest score, the resulting image, is granted.

B. Case One

The method proposed in this paper can be applied to weather-sensitive outdoor vision systems such as video surveillance, target tracking, and intelligent navigation to enhance the system’s adaptability in severe weather conditions. It can also be implemented in mobile applications, developing a camera function with better stability in scenes that are prone to blur. It is worth noting that the normal mobile phone camera enjoys a kind of function named live, which provides a sequence of images for a few seconds. This method can obtain a natural and clear image according to the image sequence provided.

In real foggy weather, we collected a video of a construction site gate (frame rate of 30f/s) through a normal Vivo Z3 mobile phone. The acquisition did not require a fixed device such as a tripod, so the captured video might appear to shake to a certain extent. In order to better illustrate only a few seconds of image sequence, which could be provided by the ordinary mobile phone’s live function, were enough for this method, only the very short first 2 seconds of the captured video (60 frames in total) was taken as input, and after processing this method, a clearer, more visually natural dehazed image was obtained. We took the first frame of captured video as the original image and applied some common image enhancement methods to process the original image as comparison. Result images are presented in Figure 3 .
FIGURE 3.

Original image and images are obtained by various methods in case one: (a) Original image. (b) Color enhancement image. (c) Brightness enhancement image. (d) Contrast enhancement image. (e) Sharpness enhancement image. (f) Frame accumulated average image. (g) Robust retinex model enhancement image. (h) Gamma corrected enhancement image. (i) Low-light image enhancement (LIME) image. (j) Multi-scale retinex (MSR) enhancement image. (k) Image handled by our method.

Show All

From Figure 3 , we can easily find that although the original image has been brighter by the way of brightness enhanced, it also lost much significant information, such as the building gate. Similarly, (g) also has the problem of losing details. For example, the top left corner of (g) missed some of the branches that existed in the original image. (f) which averaged 60 frames’ information became blurred due to the failure to solve the small movement problem between pictures. Other methods have improved image clarity from various angles, in which our method has not only a better performance in terms of quality enhancement, but also more natural in image color temperature.

Moreover, this paper evaluates all of these images from different aspects through various evaluation standards. Measured scores of each image are shown in Table 1 .
TABLE 1 Each Indicator Score for Each Image in Case One

By comparison, we can find that the improvement of (b) and (c) on each indicator is not obvious, and there is even a fallback phenomenon. For instance, BD, IE and SMD scores of (b) are lower than the corresponding values of (a). Almost all the evaluation values of (f) and (g) are less than (a). (i) and (j) perform well on all indicators except IE. Though (d), (e), and (h) have a good performance among all of the six standards, (k) is the best one among the entire image sets, since each evaluation score is improved greatly.
C. Case Two

The following is an example of this method for applications in the field of biomedical applications to assist heterogeneity detection in biological tissues. The biological tissue has optical characteristics of strong scattering and low absorption, which damage the image clarity detected by the ordinary device, and this method can assist in improving both the SNR and the visual effect.

Firstly, we simulated the heterogeneous scene of the breast tissue: a self-made device which contains milk, potato and meat. Among them, the potato and meat were used to simulate the heterogeneous body in the breast tissue. After that, we took a video clip in the same way as case one, and took the first 2 seconds of video as the input of our method. The first frame of the captured video was selected as the original image and the image was processed via using various existing image enhancement methods respectively. Lastly, the present method processed images and the images obtained by our method were put together for comparison and analysis. Getting images as shown in Figure 4 .
FIGURE 4.

Original image and images are obtained by various methods in case two: (a) Original image. (b) Color enhancement image. (c) Brightness enhancement image. (d) Contrast enhancement image. (e) Sharpness enhancement image. (f) Frame accumulated average image. (g) Robust retinex model enhancement image. (h) Gamma corrected enhancement image. (i) LIME image. (j) MSR enhancement image. (k) Image handled by our method.

Show All

Depending on Figure 4 , it can be viewed that (c) and (g) lost some details while brightening. The performance of (f) was mixed. On the one hand, it restored some of the information (such as the partial color of the potato). On the other hand, it lost some details, just like the edge of the meat was blurred. (b) and (e) enhanced the clarity with respect to (a), but the improvement was not obvious. (d), (h) and (k) achieved good visual effects, while (k) recovered more details.

Further comparison was presented in Table 2 .
TABLE 2 Each Indicator Score for Each Image in Case Two

Table 2 reported resulting image score under each indicator in the columns. (f) and (g) did not perform well due to some information missed in (a). Except for very few values, such as IE score of (i), the six indicator score of the remaining methods were basically improved compared to (a). As a whole, the performance of (e) was better than the images enhanced by other existing methods, but (k) increased more than (e) in general.
SECTION V.
Conclusion

This paper studies theory of IFOF and proposes an image enhanced method based on IFOF. According to the above two cases, it can be seen that our method has achieved a good result. This is because our method handles the difference and redundant information between frames well. The method of frame accumulated average results in poor robustness to the data due to the inability to flexibly utilize the inter-frame relationships. In the face of data sources with unstable inter-frame relationships, the image quality after this method is reduced. Moreover, our method avoids the loss of detail caused by pixel value overflowing through our automatic screening function. Such missing detail problems appear in some of the methods above, like Brightness enhancement method and robust retinex model based enhancement method.

There will be also more improvements base on this paper. Optimization of running time and reduction of deformation of the image are all worthy of studying. Considerable amount of work, expectantly, will be done in this promising area.

Authors
Figures
References
Citations
Keywords
Metrics
   Back to Results   
More Like This
A generalized contrast enhancement algorithm for seamless high contrast image across devices in Internet of Things

2015 International Conference on Green Computing and Internet of Things (ICGCIoT)

Published: 2015
Environment-Aware Multiscene Image Enhancement for Internet of Things Enabled Edge Cameras

IEEE Systems Journal

Published: 2021
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
