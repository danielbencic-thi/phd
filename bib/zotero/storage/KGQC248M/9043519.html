<!DOCTYPE html> <html class="js postmessage history draganddrop borderimage borderradius boxshadow textshadow cssgradients csstransforms csstransforms3d csstransitions generatedcontent localstorage sessionstorage" style lang=en-US><!--
 Page saved with SingleFile 
 url: https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519?arnumber=9043519 
 saved date: Thu Oct 13 2022 13:22:01 GMT+0200 (CEST)
--><meta charset=utf-8><style>:root{--sf-img-3: url("data:image/jpeg;base64,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");--sf-img-5: url("data:image/gif;base64,R0lGODlhGAAUAPcAAP//////zP//mf//Zv//M///AP/M///MzP/Mmf/MZv/MM//MAP+Z//+ZzP+Zmf+ZZv+ZM/+ZAP9m//9mzP9mmf9mZv9mM/9mAP8z//8zzP8zmf8zZv8zM/8zAP8A//8AzP8Amf8AZv8AM/8AAMz//8z/zMz/mcz/Zsz/M8z/AMzM/8zMzMzMmczMZszMM8zMAMyZ/8yZzMyZmcyZZsyZM8yZAMxm/8xmzMxmmcxmZsxmM8xmAMwz/8wzzMwzmcwzZswzM8wzAMwA/8wAzMwAmcwAZswAM8wAAJn//5n/zJn/mZn/Zpn/M5n/AJnM/5nMzJnMmZnMZpnMM5nMAJmZ/5mZzJmZmZmZZpmZM5mZAJlm/5lmzJlmmZlmZplmM5lmAJkz/5kzzJkzmZkzZpkzM5kzAJkA/5kAzJkAmZkAZpkAM5kAAGb//2b/zGb/mWb/Zmb/M2b/AGbM/2bMzGbMmWbMZmbMM2bMAGaZ/2aZzGaZmWaZZmaZM2aZAGZm/2ZmzGZmmWZmZmZmM2ZmAGYz/2YzzGYzmWYzZmYzM2YzAGYA/2YAzGYAmWYAZmYAM2YAADP//zP/zDP/mTP/ZjP/MzP/ADPM/zPMzDPMmTPMZjPMMzPMADOZ/zOZzDOZmTOZZjOZMzOZADNm/zNmzDNmmTNmZjNmMzNmADMz/zMzzDMzmTMzZjMzMzMzADMA/zMAzDMAmTMAZjMAMzMAAAD//wD/zAD/mQD/ZgD/MwD/AADM/wDMzADMmQDMZgDMMwDMAACZ/wCZzACZmQCZZgCZMwCZAABm/wBmzABmmQBmZgBmMwBmAAAz/wAzzAAzmQAzZgAzMwAzAAAA/wAAzAAAmQAAZgAAMwAAAP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAEAANgALAAAAAAYABQAAAhLALEJHEiwoMGDCBMqXMiw4IqHKxo6hEhRIjaIBDE2fGiQI0ONHjUqpOjxYsmEJAdWXJjSpMiRIF/CDClzZkuLLiPi3Mmzp8+fOwMCADs=")}</style>
<meta name=Description id=meta-description content="Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision resear">
<link rel=canonical href="https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/?arnumber=9043519">
<meta name=viewport content="width=device-width, initial-scale=1.0">
<meta name=customerIndustry content=NA>
<title>A State-of-the-Art Review on Image Synthesis With Generative Adversarial Networks | IEEE Journals &amp; Magazine | IEEE Xplore</title>
<meta property=twitter:title content="A State-of-the-Art Review on Image Synthesis With Generative Adversarial Networks">
<meta property=og:title content="A State-of-the-Art Review on Image Synthesis With Generative Adversarial Networks">
<meta property=twitter:description content="Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because of the impressive performance they achieved in various applications. In this paper, we introduce the recent research on GANs in the field of image processing, including image synthesis, image generation, image semantic editing, image-to-image translation, image super-resolution, image inpainting, and cartoon generation. We analyze and summarize the methods used in these applications which have improved the generated results. Then, we discuss the challenges faced by GANs and introduce some methods to deal with these problems. We also preview some likely future research directions in the field of GANs, such as video generation, facial animation synthesis and 3D face reconstruction. The purpose of this review is to provide insights into the research on GANs and to present the various applications based on GANs in different scenarios.">
<meta property=og:description content="Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because of the impressive performance they achieved in various applications. In this paper, we introduce the recent research on GANs in the field of image processing, including image synthesis, image generation, image semantic editing, image-to-image translation, image super-resolution, image inpainting, and cartoon generation. We analyze and summarize the methods used in these applications which have improved the generated results. Then, we discuss the challenges faced by GANs and introduce some methods to deal with these problems. We also preview some likely future research directions in the field of GANs, such as video generation, facial animation synthesis and 3D face reconstruction. The purpose of this review is to provide insights into the research on GANs and to present the various applications based on GANs in different scenarios.">
<meta name=twitter:card content=summary_large_image>
<meta property=og:image content=https://ieeexplore-ieee-org.thi.idm.oclc.org/ielx7/6287639/8948470/9043519/graphical_abstract/access-gagraphic-2982224.jpg>
<meta property=twitter:image content=https://ieeexplore-ieee-org.thi.idm.oclc.org/ielx7/6287639/8948470/9043519/graphical_abstract/access-gagraphic-2982224.jpg>
<style>.cc-window{opacity:1;transition:opacity 1s ease}.cc-link{text-decoration:underline}.cc-window{position:fixed;overflow:hidden;box-sizing:border-box;font-family:Helvetica,Calibri,Arial,sans-serif;font-size:16px;line-height:1.5em;display:-ms-flexbox;display:flex;-ms-flex-wrap:nowrap;flex-wrap:nowrap;z-index:9999}.cc-window.cc-banner{padding:1em 1.8em;width:100%;-ms-flex-direction:row;flex-direction:row}.cc-btn,.cc-link{cursor:pointer}.cc-link{opacity:.8;display:inline-block;padding:.2em}.cc-link:hover{opacity:1}.cc-link:active,.cc-link:visited{color:initial}.cc-btn{display:block;padding:.4em .8em;font-size:.9em;font-weight:700;border-width:2px;border-style:solid;text-align:center;white-space:nowrap}.cc-banner .cc-btn:last-child{min-width:140px}.cc-window.cc-banner{-ms-flex-align:center;align-items:center}.cc-banner.cc-bottom{left:0;right:0;bottom:0}.cc-banner .cc-message{-ms-flex:1;flex:1}.cc-compliance{display:-ms-flexbox;display:flex;-ms-flex-align:center;align-items:center;-ms-flex-line-pack:justify;align-content:space-between}.cc-compliance>.cc-btn{-ms-flex:1;flex:1}@media screen and (max-width:900px){.cc-btn{white-space:normal}}@media screen and (max-width:414px) and (orientation:portrait),screen and (max-width:736px) and (orientation:landscape){.cc-window.cc-bottom{bottom:0}.cc-window.cc-banner{left:0;right:0}.cc-window.cc-banner{-ms-flex-direction:column;flex-direction:column}.cc-window.cc-banner .cc-compliance{-ms-flex:1;flex:1}.cc-window .cc-message{margin-bottom:1em}.cc-window.cc-banner{-ms-flex-align:unset;align-items:unset}}</style>
<meta http-equiv=X-UA-Compatible content="IE=Edge">
<style>.MathJax_Preview{color:#888}#MathJax_Message{position:fixed;left:1px;bottom:2px;background-color:#E6E6E6;border:1px solid #959595;margin:0px;padding:2px 8px;z-index:102;color:black;font-size:80%;width:auto;white-space:nowrap}</style><meta http-equiv=origin-trial content="A+cA2PUOfIOKAdSDJOW5CP9ZlxONy1yu+hqAq72zUtKw4rLdihqRp6Nui/jUyCyegr+BUtH+C+Elv0ufn05yBQEAAACFeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="A+zsdH3aNZT/bkjT8U/o5ACzyaeNYzTvtoVmwf/KOilfv39pxY2AIsOwhQJv+YnXp98i3TqrQibIVtMWs5UHjgoAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="AxceVEhIegcDEHqLXFQ2+vPKqzCppoJYsRCZ/BdfVnbM/sUUF2BXV8lwNosyYjvoxnTh2FC8cOlAnA5uULr/zAUAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><style>.global-ng-wrapper[_ngcontent-cdr-c455]{max-width:1680px;margin:0 auto;min-height:80vh;box-sizing:border-box}@media only screen and (min-width:768px){.global-ng-wrapper[_ngcontent-cdr-c455]{padding-right:15px;padding-left:15px}}.ng2-xplore-meta-nav #global-header-cart-count{padding-right:.5rem;border-right:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav #global-header-cart-count{padding-right:0rem}}.ng2-xplore-meta-nav .xplore-meta-nav{display:flex;width:100%;background-color:#17445a;box-sizing:border-box;padding:.35rem 15px;max-width:1680px}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links{width:auto;max-width:none}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links .meta-nav-menu{-webkit-padding-start:0;padding-inline-start:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links .meta-nav-item:last-child{padding-right:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links{width:auto;max-width:none;margin-left:auto}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .nav-right{width:100%;padding-right:1rem;padding-left:1rem}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .cart-container{margin-left:auto;display:flex}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .icons-panel{padding-right:0;padding-left:0}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links{flex:none;max-width:none;margin-left:0;width:100%;box-sizing:border-box}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item{list-style:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item{padding-right:0}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item:last-child{padding-right:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{list-style:none;font-size:14px;padding-right:.75rem}@media only screen and (max-width:991px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{padding-right:.5rem}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{padding-right:0rem}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:last-child){border-right:1px solid white}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:last-child){border-right:none}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:first-child){padding-left:.75rem}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:first-child){padding-left:0rem}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item>a{text-decoration:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item>a{padding-left:.75rem}}.ng2-xplore-meta-nav .xplore-meta-nav a{color:#fff}.ng2-xplore-meta-nav .xplore-meta-nav .ieee-xplore{color:#e4a42c}.ng2-xplore-meta-nav .xplore-meta-nav .personal-signin-container{position:relative;z-index:1049}.main-header[_ngcontent-cdr-c449]{position:relative;display:flex;flex-direction:column;background-color:#14303e}.search-bar-container[_ngcontent-cdr-c449]{z-index:20}.fill-background[_ngcontent-cdr-c449]{display:flex;justify-content:center;align-items:center;position:relative;background-color:#14303e;top:auto;left:auto;transform:translate(0);width:100%;min-height:115px}.menu-link[_ngcontent-cdr-c459]{padding:.25rem .5rem;white-space:nowrap}.primary-menu[_ngcontent-cdr-c459]{flex-grow:1;min-width:301px;max-width:400px;margin-top:5px}.primary-menu[_ngcontent-cdr-c459] ul[_ngcontent-cdr-c459]{display:flex;list-style-type:none;flex-grow:1;justify-content:space-around;margin:0;padding:0;padding-left:.75rem;padding-right:1.5rem}.primary-menu[_ngcontent-cdr-c459] a[_ngcontent-cdr-c459]{color:#fff}.hamburger-menu[_ngcontent-cdr-c459]{width:33%}.hamburger-menu[_ngcontent-cdr-c459] a[_ngcontent-cdr-c459]{font-size:2rem;display:flex;color:#fff}.institution-container[_ngcontent-cdr-c459]{flex-shrink:0}.institution-container.inst-logged-in[_ngcontent-cdr-c459]{margin-top:0}.right-side-container[_ngcontent-cdr-c459]{flex-basis:50%;flex-shrink:1;display:flex;flex-direction:row-reverse}.right-side-container.inst-logged-in[_ngcontent-cdr-c459]{width:auto;padding-left:1rem;margin-left:auto}@media only screen and (max-width:767px){.right-side-container[_ngcontent-cdr-c459]{width:33%}}.inst-logged-in[_ngcontent-cdr-c459] .left-side-container[_ngcontent-cdr-c459]{min-width:465px}.left-side-container[_ngcontent-cdr-c459]{flex-basis:50%;flex-shrink:1}@media only screen and (max-width:767px){.left-side-container[_ngcontent-cdr-c459]{width:33%}}.left-side-content[_ngcontent-cdr-c459]{display:flex;justify-content:flex-start}@media only screen and (max-width:767px){.left-side-content[_ngcontent-cdr-c459]{justify-content:center}}.left-side-content[_ngcontent-cdr-c459] .xplore-logo-wrapper[_ngcontent-cdr-c459]{margin-top:-5px}.navbar-container[_ngcontent-cdr-c459]{box-sizing:border-box;display:flex;flex-direction:column;flex-grow:1;z-index:1004;width:100%;max-width:1680px;padding:18px 15px}@media only screen and (max-width:767px){.navbar-container[_ngcontent-cdr-c459]{padding-top:0}}.inst-details-container[_ngcontent-cdr-c459]{flex-grow:1}.top-navbar[_ngcontent-cdr-c459]{display:flex}@media only screen and (max-width:767px){.top-navbar[_ngcontent-cdr-c459]{padding:1rem 1rem .5rem}}.bottom-navbar[_ngcontent-cdr-c459]{display:flex;justify-content:center;padding-top:13px}.navbar-container.not-homepage[_ngcontent-cdr-c459]{position:relative;margin:0 auto;padding-bottom:1.5rem}@media screen and (-ms-high-contrast:none){.navbar-container[_ngcontent-cdr-c459]{max-width:none;width:1460px}}[_nghost-cdr-c462]{width:100%}.not-homepage [_nghost-cdr-c462] .search-bar-wrapper[_ngcontent-cdr-c462]{margin-bottom:0}.not-homepage [_nghost-cdr-c462] .search-bar[_ngcontent-cdr-c462] .below-search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462]{justify-content:unset;margin-left:auto;margin-right:0}.not-homepage [_nghost-cdr-c462] .search-bar[_ngcontent-cdr-c462] .below-search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462] .advanced-search-div[_ngcontent-cdr-c462]{background-color:transparent;padding:0;margin:0;min-width:0}@media only screen and (max-width:767px){.not-homepage [_nghost-cdr-c462] .search-bar[_ngcontent-cdr-c462] .below-search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462] .advanced-search-div[_ngcontent-cdr-c462]{padding-right:.5rem}}.not-homepage [_nghost-cdr-c462] .search-bar[_ngcontent-cdr-c462] .below-search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462] .advanced-search-div[_ngcontent-cdr-c462] a[_ngcontent-cdr-c462]{font-size:12px}@media only screen and (max-width:767px){.not-homepage [_nghost-cdr-c462] .search-bar[_ngcontent-cdr-c462] .below-search-bar[_ngcontent-cdr-c462]{padding-left:.5rem}}.search-bar[_ngcontent-cdr-c462]{max-width:960px;margin:0 auto}.search-bar[_ngcontent-cdr-c462] .below-search-bar[_ngcontent-cdr-c462]{display:flex;justify-content:center;max-width:780px;flex-grow:1;margin:0 auto}.search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462]{display:flex;margin:0 32%}@media only screen and (max-width:767px){.search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462]{flex-direction:column;margin:0 auto;margin-top:.5rem;max-width:230px}}.search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462]>div[_ngcontent-cdr-c462]{text-align:center;max-width:200px;border-radius:.2rem}@media only screen and (max-width:767px){.search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462]>div[_ngcontent-cdr-c462]{margin:1% 4%;margin-top:1rem}}.search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462]>div[_ngcontent-cdr-c462]>a[_ngcontent-cdr-c462]{color:#fff;font-family:Lato-Bold,Arial,sans-serif;font-weight:700}.search-bar[_ngcontent-cdr-c462] .advanced-search-wrapper[_ngcontent-cdr-c462]>div[_ngcontent-cdr-c462]>a[_ngcontent-cdr-c462]:hover{text-decoration:none}.search-bar-wrapper[_ngcontent-cdr-c462]{display:flex;max-width:780px;background-color:#0000001a;padding:.5em;margin:0 auto 1%;justify-content:space-between}.search-bar-wrapper[_ngcontent-cdr-c462]>.drop-down[_ngcontent-cdr-c462]{flex-grow:.08}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-cdr-c462]>.drop-down[_ngcontent-cdr-c462]{flex-grow:1;max-width:4rem}}.search-bar-wrapper[_ngcontent-cdr-c462]>.drop-down[_ngcontent-cdr-c462]>label[_ngcontent-cdr-c462]{position:relative}.search-bar-wrapper[_ngcontent-cdr-c462]>.drop-down[_ngcontent-cdr-c462]>label[_ngcontent-cdr-c462]:after{content:"";font-family:"Font Awesome 5 Pro";font-size:22px;font-weight:900;color:#e4a42c;right:4px;top:-6px;padding:0 2%;position:absolute;pointer-events:none;height:19px;width:15px}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-cdr-c462]>.drop-down[_ngcontent-cdr-c462]>label[_ngcontent-cdr-c462]:after{font-size:20px;right:5px;top:-4px;text-align:center}}.search-bar-wrapper[_ngcontent-cdr-c462]>.drop-down[_ngcontent-cdr-c462]>label[_ngcontent-cdr-c462]:before{content:"";right:4px;top:0;background:#fff;position:absolute;pointer-events:none;display:block}.search-bar-wrapper[_ngcontent-cdr-c462]>.drop-down[_ngcontent-cdr-c462]>label[_ngcontent-cdr-c462]>select[_ngcontent-cdr-c462]{padding:.35em;padding-left:.5rem;width:100%;height:100%;margin:0;color:#000;border:none;font-weight:700;background-color:#ddd;border-radius:0;-webkit-appearance:none;-moz-appearance:none}.search-bar-wrapper[_ngcontent-cdr-c462] .search-field[_ngcontent-cdr-c462]{display:flex;justify-content:space-evenly;flex-grow:1}.search-bar-wrapper[_ngcontent-cdr-c462] .search-field[_ngcontent-cdr-c462]>div[_ngcontent-cdr-c462]{flex-grow:1;padding-right:5%}.search-bar-wrapper[_ngcontent-cdr-c462] .search-field[_ngcontent-cdr-c462]>div[_ngcontent-cdr-c462]:last-child{padding-right:0}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-cdr-c462] .search-field[_ngcontent-cdr-c462]>div[_ngcontent-cdr-c462]{padding-right:0}}.search-bar-wrapper[_ngcontent-cdr-c462] .search-field[_ngcontent-cdr-c462] .global-search-bar[_ngcontent-cdr-c462]{flex-grow:1}.search-bar-wrapper[_ngcontent-cdr-c462] .search-field-icon-container[_ngcontent-cdr-c462]{display:flex}.search-bar-wrapper[_ngcontent-cdr-c462] .search-icon[_ngcontent-cdr-c462]{background-color:#e4a42c;width:3.5625rem;display:flex;align-items:center;justify-content:center;cursor:pointer}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-cdr-c462] .search-icon[_ngcontent-cdr-c462]{order:1}}.search-bar-wrapper[_ngcontent-cdr-c462] .search-icon[_ngcontent-cdr-c462]>.fa-search[_ngcontent-cdr-c462]{width:29px;height:25px;color:#000;text-align:center}.global-search-bar[_ngcontent-cdr-c462]{position:relative}.xplore-logo-container[_ngcontent-cdr-c444]{display:flex;flex-direction:column}.xplore-logo-container[_ngcontent-cdr-c444] a[_ngcontent-cdr-c444]{align-self:center}.xplore-logo-container[_ngcontent-cdr-c444] img.xplore-logo[_ngcontent-cdr-c444]{width:160px;height:40px}.ieee-logo-container[_ngcontent-cdr-c446]{display:flex;flex-direction:column}.ieee-logo-container[_ngcontent-cdr-c446] .ieee-logo[_ngcontent-cdr-c446]{width:100px;align-self:flex-end}.footer-new[_ngcontent-cdr-c454]{display:flex;background-color:#17445a;padding-top:3rem;color:#fff}@media only screen and (max-width:767px){.footer-new[_ngcontent-cdr-c454]{padding-top:.1rem}}.footer-new[_ngcontent-cdr-c454]>div[_ngcontent-cdr-c454]{padding-bottom:2rem}.footer-new[_ngcontent-cdr-c454] h3[_ngcontent-cdr-c454]{font-weight:800;font-family:"IBM Plex Serif",Arial,sans-serif}.footer-new[_ngcontent-cdr-c454] ul[_ngcontent-cdr-c454]{list-style-type:none;padding-left:0}.footer-new[_ngcontent-cdr-c454] li[_ngcontent-cdr-c454]{padding:.35rem 0;text-transform:uppercase}.footer-new[_ngcontent-cdr-c454] a[_ngcontent-cdr-c454]{width:100%;text-decoration:none;color:#fff}.footer-new[_ngcontent-cdr-c454] a[_ngcontent-cdr-c454]:hover{color:#e4a42c}.footer-new[_ngcontent-cdr-c454] .follow[_ngcontent-cdr-c454] ul[_ngcontent-cdr-c454]{display:flex}.footer-new[_ngcontent-cdr-c454] .follow[_ngcontent-cdr-c454] ul[_ngcontent-cdr-c454] li[_ngcontent-cdr-c454]{padding:0 .5rem}.footer-new[_ngcontent-cdr-c454] .follow[_ngcontent-cdr-c454] ul[_ngcontent-cdr-c454] li[_ngcontent-cdr-c454]:first-child{padding-left:0}.footer-new[_ngcontent-cdr-c454] .footer-wrapper[_ngcontent-cdr-c454]{flex-grow:1;max-width:1680px;margin:0 auto}.footer-new[_ngcontent-cdr-c454] .flexible-row-col[_ngcontent-cdr-c454]{display:flex;padding-bottom:3rem}@media only screen and (max-width:767px){.footer-new[_ngcontent-cdr-c454] .flexible-row-col[_ngcontent-cdr-c454]{padding-bottom:.1rem;flex-direction:column}}.footer-new[_ngcontent-cdr-c454] .footer-col[_ngcontent-cdr-c454]{flex-grow:1;padding-left:2rem}@media only screen and (max-width:767px){.footer-new[_ngcontent-cdr-c454] .footer-col[_ngcontent-cdr-c454]{padding-top:1rem}.footer-new[_ngcontent-cdr-c454] .footer-col[_ngcontent-cdr-c454]:first-child{padding-top:2rem}.footer-new[_ngcontent-cdr-c454] .footer-col[_ngcontent-cdr-c454]:last-child{padding-bottom:.25rem}}.footer-new[_ngcontent-cdr-c454] .footer-bottom-section[_ngcontent-cdr-c454] p[_ngcontent-cdr-c454]{margin:0;padding-left:2rem;padding-right:2rem}.footer-new[_ngcontent-cdr-c454] .footer-bottom-section[_ngcontent-cdr-c454] p[_ngcontent-cdr-c454]:last-child{padding-top:1rem}.footer-new[_ngcontent-cdr-c454] .footer-bottom-section[_ngcontent-cdr-c454] p[_ngcontent-cdr-c454] span[_ngcontent-cdr-c454]{padding-left:0}.footer-new[_ngcontent-cdr-c454] .footer-bottom-section[_ngcontent-cdr-c454] p[_ngcontent-cdr-c454] .ethics-reporting-link[_ngcontent-cdr-c454] i[_ngcontent-cdr-c454]{padding-left:.25rem}.footer-new[_ngcontent-cdr-c454] .nowrap[_ngcontent-cdr-c454]{white-space:nowrap}.signout[_ngcontent-cdr-c445]{font-size:.75rem;padding-left:1rem;padding-right:1.25rem;padding-top:.5rem}.signout[_ngcontent-cdr-c445] a[_ngcontent-cdr-c445]{display:block}.inst-detail[_ngcontent-cdr-c445]{display:flex;background:#FFF;margin-top:-18px;border-radius:0 0 .4rem .4rem;height:71px}@media only screen and (max-width:767px){.inst-detail[_ngcontent-cdr-c445]{height:auto;border-radius:0;justify-content:center;flex-wrap:wrap}}.inst-text-container[_ngcontent-cdr-c445]{padding-left:.5rem;padding-right:.5rem;margin-top:.25rem;display:flex;margin-bottom:1rem;border-right:1px solid #dddddd}.inst-text-container.no-inst-logo[_ngcontent-cdr-c445]{max-width:160px}.access-text[_ngcontent-cdr-c445]{white-space:nowrap;font-size:12px}.inst-name[_ngcontent-cdr-c445]{font-size:12px}.Typeahead-input[_ngcontent-cdr-c69]{border-radius:0}.disqus-container[_ngcontent-cdr-c196]{padding:0 1em}@media only screen and (max-width:767px){.document-sidebar[_ngcontent-cdr-c196]{position:absolute;height:calc(100% - 129px);z-index:11000;right:0;padding:0;max-width:40vw;transform:translate(40vw);transition:transform .25s ease-in-out 0s}}@media only screen and (max-width:575px){.document-sidebar[_ngcontent-cdr-c196]{max-width:80vw;transform:translate(80vw)}}@media only screen and (max-width:767px){.document-sidebar-content[_ngcontent-cdr-c196]{max-height:calc(100% - 129px);width:100%;overflow:scroll;position:absolute}}.document-sidebar.top-spacing[_ngcontent-cdr-c196]{margin-top:35px}@media only screen and (max-width:767px){.document-sidebar.top-spacing[_ngcontent-cdr-c196]{margin-top:8.5rem}}@media only screen and (min-width:768px){.document-sidebar-rel-art[_ngcontent-cdr-c196]{padding:0 15px 35px}}.header-rel-art-toggle-mobile[_ngcontent-cdr-c196]{top:-2px}.document-title[_ngcontent-cdr-c148]{letter-spacing:normal}.document-title[_ngcontent-cdr-c148]{margin:0}.document-title-fix[_ngcontent-cdr-c148]{flex-grow:1;width:100%}.pdf-btn-container[_ngcontent-cdr-c148]{margin-left:28px}.document-header-inner-container[_ngcontent-cdr-c148]{max-width:100%;width:100%}.document-header-breadcrumbs-container[_ngcontent-cdr-c148]{padding:.4rem 1rem .8rem;margin:0;font-size:.8em}.document-header-breadcrumbs-container[_ngcontent-cdr-c148] #help[_ngcontent-cdr-c148]{font-size:14px}.document-header-metrics-banner[_ngcontent-cdr-c148]{padding:.4rem 1rem .8rem;width:100%}.document-header-metrics-banner.ccby-document[_ngcontent-cdr-c148]{padding-bottom:0}.document-header-title-container[_ngcontent-cdr-c148]{padding:.4rem 1rem .8rem;display:flex}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-cdr-c148]{flex-direction:column}}.document-header-title-container[_ngcontent-cdr-c148] .right-container[_ngcontent-cdr-c148]{margin-left:auto;display:flex;flex-direction:column}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-cdr-c148] .right-container[_ngcontent-cdr-c148]{margin-left:0;margin-right:auto}}.document-banner-access[_ngcontent-cdr-c148]{width:100%;display:flex}.breadcrumbs-separator[_ngcontent-cdr-c148]{padding:.4rem}.btn-container[_ngcontent-cdr-c148]{display:flex}.btn-container[_ngcontent-cdr-c148] .cite-this-related-btn-wrapper[_ngcontent-cdr-c148] .cite-this-btn[_ngcontent-cdr-c148]{padding:.5rem;border:2px solid #069;font-weight:700}.publisher-title-tooltip[_ngcontent-cdr-c148]{margin-top:.3em;padding-right:30px}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-cdr-c148]{position:relative}}.copyright-icon[_ngcontent-cdr-c151]{font-size:1.25rem}.doc-share-tool[_ngcontent-cdr-c144] i[_ngcontent-cdr-c144]{font-size:1.2rem}.doc-share-tool[_ngcontent-cdr-c144] i.fa-share-alt[_ngcontent-cdr-c144]{color:#069}[_nghost-cdr-c167]{width:100%}.ft-toc[_ngcontent-cdr-c167]{height:52px;border-top:1px solid #e5e5e5;border-bottom:1px solid #e5e5e5}.ft-toc[_ngcontent-cdr-c167]>div[_ngcontent-cdr-c167]{margin-top:13px}.ft-toc[_ngcontent-cdr-c167] a.toc-link[_ngcontent-cdr-c167]{font-size:1.2em;font-weight:700}.ft-toc[_ngcontent-cdr-c167] a.toc-link[_ngcontent-cdr-c167]:hover{text-decoration:none}.ft-toc[_ngcontent-cdr-c167] a.toc-link[_ngcontent-cdr-c167] img[_ngcontent-cdr-c167]{position:relative;top:-2px;margin-right:.3em}.full-text-toc-wrapper[_ngcontent-cdr-c167] .previous-next-nav-ctrl[_ngcontent-cdr-c167]{overflow:visible}.stats-document-container-fullTextSection[_ngcontent-cdr-c167]{padding:0 15px}.stats-document-container-rh[_ngcontent-cdr-c167]{padding-right:1em;padding-left:1em}.toc-container[_ngcontent-cdr-c167]{margin-bottom:.7em;font-weight:700}.toc-container[_ngcontent-cdr-c167] a[_ngcontent-cdr-c167]:hover{text-decoration:none}.hide-full-text[_ngcontent-cdr-c167]{max-height:0;overflow:hidden}.accordion-header[_ngcontent-cdr-c172]{color:#069;display:flex;align-items:center}.accordion-header[_ngcontent-cdr-c172] .accordion-chevron[_ngcontent-cdr-c172] .fa[_ngcontent-cdr-c172]{font-size:1.5rem}.accordion-header[_ngcontent-cdr-c172]:hover{color:#0081c1}.document-all-references[_ngcontent-cdr-c195]{position:fixed;top:0;right:0;width:30vw;min-width:450px;padding:1rem 1rem 3rem;box-shadow:3px 10px 10px #000;overflow:auto;height:100vh;box-sizing:border-box;z-index:99999;transform:translate(100%);background-color:#fff;transition:transform .25s ease-in-out 0s}.header[_ngcontent-cdr-c195]{display:flex;align-items:center;padding:.5rem 0}.header[_ngcontent-cdr-c195] h1[_ngcontent-cdr-c195]{margin:0;font-weight:400;color:#333}.header[_ngcontent-cdr-c195] a[_ngcontent-cdr-c195]{margin-left:auto;font-size:1.25rem;color:#333}i.help-link[_ngcontent-cdr-c54]{color:#069}i.help-link[_ngcontent-cdr-c54]:hover{color:#0081c1}.help-link-icon[_ngcontent-cdr-c54]{font-size:1.2rem}.breadcrumb-help-link-icon[_ngcontent-cdr-c54]{font-size:1rem}a[_ngcontent-cdr-c54]:hover{text-decoration:none}.pdf-btn-link[_ngcontent-cdr-c115]{height:36px;background-color:#ff3500;padding:0 25px;display:flex;align-items:center;justify-content:center;border-radius:2px;color:#fff}.pdf-btn-link[_ngcontent-cdr-c115] .icon[_ngcontent-cdr-c115]{font-size:1.15rem;color:#fff;margin-right:.45rem}.pdf-btn-link[_ngcontent-cdr-c115]>span[_ngcontent-cdr-c115]{font-weight:700}.red-pdf[_ngcontent-cdr-c115]{font-size:1.3rem;margin-right:.15rem;color:#fc0d1b;font-style:normal}.red-pdf[_ngcontent-cdr-c115]:after{color:#fc0d1b}.document-authors-banner[_ngcontent-cdr-c141] .authors-container[_ngcontent-cdr-c141]{padding:0 0 0 1rem}.stats-document-authors-banner[_ngcontent-cdr-c141]{padding:.25rem 1rem .25rem 0}.authors-minimized[_ngcontent-cdr-c141]{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.toc-container[_ngcontent-cdr-c149]{padding:.6em .5em;border-bottom:1px solid #dddddd}.toc-heading[_ngcontent-cdr-c149]{font-size:1em;padding-bottom:.5em}.toc-list[_ngcontent-cdr-c149]{list-style:none;padding:0}.toc-list-item[_ngcontent-cdr-c149]{padding:.5em 0}.toc-list-link[_ngcontent-cdr-c149]{display:flex;font-size:.9em}.toc-list-icon[_ngcontent-cdr-c149]{margin-right:5px}.toc-show-more-btn[_ngcontent-cdr-c149]{font-size:.85em;font-weight:700}.cover-image-document[_ngcontent-cdr-c112] img[_ngcontent-cdr-c112]{width:25rem;height:200px;cursor:pointer;border:1px solid #069}@media only screen and (max-width:767px){.cover-image-document[_ngcontent-cdr-c112] img[_ngcontent-cdr-c112]{width:100%;max-height:100%;height:100%}}.header-rel-art-pub[_ngcontent-cdr-c194]{margin:.25rem 0}.header-rel-art-pub[_ngcontent-cdr-c194]:last-child{margin:0 0 .5rem}.header-rel-art[_ngcontent-cdr-c194]{font-size:.9375rem;color:#333;border:1px solid #e5e5e5;border-top:5px #0081C1 solid;background-color:#f8f8f8}@media only screen and (max-width:767px){.header-rel-art[_ngcontent-cdr-c194]{border-bottom:none}}.header-rel-art-action[_ngcontent-cdr-c194] a[_ngcontent-cdr-c194],.header-rel-art-title[_ngcontent-cdr-c194]{font-family:Helvetica-Nue-Bold,Arial,sans-serif}.header-rel-art-list[_ngcontent-cdr-c194]{font-family:Helvetica Regular,Arial,sans-serif}.cc-color-override-170793312.cc-window{color:rgb(255,255,255);background-color:rgb(0,0,0)}.cc-color-override-170793312 .cc-link,.cc-color-override-170793312 .cc-link:active,.cc-color-override-170793312 .cc-link:visited{color:rgb(255,255,255)}.cc-color-override-170793312 .cc-btn{color:rgb(0,0,0);border-color:transparent;background-color:rgb(255,255,255)}.cc-color-override-170793312 .cc-btn:hover,.cc-color-override-170793312 .cc-btn:focus{background-color:rgb(255,255,255)}.MathJax_Display{text-align:center;margin:1em 0em;position:relative;display:block!important;text-indent:0;max-width:none;max-height:none;min-width:0;min-height:0;width:100%}.MathJax{display:inline;font-style:normal;font-weight:normal;line-height:normal;font-size:100%;font-size-adjust:none;text-indent:0;text-align:left;text-transform:none;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;direction:ltr;max-width:none;max-height:none;min-width:0;min-height:0;border:0;padding:0;margin:0}.MathJax:focus,body :focus .MathJax{display:inline-table}.MathJax nobr{border:0;padding:0;margin:0;max-width:none;max-height:none;min-width:0;min-height:0;vertical-align:0;line-height:normal;text-decoration:none}.MathJax span{display:inline;position:static;border:0;padding:0;margin:0;vertical-align:0;line-height:normal;text-decoration:none;box-sizing:content-box}.MathJax nobr{white-space:nowrap!important}.MathJax *{transition:none;-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none}.MathJax_Processing{visibility:hidden;position:fixed;width:0;height:0;overflow:hidden}.MathJax_Processed{display:none!important}</style><meta name=cToken content=eyJhbGciOiJIUzUxMiIsInppcCI6IkRFRiJ9.eNqqVkosKFCyUoooyMkvSlXSUcosLgZyK2Dc1AqgrKGZmamZmZGFgSVQPrEEJmBgaGBZCwAAAP__.vZuj_QL1mJPzO9-JfX-BsTxdY_LJ-7HTqI2YUT8ScATnb33LtAjZKMyiYtm5lOhETPA4_ZNQwlIpaPsxO6lFJA class=sf-hidden><link type=image/x-icon rel="shortcut icon" href=data:,><style>.sf-hidden{display:none!important}</style><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:;"><style>img[src="data:,"],source[src="data:,"]{display:none!important}</style><body class="body-resp cmpl_embed_complete"><div style="visibility:hidden;overflow:hidden;position:absolute;top:0px;height:1px;width:auto;padding:0px;border:0px none;margin:0px;text-align:left;text-indent:0px;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal" class=sf-hidden></div><div role=dialog aria-live=polite aria-label=cookieconsent aria-describedby=cookieconsent:desc class="cc-window cc-banner cc-type-info cc-theme-block cc-bottom cc-color-override-170793312"><span id=cookieconsent:desc class=cc-message>IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our <a aria-label="learn more about cookies" role=button tabindex=0 class=cc-link href=https://www.ieee.org/about/help/security_privacy.html target=_blank>Privacy Policy.</a></span><div class=cc-compliance><a aria-label="dismiss cookie message" role=button tabindex=0 class="cc-btn cc-dismiss">Accept &amp; Close</a></div></div><div style=display:none id=lightningjs-usabilla_live></div><div id=MathJax_Message>Typesetting math: 12%</div><g:compress>
 <style>--></style>
 <style media="screen, print">--></style>
 <style media="screen, print">--></style>
 <style>--></style>
 <style media="screen, print">--></style>
</g:compress>
 
 
 
 
 
 
 <p class=JumpLink id=PageTop><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/# title="Click here to Skip to main content" accesskey=s>Skip to Main Content</a></p>
 <div id=global-notification class="row stats-global-notification">
 <div class="hide u-hide-important col Notification Notification--global Notification--fixed">
 <a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class="Notification-close js-close" aria-label="close message button"><i class="fa fa-close"></i></a>
 <div class=Notification-header></div>
 <div class=Notification-text></div>
 </div>
 </div>
 <div id=LayoutWrapper>
 <div class=container-fluid>
 <div class=row>
 <div class=col>
 
 
 
 
 
<div class=Header id=xplore-header data-service=true data-inst=true data-web=false style=display:none></div>
 <div id=global-alert-message></div>
 

<div class=ng2-app>
 
 
 
 
 <div class=global-content-wrapper>
 <xpl-root _nghost-cdr-c455 ng-version=13.3.11><xpl-meta-nav _ngcontent-cdr-c455><div class=ng2-xplore-meta-nav><div class="metanav-container u-flex-display-flex u-flex-justify-center"><div class="stats-metanav xplore-meta-nav"><div class="meta-nav-ieee-links hide-mobile text-sm-md-lh"><ul class="meta-nav-menu u-flex-display-flex u-m-0"><li class="meta-nav-item stats-extLink stats-Unav_exit_aaa"><a href=http://www.ieee.org/ id=u-home class=ieeeorg>IEEE.org</a><li class="meta-nav-item stats-extLink ieee-xplore">IEEE <em>Xplore</em><li class="meta-nav-item stats-extLink"><a href=http://standards.ieee.org.thi.idm.oclc.org/ id=u-standards class=exitstandardsorg>IEEE SA</a><li class="meta-nav-item stats-extLink"><a href=http://spectrum.ieee.org.thi.idm.oclc.org/ id=u-spectrum class=exitspectrum>IEEE Spectrum</a><li class="meta-nav-item stats-extLink"><a href=http://www.ieee.org/sitemap.html id=u-more class=exitmoreieeesites>More Sites</a></ul></div><div class="meta-nav-user-links u-flex-display-flex text-sm-md-lh"><ul class="u-relative u-m-0 nav-right icons-panel"><li class=u-flex-display-flex><div class="col-4 hide-desktop"></div><div class=cart-container><ul class="u-flex-display-flex u-pl-0"><li id=global-header-cart-count class="meta-nav-item stats-mnEvLinks"><a title="View Cart" tabindex=0 class="cart stats-Unav_exit_Cart" style=white-space:nowrap href="https://www.ieee.org/cart/public/myCart/page.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore"><span id=cartCount>Cart&nbsp;</span></a><div id=mc_ieee-mini-cart-include_wrapper class="content-r cart-summary product-cart" style=display:none></div></ul><ul class="u-flex-display-flex u-pl-0"><li class="meta-nav-item stats-mnEvLinks hide-desktop"><a title="Create Account" class="create-account-new stats-Unav_CreateAcct hide-desktop" href="https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&amp;sourceCode=xplore&amp;car=IEEE-Xplore&amp;autoSignin=Y&amp;signinurl=https%3A%2F%2Fieeexplore-ieee-org.thi.idm.oclc.org%2FXplore%2Flogin.jsp%3Furl%3D%2FXplore%2Fhome.jsp%26reason%3Dauthenticate&amp;url=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519?arnumber=9043519"><i class="fas fa-user-plus"></i></a><li class="meta-nav-item stats-mnEvLinks u-flex-display-flex u-ml-auto personal-signin-container hide-desktop"><a title="Sign In" class="stats-Unav_P_SignIn hide-desktop"><i aria-hidden=true class="fas fa-sign-in-alt"></i></a></ul></div><div class=text-sm-md-lh><ul class="u-flex-display-flex u-pl-0"><li class="meta-nav-item stats-mnEvLinks"><a title="Create Account" class="create-account-new stats-Unav_CreateAcct hide-mobile" href="https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&amp;sourceCode=xplore&amp;car=IEEE-Xplore&amp;autoSignin=Y&amp;signinurl=https%3A%2F%2Fieeexplore-ieee-org.thi.idm.oclc.org%2FXplore%2Flogin.jsp%3Furl%3D%2FXplore%2Fhome.jsp%26reason%3Dauthenticate&amp;url=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519?arnumber=9043519">Create Account</a><li class="meta-nav-item stats-mnEvLinks u-flex-display-flex u-ml-auto personal-signin-container"><a title="Sign In" class="stats-Unav_P_SignIn hide-mobile u-pr-05">Personal Sign In</a></ul></div></ul></div></div></div></div></xpl-meta-nav><xpl-global-notification _ngcontent-cdr-c455 _nghost-cdr-c63></xpl-global-notification><xpl-header _ngcontent-cdr-c455 _nghost-cdr-c449><div _ngcontent-cdr-c449 class=main-header><xpl-navbar _ngcontent-cdr-c449 _nghost-cdr-c459><div _ngcontent-cdr-c459 class="navbar-container not-homepage inst-logged-in"><div _ngcontent-cdr-c459 class=top-navbar><div _ngcontent-cdr-c459 class="hamburger-menu hide-desktop"><a _ngcontent-cdr-c459><i _ngcontent-cdr-c459 aria-hidden=true class="fa fa-bars"></i></a></div><div _ngcontent-cdr-c459 class=left-side-container><div _ngcontent-cdr-c459 class=left-side-content><div _ngcontent-cdr-c459 class=xplore-logo-wrapper><xpl-xplore-logo _ngcontent-cdr-c459 _nghost-cdr-c444><div _ngcontent-cdr-c444 class=xplore-logo-container><a _ngcontent-cdr-c444 accesskey=1 title="Delivering full text access to the world's highest quality technical literature in engineering and technology" alt="IEEE Advancing Technology for Humanity" href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplore/home.jsp><img _ngcontent-cdr-c444 alt="IEEE Xplore logo - Link to home" class=xplore-logo src="data:image/svg+xml;base64,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"></a></div></xpl-xplore-logo></div><div _ngcontent-cdr-c459 class="primary-menu hide-mobile text-base-md-lh"><ul _ngcontent-cdr-c459><li _ngcontent-cdr-c459><div _ngcontent-cdr-c459><a _ngcontent-cdr-c459 tabindex=0 class="menu-link stats-browse-book"> Browse <i _ngcontent-cdr-c459 aria-hidden=true class="fas fa-chevron-down"></i></a></div><li _ngcontent-cdr-c459><div _ngcontent-cdr-c459><a _ngcontent-cdr-c459 tabindex=0 class="menu-link stats-my-settings"> My Settings <i _ngcontent-cdr-c459 aria-hidden=true class="fas fa-chevron-down"></i></a></div><li _ngcontent-cdr-c459><div _ngcontent-cdr-c459><a _ngcontent-cdr-c459 tabindex=0 class="menu-link stats-get-help"> Help <i _ngcontent-cdr-c459 aria-hidden=true class="fas fa-chevron-down"></i></a></div></ul></div></div></div><div _ngcontent-cdr-c459 class="institution-container hide-mobile inst-logged-in"><div _ngcontent-cdr-c459><xpl-institution-details _ngcontent-cdr-c459 _nghost-cdr-c445><div _ngcontent-cdr-c445><div _ngcontent-cdr-c445 class=inst-detail><div _ngcontent-cdr-c445 class=u-flex-display-flex><div _ngcontent-cdr-c445 class="inst-text-container no-inst-logo"><span _ngcontent-cdr-c445 class=right-line><span _ngcontent-cdr-c445 class=access-text>Access provided by:</span><h4 _ngcontent-cdr-c445 class=inst-name>Technische Hochschule Ingolstadt</h4></span></div><div _ngcontent-cdr-c445 class=signout><a _ngcontent-cdr-c445 title="Sign Out" target=_self href="https://ieeexplore-ieee-org.thi.idm.oclc.org/servlet/Login?logout=/document/9043519/?arnumber=9043519">Sign Out</a></div></div></div></div></xpl-institution-details></div></div><div _ngcontent-cdr-c459 class="right-side-container inst-logged-in"><div _ngcontent-cdr-c459 class=row><xpl-ieee-logo _ngcontent-cdr-c459 _nghost-cdr-c446><div _ngcontent-cdr-c446 class="ieee-logo-container hide-mobile"><img _ngcontent-cdr-c446 alt="IEEE logo - Link to IEEE main site homepage" class=ieee-logo src="data:image/svg+xml;base64,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"></div></xpl-ieee-logo></div></div></div><div _ngcontent-cdr-c459 class="bottom-navbar hide-desktop"><div _ngcontent-cdr-c459 class=inst-details-container><xpl-institution-details _ngcontent-cdr-c459 _nghost-cdr-c445><div _ngcontent-cdr-c445><div _ngcontent-cdr-c445 class=inst-detail><div _ngcontent-cdr-c445 class=u-flex-display-flex><div _ngcontent-cdr-c445 class="inst-text-container no-inst-logo"><span _ngcontent-cdr-c445 class=right-line><span _ngcontent-cdr-c445 class=access-text>Access provided by:</span><h4 _ngcontent-cdr-c445 class=inst-name>Technische Hochschule Ingolstadt</h4></span></div><div _ngcontent-cdr-c445 class=signout><a _ngcontent-cdr-c445 title="Sign Out" target=_self href="https://ieeexplore-ieee-org.thi.idm.oclc.org/servlet/Login?logout=/Xplore/guesthome.jsp">Sign Out</a></div></div></div></div></xpl-institution-details></div></div></div></xpl-navbar><div _ngcontent-cdr-c449><div _ngcontent-cdr-c449 class="search-bar-container fill-background not-homepage"><xpl-search-bar-migr _ngcontent-cdr-c449 _nghost-cdr-c462><div _ngcontent-cdr-c462 class=search-bar><form _ngcontent-cdr-c462 novalidate class="search-bar-wrapper ng-untouched ng-pristine ng-valid"><div _ngcontent-cdr-c462 class=drop-down><label _ngcontent-cdr-c462><select _ngcontent-cdr-c462 aria-label="content type dropdown"><option _ngcontent-cdr-c462 selected>All<option _ngcontent-cdr-c462>Books<option _ngcontent-cdr-c462>Conferences<option _ngcontent-cdr-c462>Courses<option _ngcontent-cdr-c462>Journals &amp; Magazines<option _ngcontent-cdr-c462>Standards<option _ngcontent-cdr-c462>Authors<option _ngcontent-cdr-c462>Citations</select></label></div><div _ngcontent-cdr-c462 class="search-field all"><div _ngcontent-cdr-c462 class=search-field-icon-container><div _ngcontent-cdr-c462 class=global-search-bar><xpl-typeahead-migr _ngcontent-cdr-c462 placeholder name=search-term ulclass="search-within-results ui-autocomplete ui-front ui-menu ui-widget ui-widget-content ui-corner-all" minchars=3 _nghost-cdr-c69><div _ngcontent-cdr-c69 class="Typeahead text-sm-md-lh"><input _ngcontent-cdr-c69 autocomplete=off aria-label="Enter search text" class="Typeahead-input ng-untouched ng-pristine ng-valid" placeholder value type=text></div></xpl-typeahead-migr></div><div _ngcontent-cdr-c462 class=search-icon><button _ngcontent-cdr-c462 type=submit aria-label=Search class="fa fa-search"></button></div></div></div></form><div _ngcontent-cdr-c462 class=below-search-bar><div _ngcontent-cdr-c462 class="advanced-search-wrapper text-sm-md-lh"><div _ngcontent-cdr-c462 class=advanced-search-div><a _ngcontent-cdr-c462 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/search/advanced target=_self><span _ngcontent-cdr-c462>ADVANCED SEARCH </span><i _ngcontent-cdr-c462 aria-hidden=true class="fas fa-caret-right adv-search-arrow sf-hidden"></i></a></div></div></div></div></xpl-search-bar-migr></div></div></div></xpl-header><div _ngcontent-cdr-c455 class=global-ng-wrapper><router-outlet _ngcontent-cdr-c455></router-outlet><xpl-document-details _nghost-cdr-c196><div _ngcontent-cdr-c196 class="row document ng-document stats-document"><div _ngcontent-cdr-c196 class="document-main global-content-width-w-rr"><section _ngcontent-cdr-c196 class="document-main-leaderboard-ad col-12"><xpl-leaderboard-ad _ngcontent-cdr-c196 class=hide-desktop _nghost-cdr-c127><div _ngcontent-cdr-c127 class="Ads-leaderboard ad-panel" style=display:none><div _ngcontent-cdr-c127 class="ad-leaderboard-ad-container sf-hidden"><div _ngcontent-cdr-c127 xplgoogleadmigr class="Ads-leaderBoardTablet sf-hidden"></div><div _ngcontent-cdr-c127 xplgoogleadmigr class="Ads-leaderBoardMobile sf-hidden"></div></div></div></xpl-leaderboard-ad></section><section _ngcontent-cdr-c196 class="document-main-header row"><div _ngcontent-cdr-c196 class=col-12><xpl-document-header _ngcontent-cdr-c196 _nghost-cdr-c148><section _ngcontent-cdr-c148 class="document-header row"><div _ngcontent-cdr-c148 class="document-header-breadcrumbs-container col-12"><div _ngcontent-cdr-c148 class="breadcrumbs col text-sm-md-lh"><span _ngcontent-cdr-c148><a _ngcontent-cdr-c148 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/browse/periodicals/title/>Journals &amp; Magazines</a><span _ngcontent-cdr-c148 class=breadcrumbs-separator> &gt;</span></span><span _ngcontent-cdr-c148><a _ngcontent-cdr-c148 href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a><span _ngcontent-cdr-c148 class=breadcrumbs-separator> &gt;</span></span><span _ngcontent-cdr-c148><a _ngcontent-cdr-c148 href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/tocresult.jsp?isnumber=8948470&amp;punumber=6287639">Volume: 8</a><span _ngcontent-cdr-c148 class=breadcrumbs-separator></span></span><xpl-help-link _ngcontent-cdr-c148 id=help tooltiptype=breadcrumb _nghost-cdr-c54><a _ngcontent-cdr-c54 target=_blank tooltipclass=helplink-tooltip triggers=hover class="icon-size-md u-flex-display-inline" href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/ieee-xplore-training/working-with-documents#interactive-html aria-label="Help using Journal &amp; Magazine documents"><i _ngcontent-cdr-c54 class="fa fa-question-circle help-link breadcrumb-help-link-icon"></i></a></xpl-help-link></div></div><div _ngcontent-cdr-c148 class="document-header-inner-container row"><div _ngcontent-cdr-c148 class=col-12><div _ngcontent-cdr-c148 class="row stats-document-header"><div _ngcontent-cdr-c148 class="row document-title-fix"><div _ngcontent-cdr-c148 class="document-header-title-container col"><div _ngcontent-cdr-c148 class="left-container w-100"><h1 _ngcontent-cdr-c148 class="document-title text-2xl-md-lh"><span _ngcontent-cdr-c148>A State-of-the-Art Review on Image Synthesis With Generative Adversarial Networks</span></h1><div _ngcontent-cdr-c148 class="u-mb-1 u-mt-05 btn-container"><div _ngcontent-cdr-c148 class=publisher-title-tooltip><xpl-publisher _ngcontent-cdr-c148 tooltipplacement=right _nghost-cdr-c107><span _ngcontent-cdr-c107 class="text-base-md-lh publisher-info-container black-tooltip"><span _ngcontent-cdr-c107 xplhighlight><span _ngcontent-cdr-c107><span _ngcontent-cdr-c107 class=title>Publisher: </span><span _ngcontent-cdr-c107>IEEE</span></span></span></span></xpl-publisher></div><div _ngcontent-cdr-c148 class=cite-this-related-btn-wrapper><xpl-cite-this-modal _ngcontent-cdr-c148 _nghost-cdr-c128><div _ngcontent-cdr-c148><button _ngcontent-cdr-c148 placement=bottom class="layout-btn-white cite-this-btn">Cite This</button></div></xpl-cite-this-modal></div><div _ngcontent-cdr-c148 class="black-tooltip tool-tip-pdf-button"><div _ngcontent-cdr-c148 placement=bottom class="pdf-btn-container hide-mobile"><xpl-view-pdf _ngcontent-cdr-c148 placement=document-page-desktop _nghost-cdr-c115><div _ngcontent-cdr-c115><div _ngcontent-cdr-c115><a _ngcontent-cdr-c115 class="pdf-btn-link stats-document-lh-action-downloadPdf_2 pdf" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/stamp/stamp.jsp?tp=&amp;arnumber=9043519"><i _ngcontent-cdr-c115 class="icon-size-md icon red-pdf fas fa-file-pdf"></i><span _ngcontent-cdr-c115>PDF</span></a></div></div></xpl-view-pdf><xpl-login-modal-trigger _ngcontent-cdr-c148 _nghost-cdr-c139></xpl-login-modal-trigger></div></div></div></div><div _ngcontent-cdr-c148 class=right-container></div></div></div><div _ngcontent-cdr-c148 class=document-main-subheader><div _ngcontent-cdr-c148 class=document-main-author-banner><div _ngcontent-cdr-c148 class="document-authors-banner stats-document-authors-banner"><div _ngcontent-cdr-c148 class="row authors-banner-row u-flex-align-items-center u-flex-wrap-nowrap"><xpl-author-banner _ngcontent-cdr-c148 class=authors-banner-row-middle _nghost-cdr-c141><div _ngcontent-cdr-c141 class="document-authors-banner stats-document-authors-banner"><div _ngcontent-cdr-c141 class="row authors-banner-row u-flex-wrap-nowrap"><div _ngcontent-cdr-c141 class=authors-banner-row-middle><div _ngcontent-cdr-c141 class="authors-container stats-document-authors-banner-authorsContainer"><div _ngcontent-cdr-c141 class="authors-info-container overflow-ellipsis text-base-md-lh authors-minimized" id=indexTerms-container-1665660110120-0><span _ngcontent-cdr-c141 class=authors-info><span _ngcontent-cdr-c141 class=blue-tooltip><a _ngcontent-cdr-c141 placement="bottom auto" triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37088373427><span _ngcontent-cdr-c141>Lei Wang</span></a></span><span _ngcontent-cdr-c141>; </span></span><span _ngcontent-cdr-c141 class=authors-info><span _ngcontent-cdr-c141 class=blue-tooltip><a _ngcontent-cdr-c141 placement="bottom auto" triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37537265800><span _ngcontent-cdr-c141>Wei Chen</span></a></span><span _ngcontent-cdr-c141 class=u-px-02><a _ngcontent-cdr-c141 target=_blank href=https://orcid.org/0000-0002-7663-278X><i _ngcontent-cdr-c141 class="icon icon-orcid"></i></a></span><span _ngcontent-cdr-c141>; </span></span><span _ngcontent-cdr-c141 class=authors-info><span _ngcontent-cdr-c141 class=blue-tooltip><a _ngcontent-cdr-c141 placement="bottom auto" triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37088373563><span _ngcontent-cdr-c141>Wenjia Yang</span></a></span><span _ngcontent-cdr-c141>; </span></span><span _ngcontent-cdr-c141 class=authors-info><span _ngcontent-cdr-c141 class=blue-tooltip><a _ngcontent-cdr-c141 placement="bottom auto" triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37840808100><span _ngcontent-cdr-c141>Fangming Bi</span></a></span><span _ngcontent-cdr-c141>; </span></span><span _ngcontent-cdr-c141 class=authors-info><span _ngcontent-cdr-c141 class=blue-tooltip><a _ngcontent-cdr-c141 placement="bottom auto" triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37279171400><span _ngcontent-cdr-c141>Fei Richard Yu</span></a></span><span _ngcontent-cdr-c141 class=u-px-02><a _ngcontent-cdr-c141 target=_blank href=https://orcid.org/0000-0003-1006-7594><i _ngcontent-cdr-c141 class="icon icon-orcid"></i></a></span><span _ngcontent-cdr-c141></span></span></div></div></div></div></div></xpl-author-banner><div _ngcontent-cdr-c148 class="u-flex-display-flex u-flex-align-items-center nowrap text-base-md-lh"><div _ngcontent-cdr-c148 class="authors-view-all-link-container hide-mobile"><a _ngcontent-cdr-c148 class=text-base-md-lh>All Authors</a></div><div _ngcontent-cdr-c148 class="authors-mobile-view-all-container blue-tooltip hide-desktop"><a _ngcontent-cdr-c148 placement=bottom-right triggers=click:click class=authors-viewall-link><i _ngcontent-cdr-c148 class=authors-viewall-icon></i></a></div></div></div></div></div><div _ngcontent-cdr-c148 class="document-header-metrics-banner row ccby-document"><div _ngcontent-cdr-c148 class="document-banner col stats-document-banner"><xpl-login-modal-trigger _ngcontent-cdr-c148 _nghost-cdr-c139></xpl-login-modal-trigger><button _ngcontent-cdr-c148 class="sip-modal-button stats-document-banner-viewDocument"><div _ngcontent-cdr-c148 class=main-txt> View Document </div></button><div _ngcontent-cdr-c148 class="document-banner-metric-container row"><button _ngcontent-cdr-c148 class="document-banner-metric text-base-md-lh col"><div _ngcontent-cdr-c148 class=document-banner-metric-count>25</div><div _ngcontent-cdr-c148>Paper</div><div _ngcontent-cdr-c148>Citations</div></button><button _ngcontent-cdr-c148 class="document-banner-metric text-base-md-lh col"><div _ngcontent-cdr-c148 class=document-banner-metric-count>1</div><div _ngcontent-cdr-c148>Patent</div><div _ngcontent-cdr-c148>Citation</div></button><button _ngcontent-cdr-c148 class="document-banner-metric text-base-md-lh col"><div _ngcontent-cdr-c148 class=document-banner-metric-count>7582</div><div _ngcontent-cdr-c148><div _ngcontent-cdr-c148>Full</div><div _ngcontent-cdr-c148>Text Views</div></div></button></div><div _ngcontent-cdr-c148 class=document-banner-access><div _ngcontent-cdr-c148 class="document-access-container hide-mobile"><div _ngcontent-cdr-c148 class=document-access-icon><i _ngcontent-cdr-c148 class="icon-size-md u-mr-05 fas fa-lock-open-alt"></i><span _ngcontent-cdr-c148>Open Access</span></div></div><div _ngcontent-cdr-c148 class="document-disqus-anchor-container hide-mobile-imp"><a _ngcontent-cdr-c148 tabindex=0><i _ngcontent-cdr-c148 class="icon-size-md color-xplore-blue fas fa-comment u-mr-05"></i><span _ngcontent-cdr-c148>Comment(s)</span></a></div></div></div><div _ngcontent-cdr-c148 class=document-mobile-access-container><div _ngcontent-cdr-c148 class=document-access-icon><i _ngcontent-cdr-c148 class="icon-size-md u-mr-05 fas fa-lock-open-alt"></i></div></div><div _ngcontent-cdr-c148 class="document-disqus-anchor-container hide-desktop"><a _ngcontent-cdr-c148 tabindex=0><i _ngcontent-cdr-c148 class="icon-size-md color-xplore-blue fas fa-comment"></i></a></div><div _ngcontent-cdr-c148 class="col-7-24 black-tooltip hide-mobile"><xpl-document-toolbar _ngcontent-cdr-c148 _nghost-cdr-c147><div _ngcontent-cdr-c147 class="col-actions stats-document-container-lh u-printing-invisible-ie u-printing-invisible-ff"><div _ngcontent-cdr-c147 class=action-item-container><ul _ngcontent-cdr-c147 class="icon-size-md doc-actions doc-toolbar stats-document-lh-actions black-tooltip"><li _ngcontent-cdr-c147 placement=bottom class=doc-actions-item><a _ngcontent-cdr-c147 target=blank class="doc-actions-link stats_ReferencesView_Doc_Details_9043519" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/dwnldReferences?arnumber=9043519"><i _ngcontent-cdr-c147 class="icon-size-md color-xplore-blue fas fa-registered"></i></a><li _ngcontent-cdr-c147 placement=bottom class="doc-actions-item white-blue-border-tooltip"><xpl-document-social-media _ngcontent-cdr-c147 _nghost-cdr-c144><button _ngcontent-cdr-c144 triggers=click class=doc-share-tool><i _ngcontent-cdr-c144 aria-hidden=true class="fa fa-share-alt"></i></button></xpl-document-social-media><li _ngcontent-cdr-c147 placement=bottom class="stats-permission doc-actions-item disabled-look enable-hover"><a _ngcontent-cdr-c147 class="doc-actions-link stats_Doc_Details_Copyright_9043519"><i _ngcontent-cdr-c147 class="color-xplore-blue icon-size-md far fa-copyright"></i></a><li _ngcontent-cdr-c147 placement=bottom class="doc-actions-item white-blue-border-tooltip save-to disabled-look enable-hover"><a _ngcontent-cdr-c147 placement=bottom-right triggers=click class=doc-save-tool><i _ngcontent-cdr-c147 class="icon-size-md color-xplore-blue fas fa-folder-open"></i></a><li _ngcontent-cdr-c147 placement=bottom class=doc-actions-item><xpl-manage-alerts _ngcontent-cdr-c147 class="white-blue-border-tooltip alerts-popover" _nghost-cdr-c146><a _ngcontent-cdr-c146 triggers=click:click class="doc-actions-link stats-document-lh-action-alerts hide-mobile"><i _ngcontent-cdr-c146 class="icon-size-md color-xplore-blue fas fa-bell"></i><span _ngcontent-cdr-c146 class=doc-actions-text>Alerts</span></a><div _ngcontent-cdr-c146 class="manage-alerts-popover-content hide-desktop"><h1 _ngcontent-cdr-c146 class=header>Alerts</h1><div _ngcontent-cdr-c146 class=manage-alerts-link><a _ngcontent-cdr-c146 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/alerts/citation> Manage Content Alerts <i _ngcontent-cdr-c146 class="icon icon-courses-chevron-blue"></i></a></div><div _ngcontent-cdr-c146 class=manage-alerts-link><a _ngcontent-cdr-c146> Add to Citation Alerts <i _ngcontent-cdr-c146 class="icon icon-courses-chevron-blue"></i></a></div></div></xpl-manage-alerts></ul></div></div></xpl-document-toolbar></div></div><div _ngcontent-cdr-c148 class="ccby-indicator u-pl-2"> Under a <a _ngcontent-cdr-c148 target=_blank href=https://creativecommons.org/licenses/by/4.0/>Creative Commons License</a></div></div></div></div></div><hr _ngcontent-cdr-c148></section></xpl-document-header></div></section><div _ngcontent-cdr-c196 class="row document-main-body"><div _ngcontent-cdr-c196 class="document-main-left-trail col-5-24"><div _ngcontent-cdr-c196 class=col-24-24><div _ngcontent-cdr-c196 class=row><nav _ngcontent-cdr-c196 class="col-24-24 bg-ltgry tab-nav text-base-md-lh"><div _ngcontent-cdr-c196 id=document-tabs class="doc-tabs-list stats-document-tabs"><div _ngcontent-cdr-c196 routerlinkactive=active class="browse-pub-tab active"><a _ngcontent-cdr-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519>Abstract</a></div><xpl-full-text-toc _ngcontent-cdr-c196 class=hide-mobile _nghost-cdr-c149><div _ngcontent-cdr-c149 class=toc-container><div _ngcontent-cdr-c149 class=toc-heading>Document Sections</div><ul _ngcontent-cdr-c149 class=toc-list><li _ngcontent-cdr-c149 class=toc-list-item><a _ngcontent-cdr-c149 class=toc-list-link tabindex=0><div _ngcontent-cdr-c149 class=toc-list-icon>I.</div><div _ngcontent-cdr-c149 class=toc-list-icon></div><div _ngcontent-cdr-c149>Introduction</div></a><li _ngcontent-cdr-c149 class=toc-list-item><a _ngcontent-cdr-c149 class=toc-list-link tabindex=0><div _ngcontent-cdr-c149 class=toc-list-icon>II.</div><div _ngcontent-cdr-c149 class=toc-list-icon></div><div _ngcontent-cdr-c149>Generative Adversarial Networks</div></a><li _ngcontent-cdr-c149 class=toc-list-item><a _ngcontent-cdr-c149 class=toc-list-link tabindex=0><div _ngcontent-cdr-c149 class=toc-list-icon>III.</div><div _ngcontent-cdr-c149 class=toc-list-icon></div><div _ngcontent-cdr-c149>Image Synthesis</div></a><li _ngcontent-cdr-c149 class=toc-list-item><a _ngcontent-cdr-c149 class=toc-list-link tabindex=0><div _ngcontent-cdr-c149 class=toc-list-icon>IV.</div><div _ngcontent-cdr-c149 class=toc-list-icon></div><div _ngcontent-cdr-c149>Image-to-Image Translation</div></a><li _ngcontent-cdr-c149 class=toc-list-item><a _ngcontent-cdr-c149 class=toc-list-link tabindex=0><div _ngcontent-cdr-c149 class=toc-list-icon>V.</div><div _ngcontent-cdr-c149 class=toc-list-icon></div><div _ngcontent-cdr-c149>Image Editing</div></a></ul><button _ngcontent-cdr-c149 class=toc-show-more-btn><span _ngcontent-cdr-c149>Show Full Outline</span><i _ngcontent-cdr-c149 class="fa fa-caret-down"></i></button></div></xpl-full-text-toc><div _ngcontent-cdr-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-cdr-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/authors>Authors</a></div><div _ngcontent-cdr-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-cdr-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/figures>Figures</a></div><div _ngcontent-cdr-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-cdr-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/references>References</a></div><div _ngcontent-cdr-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-cdr-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/citations>Citations</a></div><div _ngcontent-cdr-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-cdr-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/keywords>Keywords</a></div><div _ngcontent-cdr-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-cdr-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/metrics>Metrics</a></div><div _ngcontent-cdr-c196 routerlinkactive=active class="browse-pub-tab similar"><a _ngcontent-cdr-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/similar>More Like This</a></div></div></nav></div></div></div><div _ngcontent-cdr-c196 class=document-main-content-container><xpl-left-side-bar _ngcontent-cdr-c196 _nghost-cdr-c151><div _ngcontent-cdr-c151 xplscrollsnapmigr scrollreset=true offsetfrom=100 fromelementid=mobile-tab-pane tillelementid=full-text-footer offsetto=-800 cssclasstostick=document-mobile-leftrail-stick class="col-2 col-actions ng-col-actions hide-desktop stats-document-container-lh u-printing-invisible-ie u-printing-invisible-ff col-actions-mobile-closed ng-col-actions-mobile-closed"><div _ngcontent-cdr-c151 id=left-rail-container><div _ngcontent-cdr-c151 class=doc-actions-mobile-expand-button></div><ul _ngcontent-cdr-c151 class="doc-actions stats-document-lh-actions"><li _ngcontent-cdr-c151 class=doc-actions-item><xpl-view-pdf _ngcontent-cdr-c151 placement=document-page-mobile _nghost-cdr-c115><div _ngcontent-cdr-c115><div _ngcontent-cdr-c115><a _ngcontent-cdr-c115 target=_blank class="doc-actions-link stats-document-lh-action-downloadPdf_2 pdf" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/stamp/stamp.jsp?tp=&amp;arnumber=9043519"><i _ngcontent-cdr-c115 class="icon-size-md icon red-pdf fas fa-file-pdf"></i> Download PDF </a></div></div></xpl-view-pdf><xpl-login-modal-trigger _ngcontent-cdr-c151 _nghost-cdr-c139></xpl-login-modal-trigger><li _ngcontent-cdr-c151 class=doc-actions-item><a _ngcontent-cdr-c151 target=blank class="doc-actions-link stats_ReferencesView_Doc_Details_9043519" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/dwnldReferences?arnumber=9043519"><i _ngcontent-cdr-c151 class="icon-size-md color-xplore-blue fas fa-registered"></i> View References </a><li _ngcontent-cdr-c151 class="doc-actions-item white-blue-border-tooltip"><a _ngcontent-cdr-c151 class=doc-actions-link><xpl-document-social-media _ngcontent-cdr-c151 tooltipplacement=right placement=document-page-mobile _nghost-cdr-c144><button _ngcontent-cdr-c144 triggers=click class=doc-share-tool><i _ngcontent-cdr-c144 aria-hidden=true class="fa fa-share-alt"></i></button></xpl-document-social-media></a><li _ngcontent-cdr-c151 class="stats-permission doc-actions-item disabled-look black-tooltip"><a _ngcontent-cdr-c151 placement=right triggers=click class="doc-actions-link stats_Doc_Details_Copyright_9043519"><i _ngcontent-cdr-c151 class="copyright-icon far fa-copyright"></i> Request Permissions </a><li _ngcontent-cdr-c151 class="doc-actions-item disabled-look black-tooltip"><a _ngcontent-cdr-c151 placement=right triggers=click autoclose=outside class="doc-actions-link stats-document-lh-action-downloadPdf_3"><i _ngcontent-cdr-c151 class="icon-size-md color-xplore-blue fas fa-folder-open"></i> Save to </a><li _ngcontent-cdr-c151 class=doc-actions-item><a _ngcontent-cdr-c151 class="doc-actions-link stats-document-lh-action-alerts"><i _ngcontent-cdr-c151 class="icon-size-md color-xplore-blue fas fa-bell"></i> Alerts </a></ul></div></div></xpl-left-side-bar><section _ngcontent-cdr-c196 class="tab-pane col-24-24 u-printing-display-inline-ie u-printing-display-inline-ff"><div _ngcontent-cdr-c196 id=mobile-tab-pane></div><div _ngcontent-cdr-c196 class=document-main-left-trail-content><div _ngcontent-cdr-c196><router-outlet _ngcontent-cdr-c196></router-outlet><xpl-document-abstract _nghost-cdr-c154><section _ngcontent-cdr-c154 class="document-abstract document-tab"><div _ngcontent-cdr-c154 class="col-12 hide-desktop mobile-graphical-abstract"><div _ngcontent-cdr-c154 class=abstract-graphic><xpl-graphical-abstract-modal _ngcontent-cdr-c154 styleclassname=cover-image-document _nghost-cdr-c112><a _ngcontent-cdr-c112 class="cover-image-document stats-graphical_abstract_thumbnail_9043519" aria-label="The taxonomy of GANs."><img _ngcontent-cdr-c112 src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="660" height="295"><rect fill-opacity="0"/></svg>' style="background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-3)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important"></a></xpl-graphical-abstract-modal><div _ngcontent-cdr-c154 class=abstract-graphic-caption><span _ngcontent-cdr-c154 xplmathjax>The taxonomy of GANs.</span></div></div></div><div _ngcontent-cdr-c154 class="abstract-mobile-div hide-desktop"><div _ngcontent-cdr-c154 class=row><div _ngcontent-cdr-c154 class=mobile-col-12><div _ngcontent-cdr-c154 class=u-pb-1><strong _ngcontent-cdr-c154> Abstract:</strong><span _ngcontent-cdr-c154 xplmathjax>Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because...</span><span _ngcontent-cdr-c154><a _ngcontent-cdr-c154 class=mobile-toggle-btn>View more</a></span></div></div></div><div _ngcontent-cdr-c154 class="metadata-toggle-btn mobile-content"><strong _ngcontent-cdr-c154><i _ngcontent-cdr-c154 class=icon-caret-abstract></i><span _ngcontent-cdr-c154>Metadata</span></strong></div></div><div _ngcontent-cdr-c154 class="abstract-desktop-div hide-mobile text-base-md-lh"><div _ngcontent-cdr-c154 class="abstract-text row"><div _ngcontent-cdr-c154 class=col-12><div _ngcontent-cdr-c154 class=u-mb-1><strong _ngcontent-cdr-c154> Abstract:</strong><div _ngcontent-cdr-c154 xplmathjax>Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because of the impressive performance they achieved in various applications. In this paper, we introduce the recent research on GANs in the field of image processing, including image synthesis, image generation, image semantic editing, image-to-image translation, image super-resolution, image inpainting, and cartoon generation. We analyze and summarize the methods used in these applications which have improved the generated results. Then, we discuss the challenges faced by GANs and introduce some methods to deal with these problems. We also preview some likely future research directions in the field of GANs, such as video generation, facial animation synthesis and 3D face reconstruction. The purpose of this review is to provide insights into the research on GANs and to present the various applications based on GANs in different scenarios.</div></div></div></div><div _ngcontent-cdr-c154 data-tealium_data='{"docType": "Journal"}' class="u-pb-1 stats-document-abstract-publishedIn"><strong _ngcontent-cdr-c154>Published in: </strong><a _ngcontent-cdr-c154 class=stats-document-abstract-publishedIn href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a><span _ngcontent-cdr-c154> ( <span _ngcontent-cdr-c154>Volume: 8</span>) </span></div><div _ngcontent-cdr-c154 class="row u-pt-1"><div _ngcontent-cdr-c154 class=col-6><div _ngcontent-cdr-c154 class=u-pb-1><strong _ngcontent-cdr-c154>Page(s): </strong> 63514 <span _ngcontent-cdr-c154>- 63537</span></div><div _ngcontent-cdr-c154 class="u-pb-1 doc-abstract-pubdate"><strong _ngcontent-cdr-c154>Date of Publication:</strong> 20 March 2020 <xpl-help-link _ngcontent-cdr-c154 arialabel="Get help with using Publication Dates" helplinktext="Help with using Publication Dates" helplink=http://ieeexplore.ieee.org.thi.idm.oclc.org/Xplorehelp/Help_Pubdates.html _nghost-cdr-c54><a _ngcontent-cdr-c54 target=_blank tooltipclass=helplink-tooltip triggers=hover class="icon-size-md u-flex-display-inline" href=http://ieeexplore.ieee.org.thi.idm.oclc.org/Xplorehelp/Help_Pubdates.html aria-label="Get help with using Publication Dates"><i _ngcontent-cdr-c54 class="fa fa-question-circle help-link help-link-icon"></i></a></xpl-help-link></div><div _ngcontent-cdr-c154 class=u-pb-1><div _ngcontent-cdr-c154><div _ngcontent-cdr-c154><strong _ngcontent-cdr-c154>Electronic ISSN:</strong> 2169-3536 </div></div></div></div><div _ngcontent-cdr-c154 class=col-6><div _ngcontent-cdr-c154 class=u-pb-1><strong _ngcontent-cdr-c154>INSPEC Accession Number: </strong> 19521306 </div><div _ngcontent-cdr-c154 class="u-pb-1 stats-document-abstract-doi"><strong _ngcontent-cdr-c154>DOI: </strong><a _ngcontent-cdr-c154 append-to-href="?src=document" target=_blank href=https://doi-org.thi.idm.oclc.org/10.1109/ACCESS.2020.2982224>10.1109/ACCESS.2020.2982224</a></div><div _ngcontent-cdr-c154 class="u-pb-1 doc-abstract-publisher"><xpl-publisher _ngcontent-cdr-c154 _nghost-cdr-c107><span _ngcontent-cdr-c107 class="text-base-md-lh publisher-info-container black-tooltip"><span _ngcontent-cdr-c107 xplhighlight><span _ngcontent-cdr-c107><span _ngcontent-cdr-c107 class=title>Publisher: </span><span _ngcontent-cdr-c107>IEEE</span></span></span></span></xpl-publisher></div></div><div _ngcontent-cdr-c154 class="col-12 u-pb-1 stats-document-abstract-fundedBy"><div _ngcontent-cdr-c154 role=button><strong _ngcontent-cdr-c154><i _ngcontent-cdr-c154 class=icon-caret-abstract></i>Funding Agency: </strong></div></div></div><div _ngcontent-cdr-c154 class=row><div _ngcontent-cdr-c154 class=col-12><div _ngcontent-cdr-c154 class=abstract-graphic><div _ngcontent-cdr-c154 class="row u-flex-justify-center"><xpl-graphical-abstract-modal _ngcontent-cdr-c154 styleclassname=cover-image-document _nghost-cdr-c112><a _ngcontent-cdr-c112 class="cover-image-document stats-graphical_abstract_thumbnail_9043519" aria-label="The taxonomy of GANs."><img _ngcontent-cdr-c112 src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="660" height="295"><rect fill-opacity="0"/></svg>' style="background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-3)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important"></a></xpl-graphical-abstract-modal></div><div _ngcontent-cdr-c154 class=abstract-graphic-caption><span _ngcontent-cdr-c154 xplmathjax>The taxonomy of GANs.</span></div></div></div><div _ngcontent-cdr-c154 class="show-full-abstract col-12"><a _ngcontent-cdr-c154 class=document-abstract-toggle-btn> Hide Full Abstract <i _ngcontent-cdr-c154 class="fa fa-angle-up"></i></a></div></div></div></section></xpl-document-abstract></div><xpl-leaderboard-middle-ad _ngcontent-cdr-c196 class=hide-desktop _nghost-cdr-c155><div _ngcontent-cdr-c155 class="Ads-leaderboard ad-panel"><div _ngcontent-cdr-c155 class="row u-flex-wrap-nowrap"><div _ngcontent-cdr-c155 class=ads-close-container><i _ngcontent-cdr-c155 aria-hidden=true class=ads-close-button></i></div></div><div _ngcontent-cdr-c155 class=ad-leaderboard-ad-container><div _ngcontent-cdr-c155 xplgoogleadmigr class=Ads-leaderBoardMiddleTablet><div id=div-gpt-ad-1606861708257-0 style="width:576px;height:71px;display:none;margin:0px auto;padding-bottom:0.5em"></div></div><div _ngcontent-cdr-c155 xplgoogleadmigr class=Ads-leaderBoardMiddleMobile><div id=div-gpt-ad-1606861708357-0 style="width:320px;height:50px;margin:0px auto;padding-bottom:0.5em" data-google-query-id=CP-c5LmL3foCFUNS4Aod4DoGmw><div id=google_ads_iframe_/3890430/IEEEXplore/DocDetailsMiddle_1__container__ style="border:0pt none"></div></div></div></div></div></xpl-leaderboard-middle-ad><xpl-document-full-text _ngcontent-cdr-c196 _nghost-cdr-c167><section _ngcontent-cdr-c167><div _ngcontent-cdr-c167 id=toc-wrapper class="row full-text-toc-wrapper"><div _ngcontent-cdr-c167 xplscrollsnapmigr cssclasstostick=document-toc-stick fromelementid=toc-wrapper tillelementid=full-text-footer offsetfrom=150 offsetto=-800 scrollreset=true class="col-12 u-align-center ft-toc previous-next-nav-ctrl hide-desktop"><div _ngcontent-cdr-c167 class="toc-container hide-desktop"><a _ngcontent-cdr-c167 ngclass="{'disabled': !toc}" class="toc-link {'disabled': !toc}"><img _ngcontent-cdr-c167 src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAASCAYAAABB7B6eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKT2lDQ1BQaG90b3Nob3AgSUNDIHByb2ZpbGUAAHjanVNnVFPpFj333vRCS4iAlEtvUhUIIFJCi4AUkSYqIQkQSoghodkVUcERRUUEG8igiAOOjoCMFVEsDIoK2AfkIaKOg6OIisr74Xuja9a89+bN/rXXPues852zzwfACAyWSDNRNYAMqUIeEeCDx8TG4eQuQIEKJHAAEAizZCFz/SMBAPh+PDwrIsAHvgABeNMLCADATZvAMByH/w/qQplcAYCEAcB0kThLCIAUAEB6jkKmAEBGAYCdmCZTAKAEAGDLY2LjAFAtAGAnf+bTAICd+Jl7AQBblCEVAaCRACATZYhEAGg7AKzPVopFAFgwABRmS8Q5ANgtADBJV2ZIALC3AMDOEAuyAAgMADBRiIUpAAR7AGDIIyN4AISZABRG8lc88SuuEOcqAAB4mbI8uSQ5RYFbCC1xB1dXLh4ozkkXKxQ2YQJhmkAuwnmZGTKBNA/g88wAAKCRFRHgg/P9eM4Ors7ONo62Dl8t6r8G/yJiYuP+5c+rcEAAAOF0ftH+LC+zGoA7BoBt/qIl7gRoXgugdfeLZrIPQLUAoOnaV/Nw+H48PEWhkLnZ2eXk5NhKxEJbYcpXff5nwl/AV/1s+X48/Pf14L7iJIEyXYFHBPjgwsz0TKUcz5IJhGLc5o9H/LcL//wd0yLESWK5WCoU41EScY5EmozzMqUiiUKSKcUl0v9k4t8s+wM+3zUAsGo+AXuRLahdYwP2SycQWHTA4vcAAPK7b8HUKAgDgGiD4c93/+8//UegJQCAZkmScQAAXkQkLlTKsz/HCAAARKCBKrBBG/TBGCzABhzBBdzBC/xgNoRCJMTCQhBCCmSAHHJgKayCQiiGzbAdKmAv1EAdNMBRaIaTcA4uwlW4Dj1wD/phCJ7BKLyBCQRByAgTYSHaiAFiilgjjggXmYX4IcFIBBKLJCDJiBRRIkuRNUgxUopUIFVIHfI9cgI5h1xGupE7yAAygvyGvEcxlIGyUT3UDLVDuag3GoRGogvQZHQxmo8WoJvQcrQaPYw2oefQq2gP2o8+Q8cwwOgYBzPEbDAuxsNCsTgsCZNjy7EirAyrxhqwVqwDu4n1Y8+xdwQSgUXACTYEd0IgYR5BSFhMWE7YSKggHCQ0EdoJNwkDhFHCJyKTqEu0JroR+cQYYjIxh1hILCPWEo8TLxB7iEPENyQSiUMyJ7mQAkmxpFTSEtJG0m5SI+ksqZs0SBojk8naZGuyBzmULCAryIXkneTD5DPkG+Qh8lsKnWJAcaT4U+IoUspqShnlEOU05QZlmDJBVaOaUt2ooVQRNY9aQq2htlKvUYeoEzR1mjnNgxZJS6WtopXTGmgXaPdpr+h0uhHdlR5Ol9BX0svpR+iX6AP0dwwNhhWDx4hnKBmbGAcYZxl3GK+YTKYZ04sZx1QwNzHrmOeZD5lvVVgqtip8FZHKCpVKlSaVGyovVKmqpqreqgtV81XLVI+pXlN9rkZVM1PjqQnUlqtVqp1Q61MbU2epO6iHqmeob1Q/pH5Z/YkGWcNMw09DpFGgsV/jvMYgC2MZs3gsIWsNq4Z1gTXEJrHN2Xx2KruY/R27iz2qqaE5QzNKM1ezUvOUZj8H45hx+Jx0TgnnKKeX836K3hTvKeIpG6Y0TLkxZVxrqpaXllirSKtRq0frvTau7aedpr1Fu1n7gQ5Bx0onXCdHZ4/OBZ3nU9lT3acKpxZNPTr1ri6qa6UbobtEd79up+6Ynr5egJ5Mb6feeb3n+hx9L/1U/W36p/VHDFgGswwkBtsMzhg8xTVxbzwdL8fb8VFDXcNAQ6VhlWGX4YSRudE8o9VGjUYPjGnGXOMk423GbcajJgYmISZLTepN7ppSTbmmKaY7TDtMx83MzaLN1pk1mz0x1zLnm+eb15vft2BaeFostqi2uGVJsuRaplnutrxuhVo5WaVYVVpds0atna0l1rutu6cRp7lOk06rntZnw7Dxtsm2qbcZsOXYBtuutm22fWFnYhdnt8Wuw+6TvZN9un2N/T0HDYfZDqsdWh1+c7RyFDpWOt6azpzuP33F9JbpL2dYzxDP2DPjthPLKcRpnVOb00dnF2e5c4PziIuJS4LLLpc+Lpsbxt3IveRKdPVxXeF60vWdm7Obwu2o26/uNu5p7ofcn8w0nymeWTNz0MPIQ+BR5dE/C5+VMGvfrH5PQ0+BZ7XnIy9jL5FXrdewt6V3qvdh7xc+9j5yn+M+4zw33jLeWV/MN8C3yLfLT8Nvnl+F30N/I/9k/3r/0QCngCUBZwOJgUGBWwL7+Hp8Ib+OPzrbZfay2e1BjKC5QRVBj4KtguXBrSFoyOyQrSH355jOkc5pDoVQfujW0Adh5mGLw34MJ4WHhVeGP45wiFga0TGXNXfR3ENz30T6RJZE3ptnMU85ry1KNSo+qi5qPNo3ujS6P8YuZlnM1VidWElsSxw5LiquNm5svt/87fOH4p3iC+N7F5gvyF1weaHOwvSFpxapLhIsOpZATIhOOJTwQRAqqBaMJfITdyWOCnnCHcJnIi/RNtGI2ENcKh5O8kgqTXqS7JG8NXkkxTOlLOW5hCepkLxMDUzdmzqeFpp2IG0yPTq9MYOSkZBxQqohTZO2Z+pn5mZ2y6xlhbL+xW6Lty8elQfJa7OQrAVZLQq2QqboVFoo1yoHsmdlV2a/zYnKOZarnivN7cyzytuQN5zvn//tEsIS4ZK2pYZLVy0dWOa9rGo5sjxxedsK4xUFK4ZWBqw8uIq2Km3VT6vtV5eufr0mek1rgV7ByoLBtQFr6wtVCuWFfevc1+1dT1gvWd+1YfqGnRs+FYmKrhTbF5cVf9go3HjlG4dvyr+Z3JS0qavEuWTPZtJm6ebeLZ5bDpaql+aXDm4N2dq0Dd9WtO319kXbL5fNKNu7g7ZDuaO/PLi8ZafJzs07P1SkVPRU+lQ27tLdtWHX+G7R7ht7vPY07NXbW7z3/T7JvttVAVVN1WbVZftJ+7P3P66Jqun4lvttXa1ObXHtxwPSA/0HIw6217nU1R3SPVRSj9Yr60cOxx++/p3vdy0NNg1VjZzG4iNwRHnk6fcJ3/ceDTradox7rOEH0x92HWcdL2pCmvKaRptTmvtbYlu6T8w+0dbq3nr8R9sfD5w0PFl5SvNUyWna6YLTk2fyz4ydlZ19fi753GDborZ752PO32oPb++6EHTh0kX/i+c7vDvOXPK4dPKy2+UTV7hXmq86X23qdOo8/pPTT8e7nLuarrlca7nuer21e2b36RueN87d9L158Rb/1tWeOT3dvfN6b/fF9/XfFt1+cif9zsu72Xcn7q28T7xf9EDtQdlD3YfVP1v+3Njv3H9qwHeg89HcR/cGhYPP/pH1jw9DBY+Zj8uGDYbrnjg+OTniP3L96fynQ89kzyaeF/6i/suuFxYvfvjV69fO0ZjRoZfyl5O/bXyl/erA6xmv28bCxh6+yXgzMV70VvvtwXfcdx3vo98PT+R8IH8o/2j5sfVT0Kf7kxmTk/8EA5jz/GMzLdsAAAAgY0hSTQAAeiUAAICDAAD5/wAAgOkAAHUwAADqYAAAOpgAABdvkl/FRgAAAHJJREFUeNrsUzESgDAII75VfZT617hUXWhE0Q0mCr3kAgE2rbQWXEYcOebtrGdikF0Ga6KnCSBqdMCd/yA/mcTLERWBmSFkU4qld5b7TAGEe5RVqQhIHwzBd1NWNq07uPd5XkEC/LoD/jYf2wEAAP//AwCHICkUy307YwAAAABJRU5ErkJggg=="> Contents </a></div></div></div><hr _ngcontent-cdr-c167><div _ngcontent-cdr-c167 class="row document-full-text-content"><div _ngcontent-cdr-c167 id=full-text-section class="col col-text stats-document-container-fullTextSection u-printing-display-inline-ie u-printing-display-inline-ff" style=font-size:15px><span _ngcontent-cdr-c167 id=full-text-header></span><div _ngcontent-cdr-c167><div _ngcontent-cdr-c167 xplmathjax xplfulltextdomhandler parentid=full-text-section xpllazyloadfigures class="document-text hide-full-text ng-non-bindable stats-document-dynamicFullTextOrSnippet-container">
<response><accesstype>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via <a class=vglnk href=https://creativecommons.org/licenses/by/4.0/ rel=nofollow><span>https</span><span>://</span><span>creativecommons</span><span>.</span><span>org</span><span>/</span><span>licenses</span><span>/</span><span>by</span><span>/</span><span>4</span><span>.</span><span>0</span><span>/</span></a> to obtain full-text articles and stipulations in the API documentation.</accesstype><div id=BodyWrapper class=ArticlePage xmlns:ieee=http://www.ieeexplore.ieee.org.thi.idm.oclc.org><div id=article>
<div class=section id=sec1><div class="header article-hdr"><div class=kicker>
 SECTION I.</div><h2>Introduction</h2></div><p>Artificial intelligence (AI) has aroused widespread interest in both the press and on social media. Especially with the rapid development of deep learning, image processing has made great progress. An enormous amount of images are applied in social media, which make the generative models became a hot topic in deep learning research.<p>Some generative models are promising unsupervised learning techniques with powerful semantic information representation capabilities, and are attracting more and more attention. Among them, the Variational Auto-encoder (VAE) <a ref-type=bibr anchor=ref1 id=context_ref_1_1>[1]</a> cannot generate clear enough images. The Glow <a ref-type=bibr anchor=ref2 id=context_ref_2_1>[2]</a> is a flow-based generation model, which has not been widely used so far. The Generative Adversarial Networks (GANs) have achieved impressive results in image processing, and are attracting growing interests in the academic and industrial fields.<p>Nowadays, GANs are applied to various research and applications, such as image generation <a ref-type=bibr anchor=ref3 id=context_ref_3_1>[3]</a>, image inpainting <a ref-type=bibr anchor=ref4 id=context_ref_4_1>[4]</a>, text generation <a ref-type=bibr anchor=ref5 id=context_ref_5_1>[5]</a>, medical image processing <a ref-type=bibr anchor=ref6 id=context_ref_6_1>[6]</a>–<a ref-type=bibr anchor=ref13 id=context_ref_13_1>[13]</a>, semantic segmentation <a ref-type=bibr anchor=ref14 id=context_ref_14_1>[14]</a>–<a ref-type=bibr anchor=ref17 id=context_ref_17_1>[17]</a>, image colorization <a ref-type=bibr anchor=ref18 id=context_ref_18_1>[18]</a>, <a ref-type=bibr anchor=ref19 id=context_ref_19_1>[19]</a>, image-to-image translation <a ref-type=bibr anchor=ref20 id=context_ref_20_1>[20]</a>, and art generation <a ref-type=bibr anchor=ref21 id=context_ref_21_1>[21]</a>. Besides, GANs are widely used in face synthesis and face editing, such as face age <a ref-type=bibr anchor=ref22 id=context_ref_22_1>[22]</a>–<a ref-type=bibr anchor=ref24 id=context_ref_24_1>[24]</a> and gender translation <a ref-type=bibr anchor=ref25 id=context_ref_25_1>[25]</a>.<p>The research of GANs is divided into two directions: 1) Theoretical research on GANs based on information theory or energy-based models, and focus on the unsolved problems of GANs during training, such as mode collapse, unstable training and hard to evaluate. We will briefly discuss this aspect of problems and the challenges of GANs in Section IX. 2) The applications of GANs in various computer vision tasks. Although there are still some unresolved problems, various GAN-variants have improved the performance of GANs by numerous research studies. In this work, we mainly focus on the second aspect of current research on GANs.<p>Although there have been some surveys on GANs so far, like <a ref-type=bibr anchor=ref26 id=context_ref_26_1>[26]</a>, in the field of deep learning, especially GANs are developing fast. This paper focuses on the recent research of GANs in image synthesis. It provides a comparison and analysis in terms of the pros and cons of these applications based on GANs. Besides, we analyze and summarize the methods that have been used in these applications to improve the generated images. Meanwhile, we discuss the challenges faced by GANs in terms of training and evaluating of GANs. Some methods for stable training and evaluation of GANs are provided. Then, we discuss the likely future research directions, such as video generation, facial animation synthesis, and 3D face reconstruction. The rest of the paper is organized as follows: <a ref-type=sec anchor=sec2 class=fulltext-link>Section II</a> gives a brief introduction of GANs. <a ref-type=sec anchor=sec3 class=fulltext-link>Section III</a> introduces some applications of image synthesis based on GANs. <a ref-type=sec anchor=sec4 class=fulltext-link>Section IV</a> focuses on the supervised and unsupervised methods for image-to-image translation. <a ref-type=sec anchor=sec5 class=fulltext-link>Section V</a> discusses several methods in the application of image editing. <a ref-type=sec anchor=sec6 class=fulltext-link>Section VI</a> describes several methods of cartoon generation. <a ref-type=sec anchor=sec7 class=fulltext-link>Section VII</a> reviews the current challenges and limitations of GAN-based methods, as well as previews likely future research work in the area of GANs. Conclusions are given in <a ref-type=sec anchor=sec8 class=fulltext-link>Section VIII</a>.</p></div>
<div class=section id=sec2><div class="header article-hdr"><div class=kicker>
 SECTION II.</div><h2>Generative Adversarial Networks</h2></div><p>GANs are especially successful in image tasks due to the great potential in image processing. They are considered to be the most effective method in the task of image generation and play an important role in various applications.<p>The Generative Adversarial Network (GAN) is a model that has been prevailing since Goodfellow <i>et al.</i> <a ref-type=bibr anchor=ref27 id=context_ref_27_2>[27]</a> proposed it in 2014. GAN consists of a generator G and a discriminator D, the general structure of a Generative Adversarial Network is illustrated in <a ref-type=fig anchor=fig1 class=fulltext-link>Fig. 1</a>.
<div class="figure figure-full" id=fig1><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen1-2982224-large.gif data-fig-id=fig1><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen1-2982224-small.gif data-alt="FIGURE 1. - The general structure of a generative adversarial network."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 1. </b><fig><p>The general structure of a generative adversarial network.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The generator G is used to generate realistic samples from random noise and tries to fool the discriminator D. The discriminator D is used to identify whether the sample is real or generated by the generator G. The generator and the discriminator are competing with each other until the discriminator cannot distinguish between real and fake generated images. The whole process can be regarded as a two-player minimax game where the main aim of GAN training is to achieve the Nash equilibrium <a ref-type=bibr anchor=ref28 id=context_ref_28_2>[28]</a>. The loss function of the GAN is formulated as follows:<disp-formula id=deqn1 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}&amp;\min \limits _{G} \max \limits _{D} V(D,G)=\textrm {E}_{x\sim Pdata(x)} [\textrm {log}D(x)] \\&amp; \qquad \qquad \qquad \qquad \qquad \quad {+\,\textrm {E}_{z\sim Pz(z)} [\textrm {log}(1-D(G(z)))] } \tag{1}\end{align*}</span><span class="MathJax_Display MathJax_Processed"><span class=MathJax id=MathJax-Element-1-Frame tabindex=0><nobr><span class=math id=MathJax-Span-1><span style=display:inline-block;position:relative;width:0em;height:0px;font-size:123%><span style=position:absolute><span class=mrow id=MathJax-Span-2><span class=mtable id=MathJax-Span-3 style=min-width:27.453em><span style=display:inline-block;position:relative;width:100%;height:0px;min-width:27.453em><span style=display:inline-block;position:absolute;width:23.297em;height:0px;clip:rect(-1.934em,1023.18em,1.434em,-1000em);top:0em;left:50%;margin-left:-11.648em><span style=position:absolute;clip:rect(2.877em,1000em,5.082em,-1000em);top:-4.011em;left:0em><span style=display:inline-block;position:relative;width:0em;height:0px><span style=position:absolute;clip:rect(3.848em,1000em,4.173em,-1000em);top:-4.983em;right:0em><span class=mtd id=MathJax-Span-4><span class=mrow id=MathJax-Span-5></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.848em,1000em,4.173em,-1000em);top:-3.102em;right:0em><span class=mtd id=MathJax-Span-54><span class=mrow id=MathJax-Span-55></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(2.127em,1023.18em,5.445em,-1000em);top:-4.011em;left:0em><span style=display:inline-block;position:relative;width:23.297em;height:0px><span style=position:absolute;clip:rect(3.098em,1017.43em,4.954em,-1000em);top:-4.983em;left:0em><span class=mtd id=MathJax-Span-6><span class=mrow id=MathJax-Span-7><span class=mi id=MathJax-Span-8></span><span class=munderover id=MathJax-Span-9 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:1.667em;height:0px><span style=position:absolute;clip:rect(3.179em,1001.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mo id=MathJax-Span-10 style=font-family:MathJax_Main>min</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.35em,1000.54em,4.289em,-1000em);top:-3.346em;left:0.556em><span class=texatom id=MathJax-Span-11><span class=mrow id=MathJax-Span-12><span class=mi id=MathJax-Span-13 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>G</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=munderover id=MathJax-Span-14 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:1.861em;height:0px><span style=position:absolute;clip:rect(3.4em,1001.85em,4.184em,-1000em);top:-4.011em;left:0em><span class=mo id=MathJax-Span-15 style=font-family:MathJax_Main>max</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.365em,1000.57em,4.273em,-1000em);top:-3.35em;left:0.638em><span class=texatom id=MathJax-Span-16><span class=mrow id=MathJax-Span-17><span class=mi id=MathJax-Span-18 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>D</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mi id=MathJax-Span-19 style=font-family:MathJax_Math;font-style:italic;text-rendering:optimizelegibility;padding-left:0.167em>V<span style=display:inline-block;overflow:hidden;height:1px;width:0.186em></span></span><span class=mo id=MathJax-Span-20 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-21 style=font-family:MathJax_Math;font-style:italic>D</span><span class=mo id=MathJax-Span-22 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-23 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>G</span><span class=mo id=MathJax-Span-24 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-25 style=font-family:MathJax_Main;padding-left:0.278em>=</span><span class=msubsup id=MathJax-Span-26 style=padding-left:0.278em><span style=display:inline-block;position:relative;width:4.569em;height:0px><span style=position:absolute;clip:rect(3.168em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=texatom id=MathJax-Span-27><span class=mrow id=MathJax-Span-28><span class=mtext id=MathJax-Span-29 style=font-family:MathJax_Main>E</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.825em;left:0.681em><span class=texatom id=MathJax-Span-30><span class=mrow id=MathJax-Span-31><span class=mi id=MathJax-Span-32 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-33 style=font-size:70.7%;font-family:MathJax_Main>∼</span><span class=mi id=MathJax-Span-34 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>P<span style=display:inline-block;overflow:hidden;height:1px;width:0.077em></span></span><span class=mi id=MathJax-Span-35 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>d<span style=display:inline-block;overflow:hidden;height:1px;width:0.002em></span></span><span class=mi id=MathJax-Span-36 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>a</span><span class=mi id=MathJax-Span-37 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>t</span><span class=mi id=MathJax-Span-38 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>a</span><span class=mo id=MathJax-Span-39 style=font-size:70.7%;font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-40 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-41 style=font-size:70.7%;font-family:MathJax_Main>)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-42 style=font-family:MathJax_Main>[</span><span class=texatom id=MathJax-Span-43><span class=mrow id=MathJax-Span-44><span class=mtext id=MathJax-Span-45 style=font-family:MathJax_Main>log</span></span></span><span class=mi id=MathJax-Span-46 style=font-family:MathJax_Math;font-style:italic>D</span><span class=mo id=MathJax-Span-47 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-48 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-49 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-50 style=font-family:MathJax_Main>]</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.098em,1023.18em,4.536em,-1000em);top:-3.102em;left:0em><span class=mtd id=MathJax-Span-56><span class=mrow id=MathJax-Span-57><span class=mspace id=MathJax-Span-58 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mspace id=MathJax-Span-59 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mspace id=MathJax-Span-60 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mspace id=MathJax-Span-61 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mspace id=MathJax-Span-62 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mspace id=MathJax-Span-63 style=height:0em;vertical-align:0em;width:1em;display:inline-block;overflow:hidden></span><span class=texatom id=MathJax-Span-64><span class=mrow id=MathJax-Span-65><span class=mo id=MathJax-Span-66 style=font-family:MathJax_Main>+</span><span class=mspace id=MathJax-Span-67 style=height:0em;vertical-align:0em;width:0.167em;display:inline-block;overflow:hidden></span><span class=msubsup id=MathJax-Span-68><span style=display:inline-block;position:relative;width:3.38em;height:0px><span style=position:absolute;clip:rect(3.168em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=texatom id=MathJax-Span-69><span class=mrow id=MathJax-Span-70><span class=mtext id=MathJax-Span-71 style=font-family:MathJax_Main>E</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.825em;left:0.681em><span class=texatom id=MathJax-Span-72><span class=mrow id=MathJax-Span-73><span class=mi id=MathJax-Span-74 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>z<span style=display:inline-block;overflow:hidden;height:1px;width:0.002em></span></span><span class=mo id=MathJax-Span-75 style=font-size:70.7%;font-family:MathJax_Main>∼</span><span class=mi id=MathJax-Span-76 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>P<span style=display:inline-block;overflow:hidden;height:1px;width:0.077em></span></span><span class=mi id=MathJax-Span-77 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>z<span style=display:inline-block;overflow:hidden;height:1px;width:0.002em></span></span><span class=mo id=MathJax-Span-78 style=font-size:70.7%;font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-79 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>z<span style=display:inline-block;overflow:hidden;height:1px;width:0.002em></span></span><span class=mo id=MathJax-Span-80 style=font-size:70.7%;font-family:MathJax_Main>)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-81 style=font-family:MathJax_Main>[</span><span class=texatom id=MathJax-Span-82><span class=mrow id=MathJax-Span-83><span class=mtext id=MathJax-Span-84 style=font-family:MathJax_Main>log</span></span></span><span class=mo id=MathJax-Span-85 style=font-family:MathJax_Main>(</span><span class=mn id=MathJax-Span-86 style=font-family:MathJax_Main>1</span><span class=mo id=MathJax-Span-87 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=mi id=MathJax-Span-88 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>D</span><span class=mo id=MathJax-Span-89 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-90 style=font-family:MathJax_Math;font-style:italic>G</span><span class=mo id=MathJax-Span-91 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-92 style=font-family:MathJax_Math;font-style:italic>z<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span><span class=mo id=MathJax-Span-93 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-94 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-95 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-96 style=font-family:MathJax_Main>]</span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;position:absolute;width:1.278em;height:0px;clip:rect(-0.004em,1001.18em,1.322em,-1000em);top:0em;right:0em;margin-right:0em><span style=position:absolute;clip:rect(3.098em,1001.18em,4.423em,-1000em);top:-3.102em;right:0em><span class=mtd id=mjx-eqn-1><span class=mrow id=MathJax-Span-52><span class=mtext id=MathJax-Span-53 style=font-family:MathJax_Main>(1)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span></span></span></span></nobr></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}&amp;\min \limits _{G} \max \limits _{D} V(D,G)=\textrm {E}_{x\sim Pdata(x)} [\textrm {log}D(x)] \\&amp; \qquad \qquad \qquad \qquad \qquad \quad {+\,\textrm {E}_{z\sim Pz(z)} [\textrm {log}(1-D(G(z)))] } \tag{1}\end{align*}
</span></span></disp-formula> where Pdata(x) denotes the true data distribution, Pz(z) denote the noise distribution.<p>Due to the special network structure and the generation performance of GANs, extensive research has produced numerous applications based on GANs, as shown in <a ref-type=fig anchor=fig2 class=fulltext-link>Fig. 2</a>.
<div class="figure figure-full" id=fig2><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen2-2982224-large.gif data-fig-id=fig2><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen2-2982224-small.gif data-alt="FIGURE 2. - Taxonomy of GANs."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 2. </b><fig><p>Taxonomy of GANs.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p></p></div>
<div class=section id=sec3><div class="header article-hdr"><div class=kicker>
 SECTION III.</div><h2>Image Synthesis</h2></div><p>Image synthesis has attracted people’s attention because of its wide application in social media. The GANs have achieved excellent results in the field of image synthesis, such as GauGAN <a ref-type=bibr anchor=ref29 id=context_ref_29_3>[29]</a>. A variety of image synthesis methods have emerged so far.<div class=section_2 id=sec3a><h3>A. Texture Synthesis</h3><p>Image synthesis can be divided into fine-grained texture synthesis and coarse-grained texture synthesis. The coarse-grained texture synthesis pays attention to the similarity between the input image and the output image while the fine-grained texture synthesis pursues whether the synthetic texture is similar to the ground truth.<div class=section_2 id=sec3a1><h4>1) PSGAN</h4><p>Bergmann <i>et al.</i> <a ref-type=bibr anchor=ref30 id=context_ref_30_3a1>[30]</a> proposed a new method of texture synthesis based on the Generative Adversarial Network called Periodic Spatial GAN (PSGAN). The model of PSGAN is illustrated in <a ref-type=fig anchor=fig3 class=fulltext-link>Fig. 3</a>.
<div class="figure figure-full" id=fig3><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen3-2982224-large.gif data-fig-id=fig3><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen3-2982224-small.gif data-alt="FIGURE 3. - Illustration of the PSGAN model [30]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 3. </b><fig><p>Illustration of the PSGAN model <a ref-type=bibr anchor=ref30 id=context_ref_30_3a1>[30]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss function of the PSGAN is defined as:<disp-formula id=deqn2 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}&amp;\hspace {-2pc}\min \limits _{G} \max \limits _{D} V(D,G) \\=&amp;\textstyle {\frac{1 }{ {LM}}}\mathop \Sigma \limits _{\lambda =1}^{L} \mathop \Sigma \limits _{\mu =1}^{M} \textrm {E}_{Z\sim Pz(Z)} [\textrm {log}(1-D_{\lambda \mu } (G(Z)))] \\&amp;+\,\textstyle {\frac{1 }{ {LM}}}\mathop \Sigma \limits _{\lambda =1}^{L} \mathop \Sigma \limits _{\mu =1}^{M} \textrm {E}_{X'\sim Pdata(X)} [\textrm {log}D_{\lambda \mu } ({X}')]\tag{2}\end{align*}</span><span class="MathJax_Display MathJax_Processed"><span class=MathJax id=MathJax-Element-2-Frame tabindex=0><nobr><span class=math id=MathJax-Span-97><span style=display:inline-block;position:relative;width:0em;height:0px;font-size:123%><span style=position:absolute><span class=mrow id=MathJax-Span-98><span class=mtable id=MathJax-Span-99 style=min-width:22.547em><span style=display:inline-block;position:relative;width:100%;height:0px;min-width:22.547em><span style=display:inline-block;position:absolute;width:18.391em;height:0px;clip:rect(-3.859em,1018.27em,3.359em,-1000em);top:0em;left:50%;margin-left:-9.196em><span style=position:absolute;clip:rect(3.12em,1000.72em,8.648em,-1000em);top:-6.179em;left:0em><span style=display:inline-block;position:relative;width:0.778em;height:0px><span style=position:absolute;clip:rect(3.848em,1000em,4.173em,-1000em);top:-6.907em;right:0em><span class=mtd id=MathJax-Span-100><span class=mrow id=MathJax-Span-101></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.481em,1000.72em,4.04em,-1000em);top:-4.361em;right:0em><span class=mtd id=MathJax-Span-122><span class=mrow id=MathJax-Span-123><span class=mo id=MathJax-Span-124 style=font-family:MathJax_Main>=</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.848em,1000em,4.173em,-1000em);top:-1.705em;right:0em><span class=mtd id=MathJax-Span-200><span class=mrow id=MathJax-Span-201></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:6.179em></span></span><span style=position:absolute;clip:rect(4.05em,1017.49em,11.218em,-1000em);top:-7.859em;left:0.778em><span style=display:inline-block;position:relative;width:17.613em;height:0px><span style=position:absolute;clip:rect(3.098em,1005.14em,4.954em,-1000em);top:-6.907em;left:0em><span class=mtd id=MathJax-Span-102><span class=mrow id=MathJax-Span-103><span class=mi id=MathJax-Span-104></span><span class=mspace id=MathJax-Span-105 style=height:0em;vertical-align:0em;margin-left:-2.4em></span><span class=munderover id=MathJax-Span-106 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:1.667em;height:0px><span style=position:absolute;clip:rect(3.179em,1001.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mo id=MathJax-Span-107 style=font-family:MathJax_Main>min</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.35em,1000.54em,4.289em,-1000em);top:-3.346em;left:0.556em><span class=texatom id=MathJax-Span-108><span class=mrow id=MathJax-Span-109><span class=mi id=MathJax-Span-110 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>G</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=munderover id=MathJax-Span-111 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:1.861em;height:0px><span style=position:absolute;clip:rect(3.4em,1001.85em,4.184em,-1000em);top:-4.011em;left:0em><span class=mo id=MathJax-Span-112 style=font-family:MathJax_Main>max</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.365em,1000.57em,4.273em,-1000em);top:-3.35em;left:0.638em><span class=texatom id=MathJax-Span-113><span class=mrow id=MathJax-Span-114><span class=mi id=MathJax-Span-115 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>D</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mi id=MathJax-Span-116 style=font-family:MathJax_Math;font-style:italic;text-rendering:optimizelegibility;padding-left:0.167em>V<span style=display:inline-block;overflow:hidden;height:1px;width:0.186em></span></span><span class=mo id=MathJax-Span-117 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-118 style=font-family:MathJax_Math;font-style:italic>D</span><span class=mo id=MathJax-Span-119 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-120 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>G</span><span class=mo id=MathJax-Span-121 style=font-family:MathJax_Main>)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(2.382em,1017.49em,5.064em,-1000em);top:-4.361em;left:0em><span class=mtd id=MathJax-Span-125><span class=mrow id=MathJax-Span-126><span class=mstyle id=MathJax-Span-127><span class=mrow id=MathJax-Span-128><span class=texatom id=MathJax-Span-129><span class=mrow id=MathJax-Span-130><span class=mfrac id=MathJax-Span-131><span style=display:inline-block;position:relative;width:1.345em;height:0px;margin-right:0.12em;margin-left:0.12em><span style=position:absolute;clip:rect(3.377em,1000.3em,4.173em,-1000em);top:-4.422em;left:50%;margin-left:-0.177em><span class=mn id=MathJax-Span-132 style=font-size:70.7%;font-family:MathJax_Main>1</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.365em,1001.23em,4.173em,-1000em);top:-3.616em;left:50%;margin-left:-0.612em><span class=texatom id=MathJax-Span-133><span class=mrow id=MathJax-Span-134><span class=mi id=MathJax-Span-135 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>L</span><span class=mi id=MathJax-Span-136 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>M<span style=display:inline-block;overflow:hidden;height:1px;width:0.057em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(0.854em,1001.35em,1.247em,-1000em);top:-1.304em;left:0em><span style="display:inline-block;overflow:hidden;vertical-align:0em;border-top:1.3px solid;width:1.345em;height:0px"></span><span style=display:inline-block;width:0px;height:1.084em></span></span></span></span></span></span><span class=munderover id=MathJax-Span-137 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:1.316em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.67em,4.173em,-1000em);top:-4.011em;left:0.297em><span class=texatom id=MathJax-Span-138><span class=mrow id=MathJax-Span-139><span class=mi id=MathJax-Span-140 style=font-family:MathJax_Main>Σ</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.358em,1001.26em,4.282em,-1000em);top:-3.353em;left:0em><span class=texatom id=MathJax-Span-141><span class=mrow id=MathJax-Span-142><span class=mi id=MathJax-Span-143 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>λ</span><span class=mo id=MathJax-Span-144 style=font-size:70.7%;font-family:MathJax_Main>=</span><span class=mn id=MathJax-Span-145 style=font-size:70.7%;font-family:MathJax_Main>1</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.265em,1000.46em,4.173em,-1000em);top:-4.894em;left:0.417em><span class=texatom id=MathJax-Span-146><span class=mrow id=MathJax-Span-147><span class=mi id=MathJax-Span-148 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>L</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-149></span><span class=munderover id=MathJax-Span-150 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:1.33em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.67em,4.173em,-1000em);top:-4.011em;left:0.304em><span class=texatom id=MathJax-Span-151><span class=mrow id=MathJax-Span-152><span class=mi id=MathJax-Span-153 style=font-family:MathJax_Main>Σ</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.377em,1001.28em,4.426em,-1000em);top:-3.373em;left:0em><span class=texatom id=MathJax-Span-154><span class=mrow id=MathJax-Span-155><span class=mi id=MathJax-Span-156 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>μ</span><span class=mo id=MathJax-Span-157 style=font-size:70.7%;font-family:MathJax_Main>=</span><span class=mn id=MathJax-Span-158 style=font-size:70.7%;font-family:MathJax_Main>1</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.265em,1000.74em,4.173em,-1000em);top:-4.894em;left:0.293em><span class=texatom id=MathJax-Span-159><span class=mrow id=MathJax-Span-160><span class=mi id=MathJax-Span-161 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>M<span style=display:inline-block;overflow:hidden;height:1px;width:0.057em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-162></span><span class=msubsup id=MathJax-Span-163 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:3.741em;height:0px><span style=position:absolute;clip:rect(3.168em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=texatom id=MathJax-Span-164><span class=mrow id=MathJax-Span-165><span class=mtext id=MathJax-Span-166 style=font-family:MathJax_Main>E</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.825em;left:0.681em><span class=texatom id=MathJax-Span-167><span class=mrow id=MathJax-Span-168><span class=mi id=MathJax-Span-169 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>Z<span style=display:inline-block;overflow:hidden;height:1px;width:0.028em></span></span><span class=mo id=MathJax-Span-170 style=font-size:70.7%;font-family:MathJax_Main>∼</span><span class=mi id=MathJax-Span-171 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>P<span style=display:inline-block;overflow:hidden;height:1px;width:0.077em></span></span><span class=mi id=MathJax-Span-172 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>z<span style=display:inline-block;overflow:hidden;height:1px;width:0.002em></span></span><span class=mo id=MathJax-Span-173 style=font-size:70.7%;font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-174 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>Z<span style=display:inline-block;overflow:hidden;height:1px;width:0.028em></span></span><span class=mo id=MathJax-Span-175 style=font-size:70.7%;font-family:MathJax_Main>)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-176 style=font-family:MathJax_Main>[</span><span class=texatom id=MathJax-Span-177><span class=mrow id=MathJax-Span-178><span class=mtext id=MathJax-Span-179 style=font-family:MathJax_Main>log</span></span></span><span class=mo id=MathJax-Span-180 style=font-family:MathJax_Main>(</span><span class=mn id=MathJax-Span-181 style=font-family:MathJax_Main>1</span><span class=mo id=MathJax-Span-182 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=msubsup id=MathJax-Span-183 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.742em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.8em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-184 style=font-family:MathJax_Math;font-style:italic>D</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.828em><span class=texatom id=MathJax-Span-185><span class=mrow id=MathJax-Span-186><span class=mi id=MathJax-Span-187 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>λ</span><span class=mi id=MathJax-Span-188 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>μ</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-189 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-190 style=font-family:MathJax_Math;font-style:italic>G</span><span class=mo id=MathJax-Span-191 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-192 style=font-family:MathJax_Math;font-style:italic>Z<span style=display:inline-block;overflow:hidden;height:1px;width:0.04em></span></span><span class=mo id=MathJax-Span-193 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-194 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-195 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-196 style=font-family:MathJax_Main>]</span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(2.382em,1016.75em,5.064em,-1000em);top:-1.705em;left:0em><span class=mtd id=MathJax-Span-202><span class=mrow id=MathJax-Span-203><span class=mi id=MathJax-Span-204></span><span class=mo id=MathJax-Span-205 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=mspace id=MathJax-Span-206 style=height:0em;vertical-align:0em;width:0.167em;display:inline-block;overflow:hidden></span><span class=mstyle id=MathJax-Span-207 style=padding-left:0.222em><span class=mrow id=MathJax-Span-208><span class=texatom id=MathJax-Span-209><span class=mrow id=MathJax-Span-210><span class=mfrac id=MathJax-Span-211><span style=display:inline-block;position:relative;width:1.345em;height:0px;margin-right:0.12em;margin-left:0.12em><span style=position:absolute;clip:rect(3.377em,1000.3em,4.173em,-1000em);top:-4.422em;left:50%;margin-left:-0.177em><span class=mn id=MathJax-Span-212 style=font-size:70.7%;font-family:MathJax_Main>1</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.365em,1001.23em,4.173em,-1000em);top:-3.616em;left:50%;margin-left:-0.612em><span class=texatom id=MathJax-Span-213><span class=mrow id=MathJax-Span-214><span class=mi id=MathJax-Span-215 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>L</span><span class=mi id=MathJax-Span-216 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>M<span style=display:inline-block;overflow:hidden;height:1px;width:0.057em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(0.854em,1001.35em,1.247em,-1000em);top:-1.304em;left:0em><span style="display:inline-block;overflow:hidden;vertical-align:0em;border-top:1.3px solid;width:1.345em;height:0px"></span><span style=display:inline-block;width:0px;height:1.084em></span></span></span></span></span></span><span class=munderover id=MathJax-Span-217 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:1.316em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.67em,4.173em,-1000em);top:-4.011em;left:0.297em><span class=texatom id=MathJax-Span-218><span class=mrow id=MathJax-Span-219><span class=mi id=MathJax-Span-220 style=font-family:MathJax_Main>Σ</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.358em,1001.26em,4.282em,-1000em);top:-3.353em;left:0em><span class=texatom id=MathJax-Span-221><span class=mrow id=MathJax-Span-222><span class=mi id=MathJax-Span-223 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>λ</span><span class=mo id=MathJax-Span-224 style=font-size:70.7%;font-family:MathJax_Main>=</span><span class=mn id=MathJax-Span-225 style=font-size:70.7%;font-family:MathJax_Main>1</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.265em,1000.46em,4.173em,-1000em);top:-4.894em;left:0.417em><span class=texatom id=MathJax-Span-226><span class=mrow id=MathJax-Span-227><span class=mi id=MathJax-Span-228 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>L</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-229></span><span class=munderover id=MathJax-Span-230 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:1.33em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.67em,4.173em,-1000em);top:-4.011em;left:0.304em><span class=texatom id=MathJax-Span-231><span class=mrow id=MathJax-Span-232><span class=mi id=MathJax-Span-233 style=font-family:MathJax_Main>Σ</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.377em,1001.28em,4.426em,-1000em);top:-3.373em;left:0em><span class=texatom id=MathJax-Span-234><span class=mrow id=MathJax-Span-235><span class=mi id=MathJax-Span-236 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>μ</span><span class=mo id=MathJax-Span-237 style=font-size:70.7%;font-family:MathJax_Main>=</span><span class=mn id=MathJax-Span-238 style=font-size:70.7%;font-family:MathJax_Main>1</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.265em,1000.74em,4.173em,-1000em);top:-4.894em;left:0.293em><span class=texatom id=MathJax-Span-239><span class=mrow id=MathJax-Span-240><span class=mi id=MathJax-Span-241 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>M<span style=display:inline-block;overflow:hidden;height:1px;width:0.057em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-242></span><span class=msubsup id=MathJax-Span-243 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:5.211em;height:0px><span style=position:absolute;clip:rect(3.168em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=texatom id=MathJax-Span-244><span class=mrow id=MathJax-Span-245><span class=mtext id=MathJax-Span-246 style=font-family:MathJax_Main>E</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.785em;left:0.681em><span class=texatom id=MathJax-Span-247><span class=mrow id=MathJax-Span-248><span class=msup id=MathJax-Span-249><span style=display:inline-block;position:relative;width:0.848em;height:0px><span style=position:absolute;clip:rect(3.365em,1000.6em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-250 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>X<span style=display:inline-block;overflow:hidden;height:1px;width:0.017em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-4.301em;left:0.658em><span class=mo id=MathJax-Span-251 style=font-size:50%;font-family:MathJax_Main>′</span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-252 style=font-size:70.7%;font-family:MathJax_Main>∼</span><span class=mi id=MathJax-Span-253 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>P<span style=display:inline-block;overflow:hidden;height:1px;width:0.077em></span></span><span class=mi id=MathJax-Span-254 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>d<span style=display:inline-block;overflow:hidden;height:1px;width:0.002em></span></span><span class=mi id=MathJax-Span-255 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>a</span><span class=mi id=MathJax-Span-256 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>t</span><span class=mi id=MathJax-Span-257 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>a</span><span class=mo id=MathJax-Span-258 style=font-size:70.7%;font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-259 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>X<span style=display:inline-block;overflow:hidden;height:1px;width:0.017em></span></span><span class=mo id=MathJax-Span-260 style=font-size:70.7%;font-family:MathJax_Main>)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-261 style=font-family:MathJax_Main>[</span><span class=texatom id=MathJax-Span-262><span class=mrow id=MathJax-Span-263><span class=mtext id=MathJax-Span-264 style=font-family:MathJax_Main>log</span></span></span><span class=msubsup id=MathJax-Span-265><span style=display:inline-block;position:relative;width:1.742em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.8em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-266 style=font-family:MathJax_Math;font-style:italic>D</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.828em><span class=texatom id=MathJax-Span-267><span class=mrow id=MathJax-Span-268><span class=mi id=MathJax-Span-269 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>λ</span><span class=mi id=MathJax-Span-270 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>μ</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-271 style=font-family:MathJax_Main>(</span><span class=msup id=MathJax-Span-272><span style=display:inline-block;position:relative;width:1.179em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.85em,4.173em,-1000em);top:-4.011em;left:0em><span class=texatom id=MathJax-Span-273><span class=mrow id=MathJax-Span-274><span class=mi id=MathJax-Span-275 style=font-family:MathJax_Math;font-style:italic>X<span style=display:inline-block;overflow:hidden;height:1px;width:0.024em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-4.421em;left:0.909em><span class=mo id=MathJax-Span-276 style=font-size:70.7%;font-family:MathJax_Main>′</span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-277 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-278 style=font-family:MathJax_Main>]</span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:7.859em></span></span></span><span style=display:inline-block;position:absolute;width:1.278em;height:0px;clip:rect(1.394em,1001.18em,2.719em,-1000em);top:0em;right:0em;margin-right:0em><span style=position:absolute;clip:rect(3.098em,1001.18em,4.423em,-1000em);top:-1.705em;right:0em><span class=mtd id=mjx-eqn-2><span class=mrow id=MathJax-Span-198><span class=mtext id=MathJax-Span-199 style=font-family:MathJax_Main>(2)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span></span></span></span></nobr></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}&amp;\hspace {-2pc}\min \limits _{G} \max \limits _{D} V(D,G) \\=&amp;\textstyle {\frac{1 }{ {LM}}}\mathop \Sigma \limits _{\lambda =1}^{L} \mathop \Sigma \limits _{\mu =1}^{M} \textrm {E}_{Z\sim Pz(Z)} [\textrm {log}(1-D_{\lambda \mu } (G(Z)))] \\&amp;+\,\textstyle {\frac{1 }{ {LM}}}\mathop \Sigma \limits _{\lambda =1}^{L} \mathop \Sigma \limits _{\mu =1}^{M} \textrm {E}_{X'\sim Pdata(X)} [\textrm {log}D_{\lambda \mu } ({X}')]\tag{2}\end{align*}
</span></span></disp-formula><p>PSGAN can learn multiple textures from one or more complex datasets of large images. The method can not only smoothly interpolate between samples in a structured noise space and generate novel samples that are perceptually located between the textures of the original dataset, but also accurately learn periodic textures. PSGAN has the flexibility to handle a wide range of textures and image data sources. It is a method of highly scalable and can produce output images of any size.</p></div><div class=section_2 id=sec3a2><h4>2) TextureGAN</h4><p>Xian <i>et al.</i> <a ref-type=bibr anchor=ref31 id=context_ref_31_3a2>[31]</a> proposed a texture synthesis method called TextureGAN, which combines sketch, color, and texture to synthesize images that people expect. The training process is shown in <a ref-type=fig anchor=fig4 class=fulltext-link>Fig. 4</a> and <a ref-type=fig anchor=fig5 class=fulltext-link>Fig. 5</a>.
<div class="figure figure-full" id=fig4><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen4-2982224-large.gif data-fig-id=fig4><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen4-2982224-small.gif data-alt="FIGURE 4. - TextureGAN pipeline for the ground-truth pre-training [31]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 4. </b><fig><p>TextureGAN pipeline for the ground-truth pre-training <a ref-type=bibr anchor=ref31 id=context_ref_31_3a2>[31]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig5><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen5-2982224-large.gif data-fig-id=fig5><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen5-2982224-small.gif data-alt="FIGURE 5. - TextureGAN pipeline for the external texture fine-tuning [31]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 5. </b><fig><p>TextureGAN pipeline for the external texture fine-tuning <a ref-type=bibr anchor=ref31 id=context_ref_31_3a2>[31]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function of ground-truth pre-training is defined as:<disp-formula id=deqn3 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L=L_{F} +W_{ADV} L_{ADV} +W_{S} L_{S} +W_{P} L_{P} +W_{C} L_{C}\tag{3}\end{equation*}</span><span class="MathJax_Display MathJax_Processed"><span class=MathJax id=MathJax-Element-3-Frame tabindex=0><nobr><span class=math id=MathJax-Span-279><span style=display:inline-block;position:relative;width:0em;height:0px;font-size:123%><span style=position:absolute><span class=mrow id=MathJax-Span-280><span class=mtable id=MathJax-Span-281 style=min-width:25.813em><span style=display:inline-block;position:relative;width:100%;height:0px;min-width:25.813em><span style=display:inline-block;position:absolute;width:21.657em;height:0px;clip:rect(-0.938em,1021.66em,0.438em,-1000em);top:0em;left:50%;margin-left:-10.828em><span style=position:absolute;clip:rect(3.19em,1021.66em,4.376em,-1000em);top:-4.011em;left:0em><span style=display:inline-block;position:relative;width:21.657em;height:0px><span style=position:absolute;clip:rect(3.165em,1021.66em,4.351em,-1000em);top:-3.986em;left:50%;margin-left:-10.828em><span class=mtd id=MathJax-Span-285><span class=mrow id=MathJax-Span-286><span class=mi id=MathJax-Span-287 style=font-family:MathJax_Math;font-style:italic>L</span><span class=mo id=MathJax-Span-288 style=font-family:MathJax_Main;padding-left:0.278em>=</span><span class=msubsup id=MathJax-Span-289 style=padding-left:0.278em><span style=display:inline-block;position:relative;width:1.286em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-290 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.681em><span class=texatom id=MathJax-Span-291><span class=mrow id=MathJax-Span-292><span class=mi id=MathJax-Span-293 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>F<span style=display:inline-block;overflow:hidden;height:1px;width:0.075em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-294 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-295 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:2.679em;height:0px><span style=position:absolute;clip:rect(3.165em,1001.05em,4.195em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-296 style=font-family:MathJax_Math;font-style:italic>W<span style=display:inline-block;overflow:hidden;height:1px;width:0.104em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.849em;left:0.944em><span class=texatom id=MathJax-Span-297><span class=mrow id=MathJax-Span-298><span class=mi id=MathJax-Span-299 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>A</span><span class=mi id=MathJax-Span-300 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>D</span><span class=mi id=MathJax-Span-301 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic;text-rendering:optimizelegibility>V<span style=display:inline-block;overflow:hidden;height:1px;width:0.132em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-302><span style=display:inline-block;position:relative;width:2.416em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-303 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.849em;left:0.681em><span class=texatom id=MathJax-Span-304><span class=mrow id=MathJax-Span-305><span class=mi id=MathJax-Span-306 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>A</span><span class=mi id=MathJax-Span-307 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>D</span><span class=mi id=MathJax-Span-308 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic;text-rendering:optimizelegibility>V<span style=display:inline-block;overflow:hidden;height:1px;width:0.132em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-309 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-310 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.475em;height:0px><span style=position:absolute;clip:rect(3.165em,1001.05em,4.195em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-311 style=font-family:MathJax_Math;font-style:italic>W<span style=display:inline-block;overflow:hidden;height:1px;width:0.104em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.857em;left:0.944em><span class=texatom id=MathJax-Span-312><span class=mrow id=MathJax-Span-313><span class=mi id=MathJax-Span-314 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>S<span style=display:inline-block;overflow:hidden;height:1px;width:0.023em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-315><span style=display:inline-block;position:relative;width:1.212em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-316 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.857em;left:0.681em><span class=texatom id=MathJax-Span-317><span class=mrow id=MathJax-Span-318><span class=mi id=MathJax-Span-319 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>S<span style=display:inline-block;overflow:hidden;height:1px;width:0.023em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-320 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-321 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.55em;height:0px><span style=position:absolute;clip:rect(3.165em,1001.05em,4.195em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-322 style=font-family:MathJax_Math;font-style:italic>W<span style=display:inline-block;overflow:hidden;height:1px;width:0.104em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.944em><span class=texatom id=MathJax-Span-323><span class=mrow id=MathJax-Span-324><span class=mi id=MathJax-Span-325 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>P<span style=display:inline-block;overflow:hidden;height:1px;width:0.077em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-326><span style=display:inline-block;position:relative;width:1.287em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-327 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.681em><span class=texatom id=MathJax-Span-328><span class=mrow id=MathJax-Span-329><span class=mi id=MathJax-Span-330 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>P<span style=display:inline-block;overflow:hidden;height:1px;width:0.077em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-331 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-332 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.556em;height:0px><span style=position:absolute;clip:rect(3.165em,1001.05em,4.195em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-333 style=font-family:MathJax_Math;font-style:italic>W<span style=display:inline-block;overflow:hidden;height:1px;width:0.104em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.857em;left:0.944em><span class=texatom id=MathJax-Span-334><span class=mrow id=MathJax-Span-335><span class=mi id=MathJax-Span-336 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>C<span style=display:inline-block;overflow:hidden;height:1px;width:0.032em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-337><span style=display:inline-block;position:relative;width:1.293em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-338 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.857em;left:0.681em><span class=texatom id=MathJax-Span-339><span class=mrow id=MathJax-Span-340><span class=mi id=MathJax-Span-341 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>C<span style=display:inline-block;overflow:hidden;height:1px;width:0.032em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;position:absolute;width:1.278em;height:0px;clip:rect(-0.888em,1001.18em,0.438em,-1000em);top:0em;right:0em;margin-right:0em><span style=position:absolute;clip:rect(3.098em,1001.18em,4.423em,-1000em);top:-3.986em;right:0em><span class=mtd id=mjx-eqn-3><span class=mrow id=MathJax-Span-283><span class=mtext id=MathJax-Span-284 style=font-family:MathJax_Main>(3)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span></span></span></span></nobr></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L=L_{F} +W_{ADV} L_{ADV} +W_{S} L_{S} +W_{P} L_{P} +W_{C} L_{C}\tag{3}\end{equation*}
</span></span></disp-formula><p>The objective function of external texture fine-tuning is defined as:<disp-formula id=deqn4 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L=L_{F} +W_{ADV} L_{ADV} +W_{P} L_{P}^{\prime }+W_{C} L_{C}^{\prime }+L_{t}\tag{4}\end{equation*}</span><span class="MathJax_Display MathJax_Processed"><span class=MathJax id=MathJax-Element-4-Frame tabindex=0><nobr><span class=math id=MathJax-Span-342><span style=display:inline-block;position:relative;width:0em;height:0px;font-size:123%><span style=position:absolute><span class=mrow id=MathJax-Span-343><span class=mtable id=MathJax-Span-344 style=min-width:24.137em><span style=display:inline-block;position:relative;width:100%;height:0px;min-width:24.137em><span style=display:inline-block;position:absolute;width:19.981em;height:0px;clip:rect(-0.987em,1019.98em,0.487em,-1000em);top:0em;left:50%;margin-left:-9.99em><span style=position:absolute;clip:rect(3.113em,1019.98em,4.498em,-1000em);top:-4.011em;left:0em><span style=display:inline-block;position:relative;width:19.981em;height:0px><span style=position:absolute;clip:rect(3.138em,1019.98em,4.523em,-1000em);top:-4.036em;left:50%;margin-left:-9.99em><span class=mtd id=MathJax-Span-348><span class=mrow id=MathJax-Span-349><span class=mi id=MathJax-Span-350 style=font-family:MathJax_Math;font-style:italic>L</span><span class=mo id=MathJax-Span-351 style=font-family:MathJax_Main;padding-left:0.278em>=</span><span class=msubsup id=MathJax-Span-352 style=padding-left:0.278em><span style=display:inline-block;position:relative;width:1.286em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-353 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.681em><span class=texatom id=MathJax-Span-354><span class=mrow id=MathJax-Span-355><span class=mi id=MathJax-Span-356 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>F<span style=display:inline-block;overflow:hidden;height:1px;width:0.075em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-357 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-358 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:2.679em;height:0px><span style=position:absolute;clip:rect(3.165em,1001.05em,4.195em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-359 style=font-family:MathJax_Math;font-style:italic>W<span style=display:inline-block;overflow:hidden;height:1px;width:0.104em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.849em;left:0.944em><span class=texatom id=MathJax-Span-360><span class=mrow id=MathJax-Span-361><span class=mi id=MathJax-Span-362 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>A</span><span class=mi id=MathJax-Span-363 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>D</span><span class=mi id=MathJax-Span-364 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic;text-rendering:optimizelegibility>V<span style=display:inline-block;overflow:hidden;height:1px;width:0.132em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-365><span style=display:inline-block;position:relative;width:2.416em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-366 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.849em;left:0.681em><span class=texatom id=MathJax-Span-367><span class=mrow id=MathJax-Span-368><span class=mi id=MathJax-Span-369 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>A</span><span class=mi id=MathJax-Span-370 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>D</span><span class=mi id=MathJax-Span-371 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic;text-rendering:optimizelegibility>V<span style=display:inline-block;overflow:hidden;height:1px;width:0.132em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-372 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-373 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.55em;height:0px><span style=position:absolute;clip:rect(3.165em,1001.05em,4.195em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-374 style=font-family:MathJax_Math;font-style:italic>W<span style=display:inline-block;overflow:hidden;height:1px;width:0.104em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.944em><span class=texatom id=MathJax-Span-375><span class=mrow id=MathJax-Span-376><span class=mi id=MathJax-Span-377 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>P<span style=display:inline-block;overflow:hidden;height:1px;width:0.077em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-378><span style=display:inline-block;position:relative;width:1.287em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-379 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.452em,1000.27em,4.143em,-1000em);top:-4.325em;left:0.681em><span class=texatom id=MathJax-Span-380><span class=mrow id=MathJax-Span-381><span class=mi id=MathJax-Span-382 style=font-size:70.7%;font-family:MathJax_Main>′</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.365em,1000.61em,4.173em,-1000em);top:-3.692em;left:0.681em><span class=texatom id=MathJax-Span-383><span class=mrow id=MathJax-Span-384><span class=mi id=MathJax-Span-385 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>P<span style=display:inline-block;overflow:hidden;height:1px;width:0.077em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-386 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-387 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.556em;height:0px><span style=position:absolute;clip:rect(3.165em,1001.05em,4.195em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-388 style=font-family:MathJax_Math;font-style:italic>W<span style=display:inline-block;overflow:hidden;height:1px;width:0.104em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.857em;left:0.944em><span class=texatom id=MathJax-Span-389><span class=mrow id=MathJax-Span-390><span class=mi id=MathJax-Span-391 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>C<span style=display:inline-block;overflow:hidden;height:1px;width:0.032em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-392><span style=display:inline-block;position:relative;width:1.293em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-393 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.452em,1000.27em,4.143em,-1000em);top:-4.325em;left:0.681em><span class=texatom id=MathJax-Span-394><span class=mrow id=MathJax-Span-395><span class=mi id=MathJax-Span-396 style=font-size:70.7%;font-family:MathJax_Main>′</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.35em,1000.61em,4.189em,-1000em);top:-3.677em;left:0.681em><span class=texatom id=MathJax-Span-397><span class=mrow id=MathJax-Span-398><span class=mi id=MathJax-Span-399 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>C<span style=display:inline-block;overflow:hidden;height:1px;width:0.032em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-400 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-401 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.011em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-402 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.681em><span class=texatom id=MathJax-Span-403><span class=mrow id=MathJax-Span-404><span class=mi id=MathJax-Span-405 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>t</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;position:absolute;width:1.278em;height:0px;clip:rect(-0.937em,1001.18em,0.388em,-1000em);top:0em;right:0em;margin-right:0em><span style=position:absolute;clip:rect(3.098em,1001.18em,4.423em,-1000em);top:-4.036em;right:0em><span class=mtd id=mjx-eqn-4><span class=mrow id=MathJax-Span-346><span class=mtext id=MathJax-Span-347 style=font-family:MathJax_Main>(4)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span></span></span></span></nobr></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L=L_{F} +W_{ADV} L_{ADV} +W_{P} L_{P}^{\prime }+W_{C} L_{C}^{\prime }+L_{t}\tag{4}\end{equation*}
</span></span></disp-formula><p>TextureGAN is an image synthesis method that can control the texture of generated images. It allows the users to place a texture patch anywhere on the sketch and at any scale to control the desired output texture. Besides, it can not only process various texture inputs and generate texture compositions that follow sketch outlines, but also achieve good results in the sketch and texture-based image synthesis.</p></div><div class=section_2 id=sec3a3><h4>3) Texture Mixer</h4><p>Yu <i>et al.</i> <a ref-type=bibr anchor=ref32 id=context_ref_32_3a3>[32]</a> proposed a new method that can control texture interpolation called Texture Mixer. The structure of Texture Mixer is shown in <a ref-type=fig anchor=fig6 class=fulltext-link>Fig. 6</a>.
<div class="figure figure-full" id=fig6><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen6-2982224-large.gif data-fig-id=fig6><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen6-2982224-small.gif data-alt="FIGURE 6. - A diagram of the texture mixer [32]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 6. </b><fig><p>A diagram of the texture mixer <a ref-type=bibr anchor=ref32 id=context_ref_32_3a3>[32]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The training objective is:<disp-formula id=deqn5 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}&amp;\hspace {-0.5pc}\min \limits _{E^{\ell },E^{g},G} \max \limits _{D^{rec},D^{itp}} \mathop {\textrm {E}}\limits _{S_{1},S_{2}\sim S} (\lambda _{1} L_{pix}^{rec} +\lambda _{2} L_{Gram}^{rec} +\lambda _{3} L_{adv}^{rec} \\&amp; \qquad~ \qquad ~\qquad \qquad \qquad \qquad {+\,\lambda _{4} L_{Gram}^{itp} +\lambda _{5} L_{adv}^{itp})} \tag{5}\end{align*}</span><span class="MathJax_Display MathJax_Processed"><span class=MathJax id=MathJax-Element-5-Frame tabindex=0><nobr><span class=math id=MathJax-Span-406><span style=display:inline-block;position:relative;width:0em;height:0px;font-size:123%><span style=position:absolute><span class=mrow id=MathJax-Span-407><span class=mtable id=MathJax-Span-408 style=min-width:25.703em><span style=display:inline-block;position:relative;width:100%;height:0px;min-width:25.703em><span style=display:inline-block;position:absolute;width:21.547em;height:0px;clip:rect(-2.134em,1021.45em,1.634em,-1000em);top:0em;left:50%;margin-left:-10.774em><span style=position:absolute;clip:rect(2.677em,1000em,5.295em,-1000em);top:-4.011em;left:0em><span style=display:inline-block;position:relative;width:0em;height:0px><span style=position:absolute;clip:rect(3.848em,1000em,4.173em,-1000em);top:-5.182em;right:0em><span class=mtd id=MathJax-Span-409><span class=mrow id=MathJax-Span-410></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.848em,1000em,4.173em,-1000em);top:-2.889em;right:0em><span class=mtd id=MathJax-Span-531><span class=mrow id=MathJax-Span-532></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(2.306em,1021.45em,6.024em,-1000em);top:-4.39em;left:0em><span style=display:inline-block;position:relative;width:21.547em;height:0px><span style=position:absolute;clip:rect(3.098em,1020.12em,5.218em,-1000em);top:-5.182em;left:0em><span class=mtd id=MathJax-Span-411><span class=mrow id=MathJax-Span-412><span class=mi id=MathJax-Span-413></span><span class=mspace id=MathJax-Span-414 style=height:0em;vertical-align:0em;margin-left:-0.6em></span><span class=munderover id=MathJax-Span-415 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:2.695em;height:0px><span style=position:absolute;clip:rect(3.179em,1001.65em,4.173em,-1000em);top:-4.011em;left:0.514em><span class=mo id=MathJax-Span-416 style=font-family:MathJax_Main>min</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.208em,1002.68em,4.411em,-1000em);top:-3.204em;left:0em><span class=texatom id=MathJax-Span-417><span class=mrow id=MathJax-Span-418><span class=msubsup id=MathJax-Span-419><span style=display:inline-block;position:relative;width:0.857em;height:0px><span style=position:absolute;clip:rect(3.367em,1000.54em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-420 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>E<span style=display:inline-block;overflow:hidden;height:1px;width:0.018em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-4.299em;left:0.596em><span class=texatom id=MathJax-Span-421><span class=mrow id=MathJax-Span-422><span class=mi id=MathJax-Span-423 style=font-size:50%;font-family:MathJax_Main>ℓ</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-424 style=font-size:70.7%;font-family:MathJax_Main>,</span><span class=msubsup id=MathJax-Span-425><span style=display:inline-block;position:relative;width:0.889em;height:0px><span style=position:absolute;clip:rect(3.367em,1000.54em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-426 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>E<span style=display:inline-block;overflow:hidden;height:1px;width:0.018em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-4.299em;left:0.596em><span class=texatom id=MathJax-Span-427><span class=mrow id=MathJax-Span-428><span class=mi id=MathJax-Span-429 style=font-size:50%;font-family:MathJax_Math;font-style:italic>g<span style=display:inline-block;overflow:hidden;height:1px;width:0.002em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-430 style=font-size:70.7%;font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-431 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>G</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=munderover id=MathJax-Span-432 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:2.753em;height:0px><span style=position:absolute;clip:rect(3.4em,1001.85em,4.184em,-1000em);top:-4.011em;left:0.446em><span class=mo id=MathJax-Span-433 style=font-family:MathJax_Main>max</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.228em,1002.75em,4.411em,-1000em);top:-3.213em;left:0em><span class=texatom id=MathJax-Span-434><span class=mrow id=MathJax-Span-435><span class=msubsup id=MathJax-Span-436><span style=display:inline-block;position:relative;width:1.314em;height:0px><span style=position:absolute;clip:rect(3.365em,1000.57em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-437 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>D</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-4.301em;left:0.585em><span class=texatom id=MathJax-Span-438><span class=mrow id=MathJax-Span-439><span class=mi id=MathJax-Span-440 style=font-size:50%;font-family:MathJax_Math;font-style:italic>r</span><span class=mi id=MathJax-Span-441 style=font-size:50%;font-family:MathJax_Math;font-style:italic>e</span><span class=mi id=MathJax-Span-442 style=font-size:50%;font-family:MathJax_Math;font-style:italic>c</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-443 style=font-size:70.7%;font-family:MathJax_Main>,</span><span class=msubsup id=MathJax-Span-444><span style=display:inline-block;position:relative;width:1.243em;height:0px><span style=position:absolute;clip:rect(3.365em,1000.57em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-445 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>D</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-4.301em;left:0.585em><span class=texatom id=MathJax-Span-446><span class=mrow id=MathJax-Span-447><span class=mi id=MathJax-Span-448 style=font-size:50%;font-family:MathJax_Math;font-style:italic>i</span><span class=mi id=MathJax-Span-449 style=font-size:50%;font-family:MathJax_Math;font-style:italic>t</span><span class=mi id=MathJax-Span-450 style=font-size:50%;font-family:MathJax_Math;font-style:italic>p</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=munderover id=MathJax-Span-451 style=padding-left:0.167em><span style=display:inline-block;position:relative;width:2.676em;height:0px><span style=position:absolute;clip:rect(3.168em,1000.65em,4.173em,-1000em);top:-4.011em;left:0.997em><span class=texatom id=MathJax-Span-452><span class=mrow id=MathJax-Span-453><span class=texatom id=MathJax-Span-454><span class=mrow id=MathJax-Span-455><span class=mtext id=MathJax-Span-456 style=font-family:MathJax_Main>E</span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.35em,1002.68em,4.411em,-1000em);top:-3.346em;left:0em><span class=texatom id=MathJax-Span-457><span class=mrow id=MathJax-Span-458><span class=msubsup id=MathJax-Span-459><span style=display:inline-block;position:relative;width:0.736em;height:0px><span style=position:absolute;clip:rect(3.35em,1000.46em,4.189em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-460 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>S<span style=display:inline-block;overflow:hidden;height:1px;width:0.023em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.905em;left:0.433em><span class=texatom id=MathJax-Span-461><span class=mrow id=MathJax-Span-462><span class=mn id=MathJax-Span-463 style=font-size:50%;font-family:MathJax_Main>1</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-464 style=font-size:70.7%;font-family:MathJax_Main>,</span><span class=msubsup id=MathJax-Span-465><span style=display:inline-block;position:relative;width:0.736em;height:0px><span style=position:absolute;clip:rect(3.35em,1000.46em,4.189em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-466 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>S<span style=display:inline-block;overflow:hidden;height:1px;width:0.023em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.905em;left:0.433em><span class=texatom id=MathJax-Span-467><span class=mrow id=MathJax-Span-468><span class=mn id=MathJax-Span-469 style=font-size:50%;font-family:MathJax_Main>2</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-470 style=font-size:70.7%;font-family:MathJax_Main>∼</span><span class=mi id=MathJax-Span-471 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>S<span style=display:inline-block;overflow:hidden;height:1px;width:0.023em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-472></span><span class=mo id=MathJax-Span-473 style=font-family:MathJax_Main>(</span><span class=msubsup id=MathJax-Span-474><span style=display:inline-block;position:relative;width:1.012em;height:0px><span style=position:absolute;clip:rect(3.154em,1000.56em,4.185em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-475 style=font-family:MathJax_Math;font-style:italic>λ</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.583em><span class=texatom id=MathJax-Span-476><span class=mrow id=MathJax-Span-477><span class=mn id=MathJax-Span-478 style=font-size:70.7%;font-family:MathJax_Main>1</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-479><span style=display:inline-block;position:relative;width:1.76em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-480 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.536em,1001.03em,4.181em,-1000em);top:-4.363em;left:0.681em><span class=texatom id=MathJax-Span-481><span class=mrow id=MathJax-Span-482><span class=mi id=MathJax-Span-483 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>r</span><span class=mi id=MathJax-Span-484 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>e</span><span class=mi id=MathJax-Span-485 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>c</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.381em,1001.08em,4.311em,-1000em);top:-3.708em;left:0.681em><span class=texatom id=MathJax-Span-486><span class=mrow id=MathJax-Span-487><span class=mi id=MathJax-Span-488 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>p</span><span class=mi id=MathJax-Span-489 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>i</span><span class=mi id=MathJax-Span-490 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>x</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-491 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-492 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.012em;height:0px><span style=position:absolute;clip:rect(3.154em,1000.56em,4.185em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-493 style=font-family:MathJax_Math;font-style:italic>λ</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.583em><span class=texatom id=MathJax-Span-494><span class=mrow id=MathJax-Span-495><span class=mn id=MathJax-Span-496 style=font-size:70.7%;font-family:MathJax_Main>2</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-497><span style=display:inline-block;position:relative;width:2.626em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-498 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.536em,1001.03em,4.181em,-1000em);top:-4.363em;left:0.681em><span class=texatom id=MathJax-Span-499><span class=mrow id=MathJax-Span-500><span class=mi id=MathJax-Span-501 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>r</span><span class=mi id=MathJax-Span-502 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>e</span><span class=mi id=MathJax-Span-503 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>c</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.35em,1001.95em,4.189em,-1000em);top:-3.677em;left:0.681em><span class=texatom id=MathJax-Span-504><span class=mrow id=MathJax-Span-505><span class=mi id=MathJax-Span-506 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>G</span><span class=mi id=MathJax-Span-507 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>r</span><span class=mi id=MathJax-Span-508 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>a</span><span class=mi id=MathJax-Span-509 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>m</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-510 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-511 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.012em;height:0px><span style=position:absolute;clip:rect(3.154em,1000.56em,4.185em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-512 style=font-family:MathJax_Math;font-style:italic>λ</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.583em><span class=texatom id=MathJax-Span-513><span class=mrow id=MathJax-Span-514><span class=mn id=MathJax-Span-515 style=font-size:70.7%;font-family:MathJax_Main>3</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-516><span style=display:inline-block;position:relative;width:1.843em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-517 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.536em,1001.03em,4.181em,-1000em);top:-4.363em;left:0.681em><span class=texatom id=MathJax-Span-518><span class=mrow id=MathJax-Span-519><span class=mi id=MathJax-Span-520 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>r</span><span class=mi id=MathJax-Span-521 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>e</span><span class=mi id=MathJax-Span-522 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>c</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.358em,1001.16em,4.181em,-1000em);top:-3.685em;left:0.681em><span class=texatom id=MathJax-Span-523><span class=mrow id=MathJax-Span-524><span class=mi id=MathJax-Span-525 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>a</span><span class=mi id=MathJax-Span-526 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>d<span style=display:inline-block;overflow:hidden;height:1px;width:0.002em></span></span><span class=mi id=MathJax-Span-527 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>v</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(2.899em,1021.45em,4.523em,-1000em);top:-2.889em;left:0em><span class=mtd id=MathJax-Span-533><span class=mrow id=MathJax-Span-534><span class=mspace id=MathJax-Span-535 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mtext id=MathJax-Span-536 style=font-family:MathJax_Main>&nbsp;</span><span class=mspace id=MathJax-Span-537 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mtext id=MathJax-Span-538 style=font-family:MathJax_Main>&nbsp;</span><span class=mspace id=MathJax-Span-539 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mspace id=MathJax-Span-540 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mspace id=MathJax-Span-541 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=mspace id=MathJax-Span-542 style=height:0em;vertical-align:0em;width:2em;display:inline-block;overflow:hidden></span><span class=texatom id=MathJax-Span-543><span class=mrow id=MathJax-Span-544><span class=mo id=MathJax-Span-545 style=font-family:MathJax_Main>+</span><span class=mspace id=MathJax-Span-546 style=height:0em;vertical-align:0em;width:0.167em;display:inline-block;overflow:hidden></span><span class=msubsup id=MathJax-Span-547><span style=display:inline-block;position:relative;width:1.012em;height:0px><span style=position:absolute;clip:rect(3.154em,1000.56em,4.185em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-548 style=font-family:MathJax_Math;font-style:italic>λ</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.583em><span class=texatom id=MathJax-Span-549><span class=mrow id=MathJax-Span-550><span class=mn id=MathJax-Span-551 style=font-size:70.7%;font-family:MathJax_Main>4</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-552><span style=display:inline-block;position:relative;width:2.626em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-553 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.381em,1000.93em,4.311em,-1000em);top:-4.492em;left:0.681em><span class=texatom id=MathJax-Span-554><span class=mrow id=MathJax-Span-555><span class=mi id=MathJax-Span-556 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>i</span><span class=mi id=MathJax-Span-557 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>t</span><span class=mi id=MathJax-Span-558 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>p</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.35em,1001.95em,4.189em,-1000em);top:-3.677em;left:0.681em><span class=texatom id=MathJax-Span-559><span class=mrow id=MathJax-Span-560><span class=mi id=MathJax-Span-561 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>G</span><span class=mi id=MathJax-Span-562 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>r</span><span class=mi id=MathJax-Span-563 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>a</span><span class=mi id=MathJax-Span-564 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>m</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-565 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-566 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.012em;height:0px><span style=position:absolute;clip:rect(3.154em,1000.56em,4.185em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-567 style=font-family:MathJax_Math;font-style:italic>λ</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.583em><span class=texatom id=MathJax-Span-568><span class=mrow id=MathJax-Span-569><span class=mn id=MathJax-Span-570 style=font-size:70.7%;font-family:MathJax_Main>5</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=msubsup id=MathJax-Span-571><span style=display:inline-block;position:relative;width:1.843em;height:0px><span style=position:absolute;clip:rect(3.165em,1000.65em,4.173em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-572 style=font-family:MathJax_Math;font-style:italic>L</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.381em,1000.93em,4.311em,-1000em);top:-4.492em;left:0.681em><span class=texatom id=MathJax-Span-573><span class=mrow id=MathJax-Span-574><span class=mi id=MathJax-Span-575 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>i</span><span class=mi id=MathJax-Span-576 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>t</span><span class=mi id=MathJax-Span-577 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>p</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.358em,1001.16em,4.181em,-1000em);top:-3.685em;left:0.681em><span class=texatom id=MathJax-Span-578><span class=mrow id=MathJax-Span-579><span class=mi id=MathJax-Span-580 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>a</span><span class=mi id=MathJax-Span-581 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>d<span style=display:inline-block;overflow:hidden;height:1px;width:0.002em></span></span><span class=mi id=MathJax-Span-582 style=font-size:70.7%;font-family:MathJax_Math;font-style:italic>v</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-583 style=font-family:MathJax_Main>)</span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:4.39em></span></span></span><span style=display:inline-block;position:absolute;width:1.278em;height:0px;clip:rect(0.209em,1001.18em,1.534em,-1000em);top:0em;right:0em;margin-right:0em><span style=position:absolute;clip:rect(3.098em,1001.18em,4.423em,-1000em);top:-2.889em;right:0em><span class=mtd id=mjx-eqn-5><span class=mrow id=MathJax-Span-529><span class=mtext id=MathJax-Span-530 style=font-family:MathJax_Main>(5)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span></span></span></span></nobr></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}&amp;\hspace {-0.5pc}\min \limits _{E^{\ell },E^{g},G} \max \limits _{D^{rec},D^{itp}} \mathop {\textrm {E}}\limits _{S_{1},S_{2}\sim S} (\lambda _{1} L_{pix}^{rec} +\lambda _{2} L_{Gram}^{rec} +\lambda _{3} L_{adv}^{rec} \\&amp; \qquad~ \qquad ~\qquad \qquad \qquad \qquad {+\,\lambda _{4} L_{Gram}^{itp} +\lambda _{5} L_{adv}^{itp})} \tag{5}\end{align*}
</span></span></disp-formula><p>The method utilizes deep learning and GAN to realize controllable interpolation of textures, which combines two different types of texture patterns and makes the transition natural. It proposes a neural network trained with a reconstruction task and a generation task to project the texture of the sample onto a latent space and project linear interpolation onto the image domain to ensure the quality of the intuitive control and realistic generated results. Furthermore, it is superior to many baseline methods and has a good performance in texture synthesis in the dimensions of controllability, smoothness, and realism.</p></div><div class=section_2 id=sec3a4><h4>4) Other Methods</h4><p>Li and Wand <a ref-type=bibr anchor=ref33 id=context_ref_33_3a4>[33]</a> proposed an efficient texture synthesis method called Markovian Generative Adversarial Networks (MGANs). It can not only directly decode brown noise to realistic texture but also decode the photo to the painting, which improves the quality of texture synthesis. Jetchev <i>et al.</i> <a ref-type=bibr anchor=ref34 id=context_ref_34_3a4>[34]</a> proposed an architecture called spatial GAN (SGAN) which is well-suited for texture synthesis. It is a method that can synthesize texture images with high quality and can fuse multiple different source images in complex textures.<p>The texture synthesis based on GANs adopts the method of interpolation can produce realistic details of texture and realize the natural transition of texture synthesis. Interpolation and extrapolation are two approaches to enforce constraints for GANs. The incorporation of constraints is built into the training of the GAN while the constraints are enforced after each step through projection on the space of constraints for extrapolation <a ref-type=bibr anchor=ref35 id=context_ref_35_3a4>[35]</a>. However, sometimes the texture synthesis model is difficult to converge during training and it can suffer from “mode dropping”.</p></div></div><div class=section_2 id=sec3b><h3>B. Image Super-Resolution</h3><p>The image generation model is designed to explore how to generate a desired image, while producing high-quality large images has always been a challenging task. The ability to produce high-quality and high-resolution images is an important advantage of GANs, and significant progress has been made in generating high-quality and visually realistic images. A series of models based on GANs are emerging in the purpose of producing higher-resolution images.<div class=section_2 id=sec3b1><h4>1) ProGAN</h4><p>Karras <i>et al.</i> <a ref-type=bibr anchor=ref36 id=context_ref_36_3b1>[36]</a> proposed an image generation method called ProGAN. The structure of ProGAN is shown in <a ref-type=fig anchor=fig7 class=fulltext-link>Fig. 7</a>.
<div class="figure figure-full" id=fig7><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen7-2982224-large.gif data-fig-id=fig7><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen7-2982224-small.gif data-alt="FIGURE 7. - The structure of ProGAN [36]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 7. </b><fig><p>The structure of ProGAN <a ref-type=bibr anchor=ref36 id=context_ref_36_3b1>[36]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The key idea of this approach is to gradually increase the generator and discriminator, which starts from a low resolution and adds new layers as the training progresses to make the model increase fine details. It is a method which can not only speed the training up but also greatly stabilize it. Compared with the earlier works on GANs, the quality of the results using this method is generally high, and the training is stable in high resolution. However, there are some shortcomings in this method. For example, semantic sensitivity and understanding depend on the constraints of the dataset.</p></div><div class=section_2 id=sec3b2><h4>2) Progressive Face Super-Resolution</h4><p>Kim <i>et al.</i> <a ref-type=bibr anchor=ref37 id=context_ref_37_3b2>[37]</a> proposed a novel face super-resolution (SR) method which can generate photo-realistic face images with fully retained facial details. The network architecture is shown in <a ref-type=fig anchor=fig8 class=fulltext-link>Fig. 8</a>.
<div class="figure figure-full" id=fig8><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen8-2982224-large.gif data-fig-id=fig8><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen8-2982224-small.gif data-alt="FIGURE 8. - The network architecture of [37]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 8. </b><fig><p>The network architecture of <a ref-type=bibr anchor=ref37 id=context_ref_37_3b2>[37]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss term is shown as:<disp-formula id=deqn6-7 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} L_{Ours}=&amp;\alpha L_{pixel} +\beta L_{feat} +\gamma L_{WANG} \tag{6}\\ L_{Ours}=&amp;\alpha L_{pixel} +\beta L_{feat} +\gamma L_{WANG} +\lambda L_{heatmap} \\&amp;+\,\eta L_{attention}\tag{7}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-6-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} L_{Ours}=&amp;\alpha L_{pixel} +\beta L_{feat} +\gamma L_{WANG} \tag{6}\\ L_{Ours}=&amp;\alpha L_{pixel} +\beta L_{feat} +\gamma L_{WANG} +\lambda L_{heatmap} \\&amp;+\,\eta L_{attention}\tag{7}\end{align*}
</span></span></disp-formula><p>The authors use a progressive training approach that allows stable training by dividing the network into successive steps, each step producing an output of progressively higher resolution. A novel facial attention loss has also been proposed and applied at each step to focus on restoring facial attributes in more detail by multiplying pixel differences and heatmap values. They also proposed a compressed version of the face alignment network (FAN) to extract suitable landmark heatmaps for face super-resolution (SR), and the overall training time can also be reduced. Furthermore, it can learn the restoration of facial details and generate super-resolution facial images that are similar to real ones. The results are superior to the earlier methods in terms of qualitative and quantitative measurements, especially in perceptual quality.</p></div><div class=section_2 id=sec3b3><h4>3) BigGANs</h4><p>Brock <i>et al.</i> <a ref-type=bibr anchor=ref38 id=context_ref_38_3b3>[38]</a> proposed models called BigGANs, which realized the work of generating high-resolution and diverse images from complex datasets. A typical network architecture of BigGANs is shown in <a ref-type=fig anchor=fig9 class=fulltext-link>Fig. 9</a>.
<div class="figure figure-full" id=fig9><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen9-2982224-large.gif data-fig-id=fig9><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen9-2982224-small.gif data-alt="FIGURE 9. - A typical network architecture of BigGANs [38]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 9. </b><fig><p>A typical network architecture of BigGANs <a ref-type=bibr anchor=ref38 id=context_ref_38_3b3>[38]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>This method achieves the goal of generating high-resolution and diverse samples from the complex dataset ImageNet successfully. It is the largest scale of Generative Adversarial Networks that have been trained so far and can generate images of unprecedented quality. It is far superior to the earlier methods in terms of the realism of the generated image. The authors applied orthogonal regularization to the generator to handle the specific instability of such scale and truncated the latent space to control the fidelity and variety of generated images.</p></div><div class=section_2 id=sec3b4><h4>4) StyleGAN</h4><p>Karras <i>et al.</i> <a ref-type=bibr anchor=ref39 id=context_ref_39_3b4>[39]</a> proposed an alternative generator architecture called StyleGAN. The network architecture of StyleGAN is shown in <a ref-type=fig anchor=fig10 class=fulltext-link>Fig. 10</a>.
<div class="figure figure-full" id=fig10><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen10-2982224-large.gif data-fig-id=fig10><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen10-2982224-small.gif data-alt="FIGURE 10. - The network architecture of StyleGAN [39]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 10. </b><fig><p>The network architecture of StyleGAN <a ref-type=bibr anchor=ref39 id=context_ref_39_3b4>[39]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The authors redesigned the generator architecture which can adjust its image style based on the latent code in each convolutional layer. It is able to control the entire image synthesis process which starts with very low resolution and generates high-resolution artificial images step by step. Besides, it controls the visual features by modifying the input of each level in the network separately, from coarse features to fine details. The breakthrough of StyleGAN is that it not only produces high-quality and realistic images but also provides better control and understanding of the generated images. The method implements automatic learning, unsupervised high-level attribute separation, and stochastic variation of generated images, which enables intuitive, scale-specific control synthesis of the composition. The method is superior to the traditional GAN generator architecture and can generate a high-resolution image that looks more realistic.</p></div><div class=section_2 id=sec3b5><h4>5) Other Methods</h4><p>Ledig <i>et al.</i> <a ref-type=bibr anchor=ref40 id=context_ref_40_3b5>[40]</a> proposed a generative adversarial network for image super-resolution (SR) called SRGAN, and can significantly improve the perceptual quality. Wang <i>et al.</i> <a ref-type=bibr anchor=ref41 id=context_ref_41_3b5>[41]</a> improved SRGAN to derive an Enhanced SRGAN (ESRGAN) which not only improved the problem of artifacts in SRGAN and the visual quality of generated images but also obtained more realistic and natural textures. Wang <i>et al.</i> <a ref-type=bibr anchor=ref42 id=context_ref_42_3b5>[42]</a> proposed a method to recover natural and realistic texture called SFTGAN, which is equipped with a novel Spatial Feature Transform (SFT) layer and can generate more realistic and visually pleasing textures.<p>The image super-resolution method which gets the best results trains generators and discriminators from low-resolution images, and adds a higher-resolution network layer each time to generate artificial images step by step. It can generate high-resolution and diverse images with high-quality. However, the training time is long and more GPUs are required.</p></div></div><div class=section_2 id=sec3c><h3>C. Image Inpainting</h3><p>In the past few years, deep learning technology has made significant progress in the image inpainting. Image inpainting refers to the technique of restoring and reconstructing images based on background information. The generated images are expected to look very natural and difficult to distinguish from the ground truth. High-quality image inpainting not only requires the semantics of the generated content to be reasonable but also requires that the texture of generated image clear and realistic enough. Recently, image inpainting methods based on deep learning have achieved promising results, especially based on GANs.<div class=section_2 id=sec3c1><h4>1) Deepfillv1</h4><p>Yu <i>et al.</i> <a ref-type=bibr anchor=ref43 id=context_ref_43_3c1>[43]</a> proposed a deep generative model-based image inpainting approach called Deepfillv1. The framework of Deepfillv1 is summarized in <a ref-type=fig anchor=fig11 class=fulltext-link>Fig. 11</a>.
<div class="figure figure-full" id=fig11><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen11-2982224-large.gif data-fig-id=fig11><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen11-2982224-small.gif data-alt="FIGURE 11. - Overview of the improved generative inpainting framework [43]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 11. </b><fig><p>Overview of the improved generative inpainting framework <a ref-type=bibr anchor=ref43 id=context_ref_43_3c1>[43]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>Deepfillv1 combines the solution of deep learning algorithms concerning the advantages of traditional algorithms. It further improves the generation network and can automatically repair a picture with multiple holes or large holes, which can produce images of higher quality than earlier methods. The method can synthesize novel image structures and makes use of surrounding image features to make better predictions. The authors utilized a feedforward and fully convolutional neural network to process images with multiple holes during the test time. This method is a coarse-to-fine generative image inpainting framework with a novel contextual attention module that can improve the image inpainting results by learning the feature representations for explicit matching and attending to relevant background patches.</p></div><div class=section_2 id=sec3c2><h4>2) ExGANs</h4><p>Dolhansky <i>et al.</i> <a ref-type=bibr anchor=ref44 id=context_ref_44_3c2>[44]</a> proposed a novel in-painting approach called Exemplar GANs (ExGANs). The architecture of ExGANs is shown in <a ref-type=fig anchor=fig12 class=fulltext-link>Fig 12</a>.
<div class="figure figure-full" id=fig12><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen12-2982224-large.gif data-fig-id=fig12><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen12-2982224-small.gif data-alt="FIGURE 12. - General architecture of an exemplar GAN [44]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 12. </b><fig><p>General architecture of an exemplar GAN <a ref-type=bibr anchor=ref44 id=context_ref_44_3c2>[44]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The learning objective of reference image inpainting is defined as:<disp-formula id=deqn8 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} \min \limits _{G}\max \limits _{D} V(D,G)=&amp;\textrm {E}_{x_{i},r_{i}\sim Pdata(x,r)} [\textrm {log}D(x_{i},r_{i})] \\&amp;+\,\textrm {E}_{r_{i}\sim p_{c},G(\cdot)\sim Pz} \textrm {[log1}-D(G(z_{i},r_{i}))] \\&amp;+\,\left \|{ G }\right.(z_{i},r_{i})-\left.{ {x_{i}} }\right \|_{1}\tag{8}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-7-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} \min \limits _{G}\max \limits _{D} V(D,G)=&amp;\textrm {E}_{x_{i},r_{i}\sim Pdata(x,r)} [\textrm {log}D(x_{i},r_{i})] \\&amp;+\,\textrm {E}_{r_{i}\sim p_{c},G(\cdot)\sim Pz} \textrm {[log1}-D(G(z_{i},r_{i}))] \\&amp;+\,\left \|{ G }\right.(z_{i},r_{i})-\left.{ {x_{i}} }\right \|_{1}\tag{8}\end{align*}
</span></span></disp-formula><p>The adversarial objective of code inpainting is defined as:<disp-formula id=deqn9 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}&amp;\hspace {-2pc}\min \limits _{G} \max \limits _{D} V(D,G) \\=&amp;\textrm {E}_{x_{i},c_{i}\sim Pdata(x,c)} [\textrm {log}D(x_{i},c_{i})] \\&amp;+\,\textrm {E}_{c_{i}\sim p_{c},G(\cdot)\sim Pz}[\textrm {log1}\!-\!D(G(z_{i},c_{i}))] \\&amp;+\,\left \|{ G }\right.(z_{i},c_{i})\!-\!\left.{ {x_{i}} }\right \|_{1} \!+\!\left \|{ C }\right.(G(z_{i},c_{i})\!-\!\left.{ {c_{i}} }\right \|_{2}\tag{9}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-8-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}&amp;\hspace {-2pc}\min \limits _{G} \max \limits _{D} V(D,G) \\=&amp;\textrm {E}_{x_{i},c_{i}\sim Pdata(x,c)} [\textrm {log}D(x_{i},c_{i})] \\&amp;+\,\textrm {E}_{c_{i}\sim p_{c},G(\cdot)\sim Pz}[\textrm {log1}\!-\!D(G(z_{i},c_{i}))] \\&amp;+\,\left \|{ G }\right.(z_{i},c_{i})\!-\!\left.{ {x_{i}} }\right \|_{1} \!+\!\left \|{ C }\right.(G(z_{i},c_{i})\!-\!\left.{ {c_{i}} }\right \|_{2}\tag{9}\end{align*}
</span></span></disp-formula><p>The authors use exemplar information as a reference image of the region to inpaint a person with closed eyes in a natural picture which can produce high-quality and personalized inpainting results. It can also describe the object with a perceptual code in the task of a closed-to-open eye to produce a photo-realistic and personalized image in terms of perception and semantics. ExGANs are a type of conditional GAN that can increase the descriptive power by inserting at multiple points within the adversarial network with the extra information. It is a useful method for image generation or inpainting that use reference images or perceptual codes as identifying information which has superior perceptual results.</p></div><div class=section_2 id=sec3c3><h4>3) Deepfillv2</h4><p>Yu <i>et al.</i> <a ref-type=bibr anchor=ref45 id=context_ref_45_3c3>[45]</a> proposed a novel image inpainting system based on deep learning which uses free-form masks and inputs to complete images called Deepfillv2. The architecture of Deepfillv2 is shown in <a ref-type=fig anchor=fig13 class=fulltext-link>Fig. 13</a>.
<div class="figure figure-full" id=fig13><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen13-2982224-large.gif data-fig-id=fig13><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen13-2982224-small.gif data-alt="FIGURE 13. - The architecture of Deepfillv2 [45]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 13. </b><fig><p>The architecture of Deepfillv2 <a ref-type=bibr anchor=ref45 id=context_ref_45_3c3>[45]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function is:<disp-formula id=deqn10-11 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} L_{D^{sn}}=&amp;\textrm {E}_{X\sim Pdata(X)} [ReLU(1-D^{sn}(x))] \\&amp;+\,\textrm {E}_{Z\sim Pz(Z)} [ReLU(1+D^{sn}(G(z)))] \tag{10}\\ L_{G}=&amp;-\textrm {E}_{Z\sim Pz(Z)} [D^{sn}(G(z))]\tag{11}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-9-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} L_{D^{sn}}=&amp;\textrm {E}_{X\sim Pdata(X)} [ReLU(1-D^{sn}(x))] \\&amp;+\,\textrm {E}_{Z\sim Pz(Z)} [ReLU(1+D^{sn}(G(z)))] \tag{10}\\ L_{G}=&amp;-\textrm {E}_{Z\sim Pz(Z)} [D^{sn}(G(z))]\tag{11}\end{align*}
</span></span></disp-formula><p>This method is based on gated convolutions and can handle images with free-form masks anywhere or any shapes. The authors proposed a GAN loss called SN-PatchGAN which makes the training fast and stable. It is superior to the previous methods and can produce more flexible results with higher-quality. Furthermore, it can be used to remove distracting objects, clear watermarks, edit faces and fill in missing regions. Moreover, the image inpainting system which is based on an end-to-end generative network is useful to improve inpainting results with user guidance input.</p></div><div class=section_2 id=sec3c4><h4>4) EdgeConnect</h4><p>Nazeri <i>et al.</i> <a ref-type=bibr anchor=ref46 id=context_ref_46_3c4>[46]</a> proposed a two-stage adversarial model called EdgeConnect, a novel approach for image inpainting. The structure of the EdgeConnect is shown in <a ref-type=fig anchor=fig14 class=fulltext-link>Fig. 14</a>.
<div class="figure figure-full" id=fig14><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen14-2982224-large.gif data-fig-id=fig14><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen14-2982224-small.gif data-alt="FIGURE 14. - The structure of EdgeConnect [46]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 14. </b><fig><p>The structure of EdgeConnect <a ref-type=bibr anchor=ref46 id=context_ref_46_3c4>[46]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The training objective of the edge generator network is:<disp-formula id=deqn12 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} \min \limits _{G_{1}} \max \limits _{D_{1}} L_{G_{1}} \!=\! \mathop {\textrm {min}}\limits _{G_{1}} (\lambda _{adv,1} \mathop {\textrm {max}}\limits _{D_{1}} (L_{adv,1})\!+\!\lambda _{FM} L_{FM})\tag{12}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-10-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} \min \limits _{G_{1}} \max \limits _{D_{1}} L_{G_{1}} \!=\! \mathop {\textrm {min}}\limits _{G_{1}} (\lambda _{adv,1} \mathop {\textrm {max}}\limits _{D_{1}} (L_{adv,1})\!+\!\lambda _{FM} L_{FM})\tag{12}\end{equation*}
</span></span></disp-formula><p>The loss function of the image completion network is:<disp-formula id=deqn13 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{G_{2}} =\lambda _{\ell _{1}} L_{\ell _{1}} +\lambda _{adv,2} L_{adv,2} +\lambda _{p} L_{perc} +\lambda _{s} L_{style}\tag{13}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-11-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{G_{2}} =\lambda _{\ell _{1}} L_{\ell _{1}} +\lambda _{adv,2} L_{adv,2} +\lambda _{p} L_{perc} +\lambda _{s} L_{style}\tag{13}\end{equation*}
</span></span></disp-formula><p>EdgeConnect is an image completion network that uses hallucinated edges as a priori to fill in the missing regions. It consists of an edge generator and an image completion network which can reproduce filled regions exhibiting fine details. The edge generator is used to get edges of the missing region of the image which can be regular or irregular, and the image completion network is used to fill in the missing regions. The authors proposed a new image painting method based on deep learning that can be used for image inpainting task and reconstruct reasonable structures of the missing regions. Furthermore, it does a good job of dealing with images that have multiple or irregular shapes of missing regions. It can be used for removing unwanted objects from the images or as an interactive image editing tool and get a good result in terms of quantitative and qualitative measurements. However, the current problem is that the edge generating model sometimes fails to depict the edges accurately when a large part of the image is missing or in highly textured regions.</p></div><div class=section_2 id=sec3c5><h4>5) PEN-Net</h4><p>Zeng <i>et al.</i> <a ref-type=bibr anchor=ref47 id=context_ref_47_3c5>[47]</a> proposed an image inpainting method based on deep generative models called Pyramid-context ENcoder Network (PEN-Net). The structure of the PEN-Net is shown in <a ref-type=fig anchor=fig15 class=fulltext-link>Fig. 15</a>.
<div class="figure figure-full" id=fig15><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen15-2982224-large.gif data-fig-id=fig15><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen15-2982224-small.gif data-alt="FIGURE 15. - The structure of PEN-Net [47]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 15. </b><fig><p>The structure of PEN-Net <a ref-type=bibr anchor=ref47 id=context_ref_47_3c5>[47]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The adversarial loss for the discriminator is denoted as:<disp-formula id=deqn14 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}L_{D} =&amp;\textrm {E}_{X\sim Pdata(X)} [\textrm {max}(0,1-D(x))] \\&amp; \qquad \qquad \qquad \qquad {+\,\textrm {E}_{Z\sim Pz} [\textrm {max}(0,1+D(z))] } \tag{14}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-12-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}L_{D} =&amp;\textrm {E}_{X\sim Pdata(X)} [\textrm {max}(0,1-D(x))] \\&amp; \qquad \qquad \qquad \qquad {+\,\textrm {E}_{Z\sim Pz} [\textrm {max}(0,1+D(z))] } \tag{14}\end{align*}
</span></span></disp-formula><p>The adversarial loss for the generator is denoted as:<disp-formula id=deqn15 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{G} =-\textrm {E}_{Z\sim Pz} [D(z)]\tag{15}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-13-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{G} =-\textrm {E}_{Z\sim Pz} [D(z)]\tag{15}\end{equation*}
</span></span></disp-formula><p>The PEN-Net is a method which is proposed for high-quality image inpainting and is used to fill in the missing regions of the image with plausible content. The authors put forward the idea of a pyramid-context encoder which uses a high-level semantic feature map as a guide and transfer the learned attention to the previous low-level feature map so that the network can learn the region affinity progressively. It can be used to fill in the regions in a damaged image and get a result both visually and semantically plausible. Moreover, the main idea of the method is to encode the contextual semantics learned from the full resolution input, and restore an image by decoding the semantic features back. Both visual and semantic coherence of the generated content can be ensured with the attention transferred from deep to shallow in a pyramid fashion. At the same time, the authors proposed a new loss function to make the training converge fast and generate more realistic results. The network is superior to the previous method and can generate semantically-reasonable and visually-realistic images.</p></div><div class=section_2 id=sec3c6><h4>6) Other Methods</h4><p>Yang <i>et al.</i> <a ref-type=bibr anchor=ref48 id=context_ref_48_3c6>[48]</a> proposed a multi-scale neural patch synthesis method based on deep learning which uses image content and texture constraints to optimize the task of image inpainting. The method can not only restore images with semantically plausible contents but also preserve the high-frequency details. Yeh <i>et al.</i> <a ref-type=bibr anchor=ref49 id=context_ref_49_3c6>[49]</a> proposed a new approach for the semantic image inpainting, which can achieve pixel-level photorealism and generate satisfactory results. Li <i>et al.</i> <a ref-type=bibr anchor=ref50 id=context_ref_50_3c6>[50]</a> proposed an effective face completion method based on a deep generative model, and it can restore images with a large area of missing pixels and achieve a realistic face completion result.<p>Image inpainting methods based on GANs nowadays can achieve more reasonable and semantically consistent results than traditional methods. Currently, some methods use gated convolutions to restore images with free-form masks, which can restore images with multiple holes or fill in missing areas with irregular shapes. However, the quality of the image inpainting is sensitive to the position and size of the masks.</p></div></div><div class=section_2 id=sec3d><h3>D. Face Image Synthesis</h3><p>In recent years, face image synthesis is a hot topic in photo processing because of the heavy use of pictures on social media. Due to the performance improvement of GANs, facial image processing has made great progress. A series of methods have emerged to improve the quality of face image generation.<div class=section_2 id=sec3d1><h4>1) Elegant</h4><p>Xiao <i>et al.</i> <a ref-type=bibr anchor=ref51 id=context_ref_51_3d1>[51]</a> proposed a model for transferring multiple face attributes called ELEGANT. The framework of ELEGANT is shown in <a ref-type=fig anchor=fig16 class=fulltext-link>Fig. 16</a>.
<div class="figure figure-full" id=fig16><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen16-2982224-large.gif data-fig-id=fig16><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen16-2982224-small.gif data-alt="FIGURE 16. - The framework of ELEGANT [51]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 16. </b><fig><p>The framework of ELEGANT <a ref-type=bibr anchor=ref51 id=context_ref_51_3d1>[51]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss of the discriminator is:<disp-formula id=deqn16 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{D} =L_{D_{1}} +L_{D_{2}}\tag{16}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-14-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{D} =L_{D_{1}} +L_{D_{2}}\tag{16}\end{equation*}
</span></span></disp-formula><p>The loss of the generator is:<disp-formula id=deqn17 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{G} =L_{reconstruction} +L_{adv}\tag{17}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-15-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{G} =L_{reconstruction} +L_{adv}\tag{17}\end{equation*}
</span></span></disp-formula><p>ELEGANT is an effective method for face attributes transferring. It receives two images of opposite attributes as inputs and can produce high-quality images with finer details. Furthermore, it exchanges a certain part of the encodings to transfer the same type of attributes from one image to another. This method can manipulate several attributes simultaneously by encoding different attributes into disentangled parts in the latent space. The model is based on a U-Net <a ref-type=bibr anchor=ref52 id=context_ref_52_3d1>[52]</a> structure and is trained with multi-scale discriminators which can help to improve the quality of the generated images. Besides, it can generate higher resolution images with the help of residual learning to facilitate training.</p></div><div class=section_2 id=sec3d2><h4>2) STGAN</h4><p>Liu <i>et al.</i> <a ref-type=bibr anchor=ref53 id=context_ref_53_3d2>[53]</a> proposed an arbitrary facial attribute editing model called STGAN, which achieves high-quality editing results. The structure of STGAN is shown in <a ref-type=fig anchor=fig17 class=fulltext-link>Fig. 17</a>.
<div class="figure figure-full" id=fig17><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen17-2982224-large.gif data-fig-id=fig17><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen17-2982224-small.gif data-alt="FIGURE 17. - The structure of STGAN [53]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 17. </b><fig><p>The structure of STGAN <a ref-type=bibr anchor=ref53 id=context_ref_53_3d2>[53]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function of discriminator D is formulated as:<disp-formula id=deqn18 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} \min \limits _{D} L_{D} =-L_{D_{adv}} +\lambda _{L_{1}} L_{D_{att}}\tag{18}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-16-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} \min \limits _{D} L_{D} =-L_{D_{adv}} +\lambda _{L_{1}} L_{D_{att}}\tag{18}\end{equation*}
</span></span></disp-formula><p>The objective function of generator G is formulated as:<disp-formula id=deqn19 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} \min \limits _{G} L_{G} =-L_{G_{adv}} +\lambda _{2} L_{G_{att}} +\lambda _{3} L_{rec}\tag{19}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-17-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} \min \limits _{G} L_{G} =-L_{G_{adv}} +\lambda _{2} L_{G_{att}} +\lambda _{3} L_{rec}\tag{19}\end{equation*}
</span></span></disp-formula><p>This method solves the fine-grained control on the label of the face attribute and realizes multi-attribute transformation. The model takes a difference attribute vector as input to change the related attributes instead of all target attributes in specific editing tasks. STGAN is a high-precision attribute editing model based on AttGAN <a ref-type=bibr anchor=ref54 id=context_ref_54_3d2>[54]</a> and StarGAN <a ref-type=bibr anchor=ref55 id=context_ref_55_3d2>[55]</a>. It helps to improve the generated image quality and to get a clear editing result. Besides, it can not only improve the ability of face attribute manipulation but also can be used for season translation. The authors proposed selective transfer units (STUs) to enhance attribute editing which can improve the accuracy of attribute manipulation and improve perception quality. STGAN can improve the quality of generated images and realize flexible translation of attributes by focusing on the editing attributes to be changed.</p></div><div class=section_2 id=sec3d3><h4>3) SCGAN</h4><p>Jiang <i>et al.</i> <a ref-type=bibr anchor=ref56 id=context_ref_56_3d3>[56]</a> proposed a novel image generation model called Spatially Constrained Generative Adversarial Network (SCGAN). The framework of SCGAN is shown in <a ref-type=fig anchor=fig18 class=fulltext-link>Fig. 18</a>.
<div class="figure figure-full" id=fig18><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen18-2982224-large.gif data-fig-id=fig18><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen18-2982224-small.gif data-alt="FIGURE 18. - The framework of SCGAN [56]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 18. </b><fig><p>The framework of SCGAN <a ref-type=bibr anchor=ref56 id=context_ref_56_3d3>[56]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function of SCGAN is represented as:<disp-formula id=deqn20-22 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} L_{S}=&amp;L_{seg}^{real} \tag{20}\\ L_{D}=&amp;-L_{adv} +\lambda _{cls} L_{cls}^{real} \tag{21}\\ L_{G}=&amp;L_{adv} +\lambda _{cls} L_{cla}^{fake} +\lambda _{seg} L_{seg}^{fake}\tag{22}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-18-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} L_{S}=&amp;L_{seg}^{real} \tag{20}\\ L_{D}=&amp;-L_{adv} +\lambda _{cls} L_{cls}^{real} \tag{21}\\ L_{G}=&amp;L_{adv} +\lambda _{cls} L_{cla}^{fake} +\lambda _{seg} L_{seg}^{fake}\tag{22}\end{align*}
</span></span></disp-formula><p>This method can generate images with clear edge details and can preserve spatial information. It makes the spatial constraints feasible as additional controllable signals which are decoupled from the latent vector. Moreover, the authors designed a generator network that takes a semantic segmentation, a latent vector and an attribute-level label as inputs to enhance the spatial controllability step by step. Meanwhile, the authors proposed a segmentor network to impose spatial constraints on the generator which can accelerate and stabilize the model convergence. SCGAN is an effective method that can control the spatial contents and can generate high-quality images. It can not only solve the foreground-background mismatch problem but it is also easy and fast to train. Besides, SCGAN is very effective at controlling spatial contents which can specify attributes and help to improve general visual quality and get quantitative results.</p></div><div class=section_2 id=sec3d4><h4>4) Example-Guided Image Synthesis</h4><p>Wang <i>et al.</i> <a ref-type=bibr anchor=ref57 id=context_ref_57_3d4>[57]</a> proposed an example-guided image synthesis solution by using a semantic label map and an exemplary image, and its framework is summarized as shown in <a ref-type=fig anchor=fig19 class=fulltext-link>Fig. 19</a>.
<div class="figure figure-full" id=fig19><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen19-2982224-large.gif data-fig-id=fig19><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen19-2982224-small.gif data-alt="FIGURE 19. - Overview of the framework [57]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 19. </b><fig><p>Overview of the framework <a ref-type=bibr anchor=ref57 id=context_ref_57_3d4>[57]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function is formulated as:<disp-formula id=deqn23 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} G^{\ast }=\arg \min \limits _{G} \max \limits _{D_{R},D_{SC}} L(G,D_{R},D_{SC})\tag{23}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-19-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} G^{\ast }=\arg \min \limits _{G} \max \limits _{D_{R},D_{SC}} L(G,D_{R},D_{SC})\tag{23}\end{equation*}
</span></span></disp-formula><p>This method is based on conditional generative adversarial networks aim to synthesize images from semantic label maps and using an exemplary image to indicate facial expression or full body poses. The authors proposed a novel style consistency discriminator and an adaptive semantic consistency loss to make sure that the synthesized image is consistent in style with the exemplar. Furthermore, a training data sampling strategy is also used to synthesize style-consistent results. It is an effective method that can be used on the face or street view synthesis tasks which can produce qualitative and quantitative results. Moreover, it can generate realistic and style-consistent images with the help of style consistency discriminator.</p></div><div class=section_2 id=sec3d5><h4>5) SGGAN</h4><p>Jiang <i>et al.</i> <a ref-type=bibr anchor=ref58 id=context_ref_58_3d5>[58]</a> proposed a novel multi-domain face image translation method called Segmentation Guided Generative Adversarial Networks (SGGAN). The framework of SGGAN is shown in <a ref-type=fig anchor=fig20 class=fulltext-link>Fig. 20</a>.
<div class="figure figure-full" id=fig20><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen20-2982224-large.gif data-fig-id=fig20><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen20-2982224-small.gif data-alt="FIGURE 20. - Illustration of SGGAN [58]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 20. </b><fig><p>Illustration of SGGAN <a ref-type=bibr anchor=ref58 id=context_ref_58_3d5>[58]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function of the SGGAN network is summarized as:<disp-formula id=deqn24-26 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} L_{S}=&amp;L_{seg}^{real} \tag{24}\\[-1pt] L_{D}=&amp;-L_{adv} +\lambda _{1} L_{cls}^{real} \tag{25}\\[-1pt] L_{G}=&amp;L_{adv} +\lambda _{1} L_{cls}^{fake} +\lambda _{2} L_{seg}^{fake} +\lambda _{3} L_{rec}\tag{26}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-20-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} L_{S}=&amp;L_{seg}^{real} \tag{24}\\[-1pt] L_{D}=&amp;-L_{adv} +\lambda _{1} L_{cls}^{real} \tag{25}\\[-1pt] L_{G}=&amp;L_{adv} +\lambda _{1} L_{cls}^{fake} +\lambda _{2} L_{seg}^{fake} +\lambda _{3} L_{rec}\tag{26}\end{align*}
</span></span></disp-formula><p>The method is based on a deep generative model that pays attention to higher-level and instance-specific information and can generate realistic images of high quality. It has spatial controllability in the image translation process by utilizing semantic segmentation to improve the performance of image generation and provides spatial mapping. The authors proposed a segmentor network to provide the generated images with semantic information. Besides, it can improve the quality of image generation with the ability of spatial modification. The method uses the segmentation information to guide the generation of images which can make the details clear. SGGAN can be used for face image translation by providing strong regulations during the training process.</p></div><div class=section_2 id=sec3d6><h4>6) MaskGAN</h4><p>Lee <i>et al.</i> <a ref-type=bibr anchor=ref59 id=context_ref_59_3d6>[59]</a> proposed a geometry-oriented face manipulation framework called MaskGAN. The pipeline of MaskGAN is shown in <a ref-type=fig anchor=fig21 class=fulltext-link>Fig. 21</a>.
<div class="figure figure-full" id=fig21><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen21-2982224-large.gif data-fig-id=fig21><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen21-2982224-small.gif data-alt="FIGURE 21. - The pipeline of MaskGAN [59]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 21. </b><fig><p>The pipeline of MaskGAN <a ref-type=bibr anchor=ref59 id=context_ref_59_3d6>[59]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective loss function is:<disp-formula id=deqn27 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}L_{G_{A},G_{B}} =&amp;L_{adv} (G,D_{1,2})+\lambda _{feat} L_{feat} (G,D_{1,2}) \\&amp;~ \qquad \qquad \qquad \qquad {+\,\lambda _{percept} L_{percept} (G)\,\,} \tag{27}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-21-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}L_{G_{A},G_{B}} =&amp;L_{adv} (G,D_{1,2})+\lambda _{feat} L_{feat} (G,D_{1,2}) \\&amp;~ \qquad \qquad \qquad \qquad {+\,\lambda _{percept} L_{percept} (G)\,\,} \tag{27}\end{align*}
</span></span></disp-formula><p>The method overcomes the shortcomings of operating on a predefined set of face attributes. It makes the users manipulate images with more freedom by using semantic masks as an intermediate representation, which enables diverse and interactive face manipulation. MaskGAN can achieve diverse generation results by using dense mapping networks to learn style mapping between the free-form user modified mask and the target image. Furthermore, it makes the framework more robust to manipulate by using editing behavior simulated training which models users editing behavior on the source mask. MaskGAN can be used to manipulate face image flexibly and preserve the fidelity.</p></div><div class=section_2 id=sec3d7><h4>7) Other Methods</h4><p>Lin <i>et al.</i> <a ref-type=bibr anchor=ref60 id=context_ref_60_3d7>[60]</a> proposed an unpaired image-to-image translation method called Domain-supervised GAN (DosGAN), and it uses domain information as explicit supervision and achieves conditional translation with face images in CelebA. Mokady <i>et al.</i> <a ref-type=bibr anchor=ref61 id=context_ref_61_3d7>[61]</a> proposed a novel mask-based method which uses the masks to reconstruct the face images and enables high quality and various content translations. Yin <i>et al.</i> <a ref-type=bibr anchor=ref62 id=context_ref_62_3d7>[62]</a> proposed an instance-level facial attribute transfer method which uses the geometry-aware flow as a representation for transferring the images with instance-level facial attributes.<p>The Face image synthesis method uses encoder-decoder and generative adversarial networks to solve the problem of arbitrary attribute editing. High-quality images with fine detail can be generated by this architecture which makes high-precision face attribute editing come true. However, there may have some mode collapse problems.</p></div></div><div class=section_2 id=sec3e><h3>E. Human Image Synthesis</h3><p>Human image synthesis aims to manipulate the visual appearance of the character images by transferring the pose of a character to the target pose, which can be calculated from other characters.<div class=section_2 id=sec3e1><h4>1) Text Guided Person Image Synthesis</h4><p>Zhou <i>et al.</i> <a ref-type=bibr anchor=ref63 id=context_ref_63_3e1>[63]</a> proposed an approach which can manipulate the pose and attribute of generated person images according to a specific text description. The structure is shown in <a ref-type=fig anchor=fig22 class=fulltext-link>Fig. 22</a> and <a ref-type=fig anchor=fig23 class=fulltext-link>Fig. 23</a>.
<div class="figure figure-full" id=fig22><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen22-2982224-large.gif data-fig-id=fig22><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen22-2982224-small.gif data-alt="FIGURE 22. - Text guided pose generator [63]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 22. </b><fig><p>Text guided pose generator <a ref-type=bibr anchor=ref63 id=context_ref_63_3e1>[63]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig23><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen23-2982224-large.gif data-fig-id=fig23><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen23-2982224-small.gif data-alt="FIGURE 23. - Pose and attribute transferred person image generator [63]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 23. </b><fig><p>Pose and attribute transferred person image generator <a ref-type=bibr anchor=ref63 id=context_ref_63_3e1>[63]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function of text guided pose generator is formulated as:<disp-formula id=deqn28 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{Stage-{\text I}} =L_{G_{1}} +\lambda _{1} L_{mse} +\lambda _{2} L_{cls}\tag{28}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-22-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{Stage-{\text I}} =L_{G_{1}} +\lambda _{1} L_{mse} +\lambda _{2} L_{cls}\tag{28}\end{equation*}
</span></span></disp-formula><p>The objective function of the multi-task person image generator is defined as:<disp-formula id=deqn29 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{Stage-{\text {II}}} =L_{G_{2}} +\lambda _{1} L_{1} +\lambda _{2} L_{MS}\tag{29}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-23-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{Stage-{\text {II}}} =L_{G_{2}} +\lambda _{1} L_{1} +\lambda _{2} L_{MS}\tag{29}\end{equation*}
</span></span></disp-formula><p>This method consists of text guided pose generation in the first stage and visual appearance transferred image synthesis in the second stage. The method can generate and edit images according to text description by establishing a mapping between image space and language space which extracts information from the text. The authors proposed a new image processing method based on natural language descriptions and a human pose inference network based on GAN. It uses the Visual Question Answering (VQA) perceptual score to assess the correctness of the change in attributes corresponding to a particular body part. The method first learns to infer a reasonable target human body posture according to the description and then synthesizes the appearance transferred character image based on the text and the target posture. It is an effective method that can manipulate the visual appearance by editing the generated person images based on natural language descriptions.</p></div><div class=section_2 id=sec3e2><h4>2) Progressive Pose Attention Transfer</h4><p>Zhu <i>et al.</i> <a ref-type=bibr anchor=ref64 id=context_ref_64_3e2>[64]</a> proposed a new pose transfer method based on a generative adversarial network. Its architecture is shown in <a ref-type=fig anchor=fig24 class=fulltext-link>Fig. 24</a>.
<div class="figure figure-full" id=fig24><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen24-2982224-large.gif data-fig-id=fig24><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen24-2982224-small.gif data-alt="FIGURE 24. - Generator architecture of the proposed method in [64]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 24. </b><fig><p>Generator architecture of the proposed method in <a ref-type=bibr anchor=ref64 id=context_ref_64_3e2>[64]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss function is denoted as:<disp-formula id=deqn30 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{full} =\arg \min \limits _{G} \max \limits _{D} \alpha L_{GAN} +L_{combL1}\tag{30}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-24-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{full} =\arg \min \limits _{G} \max \limits _{D} \alpha L_{GAN} +L_{combL1}\tag{30}\end{equation*}
</span></span></disp-formula><p>This method can generate person images by using Pose Attentional Transfer Blocks (PATBs) to transfer certain regions in the generator. It can generate more realistic person images that are consistent with the input images in terms of appearance and shape. Furthermore, it uses the attention mechanism to guide the deformable transfer process of the appearance and pose progressively. It can not only improve computational efficiency but also reduce the model complexity. The method uses an appearance discriminator and a shape discriminator to determine whether the appearance and pose generated by the generator are true and produces more natural results than the previous method. The network is more interpretable by its attention masks which make the progressive pose-attentional transfer process visible. Moreover, it is capable of generating realistic images in both qualitative and quantitative measurements.</p></div><div class=section_2 id=sec3e3><h4>3) Semantic Parsing Transformation</h4><p>Song <i>et al.</i> <a ref-type=bibr anchor=ref65 id=context_ref_65_3e3>[65]</a> proposed an unsupervised person image generation approach. Its framework is shown in <a ref-type=fig anchor=fig25 class=fulltext-link>Fig. 25</a>.
<div class="figure figure-full" id=fig25><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen25-2982224-large.gif data-fig-id=fig25><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen25-2982224-small.gif data-alt="FIGURE 25. - The framework for unsupervised person image generation [65]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 25. </b><fig><p>The framework for unsupervised person image generation <a ref-type=bibr anchor=ref65 id=context_ref_65_3e3>[65]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss function of the semantic generative network is denoted as follows:<disp-formula id=deqn31 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{S}^{total} =L_{S}^{adv} +\lambda ^{ce}L_{S}^{ce}\tag{31}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-25-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{S}^{total} =L_{S}^{adv} +\lambda ^{ce}L_{S}^{ce}\tag{31}\end{equation*}
</span></span></disp-formula><p>The loss function of the appearance generative network is denoted as follows:<disp-formula id=deqn32 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}L_{A}^{total} =&amp;L_{A}^{adv} +\lambda ^{pose}L_{A}^{pose} +\lambda _{A}^{cont} L_{A}^{cont} \\&amp; \qquad \qquad \qquad \qquad \qquad \quad {+\,\lambda ^{sty}L_{A}^{sty} +L_{A}^{face}} \tag{32}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-26-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}L_{A}^{total} =&amp;L_{A}^{adv} +\lambda ^{pose}L_{A}^{pose} +\lambda _{A}^{cont} L_{A}^{cont} \\&amp; \qquad \qquad \qquad \qquad \qquad \quad {+\,\lambda ^{sty}L_{A}^{sty} +L_{A}^{face}} \tag{32}\end{align*}
</span></span></disp-formula><p>The approach is divided into two subtasks which reduce the complexity of learning a direct mapping between human bodies with different poses. The semantic parsing transformation task is based on a semantic generative network that can transform between semantic parsing maps and simplify the non-rigid deformation learning. The appearance generation task is based on an appearance generative network that can synthesize semantic-aware textures. It is an unsupervised pose-guided person image generation method which can keep the clothing attributes and better body shapes. Moreover, it can be used to transfer clothing texture or control image manipulation. However, the problem is that the model would fail if there is an error in the conditional semantic map.</p></div><div class=section_2 id=sec3e4><h4>4) Coordinate-Based Texture INPAINTING</h4><p>Grigorev <i>et al.</i> <a ref-type=bibr anchor=ref66 id=context_ref_66_3e4>[66]</a> proposed a pose-guided human image generation approach based on deep learning. Its framework is shown in <a ref-type=fig anchor=fig26 class=fulltext-link>Fig. 26</a> and <a ref-type=fig anchor=fig27 class=fulltext-link>Fig. 27</a>.
<div class="figure figure-full" id=fig26><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen26-2982224-large.gif data-fig-id=fig26><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen26-2982224-small.gif data-alt="FIGURE 26. - The coordinate-based texture inpainting [66]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 26. </b><fig><p>The coordinate-based texture inpainting <a ref-type=bibr anchor=ref66 id=context_ref_66_3e4>[66]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig27><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen27-2982224-large.gif data-fig-id=fig27><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen27-2982224-small.gif data-alt="FIGURE 27. - The final resynthesis [66]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 27. </b><fig><p>The final resynthesis <a ref-type=bibr anchor=ref66 id=context_ref_66_3e4>[66]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The main idea of the method is to complete the texture of the human body by using a new inpainting method which estimates the appropriate source location for each part of the body surface. It establishes the correspondence between source and target view by warping the correspondence field between input image and texture into the target image coordinate frame according to the desired pose. The method uses the estimated correspondence field to guide the deformable skip connections in a fully-convolutional architecture which helps to synthesize the output image. It is a new method based on coordinate-based texture inpainting which can produce more texture details. Moreover, it works by estimating the texture of the human body based on a single photograph that can be used for garment transfer or pose-guided face resynthesis.</p></div><div class=section_2 id=sec3e5><h4>5) Other Methods</h4><p>Tang <i>et al.</i> <a ref-type=bibr anchor=ref67 id=context_ref_67_3e5>[67]</a> proposed a keypoint-guided image generation method called Cycle In Cycle Generative Adversarial Network, which can generate photo-realistic person pose images. Ma <i>et al.</i> <a ref-type=bibr anchor=ref68 id=context_ref_68_3e5>[68]</a> proposed a person image generation method called Pose Guided Person Generation Network, and it can synthesize high-quality person images with arbitrary poses based on a person image and a pose. Ma <i>et al.</i> <a ref-type=bibr anchor=ref69 id=context_ref_69_3e5>[69]</a> proposed a person image generation approach, which can not only learn a disentangled representation of the image factors but also generate realistic person images based on a two-stage reconstruction pipeline.<p>Human image synthesis methods are usually based on a person image and an arbitrary pose to manipulate the visual appearance of a person image. It is possible to reconstruct detail-rich textures for pose-guided human image generation. However, sometimes the texture and the generated images are blurred.</p></div></div></div>
<div class=section id=sec4><div class="header article-hdr"><div class=kicker>
 SECTION IV.</div><h2>Image-to-Image Translation</h2></div><p>Recently, image-to-image translation has made great progress. The goal of image translation is to learn the mapping from the source image domain to the target image domain, which changes the style or some other properties of the source domain to the target domain while keeps the image content unchanged.<div class=section_2 id=sec4a><h3>A. Image-to-Image Translation</h3><p>Image-to-image translation using generative adversarial networks has drawn great attention in both supervised learning and unsupervised learning research. Noise-to-image GANs generate realistic images from random noise samples while image-to-image GANs generate diverse images from images. Many GAN-variants have been proposed, which achieved good results in image-to-image translation tasks.<div class=section_2 id=sec4a1><h4>1) CycleGAN</h4><p>Zhu <i>et al.</i> <a ref-type=bibr anchor=ref70 id=context_ref_70_4a1>[70]</a> presented an unpaired image-to-image translation approach called CycleGAN. The model of CycleGAN is shown in <a ref-type=fig anchor=fig28 class=fulltext-link>Fig. 28</a>.
<div class="figure figure-full" id=fig28><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen28-2982224-large.gif data-fig-id=fig28><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen28-2982224-small.gif data-alt="FIGURE 28. - The model of CycleGAN [70]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 28. </b><fig><p>The model of CycleGAN <a ref-type=bibr anchor=ref70 id=context_ref_70_4a1>[70]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective is:<disp-formula id=deqn33 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} G^{\ast },F^{\ast }=&amp;\arg \min \limits _{G,F} \max \limits _{D_{X},D_{Y} } L(G,F,D_{X},D_{Y}) \\=&amp;L_{GAN} (G,D_{Y},X,Y) \\&amp;+\,L_{GAN} (F,D_{X},Y,X) \\&amp;+\,\lambda L_{cyc} (G,F)\tag{33}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-27-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} G^{\ast },F^{\ast }=&amp;\arg \min \limits _{G,F} \max \limits _{D_{X},D_{Y} } L(G,F,D_{X},D_{Y}) \\=&amp;L_{GAN} (G,D_{Y},X,Y) \\&amp;+\,L_{GAN} (F,D_{X},Y,X) \\&amp;+\,\lambda L_{cyc} (G,F)\tag{33}\end{align*}
</span></span></disp-formula><p>CycleGAN is an innovation of method in the field of unsupervised image translation research. Based on CycleGAN, various unsupervised image translation studies have emerged. It proposed the cycle consistency loss which can learn the mapping without a training set of aligned image pairs. The method achieves good results on many translation tasks involve color and texture changes, such as collection style transfer, object transfiguration, season transfer. However, it fails when it requires geometric changes.</p></div><div class=section_2 id=sec4a2><h4>2) Unit</h4><p>Liu <i>et al.</i> <a ref-type=bibr anchor=ref71 id=context_ref_71_4a2>[71]</a> proposed an unsupervised image-to-image translation framework called UNsupervised Image-to-image. Translation (UNIT) based on Coupled GANs <a ref-type=bibr anchor=ref72 id=context_ref_72_4a2>[72]</a>. The framework of UNIT is shown in <a ref-type=fig anchor=fig29 class=fulltext-link>Fig. 29</a>.
<div class="figure figure-full" id=fig29><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen29-2982224-large.gif data-fig-id=fig29><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen29-2982224-small.gif data-alt="FIGURE 29. - The framework of UNIT [71]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 29. </b><fig><p>The framework of UNIT <a ref-type=bibr anchor=ref71 id=context_ref_71_4a2>[71]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function is defined as:<disp-formula id=deqn34 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}&amp;\hspace {-2pc}\min \limits _{E_{1},E_{2},\textrm {G}_{1},G_{2}} \max \limits _{D_{1},D_{2}} L_{VAE_{1}} (E_{1},G_{1})+L_{GAN_{1}} (E_{2},G_{1},D_{1}) \\&amp;+\,L_{CC_{1}} (E_{1},G_{1},E_{2},G_{2}) \\&amp;\times \, L_{VAE_{2}} (E_{2},G_{2})+L_{GAN_{2}} (E_{1},G_{2},D_{2}) \\&amp;+\,L_{CC_{2}} (E_{2},G_{2},E_{1},G_{1})\tag{34}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-28-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}&amp;\hspace {-2pc}\min \limits _{E_{1},E_{2},\textrm {G}_{1},G_{2}} \max \limits _{D_{1},D_{2}} L_{VAE_{1}} (E_{1},G_{1})+L_{GAN_{1}} (E_{2},G_{1},D_{1}) \\&amp;+\,L_{CC_{1}} (E_{1},G_{1},E_{2},G_{2}) \\&amp;\times \, L_{VAE_{2}} (E_{2},G_{2})+L_{GAN_{2}} (E_{1},G_{2},D_{2}) \\&amp;+\,L_{CC_{2}} (E_{2},G_{2},E_{1},G_{1})\tag{34}\end{align*}
</span></span></disp-formula><p>The method is based on generative adversarial networks as well as variational autoencoders. It generates corresponding images in two domains by adversarial training objective interacts with a weight-sharing constraint to enforce a shared-latent space. Besides, it relates the translated images with the input images in the respective domains by using variational autoencoders. UNIT is a method which can not only present image translation results with high quality but also can be used for various unsupervised image translation tasks, such as street scene image translation, or face image translation. However, there are two limitations to this framework. On the one hand, as a result of the Gaussian latent space assumption, the translation model is unimodal. On the other hand, the saddle point searching problem may cause the training unstable.</p></div><div class=section_2 id=sec4a3><h4>3) MUNIT</h4><p>Xun Huang <i>et al.</i> <a ref-type=bibr anchor=ref73 id=context_ref_73_4a3>[73]</a> proposed an unsupervised image-to-image translation framework called Multimodal Unsupervised Image-to-image Translation (MUNIT). The architecture of MUNIT is shown in <a ref-type=fig anchor=fig30 class=fulltext-link>Fig. 30</a>.
<div class="figure figure-full" id=fig30><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen30-2982224-large.gif data-fig-id=fig30><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen30-2982224-small.gif data-alt="FIGURE 30. - The method overview of DRIT [74]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 30. </b><fig><p>The method overview of DRIT <a ref-type=bibr anchor=ref74 id=context_ref_74_4a3>[74]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function is defined as:<disp-formula id=deqn35 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}&amp;\hspace {-2pc}\min \limits _{E_{1},E_{2},\textrm {G}_{1},G_{2}} \max \limits _{D_{1},D_{2}} L(E_{1},E_{2},G_{1},G_{2},D_{1},D_{2}) \\=&amp;L_{GAN}^{x_{1}} +L_{GAN}^{x_{2}} +\lambda _{x} (L_{recon}^{x_{1}} +L_{recon}^{x_{2}}) \\&amp;+\,\lambda _{c} (L_{recon}^{c_{1}} +L_{recon}^{c_{2}})+\lambda _{s} (L_{recon}^{s_{1}} +L_{recon}^{s_{2}})\tag{35}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-29-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}&amp;\hspace {-2pc}\min \limits _{E_{1},E_{2},\textrm {G}_{1},G_{2}} \max \limits _{D_{1},D_{2}} L(E_{1},E_{2},G_{1},G_{2},D_{1},D_{2}) \\=&amp;L_{GAN}^{x_{1}} +L_{GAN}^{x_{2}} +\lambda _{x} (L_{recon}^{x_{1}} +L_{recon}^{x_{2}}) \\&amp;+\,\lambda _{c} (L_{recon}^{c_{1}} +L_{recon}^{c_{2}})+\lambda _{s} (L_{recon}^{s_{1}} +L_{recon}^{s_{2}})\tag{35}\end{align*}
</span></span></disp-formula><p>MUNIT can generate diverse results from the source domains which are multimodal conditional distribution. It trains two auto-encoders, one encodes the content of the image, and the other encodes the style, which enables the generation of multimodal images. Furthermore, it decomposes the image representation into a content code that is domain-invariant, and a style code to captures domain-specific properties. The method recombines the content code with a random style code sampled from the style space of the target domain to translate the image to another domain. Moreover, two domains that share the same content distribution with different style distributions. It can control the style of translation results according to an example style image that achieves high quality and diversity.</p></div><div class=section_2 id=sec4a4><h4>4) DRIT</h4><p>Hsin-Ying Lee <i>et al.</i> <a ref-type=bibr anchor=ref74 id=context_ref_74_4a4>[74]</a> proposed an image-to-image translation approach termed DRIT. The method of DRIT is shown in <a ref-type=fig anchor=fig31 class=fulltext-link>Fig. 31</a>.
<div class="figure figure-full" id=fig31><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen31-2982224-large.gif data-fig-id=fig31><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen31-2982224-small.gif data-alt="FIGURE 31. - The architecture of MUNIT [73]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 31. </b><fig><p>The architecture of MUNIT <a ref-type=bibr anchor=ref73 id=context_ref_73_4a4>[73]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function of the network is:<disp-formula id=deqn36 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}&amp;\min \limits _{G,E^{c},E^{a}} \max \limits _{D,D^{c}} \lambda _{adv}^{content} L_{adv}^{c} +\lambda _{1}^{cc} L_{1}^{cc} +\lambda _{adv}^{domain} L_{adv}^{domain} \\&amp; \qquad \qquad \quad {+\,\lambda _{1}^{recon} L_{1}^{recon} +\lambda _{1}^{latent} L_{1}^{latent} +\lambda _{KL} L_{KL}} \tag{36}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-30-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}&amp;\min \limits _{G,E^{c},E^{a}} \max \limits _{D,D^{c}} \lambda _{adv}^{content} L_{adv}^{c} +\lambda _{1}^{cc} L_{1}^{cc} +\lambda _{adv}^{domain} L_{adv}^{domain} \\&amp; \qquad \qquad \quad {+\,\lambda _{1}^{recon} L_{1}^{recon} +\lambda _{1}^{latent} L_{1}^{latent} +\lambda _{KL} L_{KL}} \tag{36}\end{align*}
</span></span></disp-formula><p>DRIT is a method that is capable of generating realistic and diverse results without aligned training pairs based on disentangled representation. The generator for each domain in DRIT consists of two encoders, one encodes the content of the image and the other encodes the style of the image, which makes a domain-invariant content space to capture the shared information across domains as well as a domain-specific attribute space. It can generate diverse results with the encoded content features from an image and attribute vectors from the attribute space. Furthermore, it facilitates the factorization of the domain-invariant content space along with domain-specific attribute space by using a content discriminator and trains the model with paired images by using a cross-cycle consistency loss according to disentangled representations. Moreover, it can produce qualitative and quantitative outputs on a wide range of tasks in the absence of paired data. Meanwhile, the approach called DRIT ++ <a ref-type=bibr anchor=ref75 id=context_ref_75_4a4>[75]</a> seeks regularization term to alleviate the mode collapse problem in DRIT, especially in shape-variation translation.</p></div><div class=section_2 id=sec4a5><h4>5) TransGaGa</h4><p>Wu <i>et al.</i> <a ref-type=bibr anchor=ref76 id=context_ref_76_4a5>[76]</a> proposed a geometry-aware disentangle-and-translate framework which can be used for unsupervised image-to-image translation called TransGaGa. The architecture of TransGaGa is shown in <a ref-type=fig anchor=fig32 class=fulltext-link>Fig. 32</a>.
<div class="figure figure-full" id=fig32><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen32-2982224-large.gif data-fig-id=fig32><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen32-2982224-small.gif data-alt="FIGURE 32. - The architecture of TransGaGa [76]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 32. </b><fig><p>The architecture of TransGaGa <a ref-type=bibr anchor=ref76 id=context_ref_76_4a5>[76]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss function of the method is:<disp-formula id=deqn37 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}L_{total} =&amp;L_{CVAE} +L_{prior} +L_{con}^{s} +L_{cyc}^{s} +L_{cyc}^{g} +L_{cyc}^{pix} +L_{adv}^{a} \\&amp; \qquad \qquad \qquad \qquad \qquad \qquad \quad {+\,L_{adv}^{g} +L_{adv}^{pix}} \tag{37}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-31-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}L_{total} =&amp;L_{CVAE} +L_{prior} +L_{con}^{s} +L_{cyc}^{s} +L_{cyc}^{g} +L_{cyc}^{pix} +L_{adv}^{a} \\&amp; \qquad \qquad \qquad \qquad \qquad \qquad \quad {+\,L_{adv}^{g} +L_{adv}^{pix}} \tag{37}\end{align*}
</span></span></disp-formula><p>This method can learn a mapping between two visual domains as well as the translation across large geometry variations. It learns a translation which is built on appearance and geometry space separately by disentangling the image space into an appearance space and a geometry latent space to decompose image-to-image translation into two separate problems. Furthermore, it proposed a geometry prior loss and a conditional VAE loss that can learn independent but complementary representations. TransGaGa is capable of dealing with complex objects image-to-image translation tasks such as near-rigid or non-rigid objects translation. Besides, it supports multimodal translation and achieves qualitative and quantitative results.</p></div><div class=section_2 id=sec4a6><h4>6) RelGAN</h4><p>Lin <i>et al.</i> <a ref-type=bibr anchor=ref77 id=context_ref_77_4a6>[77]</a> proposed a multi-domain image-to-image translation method based on relative attributes called RelGAN. The model of RelGAN is shown in <a ref-type=fig anchor=fig33 class=fulltext-link>Fig. 33</a>.
<div class="figure figure-full" id=fig33><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen33-2982224-large.gif data-fig-id=fig33><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen33-2982224-small.gif data-alt="FIGURE 33. - The model of RelGAN [77]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 33. </b><fig><p>The model of RelGAN <a ref-type=bibr anchor=ref77 id=context_ref_77_4a6>[77]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss function of the method is formulated as:<disp-formula id=deqn38-39 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} \min \limits _{D} L^{D}=&amp;-L_{Real} +\lambda _{1} L_{Match}^{D} +\lambda _{2} L_{Interp}^{D} \tag{38}\\ \min \limits _{G} L^{G}=&amp;L_{Real} +\lambda _{1} L_{Match}^{G} +\lambda _{2} L_{Interp}^{G} +\lambda _{3} L_{Cycle} \\&amp;+\,\lambda _{4} L_{Self} \;+\lambda _{5} L_{Ortho}\tag{39}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-32-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} \min \limits _{D} L^{D}=&amp;-L_{Real} +\lambda _{1} L_{Match}^{D} +\lambda _{2} L_{Interp}^{D} \tag{38}\\ \min \limits _{G} L^{G}=&amp;L_{Real} +\lambda _{1} L_{Match}^{G} +\lambda _{2} L_{Interp}^{G} +\lambda _{3} L_{Cycle} \\&amp;+\,\lambda _{4} L_{Self} \;+\lambda _{5} L_{Ortho}\tag{39}\end{align*}
</span></span></disp-formula><p>The method takes relative attributes as input to describe the selected attributes that need to be changed. It is able to produce images by changing specific properties of interest in a continuous manner and keep the other attributes unchanged. RelGAN helps to improve interpolation quality by training on real-valued relative attributes instead of binary-valued attributes with additional discriminators. It is can be used for facial attribute transfer and interpolation. Furthermore, it achieves quantitative and qualitative results in multi-domain image-to-image translation tasks.</p></div><div class=section_2 id=sec4a7><h4>7) Other Methods</h4><p>Li <i>et al.</i> <a ref-type=bibr anchor=ref78 id=context_ref_78_4a7>[78]</a> proposed an Attribute Guided UIT (Unpaired Image-to-Image Translation) approach termed AGUIT, which can perform image translation tasks by adopting a novel semi-supervised learning process and decomposing the image representation into domain-invariant content code and domain-specific style code. Chang <i>et al.</i> <a ref-type=bibr anchor=ref79 id=context_ref_79_4a7>[79]</a> proposed an image-to-image translation approach called Sym-parameterized Generative Network (SGN), and it focuses on the loss area and infers translations of images in mixed domains by learning the combined characteristics of each domain. Tomei <i>et al.</i> <a ref-type=bibr anchor=ref80 id=context_ref_80_4a7>[80]</a> proposed a semantic-aware approach that can reduce the gap between visual features of artistic and realistic data by translating artworks to photo-realistic visualizations. Mo <i>et al.</i> <a ref-type=bibr anchor=ref81 id=context_ref_81_4a7>[81]</a> proposed an unsupervised image-to-image translation approach called instance-aware GAN (InstaGAN), which can not only incorporate the instance information but also improve the multi-instance transfiguration.<p>The style transfer method widely adopts an encoder-decoder-discriminator (EDD) architecture and can produce diverse outputs. However, it may generate images with artifacts sometimes. Besides, the training may be unstable and there may have mode collapse problems.</p></div></div></div>
<div class=section id=sec5><div class="header article-hdr"><div class=kicker>
 SECTION V.</div><h2>Image Editing</h2></div><div class=section_2 id=sec5a><h3>A. Image Editing</h3><p>The image editing is an interesting but challenging task in computer vision. It mainly manipulates images through color and geometric interactions to complete tasks such as image deformation and blending. Recently, image editing based on deep learning has received more and more attention, especially with the development of GANs. Image editing using GANs has made great progress and becomes a highly recognized subject in computer vision. A series of image editing methods have appeared.<div class=section_2 id=sec5a1><h4>1) SC-FEGAN</h4><p>Jo and Park <a ref-type=bibr anchor=ref82 id=context_ref_82_5a1>[82]</a> proposed a face editing system called SC-FEGAN based on Generative Adversarial Network, and it can synthesize images with high quality by using intuitive user inputs. The architecture of SC-FEGAN is shown in <a ref-type=fig anchor=fig34 class=fulltext-link>Fig. 34</a>.
<div class="figure figure-full" id=fig34><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen34-2982224-large.gif data-fig-id=fig34><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen34-2982224-small.gif data-alt="FIGURE 34. - The network architecture of SC-FEGAN [82]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 34. </b><fig><p>The network architecture of SC-FEGAN <a ref-type=bibr anchor=ref82 id=context_ref_82_5a1>[82]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss functions are shown below:<disp-formula id=deqn40-42 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} L_{G\_{}SN}=&amp;-IE[D(I_{comp})] \tag{40}\\ L_{G}=&amp;L_{per-pixel} +\alpha L_{percept} +\beta L_{G\_{}SN} \\&amp;+\,\gamma (L_{style} (I_{gen})+L_{style} (I_{comp}))+\nu L_{tv} \\&amp;+\,\varepsilon IE[D(I_{gt})^{2}] \tag{41}\\ L_{D}=&amp;IE[1-D(I_{gt})]+IE[1+D(I_{comp})]+\theta L_{GP} \\ {}\tag{42}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-33-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} L_{G\_{}SN}=&amp;-IE[D(I_{comp})] \tag{40}\\ L_{G}=&amp;L_{per-pixel} +\alpha L_{percept} +\beta L_{G\_{}SN} \\&amp;+\,\gamma (L_{style} (I_{gen})+L_{style} (I_{comp}))+\nu L_{tv} \\&amp;+\,\varepsilon IE[D(I_{gt})^{2}] \tag{41}\\ L_{D}=&amp;IE[1-D(I_{gt})]+IE[1+D(I_{comp})]+\theta L_{GP} \\ {}\tag{42}\end{align*}
</span></span></disp-formula><p>SC-FEGAN is a face editing method that uses a free-form mask, sketch and color as an input. The method can restore the area of any shape and reconstruct detail-rich textures of large regions. It generates images guided with sketches and color by using an end-to-end trainable convolutional network and free-form user input with color and shape. In addition, the method is able to generate realistic results by training an additional style loss. It can generate high quality and realistic results with the proposed network architecture and loss functions.</p></div><div class=section_2 id=sec5a2><h4>2) FE-GAN</h4><p>Dong <i>et al.</i> <a ref-type=bibr anchor=ref83 id=context_ref_83_5a2>[83]</a> proposed an image editing approach called Fashion Editing Generative Adversarial Network (FE-GAN) by using a multi-scale attention normalization. The architecture of FE-GAN is shown in <a ref-type=fig anchor=fig35 class=fulltext-link>Fig. 35</a>.
<div class="figure figure-full" id=fig35><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen35-2982224-large.gif data-fig-id=fig35><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen35-2982224-small.gif data-alt="FIGURE 35. - The network architecture of FE-GAN [83]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 35. </b><fig><p>The network architecture of FE-GAN <a ref-type=bibr anchor=ref83 id=context_ref_83_5a2>[83]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function of the free-form parsing network is formulated as:<disp-formula id=deqn43 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{free-form-parser} =\gamma _{1} L_{parsing} +\gamma _{2} L_{feat} +\gamma _{3} L_{adv}\tag{43}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-34-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{free-form-parser} =\gamma _{1} L_{parsing} +\gamma _{2} L_{feat} +\gamma _{3} L_{adv}\tag{43}\end{equation*}
</span></span></disp-formula><p>The objective function of the parsing-aware inpainting network is formulated as:<disp-formula id=deqn44 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}L_{inpainter} =&amp;\lambda _{1} L_{mask} +\lambda _{2} L_{foreground} +\lambda _{3} L_{face} +\lambda _{4} L_{faceTV} \\&amp; \qquad {+\,\lambda _{5} L_{perceptual} +\lambda _{6} L_{style} +\lambda _{7} L_{adv} } \tag{44}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-35-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}L_{inpainter} =&amp;\lambda _{1} L_{mask} +\lambda _{2} L_{foreground} +\lambda _{3} L_{face} +\lambda _{4} L_{faceTV} \\&amp; \qquad {+\,\lambda _{5} L_{perceptual} +\lambda _{6} L_{style} +\lambda _{7} L_{adv} } \tag{44}\end{align*}
</span></span></disp-formula><p>FE-GAN uses sketches and color strokes to manipulate and edit fashion images. It is able to leverage the semantic structural information to edit fashion images by free-forms sketches and sparse color strokes. The method controls the human parsing generation with the sketch and color by using a free-form parsing network. It renders detailed textures with semantic guidance from the human parsing map by using a parsing-aware inpainting network. Furthermore, it improves the quality of the generated images by using a new attention normalization layer in the decoder of the inpainting network. The method can generate high-quality images with convincing details by using a foreground-based partial convolutional encoder.</p></div><div class=section_2 id=sec5a3><h4>3) Mask-Guided Portrait Editing</h4><p>Gu <i>et al.</i> <a ref-type=bibr anchor=ref84 id=context_ref_84_5a3>[84]</a> proposed a portrait editing framework based on mask-guided conditional GANs, and it uses the face masks to guide the image generation. Its framework is shown in <a ref-type=fig anchor=fig36 class=fulltext-link>Fig. 36</a>.
<div class="figure figure-full" id=fig36><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen36-2982224-large.gif data-fig-id=fig36><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen36-2982224-small.gif data-alt="FIGURE 36. - The framework for mask-guided portrait editing in [84]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 36. </b><fig><p>The framework for mask-guided portrait editing in <a ref-type=bibr anchor=ref84 id=context_ref_84_5a3>[84]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss function is:<disp-formula id=deqn45 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}L_{G} =&amp;\lambda _{local} L_{local} +\lambda _{global} L_{global} +\lambda _{GD} L_{GD} \\&amp; \qquad \qquad \qquad \qquad \qquad \qquad \qquad {+\,\lambda _{GP} L_{GP} } \tag{45}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-36-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}L_{G} =&amp;\lambda _{local} L_{local} +\lambda _{global} L_{global} +\lambda _{GD} L_{GD} \\&amp; \qquad \qquad \qquad \qquad \qquad \qquad \qquad {+\,\lambda _{GP} L_{GP} } \tag{45}\end{align*}
</span></span></disp-formula><p>The method is guided by face masks which can generate diverse images with high-quality. It controls the synthesis and editing of facial images by learning feature embeddings for each face component separately. It also helps to improve the performance of image translation and local face editing. This method can edit face components in the generated images with the help of changeable input facial masks and the source image. Moreover, it leverages the input masks to synthesize facial data which can be used for the face paring model. The method can produce realistic outputs and realize face editing.</p></div><div class=section_2 id=sec5a4><h4>4) FaceShapeGene</h4><p>Xu <i>et al.</i> <a ref-type=bibr anchor=ref85 id=context_ref_85_5a4>[85]</a> proposed a face image editing approach termed FaceShapeGene, and it can compute a disentangled shape representation for face images. The pipeline of FaceShapeGene is shown in <a ref-type=fig anchor=fig37 class=fulltext-link>Fig. 37</a>.
<div class="figure figure-full" id=fig37><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen37-2982224-large.gif data-fig-id=fig37><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen37-2982224-small.gif data-alt="FIGURE 37. - The pipeline of FaceShapeGene [85]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 37. </b><fig><p>The pipeline of FaceShapeGene <a ref-type=bibr anchor=ref85 id=context_ref_85_5a4>[85]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function is:<disp-formula id=deqn46 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}L_{total} =&amp;L_{cycle,I} +\lambda _{CL} L_{cyc,L} +\lambda _{GI} L_{GAN,I} \\&amp; \qquad \qquad \qquad ~~\qquad {+\,\lambda _{GL} L_{GAN,L} +\lambda _{ID} L_{ID} } \tag{46}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-37-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}L_{total} =&amp;L_{cycle,I} +\lambda _{CL} L_{cyc,L} +\lambda _{GI} L_{GAN,I} \\&amp; \qquad \qquad \qquad ~~\qquad {+\,\lambda _{GL} L_{GAN,L} +\lambda _{ID} L_{ID} } \tag{46}\end{align*}
</span></span></disp-formula><p>The FaceShapeGene realizes face editing tasks by encoding the shape information of each semantic facial part into a 1D latent vector separately. The authors proposed a shape-remix network to recombine the part-wise latent vectors from different individuals which produces remixed face shape in the form of a label map. They also use a conditional label-to-face transformer to perform part-wise face editing while preserving the identity of the subject. Furthermore, it trains the system in an unsupervised manner by using a cyclic training strategy. The method can disentangle the shape features of different semantic parts correctly and achieve partial editing with realistic results.</p></div><div class=section_2 id=sec5a5><h4>5) Other Methods</h4><p>Shen <i>et al.</i> <a ref-type=bibr anchor=ref86 id=context_ref_86_5a5>[86]</a> proposed a semantic face editing approach termed InterFaceGAN, which can synthesize high-fidelity image by semantic face editing in latent space. Portenier <i>et al.</i> <a ref-type=bibr anchor=ref87 id=context_ref_87_5a5>[87]</a> proposed a face image editing approach called FaceShop, and it can produce high quality and semantically consistent results.<p>In recent years, image editing using GANs has made great progress and achieve good results. Mask-guided image editing method is widely used and it can synthesize realistic with high quality. However, most of the methods proposed at present can only be used for the task of face portrait editing, and there will be artifacts and blurred results when performing whole-body editing.</p></div></div></div>
<div class=section id=sec6><div class="header article-hdr"><div class=kicker>
 SECTION VI.</div><h2>Cartoon Generation</h2></div><div class=section_2 id=sec6a><h3>A. Cartoon Generation</h3><p>The cartoon is popular with young people because of its interesting story. GANs have also attracted the interest of researchers in the field of cartoon generation, and they proposed a series of fresh and interesting cartoon generation methods.<div class=section_2 id=sec6a1><h4>1) CartoonGAN</h4><p>Chen <i>et al.</i> <a ref-type=bibr anchor=ref88 id=context_ref_88_6a1>[88]</a> proposed a photo cartoonization solution called CartoonGAN, and it can transform a photo of a real-world scene into a cartoon style image. The architecture of CartoonGAN is shown in <a ref-type=fig anchor=fig38 class=fulltext-link>Fig. 38</a>.
<div class="figure figure-full" id=fig38><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen38-2982224-large.gif data-fig-id=fig38><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen38-2982224-small.gif data-alt="FIGURE 38. - The architecture of the CartoonGAN [88]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 38. </b><fig><p>The architecture of the CartoonGAN <a ref-type=bibr anchor=ref88 id=context_ref_88_6a1>[88]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective loss function is:<disp-formula id=deqn47 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} (G^{\ast },D^{\ast })=&amp;\arg \min \limits _{G} \max \limits _{D} L(G,D) \\=&amp;L_{adv} (G,D)+\omega L_{con} (G,D)\tag{47}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-38-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} (G^{\ast },D^{\ast })=&amp;\arg \min \limits _{G} \max \limits _{D} L(G,D) \\=&amp;L_{adv} (G,D)+\omega L_{con} (G,D)\tag{47}\end{align*}
</span></span></disp-formula><p>CartoonGAN is a cartoon generation method based on a generative adversarial network, which is easy to use by training with unpaired photos and cartoon images. The method is capable of generating high-quality cartoon images with clear edges and smooth color shading from real-world photos, following the style of specific artists. It copes with the substantial style variation between photos and cartoons by proposing a semantic content loss, which is formulated as a sparse regularization of high-level feature maps in the VGG network. CartoonGAN is able to preserve clear edges by proposing an edge-promoting adversarial loss. In addition, it is capable of improving the convergence of the network to the target manifold by introducing an initialization phase. The method can produce cartoon images with high-quality from real-world photos.</p></div><div class=section_2 id=sec6a2><h4>2) PI-REC</h4><p>You <i>et al.</i> <a ref-type=bibr anchor=ref89 id=context_ref_89_6a2>[89]</a> proposed an image reconstruction approach called PI-REC, which can generate images from binary sparse edge and flat color domain. The architecture of PI-REC is shown in <a ref-type=fig anchor=fig39 class=fulltext-link>Fig. 39</a>.
<div class="figure figure-full" id=fig39><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen39-2982224-large.gif data-fig-id=fig39><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen39-2982224-small.gif data-alt="FIGURE 39. - The network architecture of PI-REC [89]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 39. </b><fig><p>The network architecture of PI-REC <a ref-type=bibr anchor=ref89 id=context_ref_89_6a2>[89]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The loss function is calculated as below:<disp-formula id=deqn48-49 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} L_{G_{1}}=&amp;\alpha L_{per-pixel}\! +\!\beta L_{GAN-G} \!+\!\gamma L_{feature} +\delta L_{style}\qquad ~~\tag{48}\\ L_{D_{1}}=&amp;L_{GAN-D}\tag{49}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-39-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} L_{G_{1}}=&amp;\alpha L_{per-pixel}\! +\!\beta L_{GAN-G} \!+\!\gamma L_{feature} +\delta L_{style}\qquad ~~\tag{48}\\ L_{D_{1}}=&amp;L_{GAN-D}\tag{49}\end{align*}
</span></span></disp-formula><p>PI-REC is able to reconstruct images by inputting binary sparse edge and flat color domain, which can not only control the content and style of generated images freely and accurately but also produce refined reconstruction results with high-quality. The method consists of three phases: Imitation Phase to initialize the networks, Generating Phase aims at reconstructing preliminary images, and Refinement Phase to fine-tune preliminary images and produce outputs with details. Besides, it can be used for hand-drawn draft translation tasks by utilizing parameter confusion operation, which obtains remarkable results. Furthermore, it is able to create anime characters by feeding the well-trained model with edge and color domain extracted from realistic photos, which improves the controllability and interpretability and generates abundant high-frequency details.</p></div><div class=section_2 id=sec6a3><h4>3) Internal Representation Collaging</h4><p>Suzuki <i>et al.</i> <a ref-type=bibr anchor=ref90 id=context_ref_90_6a3>[90]</a> proposed an image synthesis strategy based on CNN, and it can manipulate the feature-space representation of the image in a trained GAN model to change the semantic information of an image over an arbitrary region. The algorithm of applying the feature-space collaging is shown in <a ref-type=fig anchor=fig40 class=fulltext-link>Fig. 40</a>.
<div class="figure figure-full" id=fig40><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen40-2982224-large.gif data-fig-id=fig40><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen40-2982224-small.gif data-alt="FIGURE 40. - The algorithm of applying the feature-space collaging [90]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 40. </b><fig><p>The algorithm of applying the feature-space collaging <a ref-type=bibr anchor=ref90 id=context_ref_90_6a3>[90]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The method can be used to edit artificial images or real images by using spatial conditional batch normalization (sCBN), which is a type of conditional batch normalization with user-specifiable spatial weight maps. It can modify the intermediate features directly and by using feature-blending in any GAN with conditional normalization layers. Besides, it is able to be used to edit anime face which can synthesize realistic results. However, the problem is that it may perform poorly in the transformation of a specific individual, and some information is bound to be lost in the process of projecting the target images to the restricted space of images.</p></div><div class=section_2 id=sec6a4><h4>4) U-GAT-IT</h4><p>Kim <i>et al.</i> <a ref-type=bibr anchor=ref91 id=context_ref_91_6a4>[91]</a> proposed an image-to-image translation approach termed U-GAT-IT, and it can generate realistic anime images by using adaptive layer-instance normalization. The architecture of U-GAT-IT is shown in <a ref-type=fig anchor=fig41 class=fulltext-link>Fig. 41</a>.
<div class="figure figure-full" id=fig41><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen41-2982224-large.gif data-fig-id=fig41><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen41-2982224-small.gif data-alt="FIGURE 41. - The architecture of U-GAT-IT [91]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 41. </b><fig><p>The architecture of U-GAT-IT <a ref-type=bibr anchor=ref91 id=context_ref_91_6a4>[91]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The objective function is:<disp-formula id=deqn50 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*}&amp;\min \limits _{G_{s\to t},G_{t\to s},\eta _{s},\eta _{t}} \max \limits _{D_{s},D_{t},\eta D_{s},\eta D_{t}} \lambda _{1} L_{gan} +\lambda _{2} L_{cycle} +\lambda _{3} L_{identity} \\&amp; \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad {+\,\lambda _{4} L_{cam} } \tag{50}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-40-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}&amp;\min \limits _{G_{s\to t},G_{t\to s},\eta _{s},\eta _{t}} \max \limits _{D_{s},D_{t},\eta D_{s},\eta D_{t}} \lambda _{1} L_{gan} +\lambda _{2} L_{cycle} +\lambda _{3} L_{identity} \\&amp; \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad {+\,\lambda _{4} L_{cam} } \tag{50}\end{align*}
</span></span></disp-formula><p>The U-GAT-IT is an unsupervised image-to-image translation approach, which can translate images with holistic or large shape changes by handling the geometric changes between domains. It incorporates a learnable normalization function and an attention module to distinguish between the source and target domains. The attention module is based on the attention map and obtained by the auxiliary classifier to guide the model to focus on more important regions. Furthermore, the attention-guided model is able to control the amount of change in shape and texture flexibly with the proposed Adaptive Layer-Instance Normalization (AdaLIN). Moreover, it is a method that can produce anime face with more visually pleasing results based on the attention module and AdaLIN.</p></div><div class=section_2 id=sec6a5><h4>5) Landmark Assisted CycleGAN</h4><p>Wu <i>et al.</i> <a ref-type=bibr anchor=ref92 id=context_ref_92_6a5>[92]</a> proposed a cartoon face generation approach based on CycleGAN, and it trains with unpaired data between real faces and cartoon ones, the architecture is shown in <a ref-type=fig anchor=fig42 class=fulltext-link>Fig. 42</a>.
<div class="figure figure-full" id=fig42><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen42-2982224-large.gif data-fig-id=fig42><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen42-2982224-small.gif data-alt="FIGURE 42. - The architecture of the cartoon-face landmark-assisted CycleGAN [92]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 42. </b><fig><p>The architecture of the cartoon-face landmark-assisted CycleGAN <a ref-type=bibr anchor=ref92 id=context_ref_92_6a5>[92]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9043519/all-figures class=all>Show All</a></p></div><p><p>The landmark consistency loss is:<disp-formula id=deqn51 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} L_{c} (G_{(X,L)\to Y})=\left \|{ {R_{Y} (G_{(X,L)\to Y} (x,\ell))-\ell } }\right \|_{2}\tag{51}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-41-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} L_{c} (G_{(X,L)\to Y})=\left \|{ {R_{Y} (G_{(X,L)\to Y} (x,\ell))-\ell } }\right \|_{2}\tag{51}\end{equation*}
</span></span></disp-formula><p>The method is able to generate a cartoon face with high quality which captures the essential facial features of a person by proposing a landmark consistency loss and training a local discriminator in CycleGAN. It can produce cartoon faces with high-quality based on the conditional generator and discriminator, which enforces structural consistency in landmarks. Besides, it is a method guided by facial landmarks that can constrain the facial structure between two domains and can generate impressive high-quality cartoon faces according to the input human faces.</p></div><div class=section_2 id=sec6a6><h4>6) Other Methods</h4><p>Taigman <i>et al.</i> <a ref-type=bibr anchor=ref93 id=context_ref_93_6a6>[93]</a> proposed an image generation approach called Domain Transfer Network (DTN), which can transfer from face photos to emoji. Jin <i>et al.</i> <a ref-type=bibr anchor=ref94 id=context_ref_94_6a6>[94]</a> proposed a method, which can generate facial images of anime characters with a promising result. Li <a ref-type=bibr anchor=ref95 id=context_ref_95_6a6>[95]</a> proposed an image translation method called TwinGAN, and it achieves unsupervised image translation from a human to anime characters. Ma <i>et al.</i> <a ref-type=bibr anchor=ref96 id=context_ref_96_6a6>[96]</a> proposed an instance-level image translation approach called DA-GAN, which can synthesize animation face from a human face. Hamada <i>et al.</i> <a ref-type=bibr anchor=ref97 id=context_ref_97_6a6>[97]</a> proposed an anime generation method called Progressive Structure-conditional Generative Adversarial Networks (PSGAN), and it is able to generate character images with full-body and high-resolution based on structural information. Gokaslan <i>et al.</i> <a ref-type=bibr anchor=ref98 id=context_ref_98_6a6>[98]</a> proposed an image-to-image translation approach termed the GANimorph, which can translate human faces to anime faces. Cao <i>et al.</i> <a ref-type=bibr anchor=ref99 id=context_ref_99_6a6>[99]</a> proposed a photo-to-caricature translation approach, and it is able to transfer from face photos to caricatures. Xiang and Li <a ref-type=bibr anchor=ref100 id=context_ref_100_6a6>[100]</a> proposed a Generative Adversarial Disentanglement Network, which can generate high-fidelity anime portraits.<p>Cartoon generation based on GANs mostly uses the normalization method. It can produce high-quality cartoon images from real-world photos. However, it sometimes generates images with artifacts. Besides, it may not perform well in the image translation of a particular individual.</p></div></div></div>
<div class=section id=sec7><div class="header article-hdr"><div class=kicker>
 SECTION VII.</div><h2>Discussion</h2></div><p>So far, we have discussed some of the applications of GANs in the field of image synthesis. <a ref-type=table anchor=table1 class=fulltext-link>Table 1</a> gives a summary and comparison between different methods based on GANs in terms of pros and cons.<div class="figure figure-full table" id=table1><div class=figcaption><b class=title>TABLE 1 </b>
A brief summary of different GANs.</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen.t1-2982224-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9043519/chen.t1-2982224-small.gif data-alt="Table 1- 
A brief summary of different GANs."><div class=zoom title="View Larger Image"></div></a></div></div><p><div class=section_2 id=sec7a><h3>A. Open Questions</h3><p>GANs can not only learn a highly nonlinear mapping from latent space to data space but also can utilize a large amount of unlabeled image data for deep representation learning. Compared with other frameworks, GANs tend to produce better results with realistic and clear images, which have attracted extensive attention. Due to the great potential and wide applicability of GANs, the researchers are constantly attracted to the research of GANs. However, there are still some problems that have not been completely solved in training and evaluating GANs, such as mode collapse, unstable training problem, and vanishing gradient problem. In addition, GANs are also faced with the problem of non-convergence and sensitivity to hyperparameters. At present, these problems remain to be solved, which need continuous research and efforts from the researchers, and many improved GAN-variants have emerged, including Least Square GAN (LSGAN) <a ref-type=bibr anchor=ref101 id=context_ref_101_7a>[101]</a>, Wasserstein GAN (WGAN) <a ref-type=bibr anchor=ref102 id=context_ref_102_7a>[102]</a>, WGAN-GP <a ref-type=bibr anchor=ref103 id=context_ref_103_7a>[103]</a>, and Spectral Normalized GANs (SNGAN) <a ref-type=bibr anchor=ref104 id=context_ref_104_7a>[104]</a>. These models not only greatly improved the quality and the stability of GANs, but also make it easy to converge and aim to solve the problem of unstable training. However, the problem of collapse during training and the mode collapse have not been completely solved due to the high dimensional characteristics of image data. By using maximum likelihood pre-training, with the help of adversarial fine-tuning is now an effective solution to deal with mode collapse. Other techniques that can be used to stabilize and improve GANs training performance like large batch sizes, dense rewards and discriminator regularization <a ref-type=bibr anchor=ref105 id=context_ref_105_7a>[105]</a>. Recently, Liu <i>et al.</i> <a ref-type=bibr anchor=ref106 id=context_ref_106_7a>[106]</a> proposed an approach called WGAN-QC, which can stabilize and speed up the training process based on the quadratic transport cost. Petroski Such <i>et al.</i> <a ref-type=bibr anchor=ref107 id=context_ref_107_7a>[107]</a> proposed an approach called Generative Teaching Networks (GTNs), and it can stabilize and prevent mode collapse of GAN training.<p>On the other hand, how to evaluate the quality of generated images still lacks effective means. Generative Adversarial Networks are the most popular image generation methods today, but the way to evaluate and compare images produced by GANs is still an extremely challenging task. Many earlier studies on image synthesis based on GANs only used subjective visual assessments. Although it is very hard to quantify the quality of generated images, some studies of evaluating the GANs have begun to appear. For example, the Inception score (IS) <a ref-type=bibr anchor=ref108 id=context_ref_108_7a>[108]</a> and Fréchet Inception Distance (FID) <a ref-type=bibr anchor=ref109 id=context_ref_109_7a>[109]</a> are the most widely adopted evaluation metrics for quantitatively evaluating generated images. Besides, Bau <i>et al.</i> <a ref-type=bibr anchor=ref110 id=context_ref_110_7a>[110]</a> proposed an approach to visualize and understand GANs at the scene-level. Zhou <i>et al.</i> <a ref-type=bibr anchor=ref111 id=context_ref_111_7a>[111]</a> proposed a method called HUMAN EYE PERCEPTUAL EVALUATION (HYPE) to establish a gold standard human benchmark for generative realism. Grnarova <i>et al.</i> <a ref-type=bibr anchor=ref112 id=context_ref_112_7a>[112]</a> propose an evaluation measure to monitor the training progress, which is able to detect failure modes, like unstable mode collapse and divergence. Other evaluation methods are studied as <a ref-type=bibr anchor=ref113 id=context_ref_113_7a>[113]</a>, <a ref-type=bibr anchor=ref114 id=context_ref_114_7a>[114]</a>.<p>With the successive development of methods for training and evaluation of GANs and the great progress that has been done on the GANs, the generative adversarial networks will be more and more widely used in various applications.</p></div><div class=section_2 id=sec7b><h3>B. Future Opportunities</h3><p>With the impetus of GANs, the research in the field of computer vision has been greatly developed in recent years, and various applications have emerged. Most of these applications involve image processing. Although there have been some studies involving video processing, such as video generation <a ref-type=bibr anchor=ref115 id=context_ref_115_7b>[115]</a>, video colorization <a ref-type=bibr anchor=ref116 id=context_ref_116_7b>[116]</a>, <a ref-type=bibr anchor=ref117 id=context_ref_117_7b>[117]</a>, video inpainting <a ref-type=bibr anchor=ref118 id=context_ref_118_7b>[118]</a>, motion transfer <a ref-type=bibr anchor=ref119 id=context_ref_119_7b>[119]</a>, and facial animation synthesis <a ref-type=bibr anchor=ref120 id=context_ref_120_7b>[120]</a>–<a ref-type=bibr anchor=ref123 id=context_ref_123_7b>[123]</a>, the research on video using GANs is limited. In addition, although GANs have been applied to the generation and synthesis of 3D models, such as 3D colorization <a ref-type=bibr anchor=ref124 id=context_ref_124_7b>[124]</a>, 3D face reconstruction <a ref-type=bibr anchor=ref125 id=context_ref_125_7b>[125]</a>, <a ref-type=bibr anchor=ref126 id=context_ref_126_7b>[126]</a>, 3D character animation <a ref-type=bibr anchor=ref127 id=context_ref_127_7b>[127]</a>, and 3D textured object generation <a ref-type=bibr anchor=ref128 id=context_ref_128_7b>[128]</a>, the results are far from perfect. At present, GANs are still based on large amounts of training data. It is an inevitable trend to reduce the use of data in the future. Although there is already some weakly supervised learning research <a ref-type=bibr anchor=ref129 id=context_ref_129_7b>[129]</a>, <a ref-type=bibr anchor=ref130 id=context_ref_130_7b>[130]</a>, these are still very limited, and the results are far from optimal.<p>Besides, GANs have great potential in data augmentation, due to its ability to synthesize high-quality images, especially in areas with data paucity, such as medical image analysis <a ref-type=bibr anchor=ref131 id=context_ref_131_7b>[131]</a>. Frid-Adar <i>et al.</i> <a ref-type=bibr anchor=ref132 id=context_ref_132_7b>[132]</a> presented a GAN-based method to generate synthetic medical images for data augmentation, which can improve the performance of medical problems with limited data. Han <i>et al.</i> <a ref-type=bibr anchor=ref133 id=context_ref_133_7b>[133]</a> proposed a two-step GAN-based data augmentation method to minimize the number of annotated images required for the medical imaging tasks. Sandfort <i>et al.</i> <a ref-type=bibr anchor=ref134 id=context_ref_134_7b>[134]</a> used a GAN-based method for data augmentation to improve the performance of tasks in medical imaging. Han <i>et al.</i> <a ref-type=bibr anchor=ref135 id=context_ref_135_7b>[135]</a> proposed a data augmentation approach called Conditional Progressive Growing of GANs (CPGGANs) to minimize expert physicians’ annotation in medical applications. Schlegl <i>et al.</i> <a ref-type=bibr anchor=ref136 id=context_ref_136_7b>[136]</a> presented an approach called fast AnoGAN (f-AnoGAN), which can identify anomalous images on a variety of biomedical data. Han <i>et al.</i> <a ref-type=bibr anchor=ref137 id=context_ref_137_7b>[137]</a> proposed a data augmentation method called 3D Multi-Conditional GAN (MCGAN), and it can help to overcome medical data paucity.<p>Other directions that are equally noteworthy, such as in the modular <a ref-type=bibr anchor=ref138 id=context_ref_138_7b>[138]</a> and game areas <a ref-type=bibr anchor=ref139 id=context_ref_139_7b>[139]</a>, have rarely been studied. Recently, Lin <i>et al.</i> <a ref-type=bibr anchor=ref140 id=context_ref_140_7b>[140]</a> proposed a novel method called Conditional Coordinate GAN (COCO-GAN), which uses the spatial coordinates as the condition to generate images by parts, and it achieves a high generation quality. Particularly, this approach can generate images larger than any training sample and can be used for large field-of-view generation. We conclude that there are opportunities for future research and application on GANs, especially in these areas.</p></div></div>
<div class=section id=sec8><div class="header article-hdr"><div class=kicker>
 SECTION VIII.</div><h2>Conclusion</h2></div><p>In this paper, we reviewed some basics of GANs and described some applications in the field of image synthesis based on GANs. The pros and cons of these GANs applications are also provided. Besides, we summarized the methods used in GANs applications which improved the performance of generated images. Although the research on GANs is becoming more and more mature, GANs are still faced with some challenges, such as unstable training and hard to evaluate, for which we introduced some methods for training and evaluating of GANs. We think there are some likely future research directions, such as video generation, facial animation synthesis, and 3D face reconstruction. The performance of GANs will continue to improve as various GAN-variants are proposed and GANs applications still need exploring. We expect more interesting applications based on GANs to appear in the future.</p></div>
</div></div></response>
</div><xpl-reference-pop-up _ngcontent-cdr-c167 parentid=full-text-section _nghost-cdr-c161></xpl-reference-pop-up><span _ngcontent-cdr-c167 id=full-text-footer></span></div></div><div _ngcontent-cdr-c167 class="col-3-24 u-pr-1 u-pl-1 col-buttons stats-document-container-rh u-printing-invisible-ie u-printing-invisible-ff"><xpl-document-buttons _ngcontent-cdr-c167 _nghost-cdr-c166><div _ngcontent-cdr-c166 xplscrollsnapmigr cssclasstostick=document-side-menu-stick fromelementid=toc-wrapper tillelementid=full-text-footer offsetfrom=150 offsetto=-800 scrollreset=true id=scroll-snap-buttons-container class="document-doc-buttons stats-document-container-buttons"><ul _ngcontent-cdr-c166 class=tools><li _ngcontent-cdr-c166 id=search-popover xplpopoveranimateonscroll animateelementidposition=1 animatecontainerelementid=search-popover class="blue-tooltip increased-width special-left-tooltip"><a _ngcontent-cdr-c166 triggers=click placement=left><i _ngcontent-cdr-c166 class="fas fa-search icon-size-md color-gray-dark"></i></a><li _ngcontent-cdr-c166 id=resizer-popover xplpopoveranimateonscroll animateelementidposition=1 animatecontainerelementid=resizer-popover class="blue-tooltip special-left-tooltip"><a _ngcontent-cdr-c166 min-size=10 max-size=20 placement=left triggers=click><i _ngcontent-cdr-c166 class="far fa-text-size icon-size-md color-gray-dark"></i></a></ul><xpl-back-to-top-button _ngcontent-cdr-c166 section=full-text-header _nghost-cdr-c67><ul _ngcontent-cdr-c67 class="back-to-top sf-hidden"></ul></xpl-back-to-top-button></div></xpl-document-buttons></div></div></section></xpl-document-full-text><xpl-accordian-section _ngcontent-cdr-c196 _nghost-cdr-c192><div _ngcontent-cdr-c192 role=tablist class="document-accordion-section-container hide-mobile"><xpl-document-accordion _ngcontent-cdr-c192 class=accordion-panel-container _nghost-cdr-c172><div _ngcontent-cdr-c172><div _ngcontent-cdr-c172 role=tab class="accordion-header accordion-button" id=authors-header aria-expanded=false aria-disabled=false><a _ngcontent-cdr-c192 id=authors class="accordion-link text-base-md-lh">Authors</a><div _ngcontent-cdr-c172 class=accordion-chevron><i _ngcontent-cdr-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-cdr-c172><div _ngcontent-cdr-c172 role=tab class="accordion-header accordion-button" id=figures-header aria-expanded=false aria-disabled=false><a _ngcontent-cdr-c192 id=figures class="accordion-link text-base-md-lh">Figures</a><div _ngcontent-cdr-c172 class=accordion-chevron><i _ngcontent-cdr-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-cdr-c172><div _ngcontent-cdr-c172 role=tab class="accordion-header accordion-button" id=references-header aria-expanded=false aria-disabled=false><a _ngcontent-cdr-c192 id=references class="accordion-link text-base-md-lh">References</a><div _ngcontent-cdr-c172 class=accordion-chevron><i _ngcontent-cdr-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-cdr-c172><div _ngcontent-cdr-c172 role=tab class="accordion-header accordion-button" id=citations-header aria-expanded=false aria-disabled=false><a _ngcontent-cdr-c192 id=citations class="accordion-link text-base-md-lh">Citations</a><div _ngcontent-cdr-c172 class=accordion-chevron><i _ngcontent-cdr-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-cdr-c172><div _ngcontent-cdr-c172 role=tab class="accordion-header accordion-button" id=keywords-header aria-expanded=false aria-disabled=false><a _ngcontent-cdr-c192 id=keywords class="accordion-link text-base-md-lh">Keywords</a><div _ngcontent-cdr-c172 class=accordion-chevron><i _ngcontent-cdr-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-cdr-c172><div _ngcontent-cdr-c172 role=tab class="accordion-header accordion-button" id=metrics-header aria-expanded=false aria-disabled=false><a _ngcontent-cdr-c192 id=metrics class="accordion-link text-base-md-lh">Metrics</a><div _ngcontent-cdr-c172 class=accordion-chevron><i _ngcontent-cdr-c172 class="fa fa-angle-down"></i></div></div></div></xpl-document-accordion></div></xpl-accordian-section></div></section></div><div _ngcontent-cdr-c196 class="document-disqus-container col-24-24"><div _ngcontent-cdr-c196 class=row><div _ngcontent-cdr-c196 class="col-12 disqus-container"><xpl-disqus-migr _ngcontent-cdr-c196 page=document _nghost-cdr-c193><div id=disqus_recommendations style=margin-bottom:12px></div><div _ngcontent-cdr-c193 id=disqus_thread></div></xpl-disqus-migr></div></div></div></div></div><div _ngcontent-cdr-c196 class="document-sidebar global-right-rail top-spacing"><div _ngcontent-cdr-c196 class=header-rel-art-toggle-mobile><i _ngcontent-cdr-c196 class=header-rel-art-toggle-icon></i></div><div _ngcontent-cdr-c196 class=document-sidebar-content><div _ngcontent-cdr-c196 class=hide-mobile><xpl-leaderboard-ad _ngcontent-cdr-c196 _nghost-cdr-c127><div _ngcontent-cdr-c127 class="Ads-leaderboard ad-panel"><div _ngcontent-cdr-c127 class="row u-flex-wrap-nowrap"></div><div _ngcontent-cdr-c127 class=ad-leaderboard-ad-container><div _ngcontent-cdr-c127 xplgoogleadmigr><div id=div-gpt-ad-1606861783116-0 style="width:300px;height:250px;display:none;margin:0px auto"></div></div></div></div></xpl-leaderboard-ad></div><div _ngcontent-cdr-c196 class=document-sidebar-rel-art><xpl-related-article-list _ngcontent-cdr-c196 _nghost-cdr-c194><div _ngcontent-cdr-c194 class=stats-document-header-relatedArticles><div _ngcontent-cdr-c194 class=header-rel-art><div _ngcontent-cdr-c194 class="header-rel-art-title text-base-md-lh"> More Like This </div><div _ngcontent-cdr-c194 class=header-rel-art-list><div _ngcontent-cdr-c194 class=header-rel-art-item><div _ngcontent-cdr-c194 class="row text-base-md-lh"><a _ngcontent-cdr-c194 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9684616/><span _ngcontent-cdr-c194>Range Measurement by Computer Vision Systems Based on Invariant Moments</span></a></div><p _ngcontent-cdr-c194 class="header-rel-art-pub text-sm-md-lh">2021 25th International Computer Science and Engineering Conference (ICSEC)<p _ngcontent-cdr-c194 class="header-rel-art-pub text-sm-md-lh">Published: 2021</p></div><div _ngcontent-cdr-c194 class=header-rel-art-item><div _ngcontent-cdr-c194 class="row text-base-md-lh"><a _ngcontent-cdr-c194 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/6715127/><span _ngcontent-cdr-c194>Image resolution improvement in digital holographic microscope with image restoration and PSF upscaling</span></a></div><p _ngcontent-cdr-c194 class="header-rel-art-pub text-sm-md-lh">Technical Digest of the Eighteenth Microoptics Conference<p _ngcontent-cdr-c194 class="header-rel-art-pub text-sm-md-lh">Published: 2013</p></div></div><div _ngcontent-cdr-c194 class="header-rel-art-action text-base-md-lh"><a _ngcontent-cdr-c194>Show More</a></div></div></div></xpl-related-article-list></div><div _ngcontent-cdr-c196 class=hide-mobile><xpl-leaderboard-middle-ad _ngcontent-cdr-c196 _nghost-cdr-c155><div _ngcontent-cdr-c155 class="Ads-leaderboard ad-panel"><div _ngcontent-cdr-c155 class="row u-flex-wrap-nowrap"></div><div _ngcontent-cdr-c155 class=ad-leaderboard-ad-container><div _ngcontent-cdr-c155 xplgoogleadmigr><div id=div-gpt-ad-1606861708157-0 style="width:300px;height:600px;display:none;margin:0px auto"></div></div></div></div></xpl-leaderboard-middle-ad></div></div></div><xpl-reference-panel _ngcontent-cdr-c196 _nghost-cdr-c195><section _ngcontent-cdr-c195 id=references-anchor class="document-all-references hide-mobile panel-closed"><div _ngcontent-cdr-c195 class=header><h1 _ngcontent-cdr-c195>References</h1><a _ngcontent-cdr-c195><i _ngcontent-cdr-c195 class="fas fa-times"></i></a></div><div _ngcontent-cdr-c195 id=references-section-container class=document-ft-section-container><div _ngcontent-cdr-c195><b _ngcontent-cdr-c195>References is not available for this document.</b></div></div></section></xpl-reference-panel></div></xpl-document-details></div><xpl-footer _ngcontent-cdr-c455 _nghost-cdr-c454><footer _ngcontent-cdr-c454 id=xplore-footer class="stats-footer footer-new"><div _ngcontent-cdr-c454 class=footer-wrapper><div _ngcontent-cdr-c454 class=flexible-row-col><div _ngcontent-cdr-c454 class=footer-col><h3 _ngcontent-cdr-c454 class=text-base-md-lh>IEEE Personal Account</h3><ul _ngcontent-cdr-c454 class=text-sm-md-lh><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_blank href="https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Change username/password</a></ul></div><div _ngcontent-cdr-c454 class=footer-col><h3 _ngcontent-cdr-c454 class=text-base-md-lh>Purchase Details</h3><ul _ngcontent-cdr-c454 class=text-sm-md-lh><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_blank href="https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_blank href=https://ieeexplore-ieee-org.thi.idm.oclc.org/articleSale/purchaseHistory.jsp>View Purchased Documents</a></ul></div><div _ngcontent-cdr-c454 class=footer-col><h3 _ngcontent-cdr-c454 class=text-base-md-lh>Profile Information</h3><ul _ngcontent-cdr-c454 class=text-sm-md-lh><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_blank href="https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_blank href="https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_blank href="https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Technical interests</a></ul></div><div _ngcontent-cdr-c454 class="footer-col need-help"><h3 _ngcontent-cdr-c454 class=text-base-md-lh>Need Help?</h3><ul _ngcontent-cdr-c454 class=text-sm-md-lh><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 href=tel:+1-800-678-4333> US &amp; Canada: +1 800 678 4333 </a><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 href=tel:+1-732-981-0060> Worldwide: +1 732 981 0060 </a><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact> Contact &amp; Support </a></ul></div><div _ngcontent-cdr-c454 class="footer-col follow"><h3 _ngcontent-cdr-c454 class=text-base-md-lh>Follow</h3><ul _ngcontent-cdr-c454 class=icon-size-md><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_blank href=https://www.facebook.com/IEEEXploreDigitalLibrary/ aria-label="Follow us on Facebook" title="Follow IEEE Xplore on Facebook, opens in a new tab"><i _ngcontent-cdr-c454 aria-hidden=true class="fab fa-facebook-f"></i></a><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_blank href=https://www.linkedin.com/showcase/ieee-xplore aria-label="Follow us on LinkedIn" title="Follow IEEE Xplore on LinkedIn, opens in a new tab"><i _ngcontent-cdr-c454 aria-hidden=true class="fab fa-linkedin-in"></i></a><li _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_blank href="https://twitter.com/IEEEXplore?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor" aria-label="Follow us on Twitter" title="Follow IEEE Xplore on Twitter, opens in a new tab"><i _ngcontent-cdr-c454 aria-hidden=true class="fab fa-twitter"></i></a></ul></div></div><div _ngcontent-cdr-c454 class=footer-bottom-section><p _ngcontent-cdr-c454 class=text-sm-md-lh><span _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/about-ieee-xplore.html>About IEEE <em _ngcontent-cdr-c454>Xplore</em></a></span> | <span _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact>Contact Us</a></span> | <span _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_start.html>Help</a></span> | <span _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/accessibility-statement.html>Accessibility</a></span> | <span _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_Terms_of_Use.html>Terms of Use</a></span> | <span _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_self href=http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html>Nondiscrimination Policy</a></span> | <span _ngcontent-cdr-c454 class=ethics-reporting-link><a _ngcontent-cdr-c454 target=_blank href=http://www.ieee-ethics-reporting.org/>IEEE Ethics Reporting<i _ngcontent-cdr-c454 class="fa fa-external-link-alt"></i></a></span> | <span _ngcontent-cdr-c454><a _ngcontent-cdr-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/overview-of-ieee-xplore/ieee-xplore-sitemap>Sitemap</a></span> | <span _ngcontent-cdr-c454 class=nowrap><a _ngcontent-cdr-c454 target=_self href=http://www.ieee.org/about/help/security_privacy.html>Privacy &amp; Opting Out of Cookies</a></span><p _ngcontent-cdr-c454> A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. <p _ngcontent-cdr-c454> © Copyright 2022 IEEE - All rights reserved. </p></div></div></footer></xpl-footer></xpl-root>
 </div>
 
 
 
</div>
 
<section id=xploreFooter class=hide-desktop>
 
 <div class="Footer stats-footer hide-mobile">
 <div class="pure-g Footer-sections">
 <div class=pure-u-1-4>
 <h3 class=Footer-header>IEEE Account</h3>
 <ul class=Footer-list>
 <li><a href="https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Change Username/Password</a></li>
 <li><a href="https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Update Address</a></li>
 </ul>
 </div>
 <div class=pure-u-1-4>
 <h3 class=Footer-header>Purchase Details</h3>
 <ul class=Footer-list>
 <li><a href="https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a></li>
 <li><a href="https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Order History</a></li>
 <li><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/articleSale/purchaseHistory.jsp>View Purchased Documents</a></li>
 </ul>
 </div>
 <div class=pure-u-1-4>
 <h3 class=Footer-header>Profile Information</h3>
 <ul class=Footer-list>
 <li><a href="https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a></li>
 <li><a href="https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a></li>
 <li><a href="https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Technical Interests</a></li>
 </ul>
 </div>
 <div class=pure-u-1-4>
 <h3 class=Footer-header>Need Help?</h3>
 <ul class=Footer-list>
 <li><strong>US &amp; Canada:</strong> +1 800 678 4333</li>
 <li><strong>Worldwide: </strong> +1 732 981 0060<br>
 </li>
 <li><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact>Contact &amp; Support</a></li>
 </ul>
 </div>
 </div>
 <div class=row>
 <div class="col-12 Footer-bottom">
 <div class="Footer-bottom-inner-div row">
 <div class=col>
 <ul class="Menu Menu--horizontal Menu--dividers u-mb-1">
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/about-ieee-xplore.html>About IEEE <em>Xplore</em></a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact>Contact Us</a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_start.html target=blank>Help</a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/accessibility-statement.html target=blank>Accessibility</a></li> 
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_Terms_of_Use.html target=_blank>Terms of Use</a></li>
 <li class=Menu-item><a href=http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html>Nondiscrimination Policy</a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/sitemap.jsp>Sitemap</a></li>
 <li class=Menu-item><a href=http://www.ieee.org/about/help/security_privacy.html target=blank>Privacy &amp; Opting Out of Cookies</a></li>
 </ul>
 <p class=Footer-bottom-terms>
 A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.<br>© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
 </p>
 </div>
 <div><i class=logo-ieee-white></i></div>
 </div>
 
 </div>
 </div>
 </div>
 
 
</section>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<style>--></style>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div style=width:1263px id=popup_overlay></div>
<g:compress>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</g:compress>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 </div>
 </div>
 </div>
 
 
 
 </div>
 
<div id=cboxOverlay style=display:none></div><div id=colorbox role=dialog tabindex=-1 style=display:none></div><div class=usabilla_live_button_container id=usabilla_live_button_container_994698774 role=button tabindex=0 aria-label="Usabilla Feedback Button"><style nonce=e9930a118e08 class=sf-hidden>div.usabilla_live_button_container#usabilla_live_button_container_994698774[role="button"]{margin-left:-62.5px;width:125px;height:50px;position:fixed;z-index:999;bottom:0;left:95%}</style></div><div style="position:absolute;width:0px;height:0px;overflow:hidden;padding:0px;border:0px none;margin:0px"><div id=MathJax_Font_Test style="position:absolute;visibility:hidden;top:0px;left:0px;width:auto;min-width:0px;max-width:none;padding:0px;border:0px none;margin:0px;white-space:nowrap;text-align:left;text-indent:0px;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal;font-size:40px;font-weight:normal;font-style:italic;font-size-adjust:none;font-family:MathJax_Math,sans-serif" class=sf-hidden></div></div>