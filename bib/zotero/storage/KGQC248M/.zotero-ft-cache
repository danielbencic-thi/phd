IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Typesetting math: 12%

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

        Cart 
        Create Account
        Personal Sign In 

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Journals & Magazines > IEEE Access > Volume: 8
A State-of-the-Art Review on Image Synthesis With Generative Adversarial Networks
Publisher: IEEE
Cite This
PDF
Lei Wang ; Wei Chen ; Wenjia Yang ; Fangming Bi ; Fei Richard Yu
All Authors
View Document
25
Paper
Citations
1
Patent
Citation
7582
Full
Text Views
Open Access
Comment(s)

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Under a Creative Commons License
Abstract
Document Sections

    I.
    Introduction
    II.
    Generative Adversarial Networks
    III.
    Image Synthesis
    IV.
    Image-to-Image Translation
    V.
    Image Editing

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
More Like This

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

The taxonomy of GANs.
Abstract: Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because... View more
Metadata
Abstract:
Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because of the impressive performance they achieved in various applications. In this paper, we introduce the recent research on GANs in the field of image processing, including image synthesis, image generation, image semantic editing, image-to-image translation, image super-resolution, image inpainting, and cartoon generation. We analyze and summarize the methods used in these applications which have improved the generated results. Then, we discuss the challenges faced by GANs and introduce some methods to deal with these problems. We also preview some likely future research directions in the field of GANs, such as video generation, facial animation synthesis and 3D face reconstruction. The purpose of this review is to provide insights into the research on GANs and to present the various applications based on GANs in different scenarios.
Published in: IEEE Access ( Volume: 8 )
Page(s): 63514 - 63537
Date of Publication: 20 March 2020
Electronic ISSN: 2169-3536
INSPEC Accession Number: 19521306
DOI: 10.1109/ACCESS.2020.2982224
Publisher: IEEE
Funding Agency:
The taxonomy of GANs.
Hide Full Abstract
Contents
CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https :// creativecommons . org / licenses / by / 4 . 0 / to obtain full-text articles and stipulations in the API documentation.
SECTION I.
Introduction

Artificial intelligence (AI) has aroused widespread interest in both the press and on social media. Especially with the rapid development of deep learning, image processing has made great progress. An enormous amount of images are applied in social media, which make the generative models became a hot topic in deep learning research.

Some generative models are promising unsupervised learning techniques with powerful semantic information representation capabilities, and are attracting more and more attention. Among them, the Variational Auto-encoder (VAE) [1] cannot generate clear enough images. The Glow [2] is a flow-based generation model, which has not been widely used so far. The Generative Adversarial Networks (GANs) have achieved impressive results in image processing, and are attracting growing interests in the academic and industrial fields.

Nowadays, GANs are applied to various research and applications, such as image generation [3] , image inpainting [4] , text generation [5] , medical image processing [6] – [13] , semantic segmentation [14] – [17] , image colorization [18] , [19] , image-to-image translation [20] , and art generation [21] . Besides, GANs are widely used in face synthesis and face editing, such as face age [22] – [24] and gender translation [25] .

The research of GANs is divided into two directions: 1) Theoretical research on GANs based on information theory or energy-based models, and focus on the unsolved problems of GANs during training, such as mode collapse, unstable training and hard to evaluate. We will briefly discuss this aspect of problems and the challenges of GANs in Section IX. 2) The applications of GANs in various computer vision tasks. Although there are still some unresolved problems, various GAN-variants have improved the performance of GANs by numerous research studies. In this work, we mainly focus on the second aspect of current research on GANs.

Although there have been some surveys on GANs so far, like [26] , in the field of deep learning, especially GANs are developing fast. This paper focuses on the recent research of GANs in image synthesis. It provides a comparison and analysis in terms of the pros and cons of these applications based on GANs. Besides, we analyze and summarize the methods that have been used in these applications to improve the generated images. Meanwhile, we discuss the challenges faced by GANs in terms of training and evaluating of GANs. Some methods for stable training and evaluation of GANs are provided. Then, we discuss the likely future research directions, such as video generation, facial animation synthesis, and 3D face reconstruction. The rest of the paper is organized as follows: Section II gives a brief introduction of GANs. Section III introduces some applications of image synthesis based on GANs. Section IV focuses on the supervised and unsupervised methods for image-to-image translation. Section V discusses several methods in the application of image editing. Section VI describes several methods of cartoon generation. Section VII reviews the current challenges and limitations of GAN-based methods, as well as previews likely future research work in the area of GANs. Conclusions are given in Section VIII .
SECTION II.
Generative Adversarial Networks

GANs are especially successful in image tasks due to the great potential in image processing. They are considered to be the most effective method in the task of image generation and play an important role in various applications.

The Generative Adversarial Network (GAN) is a model that has been prevailing since Goodfellow et al. [27] proposed it in 2014. GAN consists of a generator G and a discriminator D, the general structure of a Generative Adversarial Network is illustrated in Fig. 1 .
FIGURE 1.

The general structure of a generative adversarial network.

Show All

The generator G is used to generate realistic samples from random noise and tries to fool the discriminator D. The discriminator D is used to identify whether the sample is real or generated by the generator G. The generator and the discriminator are competing with each other until the discriminator cannot distinguish between real and fake generated images. The whole process can be regarded as a two-player minimax game where the main aim of GAN training is to achieve the Nash equilibrium [28] . The loss function of the GAN is formulated as follows: \begin{align*}&\min \limits _{G} \max \limits _{D} V(D,G)=\textrm {E}_{x\sim Pdata(x)} [\textrm {log}D(x)] \\& \qquad \qquad \qquad \qquad \qquad \quad {+\,\textrm {E}_{z\sim Pz(z)} [\textrm {log}(1-D(G(z)))] } \tag{1}\end{align*} min G max D V ( D , G ) = E x ∼ P d a t a ( x ) [ log D ( x ) ] + E z ∼ P z ( z ) [ log ( 1 − D ( G ( z ) ) ) ] (1) View Source Right-click on figure for MathML and additional features. \begin{align*}&\min \limits _{G} \max \limits _{D} V(D,G)=\textrm {E}_{x\sim Pdata(x)} [\textrm {log}D(x)] \\& \qquad \qquad \qquad \qquad \qquad \quad {+\,\textrm {E}_{z\sim Pz(z)} [\textrm {log}(1-D(G(z)))] } \tag{1}\end{align*} where Pdata(x) denotes the true data distribution, Pz(z) denote the noise distribution.

Due to the special network structure and the generation performance of GANs, extensive research has produced numerous applications based on GANs, as shown in Fig. 2 .
FIGURE 2.

Taxonomy of GANs.

Show All

SECTION III.
Image Synthesis

Image synthesis has attracted people’s attention because of its wide application in social media. The GANs have achieved excellent results in the field of image synthesis, such as GauGAN [29] . A variety of image synthesis methods have emerged so far.
A. Texture Synthesis

Image synthesis can be divided into fine-grained texture synthesis and coarse-grained texture synthesis. The coarse-grained texture synthesis pays attention to the similarity between the input image and the output image while the fine-grained texture synthesis pursues whether the synthetic texture is similar to the ground truth.
1) PSGAN

Bergmann et al. [30] proposed a new method of texture synthesis based on the Generative Adversarial Network called Periodic Spatial GAN (PSGAN). The model of PSGAN is illustrated in Fig. 3 .
FIGURE 3.

Illustration of the PSGAN model [30] .

Show All

The loss function of the PSGAN is defined as: \begin{align*}&\hspace {-2pc}\min \limits _{G} \max \limits _{D} V(D,G) \\=&\textstyle {\frac{1 }{ {LM}}}\mathop \Sigma \limits _{\lambda =1}^{L} \mathop \Sigma \limits _{\mu =1}^{M} \textrm {E}_{Z\sim Pz(Z)} [\textrm {log}(1-D_{\lambda \mu } (G(Z)))] \\&+\,\textstyle {\frac{1 }{ {LM}}}\mathop \Sigma \limits _{\lambda =1}^{L} \mathop \Sigma \limits _{\mu =1}^{M} \textrm {E}_{X'\sim Pdata(X)} [\textrm {log}D_{\lambda \mu } ({X}')]\tag{2}\end{align*} = min G max D V ( D , G ) 1 L M Σ λ = 1 L Σ μ = 1 M E Z ∼ P z ( Z ) [ log ( 1 − D λ μ ( G ( Z ) ) ) ] + 1 L M Σ λ = 1 L Σ μ = 1 M E X ′ ∼ P d a t a ( X ) [ log D λ μ ( X ′ ) ] (2) View Source Right-click on figure for MathML and additional features. \begin{align*}&\hspace {-2pc}\min \limits _{G} \max \limits _{D} V(D,G) \\=&\textstyle {\frac{1 }{ {LM}}}\mathop \Sigma \limits _{\lambda =1}^{L} \mathop \Sigma \limits _{\mu =1}^{M} \textrm {E}_{Z\sim Pz(Z)} [\textrm {log}(1-D_{\lambda \mu } (G(Z)))] \\&+\,\textstyle {\frac{1 }{ {LM}}}\mathop \Sigma \limits _{\lambda =1}^{L} \mathop \Sigma \limits _{\mu =1}^{M} \textrm {E}_{X'\sim Pdata(X)} [\textrm {log}D_{\lambda \mu } ({X}')]\tag{2}\end{align*}

PSGAN can learn multiple textures from one or more complex datasets of large images. The method can not only smoothly interpolate between samples in a structured noise space and generate novel samples that are perceptually located between the textures of the original dataset, but also accurately learn periodic textures. PSGAN has the flexibility to handle a wide range of textures and image data sources. It is a method of highly scalable and can produce output images of any size.
2) TextureGAN

Xian et al. [31] proposed a texture synthesis method called TextureGAN, which combines sketch, color, and texture to synthesize images that people expect. The training process is shown in Fig. 4 and Fig. 5 .
FIGURE 4.

TextureGAN pipeline for the ground-truth pre-training [31] .

Show All
FIGURE 5.

TextureGAN pipeline for the external texture fine-tuning [31] .

Show All

The objective function of ground-truth pre-training is defined as: \begin{equation*} L=L_{F} +W_{ADV} L_{ADV} +W_{S} L_{S} +W_{P} L_{P} +W_{C} L_{C}\tag{3}\end{equation*} L = L F + W A D V L A D V + W S L S + W P L P + W C L C (3) View Source Right-click on figure for MathML and additional features. \begin{equation*} L=L_{F} +W_{ADV} L_{ADV} +W_{S} L_{S} +W_{P} L_{P} +W_{C} L_{C}\tag{3}\end{equation*}

The objective function of external texture fine-tuning is defined as: \begin{equation*} L=L_{F} +W_{ADV} L_{ADV} +W_{P} L_{P}^{\prime }+W_{C} L_{C}^{\prime }+L_{t}\tag{4}\end{equation*} L = L F + W A D V L A D V + W P L ′ P + W C L ′ C + L t (4) View Source Right-click on figure for MathML and additional features. \begin{equation*} L=L_{F} +W_{ADV} L_{ADV} +W_{P} L_{P}^{\prime }+W_{C} L_{C}^{\prime }+L_{t}\tag{4}\end{equation*}

TextureGAN is an image synthesis method that can control the texture of generated images. It allows the users to place a texture patch anywhere on the sketch and at any scale to control the desired output texture. Besides, it can not only process various texture inputs and generate texture compositions that follow sketch outlines, but also achieve good results in the sketch and texture-based image synthesis.
3) Texture Mixer

Yu et al. [32] proposed a new method that can control texture interpolation called Texture Mixer. The structure of Texture Mixer is shown in Fig. 6 .
FIGURE 6.

A diagram of the texture mixer [32] .

Show All

The training objective is: \begin{align*}&\hspace {-0.5pc}\min \limits _{E^{\ell },E^{g},G} \max \limits _{D^{rec},D^{itp}} \mathop {\textrm {E}}\limits _{S_{1},S_{2}\sim S} (\lambda _{1} L_{pix}^{rec} +\lambda _{2} L_{Gram}^{rec} +\lambda _{3} L_{adv}^{rec} \\& \qquad~ \qquad ~\qquad \qquad \qquad \qquad {+\,\lambda _{4} L_{Gram}^{itp} +\lambda _{5} L_{adv}^{itp})} \tag{5}\end{align*} min E ℓ , E g , G max D r e c , D i t p E S 1 , S 2 ∼ S ( λ 1 L r e c p i x + λ 2 L r e c G r a m + λ 3 L r e c a d v     + λ 4 L i t p G r a m + λ 5 L i t p a d v ) (5) View Source Right-click on figure for MathML and additional features. \begin{align*}&\hspace {-0.5pc}\min \limits _{E^{\ell },E^{g},G} \max \limits _{D^{rec},D^{itp}} \mathop {\textrm {E}}\limits _{S_{1},S_{2}\sim S} (\lambda _{1} L_{pix}^{rec} +\lambda _{2} L_{Gram}^{rec} +\lambda _{3} L_{adv}^{rec} \\& \qquad~ \qquad ~\qquad \qquad \qquad \qquad {+\,\lambda _{4} L_{Gram}^{itp} +\lambda _{5} L_{adv}^{itp})} \tag{5}\end{align*}

The method utilizes deep learning and GAN to realize controllable interpolation of textures, which combines two different types of texture patterns and makes the transition natural. It proposes a neural network trained with a reconstruction task and a generation task to project the texture of the sample onto a latent space and project linear interpolation onto the image domain to ensure the quality of the intuitive control and realistic generated results. Furthermore, it is superior to many baseline methods and has a good performance in texture synthesis in the dimensions of controllability, smoothness, and realism.
4) Other Methods

Li and Wand [33] proposed an efficient texture synthesis method called Markovian Generative Adversarial Networks (MGANs). It can not only directly decode brown noise to realistic texture but also decode the photo to the painting, which improves the quality of texture synthesis. Jetchev et al. [34] proposed an architecture called spatial GAN (SGAN) which is well-suited for texture synthesis. It is a method that can synthesize texture images with high quality and can fuse multiple different source images in complex textures.

The texture synthesis based on GANs adopts the method of interpolation can produce realistic details of texture and realize the natural transition of texture synthesis. Interpolation and extrapolation are two approaches to enforce constraints for GANs. The incorporation of constraints is built into the training of the GAN while the constraints are enforced after each step through projection on the space of constraints for extrapolation [35] . However, sometimes the texture synthesis model is difficult to converge during training and it can suffer from “mode dropping”.
B. Image Super-Resolution

The image generation model is designed to explore how to generate a desired image, while producing high-quality large images has always been a challenging task. The ability to produce high-quality and high-resolution images is an important advantage of GANs, and significant progress has been made in generating high-quality and visually realistic images. A series of models based on GANs are emerging in the purpose of producing higher-resolution images.
1) ProGAN

Karras et al. [36] proposed an image generation method called ProGAN. The structure of ProGAN is shown in Fig. 7 .
FIGURE 7.

The structure of ProGAN [36] .

Show All

The key idea of this approach is to gradually increase the generator and discriminator, which starts from a low resolution and adds new layers as the training progresses to make the model increase fine details. It is a method which can not only speed the training up but also greatly stabilize it. Compared with the earlier works on GANs, the quality of the results using this method is generally high, and the training is stable in high resolution. However, there are some shortcomings in this method. For example, semantic sensitivity and understanding depend on the constraints of the dataset.
2) Progressive Face Super-Resolution

Kim et al. [37] proposed a novel face super-resolution (SR) method which can generate photo-realistic face images with fully retained facial details. The network architecture is shown in Fig. 8 .
FIGURE 8.

The network architecture of [37] .

Show All

The loss term is shown as: \begin{align*} L_{Ours}=&\alpha L_{pixel} +\beta L_{feat} +\gamma L_{WANG} \tag{6}\\ L_{Ours}=&\alpha L_{pixel} +\beta L_{feat} +\gamma L_{WANG} +\lambda L_{heatmap} \\&+\,\eta L_{attention}\tag{7}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} L_{Ours}=&\alpha L_{pixel} +\beta L_{feat} +\gamma L_{WANG} \tag{6}\\ L_{Ours}=&\alpha L_{pixel} +\beta L_{feat} +\gamma L_{WANG} +\lambda L_{heatmap} \\&+\,\eta L_{attention}\tag{7}\end{align*}

The authors use a progressive training approach that allows stable training by dividing the network into successive steps, each step producing an output of progressively higher resolution. A novel facial attention loss has also been proposed and applied at each step to focus on restoring facial attributes in more detail by multiplying pixel differences and heatmap values. They also proposed a compressed version of the face alignment network (FAN) to extract suitable landmark heatmaps for face super-resolution (SR), and the overall training time can also be reduced. Furthermore, it can learn the restoration of facial details and generate super-resolution facial images that are similar to real ones. The results are superior to the earlier methods in terms of qualitative and quantitative measurements, especially in perceptual quality.
3) BigGANs

Brock et al. [38] proposed models called BigGANs, which realized the work of generating high-resolution and diverse images from complex datasets. A typical network architecture of BigGANs is shown in Fig. 9 .
FIGURE 9.

A typical network architecture of BigGANs [38] .

Show All

This method achieves the goal of generating high-resolution and diverse samples from the complex dataset ImageNet successfully. It is the largest scale of Generative Adversarial Networks that have been trained so far and can generate images of unprecedented quality. It is far superior to the earlier methods in terms of the realism of the generated image. The authors applied orthogonal regularization to the generator to handle the specific instability of such scale and truncated the latent space to control the fidelity and variety of generated images.
4) StyleGAN

Karras et al. [39] proposed an alternative generator architecture called StyleGAN. The network architecture of StyleGAN is shown in Fig. 10 .
FIGURE 10.

The network architecture of StyleGAN [39] .

Show All

The authors redesigned the generator architecture which can adjust its image style based on the latent code in each convolutional layer. It is able to control the entire image synthesis process which starts with very low resolution and generates high-resolution artificial images step by step. Besides, it controls the visual features by modifying the input of each level in the network separately, from coarse features to fine details. The breakthrough of StyleGAN is that it not only produces high-quality and realistic images but also provides better control and understanding of the generated images. The method implements automatic learning, unsupervised high-level attribute separation, and stochastic variation of generated images, which enables intuitive, scale-specific control synthesis of the composition. The method is superior to the traditional GAN generator architecture and can generate a high-resolution image that looks more realistic.
5) Other Methods

Ledig et al. [40] proposed a generative adversarial network for image super-resolution (SR) called SRGAN, and can significantly improve the perceptual quality. Wang et al. [41] improved SRGAN to derive an Enhanced SRGAN (ESRGAN) which not only improved the problem of artifacts in SRGAN and the visual quality of generated images but also obtained more realistic and natural textures. Wang et al. [42] proposed a method to recover natural and realistic texture called SFTGAN, which is equipped with a novel Spatial Feature Transform (SFT) layer and can generate more realistic and visually pleasing textures.

The image super-resolution method which gets the best results trains generators and discriminators from low-resolution images, and adds a higher-resolution network layer each time to generate artificial images step by step. It can generate high-resolution and diverse images with high-quality. However, the training time is long and more GPUs are required.
C. Image Inpainting

In the past few years, deep learning technology has made significant progress in the image inpainting. Image inpainting refers to the technique of restoring and reconstructing images based on background information. The generated images are expected to look very natural and difficult to distinguish from the ground truth. High-quality image inpainting not only requires the semantics of the generated content to be reasonable but also requires that the texture of generated image clear and realistic enough. Recently, image inpainting methods based on deep learning have achieved promising results, especially based on GANs.
1) Deepfillv1

Yu et al. [43] proposed a deep generative model-based image inpainting approach called Deepfillv1. The framework of Deepfillv1 is summarized in Fig. 11 .
FIGURE 11.

Overview of the improved generative inpainting framework [43] .

Show All

Deepfillv1 combines the solution of deep learning algorithms concerning the advantages of traditional algorithms. It further improves the generation network and can automatically repair a picture with multiple holes or large holes, which can produce images of higher quality than earlier methods. The method can synthesize novel image structures and makes use of surrounding image features to make better predictions. The authors utilized a feedforward and fully convolutional neural network to process images with multiple holes during the test time. This method is a coarse-to-fine generative image inpainting framework with a novel contextual attention module that can improve the image inpainting results by learning the feature representations for explicit matching and attending to relevant background patches.
2) ExGANs

Dolhansky et al. [44] proposed a novel in-painting approach called Exemplar GANs (ExGANs). The architecture of ExGANs is shown in Fig 12 .
FIGURE 12.

General architecture of an exemplar GAN [44] .

Show All

The learning objective of reference image inpainting is defined as: \begin{align*} \min \limits _{G}\max \limits _{D} V(D,G)=&\textrm {E}_{x_{i},r_{i}\sim Pdata(x,r)} [\textrm {log}D(x_{i},r_{i})] \\&+\,\textrm {E}_{r_{i}\sim p_{c},G(\cdot)\sim Pz} \textrm {[log1}-D(G(z_{i},r_{i}))] \\&+\,\left \|{ G }\right.(z_{i},r_{i})-\left.{ {x_{i}} }\right \|_{1}\tag{8}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} \min \limits _{G}\max \limits _{D} V(D,G)=&\textrm {E}_{x_{i},r_{i}\sim Pdata(x,r)} [\textrm {log}D(x_{i},r_{i})] \\&+\,\textrm {E}_{r_{i}\sim p_{c},G(\cdot)\sim Pz} \textrm {[log1}-D(G(z_{i},r_{i}))] \\&+\,\left \|{ G }\right.(z_{i},r_{i})-\left.{ {x_{i}} }\right \|_{1}\tag{8}\end{align*}

The adversarial objective of code inpainting is defined as: \begin{align*}&\hspace {-2pc}\min \limits _{G} \max \limits _{D} V(D,G) \\=&\textrm {E}_{x_{i},c_{i}\sim Pdata(x,c)} [\textrm {log}D(x_{i},c_{i})] \\&+\,\textrm {E}_{c_{i}\sim p_{c},G(\cdot)\sim Pz}[\textrm {log1}\!-\!D(G(z_{i},c_{i}))] \\&+\,\left \|{ G }\right.(z_{i},c_{i})\!-\!\left.{ {x_{i}} }\right \|_{1} \!+\!\left \|{ C }\right.(G(z_{i},c_{i})\!-\!\left.{ {c_{i}} }\right \|_{2}\tag{9}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}&\hspace {-2pc}\min \limits _{G} \max \limits _{D} V(D,G) \\=&\textrm {E}_{x_{i},c_{i}\sim Pdata(x,c)} [\textrm {log}D(x_{i},c_{i})] \\&+\,\textrm {E}_{c_{i}\sim p_{c},G(\cdot)\sim Pz}[\textrm {log1}\!-\!D(G(z_{i},c_{i}))] \\&+\,\left \|{ G }\right.(z_{i},c_{i})\!-\!\left.{ {x_{i}} }\right \|_{1} \!+\!\left \|{ C }\right.(G(z_{i},c_{i})\!-\!\left.{ {c_{i}} }\right \|_{2}\tag{9}\end{align*}

The authors use exemplar information as a reference image of the region to inpaint a person with closed eyes in a natural picture which can produce high-quality and personalized inpainting results. It can also describe the object with a perceptual code in the task of a closed-to-open eye to produce a photo-realistic and personalized image in terms of perception and semantics. ExGANs are a type of conditional GAN that can increase the descriptive power by inserting at multiple points within the adversarial network with the extra information. It is a useful method for image generation or inpainting that use reference images or perceptual codes as identifying information which has superior perceptual results.
3) Deepfillv2

Yu et al. [45] proposed a novel image inpainting system based on deep learning which uses free-form masks and inputs to complete images called Deepfillv2. The architecture of Deepfillv2 is shown in Fig. 13 .
FIGURE 13.

The architecture of Deepfillv2 [45] .

Show All

The objective function is: \begin{align*} L_{D^{sn}}=&\textrm {E}_{X\sim Pdata(X)} [ReLU(1-D^{sn}(x))] \\&+\,\textrm {E}_{Z\sim Pz(Z)} [ReLU(1+D^{sn}(G(z)))] \tag{10}\\ L_{G}=&-\textrm {E}_{Z\sim Pz(Z)} [D^{sn}(G(z))]\tag{11}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} L_{D^{sn}}=&\textrm {E}_{X\sim Pdata(X)} [ReLU(1-D^{sn}(x))] \\&+\,\textrm {E}_{Z\sim Pz(Z)} [ReLU(1+D^{sn}(G(z)))] \tag{10}\\ L_{G}=&-\textrm {E}_{Z\sim Pz(Z)} [D^{sn}(G(z))]\tag{11}\end{align*}

This method is based on gated convolutions and can handle images with free-form masks anywhere or any shapes. The authors proposed a GAN loss called SN-PatchGAN which makes the training fast and stable. It is superior to the previous methods and can produce more flexible results with higher-quality. Furthermore, it can be used to remove distracting objects, clear watermarks, edit faces and fill in missing regions. Moreover, the image inpainting system which is based on an end-to-end generative network is useful to improve inpainting results with user guidance input.
4) EdgeConnect

Nazeri et al. [46] proposed a two-stage adversarial model called EdgeConnect, a novel approach for image inpainting. The structure of the EdgeConnect is shown in Fig. 14 .
FIGURE 14.

The structure of EdgeConnect [46] .

Show All

The training objective of the edge generator network is: \begin{equation*} \min \limits _{G_{1}} \max \limits _{D_{1}} L_{G_{1}} \!=\! \mathop {\textrm {min}}\limits _{G_{1}} (\lambda _{adv,1} \mathop {\textrm {max}}\limits _{D_{1}} (L_{adv,1})\!+\!\lambda _{FM} L_{FM})\tag{12}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \min \limits _{G_{1}} \max \limits _{D_{1}} L_{G_{1}} \!=\! \mathop {\textrm {min}}\limits _{G_{1}} (\lambda _{adv,1} \mathop {\textrm {max}}\limits _{D_{1}} (L_{adv,1})\!+\!\lambda _{FM} L_{FM})\tag{12}\end{equation*}

The loss function of the image completion network is: \begin{equation*} L_{G_{2}} =\lambda _{\ell _{1}} L_{\ell _{1}} +\lambda _{adv,2} L_{adv,2} +\lambda _{p} L_{perc} +\lambda _{s} L_{style}\tag{13}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{G_{2}} =\lambda _{\ell _{1}} L_{\ell _{1}} +\lambda _{adv,2} L_{adv,2} +\lambda _{p} L_{perc} +\lambda _{s} L_{style}\tag{13}\end{equation*}

EdgeConnect is an image completion network that uses hallucinated edges as a priori to fill in the missing regions. It consists of an edge generator and an image completion network which can reproduce filled regions exhibiting fine details. The edge generator is used to get edges of the missing region of the image which can be regular or irregular, and the image completion network is used to fill in the missing regions. The authors proposed a new image painting method based on deep learning that can be used for image inpainting task and reconstruct reasonable structures of the missing regions. Furthermore, it does a good job of dealing with images that have multiple or irregular shapes of missing regions. It can be used for removing unwanted objects from the images or as an interactive image editing tool and get a good result in terms of quantitative and qualitative measurements. However, the current problem is that the edge generating model sometimes fails to depict the edges accurately when a large part of the image is missing or in highly textured regions.
5) PEN-Net

Zeng et al. [47] proposed an image inpainting method based on deep generative models called Pyramid-context ENcoder Network (PEN-Net). The structure of the PEN-Net is shown in Fig. 15 .
FIGURE 15.

The structure of PEN-Net [47] .

Show All

The adversarial loss for the discriminator is denoted as: \begin{align*}L_{D} =&\textrm {E}_{X\sim Pdata(X)} [\textrm {max}(0,1-D(x))] \\& \qquad \qquad \qquad \qquad {+\,\textrm {E}_{Z\sim Pz} [\textrm {max}(0,1+D(z))] } \tag{14}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}L_{D} =&\textrm {E}_{X\sim Pdata(X)} [\textrm {max}(0,1-D(x))] \\& \qquad \qquad \qquad \qquad {+\,\textrm {E}_{Z\sim Pz} [\textrm {max}(0,1+D(z))] } \tag{14}\end{align*}

The adversarial loss for the generator is denoted as: \begin{equation*} L_{G} =-\textrm {E}_{Z\sim Pz} [D(z)]\tag{15}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{G} =-\textrm {E}_{Z\sim Pz} [D(z)]\tag{15}\end{equation*}

The PEN-Net is a method which is proposed for high-quality image inpainting and is used to fill in the missing regions of the image with plausible content. The authors put forward the idea of a pyramid-context encoder which uses a high-level semantic feature map as a guide and transfer the learned attention to the previous low-level feature map so that the network can learn the region affinity progressively. It can be used to fill in the regions in a damaged image and get a result both visually and semantically plausible. Moreover, the main idea of the method is to encode the contextual semantics learned from the full resolution input, and restore an image by decoding the semantic features back. Both visual and semantic coherence of the generated content can be ensured with the attention transferred from deep to shallow in a pyramid fashion. At the same time, the authors proposed a new loss function to make the training converge fast and generate more realistic results. The network is superior to the previous method and can generate semantically-reasonable and visually-realistic images.
6) Other Methods

Yang et al. [48] proposed a multi-scale neural patch synthesis method based on deep learning which uses image content and texture constraints to optimize the task of image inpainting. The method can not only restore images with semantically plausible contents but also preserve the high-frequency details. Yeh et al. [49] proposed a new approach for the semantic image inpainting, which can achieve pixel-level photorealism and generate satisfactory results. Li et al. [50] proposed an effective face completion method based on a deep generative model, and it can restore images with a large area of missing pixels and achieve a realistic face completion result.

Image inpainting methods based on GANs nowadays can achieve more reasonable and semantically consistent results than traditional methods. Currently, some methods use gated convolutions to restore images with free-form masks, which can restore images with multiple holes or fill in missing areas with irregular shapes. However, the quality of the image inpainting is sensitive to the position and size of the masks.
D. Face Image Synthesis

In recent years, face image synthesis is a hot topic in photo processing because of the heavy use of pictures on social media. Due to the performance improvement of GANs, facial image processing has made great progress. A series of methods have emerged to improve the quality of face image generation.
1) Elegant

Xiao et al. [51] proposed a model for transferring multiple face attributes called ELEGANT. The framework of ELEGANT is shown in Fig. 16 .
FIGURE 16.

The framework of ELEGANT [51] .

Show All

The loss of the discriminator is: \begin{equation*} L_{D} =L_{D_{1}} +L_{D_{2}}\tag{16}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{D} =L_{D_{1}} +L_{D_{2}}\tag{16}\end{equation*}

The loss of the generator is: \begin{equation*} L_{G} =L_{reconstruction} +L_{adv}\tag{17}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{G} =L_{reconstruction} +L_{adv}\tag{17}\end{equation*}

ELEGANT is an effective method for face attributes transferring. It receives two images of opposite attributes as inputs and can produce high-quality images with finer details. Furthermore, it exchanges a certain part of the encodings to transfer the same type of attributes from one image to another. This method can manipulate several attributes simultaneously by encoding different attributes into disentangled parts in the latent space. The model is based on a U-Net [52] structure and is trained with multi-scale discriminators which can help to improve the quality of the generated images. Besides, it can generate higher resolution images with the help of residual learning to facilitate training.
2) STGAN

Liu et al. [53] proposed an arbitrary facial attribute editing model called STGAN, which achieves high-quality editing results. The structure of STGAN is shown in Fig. 17 .
FIGURE 17.

The structure of STGAN [53] .

Show All

The objective function of discriminator D is formulated as: \begin{equation*} \min \limits _{D} L_{D} =-L_{D_{adv}} +\lambda _{L_{1}} L_{D_{att}}\tag{18}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \min \limits _{D} L_{D} =-L_{D_{adv}} +\lambda _{L_{1}} L_{D_{att}}\tag{18}\end{equation*}

The objective function of generator G is formulated as: \begin{equation*} \min \limits _{G} L_{G} =-L_{G_{adv}} +\lambda _{2} L_{G_{att}} +\lambda _{3} L_{rec}\tag{19}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \min \limits _{G} L_{G} =-L_{G_{adv}} +\lambda _{2} L_{G_{att}} +\lambda _{3} L_{rec}\tag{19}\end{equation*}

This method solves the fine-grained control on the label of the face attribute and realizes multi-attribute transformation. The model takes a difference attribute vector as input to change the related attributes instead of all target attributes in specific editing tasks. STGAN is a high-precision attribute editing model based on AttGAN [54] and StarGAN [55] . It helps to improve the generated image quality and to get a clear editing result. Besides, it can not only improve the ability of face attribute manipulation but also can be used for season translation. The authors proposed selective transfer units (STUs) to enhance attribute editing which can improve the accuracy of attribute manipulation and improve perception quality. STGAN can improve the quality of generated images and realize flexible translation of attributes by focusing on the editing attributes to be changed.
3) SCGAN

Jiang et al. [56] proposed a novel image generation model called Spatially Constrained Generative Adversarial Network (SCGAN). The framework of SCGAN is shown in Fig. 18 .
FIGURE 18.

The framework of SCGAN [56] .

Show All

The objective function of SCGAN is represented as: \begin{align*} L_{S}=&L_{seg}^{real} \tag{20}\\ L_{D}=&-L_{adv} +\lambda _{cls} L_{cls}^{real} \tag{21}\\ L_{G}=&L_{adv} +\lambda _{cls} L_{cla}^{fake} +\lambda _{seg} L_{seg}^{fake}\tag{22}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} L_{S}=&L_{seg}^{real} \tag{20}\\ L_{D}=&-L_{adv} +\lambda _{cls} L_{cls}^{real} \tag{21}\\ L_{G}=&L_{adv} +\lambda _{cls} L_{cla}^{fake} +\lambda _{seg} L_{seg}^{fake}\tag{22}\end{align*}

This method can generate images with clear edge details and can preserve spatial information. It makes the spatial constraints feasible as additional controllable signals which are decoupled from the latent vector. Moreover, the authors designed a generator network that takes a semantic segmentation, a latent vector and an attribute-level label as inputs to enhance the spatial controllability step by step. Meanwhile, the authors proposed a segmentor network to impose spatial constraints on the generator which can accelerate and stabilize the model convergence. SCGAN is an effective method that can control the spatial contents and can generate high-quality images. It can not only solve the foreground-background mismatch problem but it is also easy and fast to train. Besides, SCGAN is very effective at controlling spatial contents which can specify attributes and help to improve general visual quality and get quantitative results.
4) Example-Guided Image Synthesis

Wang et al. [57] proposed an example-guided image synthesis solution by using a semantic label map and an exemplary image, and its framework is summarized as shown in Fig. 19 .
FIGURE 19.

Overview of the framework [57] .

Show All

The objective function is formulated as: \begin{equation*} G^{\ast }=\arg \min \limits _{G} \max \limits _{D_{R},D_{SC}} L(G,D_{R},D_{SC})\tag{23}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} G^{\ast }=\arg \min \limits _{G} \max \limits _{D_{R},D_{SC}} L(G,D_{R},D_{SC})\tag{23}\end{equation*}

This method is based on conditional generative adversarial networks aim to synthesize images from semantic label maps and using an exemplary image to indicate facial expression or full body poses. The authors proposed a novel style consistency discriminator and an adaptive semantic consistency loss to make sure that the synthesized image is consistent in style with the exemplar. Furthermore, a training data sampling strategy is also used to synthesize style-consistent results. It is an effective method that can be used on the face or street view synthesis tasks which can produce qualitative and quantitative results. Moreover, it can generate realistic and style-consistent images with the help of style consistency discriminator.
5) SGGAN

Jiang et al. [58] proposed a novel multi-domain face image translation method called Segmentation Guided Generative Adversarial Networks (SGGAN). The framework of SGGAN is shown in Fig. 20 .
FIGURE 20.

Illustration of SGGAN [58] .

Show All

The objective function of the SGGAN network is summarized as: \begin{align*} L_{S}=&L_{seg}^{real} \tag{24}\\[-1pt] L_{D}=&-L_{adv} +\lambda _{1} L_{cls}^{real} \tag{25}\\[-1pt] L_{G}=&L_{adv} +\lambda _{1} L_{cls}^{fake} +\lambda _{2} L_{seg}^{fake} +\lambda _{3} L_{rec}\tag{26}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} L_{S}=&L_{seg}^{real} \tag{24}\\[-1pt] L_{D}=&-L_{adv} +\lambda _{1} L_{cls}^{real} \tag{25}\\[-1pt] L_{G}=&L_{adv} +\lambda _{1} L_{cls}^{fake} +\lambda _{2} L_{seg}^{fake} +\lambda _{3} L_{rec}\tag{26}\end{align*}

The method is based on a deep generative model that pays attention to higher-level and instance-specific information and can generate realistic images of high quality. It has spatial controllability in the image translation process by utilizing semantic segmentation to improve the performance of image generation and provides spatial mapping. The authors proposed a segmentor network to provide the generated images with semantic information. Besides, it can improve the quality of image generation with the ability of spatial modification. The method uses the segmentation information to guide the generation of images which can make the details clear. SGGAN can be used for face image translation by providing strong regulations during the training process.
6) MaskGAN

Lee et al. [59] proposed a geometry-oriented face manipulation framework called MaskGAN. The pipeline of MaskGAN is shown in Fig. 21 .
FIGURE 21.

The pipeline of MaskGAN [59] .

Show All

The objective loss function is: \begin{align*}L_{G_{A},G_{B}} =&L_{adv} (G,D_{1,2})+\lambda _{feat} L_{feat} (G,D_{1,2}) \\&~ \qquad \qquad \qquad \qquad {+\,\lambda _{percept} L_{percept} (G)\,\,} \tag{27}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}L_{G_{A},G_{B}} =&L_{adv} (G,D_{1,2})+\lambda _{feat} L_{feat} (G,D_{1,2}) \\&~ \qquad \qquad \qquad \qquad {+\,\lambda _{percept} L_{percept} (G)\,\,} \tag{27}\end{align*}

The method overcomes the shortcomings of operating on a predefined set of face attributes. It makes the users manipulate images with more freedom by using semantic masks as an intermediate representation, which enables diverse and interactive face manipulation. MaskGAN can achieve diverse generation results by using dense mapping networks to learn style mapping between the free-form user modified mask and the target image. Furthermore, it makes the framework more robust to manipulate by using editing behavior simulated training which models users editing behavior on the source mask. MaskGAN can be used to manipulate face image flexibly and preserve the fidelity.
7) Other Methods

Lin et al. [60] proposed an unpaired image-to-image translation method called Domain-supervised GAN (DosGAN), and it uses domain information as explicit supervision and achieves conditional translation with face images in CelebA. Mokady et al. [61] proposed a novel mask-based method which uses the masks to reconstruct the face images and enables high quality and various content translations. Yin et al. [62] proposed an instance-level facial attribute transfer method which uses the geometry-aware flow as a representation for transferring the images with instance-level facial attributes.

The Face image synthesis method uses encoder-decoder and generative adversarial networks to solve the problem of arbitrary attribute editing. High-quality images with fine detail can be generated by this architecture which makes high-precision face attribute editing come true. However, there may have some mode collapse problems.
E. Human Image Synthesis

Human image synthesis aims to manipulate the visual appearance of the character images by transferring the pose of a character to the target pose, which can be calculated from other characters.
1) Text Guided Person Image Synthesis

Zhou et al. [63] proposed an approach which can manipulate the pose and attribute of generated person images according to a specific text description. The structure is shown in Fig. 22 and Fig. 23 .
FIGURE 22.

Text guided pose generator [63] .

Show All
FIGURE 23.

Pose and attribute transferred person image generator [63] .

Show All

The objective function of text guided pose generator is formulated as: \begin{equation*} L_{Stage-{\text I}} =L_{G_{1}} +\lambda _{1} L_{mse} +\lambda _{2} L_{cls}\tag{28}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{Stage-{\text I}} =L_{G_{1}} +\lambda _{1} L_{mse} +\lambda _{2} L_{cls}\tag{28}\end{equation*}

The objective function of the multi-task person image generator is defined as: \begin{equation*} L_{Stage-{\text {II}}} =L_{G_{2}} +\lambda _{1} L_{1} +\lambda _{2} L_{MS}\tag{29}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{Stage-{\text {II}}} =L_{G_{2}} +\lambda _{1} L_{1} +\lambda _{2} L_{MS}\tag{29}\end{equation*}

This method consists of text guided pose generation in the first stage and visual appearance transferred image synthesis in the second stage. The method can generate and edit images according to text description by establishing a mapping between image space and language space which extracts information from the text. The authors proposed a new image processing method based on natural language descriptions and a human pose inference network based on GAN. It uses the Visual Question Answering (VQA) perceptual score to assess the correctness of the change in attributes corresponding to a particular body part. The method first learns to infer a reasonable target human body posture according to the description and then synthesizes the appearance transferred character image based on the text and the target posture. It is an effective method that can manipulate the visual appearance by editing the generated person images based on natural language descriptions.
2) Progressive Pose Attention Transfer

Zhu et al. [64] proposed a new pose transfer method based on a generative adversarial network. Its architecture is shown in Fig. 24 .
FIGURE 24.

Generator architecture of the proposed method in [64] .

Show All

The loss function is denoted as: \begin{equation*} L_{full} =\arg \min \limits _{G} \max \limits _{D} \alpha L_{GAN} +L_{combL1}\tag{30}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{full} =\arg \min \limits _{G} \max \limits _{D} \alpha L_{GAN} +L_{combL1}\tag{30}\end{equation*}

This method can generate person images by using Pose Attentional Transfer Blocks (PATBs) to transfer certain regions in the generator. It can generate more realistic person images that are consistent with the input images in terms of appearance and shape. Furthermore, it uses the attention mechanism to guide the deformable transfer process of the appearance and pose progressively. It can not only improve computational efficiency but also reduce the model complexity. The method uses an appearance discriminator and a shape discriminator to determine whether the appearance and pose generated by the generator are true and produces more natural results than the previous method. The network is more interpretable by its attention masks which make the progressive pose-attentional transfer process visible. Moreover, it is capable of generating realistic images in both qualitative and quantitative measurements.
3) Semantic Parsing Transformation

Song et al. [65] proposed an unsupervised person image generation approach. Its framework is shown in Fig. 25 .
FIGURE 25.

The framework for unsupervised person image generation [65] .

Show All

The loss function of the semantic generative network is denoted as follows: \begin{equation*} L_{S}^{total} =L_{S}^{adv} +\lambda ^{ce}L_{S}^{ce}\tag{31}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{S}^{total} =L_{S}^{adv} +\lambda ^{ce}L_{S}^{ce}\tag{31}\end{equation*}

The loss function of the appearance generative network is denoted as follows: \begin{align*}L_{A}^{total} =&L_{A}^{adv} +\lambda ^{pose}L_{A}^{pose} +\lambda _{A}^{cont} L_{A}^{cont} \\& \qquad \qquad \qquad \qquad \qquad \quad {+\,\lambda ^{sty}L_{A}^{sty} +L_{A}^{face}} \tag{32}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}L_{A}^{total} =&L_{A}^{adv} +\lambda ^{pose}L_{A}^{pose} +\lambda _{A}^{cont} L_{A}^{cont} \\& \qquad \qquad \qquad \qquad \qquad \quad {+\,\lambda ^{sty}L_{A}^{sty} +L_{A}^{face}} \tag{32}\end{align*}

The approach is divided into two subtasks which reduce the complexity of learning a direct mapping between human bodies with different poses. The semantic parsing transformation task is based on a semantic generative network that can transform between semantic parsing maps and simplify the non-rigid deformation learning. The appearance generation task is based on an appearance generative network that can synthesize semantic-aware textures. It is an unsupervised pose-guided person image generation method which can keep the clothing attributes and better body shapes. Moreover, it can be used to transfer clothing texture or control image manipulation. However, the problem is that the model would fail if there is an error in the conditional semantic map.
4) Coordinate-Based Texture INPAINTING

Grigorev et al. [66] proposed a pose-guided human image generation approach based on deep learning. Its framework is shown in Fig. 26 and Fig. 27 .
FIGURE 26.

The coordinate-based texture inpainting [66] .

Show All
FIGURE 27.

The final resynthesis [66] .

Show All

The main idea of the method is to complete the texture of the human body by using a new inpainting method which estimates the appropriate source location for each part of the body surface. It establishes the correspondence between source and target view by warping the correspondence field between input image and texture into the target image coordinate frame according to the desired pose. The method uses the estimated correspondence field to guide the deformable skip connections in a fully-convolutional architecture which helps to synthesize the output image. It is a new method based on coordinate-based texture inpainting which can produce more texture details. Moreover, it works by estimating the texture of the human body based on a single photograph that can be used for garment transfer or pose-guided face resynthesis.
5) Other Methods

Tang et al. [67] proposed a keypoint-guided image generation method called Cycle In Cycle Generative Adversarial Network, which can generate photo-realistic person pose images. Ma et al. [68] proposed a person image generation method called Pose Guided Person Generation Network, and it can synthesize high-quality person images with arbitrary poses based on a person image and a pose. Ma et al. [69] proposed a person image generation approach, which can not only learn a disentangled representation of the image factors but also generate realistic person images based on a two-stage reconstruction pipeline.

Human image synthesis methods are usually based on a person image and an arbitrary pose to manipulate the visual appearance of a person image. It is possible to reconstruct detail-rich textures for pose-guided human image generation. However, sometimes the texture and the generated images are blurred.
SECTION IV.
Image-to-Image Translation

Recently, image-to-image translation has made great progress. The goal of image translation is to learn the mapping from the source image domain to the target image domain, which changes the style or some other properties of the source domain to the target domain while keeps the image content unchanged.
A. Image-to-Image Translation

Image-to-image translation using generative adversarial networks has drawn great attention in both supervised learning and unsupervised learning research. Noise-to-image GANs generate realistic images from random noise samples while image-to-image GANs generate diverse images from images. Many GAN-variants have been proposed, which achieved good results in image-to-image translation tasks.
1) CycleGAN

Zhu et al. [70] presented an unpaired image-to-image translation approach called CycleGAN. The model of CycleGAN is shown in Fig. 28 .
FIGURE 28.

The model of CycleGAN [70] .

Show All

The objective is: \begin{align*} G^{\ast },F^{\ast }=&\arg \min \limits _{G,F} \max \limits _{D_{X},D_{Y} } L(G,F,D_{X},D_{Y}) \\=&L_{GAN} (G,D_{Y},X,Y) \\&+\,L_{GAN} (F,D_{X},Y,X) \\&+\,\lambda L_{cyc} (G,F)\tag{33}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} G^{\ast },F^{\ast }=&\arg \min \limits _{G,F} \max \limits _{D_{X},D_{Y} } L(G,F,D_{X},D_{Y}) \\=&L_{GAN} (G,D_{Y},X,Y) \\&+\,L_{GAN} (F,D_{X},Y,X) \\&+\,\lambda L_{cyc} (G,F)\tag{33}\end{align*}

CycleGAN is an innovation of method in the field of unsupervised image translation research. Based on CycleGAN, various unsupervised image translation studies have emerged. It proposed the cycle consistency loss which can learn the mapping without a training set of aligned image pairs. The method achieves good results on many translation tasks involve color and texture changes, such as collection style transfer, object transfiguration, season transfer. However, it fails when it requires geometric changes.
2) Unit

Liu et al. [71] proposed an unsupervised image-to-image translation framework called UNsupervised Image-to-image. Translation (UNIT) based on Coupled GANs [72] . The framework of UNIT is shown in Fig. 29 .
FIGURE 29.

The framework of UNIT [71] .

Show All

The objective function is defined as: \begin{align*}&\hspace {-2pc}\min \limits _{E_{1},E_{2},\textrm {G}_{1},G_{2}} \max \limits _{D_{1},D_{2}} L_{VAE_{1}} (E_{1},G_{1})+L_{GAN_{1}} (E_{2},G_{1},D_{1}) \\&+\,L_{CC_{1}} (E_{1},G_{1},E_{2},G_{2}) \\&\times \, L_{VAE_{2}} (E_{2},G_{2})+L_{GAN_{2}} (E_{1},G_{2},D_{2}) \\&+\,L_{CC_{2}} (E_{2},G_{2},E_{1},G_{1})\tag{34}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}&\hspace {-2pc}\min \limits _{E_{1},E_{2},\textrm {G}_{1},G_{2}} \max \limits _{D_{1},D_{2}} L_{VAE_{1}} (E_{1},G_{1})+L_{GAN_{1}} (E_{2},G_{1},D_{1}) \\&+\,L_{CC_{1}} (E_{1},G_{1},E_{2},G_{2}) \\&\times \, L_{VAE_{2}} (E_{2},G_{2})+L_{GAN_{2}} (E_{1},G_{2},D_{2}) \\&+\,L_{CC_{2}} (E_{2},G_{2},E_{1},G_{1})\tag{34}\end{align*}

The method is based on generative adversarial networks as well as variational autoencoders. It generates corresponding images in two domains by adversarial training objective interacts with a weight-sharing constraint to enforce a shared-latent space. Besides, it relates the translated images with the input images in the respective domains by using variational autoencoders. UNIT is a method which can not only present image translation results with high quality but also can be used for various unsupervised image translation tasks, such as street scene image translation, or face image translation. However, there are two limitations to this framework. On the one hand, as a result of the Gaussian latent space assumption, the translation model is unimodal. On the other hand, the saddle point searching problem may cause the training unstable.
3) MUNIT

Xun Huang et al. [73] proposed an unsupervised image-to-image translation framework called Multimodal Unsupervised Image-to-image Translation (MUNIT). The architecture of MUNIT is shown in Fig. 30 .
FIGURE 30.

The method overview of DRIT [74] .

Show All

The objective function is defined as: \begin{align*}&\hspace {-2pc}\min \limits _{E_{1},E_{2},\textrm {G}_{1},G_{2}} \max \limits _{D_{1},D_{2}} L(E_{1},E_{2},G_{1},G_{2},D_{1},D_{2}) \\=&L_{GAN}^{x_{1}} +L_{GAN}^{x_{2}} +\lambda _{x} (L_{recon}^{x_{1}} +L_{recon}^{x_{2}}) \\&+\,\lambda _{c} (L_{recon}^{c_{1}} +L_{recon}^{c_{2}})+\lambda _{s} (L_{recon}^{s_{1}} +L_{recon}^{s_{2}})\tag{35}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}&\hspace {-2pc}\min \limits _{E_{1},E_{2},\textrm {G}_{1},G_{2}} \max \limits _{D_{1},D_{2}} L(E_{1},E_{2},G_{1},G_{2},D_{1},D_{2}) \\=&L_{GAN}^{x_{1}} +L_{GAN}^{x_{2}} +\lambda _{x} (L_{recon}^{x_{1}} +L_{recon}^{x_{2}}) \\&+\,\lambda _{c} (L_{recon}^{c_{1}} +L_{recon}^{c_{2}})+\lambda _{s} (L_{recon}^{s_{1}} +L_{recon}^{s_{2}})\tag{35}\end{align*}

MUNIT can generate diverse results from the source domains which are multimodal conditional distribution. It trains two auto-encoders, one encodes the content of the image, and the other encodes the style, which enables the generation of multimodal images. Furthermore, it decomposes the image representation into a content code that is domain-invariant, and a style code to captures domain-specific properties. The method recombines the content code with a random style code sampled from the style space of the target domain to translate the image to another domain. Moreover, two domains that share the same content distribution with different style distributions. It can control the style of translation results according to an example style image that achieves high quality and diversity.
4) DRIT

Hsin-Ying Lee et al. [74] proposed an image-to-image translation approach termed DRIT. The method of DRIT is shown in Fig. 31 .
FIGURE 31.

The architecture of MUNIT [73] .

Show All

The objective function of the network is: \begin{align*}&\min \limits _{G,E^{c},E^{a}} \max \limits _{D,D^{c}} \lambda _{adv}^{content} L_{adv}^{c} +\lambda _{1}^{cc} L_{1}^{cc} +\lambda _{adv}^{domain} L_{adv}^{domain} \\& \qquad \qquad \quad {+\,\lambda _{1}^{recon} L_{1}^{recon} +\lambda _{1}^{latent} L_{1}^{latent} +\lambda _{KL} L_{KL}} \tag{36}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}&\min \limits _{G,E^{c},E^{a}} \max \limits _{D,D^{c}} \lambda _{adv}^{content} L_{adv}^{c} +\lambda _{1}^{cc} L_{1}^{cc} +\lambda _{adv}^{domain} L_{adv}^{domain} \\& \qquad \qquad \quad {+\,\lambda _{1}^{recon} L_{1}^{recon} +\lambda _{1}^{latent} L_{1}^{latent} +\lambda _{KL} L_{KL}} \tag{36}\end{align*}

DRIT is a method that is capable of generating realistic and diverse results without aligned training pairs based on disentangled representation. The generator for each domain in DRIT consists of two encoders, one encodes the content of the image and the other encodes the style of the image, which makes a domain-invariant content space to capture the shared information across domains as well as a domain-specific attribute space. It can generate diverse results with the encoded content features from an image and attribute vectors from the attribute space. Furthermore, it facilitates the factorization of the domain-invariant content space along with domain-specific attribute space by using a content discriminator and trains the model with paired images by using a cross-cycle consistency loss according to disentangled representations. Moreover, it can produce qualitative and quantitative outputs on a wide range of tasks in the absence of paired data. Meanwhile, the approach called DRIT ++ [75] seeks regularization term to alleviate the mode collapse problem in DRIT, especially in shape-variation translation.
5) TransGaGa

Wu et al. [76] proposed a geometry-aware disentangle-and-translate framework which can be used for unsupervised image-to-image translation called TransGaGa. The architecture of TransGaGa is shown in Fig. 32 .
FIGURE 32.

The architecture of TransGaGa [76] .

Show All

The loss function of the method is: \begin{align*}L_{total} =&L_{CVAE} +L_{prior} +L_{con}^{s} +L_{cyc}^{s} +L_{cyc}^{g} +L_{cyc}^{pix} +L_{adv}^{a} \\& \qquad \qquad \qquad \qquad \qquad \qquad \quad {+\,L_{adv}^{g} +L_{adv}^{pix}} \tag{37}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}L_{total} =&L_{CVAE} +L_{prior} +L_{con}^{s} +L_{cyc}^{s} +L_{cyc}^{g} +L_{cyc}^{pix} +L_{adv}^{a} \\& \qquad \qquad \qquad \qquad \qquad \qquad \quad {+\,L_{adv}^{g} +L_{adv}^{pix}} \tag{37}\end{align*}

This method can learn a mapping between two visual domains as well as the translation across large geometry variations. It learns a translation which is built on appearance and geometry space separately by disentangling the image space into an appearance space and a geometry latent space to decompose image-to-image translation into two separate problems. Furthermore, it proposed a geometry prior loss and a conditional VAE loss that can learn independent but complementary representations. TransGaGa is capable of dealing with complex objects image-to-image translation tasks such as near-rigid or non-rigid objects translation. Besides, it supports multimodal translation and achieves qualitative and quantitative results.
6) RelGAN

Lin et al. [77] proposed a multi-domain image-to-image translation method based on relative attributes called RelGAN. The model of RelGAN is shown in Fig. 33 .
FIGURE 33.

The model of RelGAN [77] .

Show All

The loss function of the method is formulated as: \begin{align*} \min \limits _{D} L^{D}=&-L_{Real} +\lambda _{1} L_{Match}^{D} +\lambda _{2} L_{Interp}^{D} \tag{38}\\ \min \limits _{G} L^{G}=&L_{Real} +\lambda _{1} L_{Match}^{G} +\lambda _{2} L_{Interp}^{G} +\lambda _{3} L_{Cycle} \\&+\,\lambda _{4} L_{Self} \;+\lambda _{5} L_{Ortho}\tag{39}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} \min \limits _{D} L^{D}=&-L_{Real} +\lambda _{1} L_{Match}^{D} +\lambda _{2} L_{Interp}^{D} \tag{38}\\ \min \limits _{G} L^{G}=&L_{Real} +\lambda _{1} L_{Match}^{G} +\lambda _{2} L_{Interp}^{G} +\lambda _{3} L_{Cycle} \\&+\,\lambda _{4} L_{Self} \;+\lambda _{5} L_{Ortho}\tag{39}\end{align*}

The method takes relative attributes as input to describe the selected attributes that need to be changed. It is able to produce images by changing specific properties of interest in a continuous manner and keep the other attributes unchanged. RelGAN helps to improve interpolation quality by training on real-valued relative attributes instead of binary-valued attributes with additional discriminators. It is can be used for facial attribute transfer and interpolation. Furthermore, it achieves quantitative and qualitative results in multi-domain image-to-image translation tasks.
7) Other Methods

Li et al. [78] proposed an Attribute Guided UIT (Unpaired Image-to-Image Translation) approach termed AGUIT, which can perform image translation tasks by adopting a novel semi-supervised learning process and decomposing the image representation into domain-invariant content code and domain-specific style code. Chang et al. [79] proposed an image-to-image translation approach called Sym-parameterized Generative Network (SGN), and it focuses on the loss area and infers translations of images in mixed domains by learning the combined characteristics of each domain. Tomei et al. [80] proposed a semantic-aware approach that can reduce the gap between visual features of artistic and realistic data by translating artworks to photo-realistic visualizations. Mo et al. [81] proposed an unsupervised image-to-image translation approach called instance-aware GAN (InstaGAN), which can not only incorporate the instance information but also improve the multi-instance transfiguration.

The style transfer method widely adopts an encoder-decoder-discriminator (EDD) architecture and can produce diverse outputs. However, it may generate images with artifacts sometimes. Besides, the training may be unstable and there may have mode collapse problems.
SECTION V.
Image Editing
A. Image Editing

The image editing is an interesting but challenging task in computer vision. It mainly manipulates images through color and geometric interactions to complete tasks such as image deformation and blending. Recently, image editing based on deep learning has received more and more attention, especially with the development of GANs. Image editing using GANs has made great progress and becomes a highly recognized subject in computer vision. A series of image editing methods have appeared.
1) SC-FEGAN

Jo and Park [82] proposed a face editing system called SC-FEGAN based on Generative Adversarial Network, and it can synthesize images with high quality by using intuitive user inputs. The architecture of SC-FEGAN is shown in Fig. 34 .
FIGURE 34.

The network architecture of SC-FEGAN [82] .

Show All

The loss functions are shown below: \begin{align*} L_{G\_{}SN}=&-IE[D(I_{comp})] \tag{40}\\ L_{G}=&L_{per-pixel} +\alpha L_{percept} +\beta L_{G\_{}SN} \\&+\,\gamma (L_{style} (I_{gen})+L_{style} (I_{comp}))+\nu L_{tv} \\&+\,\varepsilon IE[D(I_{gt})^{2}] \tag{41}\\ L_{D}=&IE[1-D(I_{gt})]+IE[1+D(I_{comp})]+\theta L_{GP} \\ {}\tag{42}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} L_{G\_{}SN}=&-IE[D(I_{comp})] \tag{40}\\ L_{G}=&L_{per-pixel} +\alpha L_{percept} +\beta L_{G\_{}SN} \\&+\,\gamma (L_{style} (I_{gen})+L_{style} (I_{comp}))+\nu L_{tv} \\&+\,\varepsilon IE[D(I_{gt})^{2}] \tag{41}\\ L_{D}=&IE[1-D(I_{gt})]+IE[1+D(I_{comp})]+\theta L_{GP} \\ {}\tag{42}\end{align*}

SC-FEGAN is a face editing method that uses a free-form mask, sketch and color as an input. The method can restore the area of any shape and reconstruct detail-rich textures of large regions. It generates images guided with sketches and color by using an end-to-end trainable convolutional network and free-form user input with color and shape. In addition, the method is able to generate realistic results by training an additional style loss. It can generate high quality and realistic results with the proposed network architecture and loss functions.
2) FE-GAN

Dong et al. [83] proposed an image editing approach called Fashion Editing Generative Adversarial Network (FE-GAN) by using a multi-scale attention normalization. The architecture of FE-GAN is shown in Fig. 35 .
FIGURE 35.

The network architecture of FE-GAN [83] .

Show All

The objective function of the free-form parsing network is formulated as: \begin{equation*} L_{free-form-parser} =\gamma _{1} L_{parsing} +\gamma _{2} L_{feat} +\gamma _{3} L_{adv}\tag{43}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{free-form-parser} =\gamma _{1} L_{parsing} +\gamma _{2} L_{feat} +\gamma _{3} L_{adv}\tag{43}\end{equation*}

The objective function of the parsing-aware inpainting network is formulated as: \begin{align*}L_{inpainter} =&\lambda _{1} L_{mask} +\lambda _{2} L_{foreground} +\lambda _{3} L_{face} +\lambda _{4} L_{faceTV} \\& \qquad {+\,\lambda _{5} L_{perceptual} +\lambda _{6} L_{style} +\lambda _{7} L_{adv} } \tag{44}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}L_{inpainter} =&\lambda _{1} L_{mask} +\lambda _{2} L_{foreground} +\lambda _{3} L_{face} +\lambda _{4} L_{faceTV} \\& \qquad {+\,\lambda _{5} L_{perceptual} +\lambda _{6} L_{style} +\lambda _{7} L_{adv} } \tag{44}\end{align*}

FE-GAN uses sketches and color strokes to manipulate and edit fashion images. It is able to leverage the semantic structural information to edit fashion images by free-forms sketches and sparse color strokes. The method controls the human parsing generation with the sketch and color by using a free-form parsing network. It renders detailed textures with semantic guidance from the human parsing map by using a parsing-aware inpainting network. Furthermore, it improves the quality of the generated images by using a new attention normalization layer in the decoder of the inpainting network. The method can generate high-quality images with convincing details by using a foreground-based partial convolutional encoder.
3) Mask-Guided Portrait Editing

Gu et al. [84] proposed a portrait editing framework based on mask-guided conditional GANs, and it uses the face masks to guide the image generation. Its framework is shown in Fig. 36 .
FIGURE 36.

The framework for mask-guided portrait editing in [84] .

Show All

The loss function is: \begin{align*}L_{G} =&\lambda _{local} L_{local} +\lambda _{global} L_{global} +\lambda _{GD} L_{GD} \\& \qquad \qquad \qquad \qquad \qquad \qquad \qquad {+\,\lambda _{GP} L_{GP} } \tag{45}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}L_{G} =&\lambda _{local} L_{local} +\lambda _{global} L_{global} +\lambda _{GD} L_{GD} \\& \qquad \qquad \qquad \qquad \qquad \qquad \qquad {+\,\lambda _{GP} L_{GP} } \tag{45}\end{align*}

The method is guided by face masks which can generate diverse images with high-quality. It controls the synthesis and editing of facial images by learning feature embeddings for each face component separately. It also helps to improve the performance of image translation and local face editing. This method can edit face components in the generated images with the help of changeable input facial masks and the source image. Moreover, it leverages the input masks to synthesize facial data which can be used for the face paring model. The method can produce realistic outputs and realize face editing.
4) FaceShapeGene

Xu et al. [85] proposed a face image editing approach termed FaceShapeGene, and it can compute a disentangled shape representation for face images. The pipeline of FaceShapeGene is shown in Fig. 37 .
FIGURE 37.

The pipeline of FaceShapeGene [85] .

Show All

The objective function is: \begin{align*}L_{total} =&L_{cycle,I} +\lambda _{CL} L_{cyc,L} +\lambda _{GI} L_{GAN,I} \\& \qquad \qquad \qquad ~~\qquad {+\,\lambda _{GL} L_{GAN,L} +\lambda _{ID} L_{ID} } \tag{46}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}L_{total} =&L_{cycle,I} +\lambda _{CL} L_{cyc,L} +\lambda _{GI} L_{GAN,I} \\& \qquad \qquad \qquad ~~\qquad {+\,\lambda _{GL} L_{GAN,L} +\lambda _{ID} L_{ID} } \tag{46}\end{align*}

The FaceShapeGene realizes face editing tasks by encoding the shape information of each semantic facial part into a 1D latent vector separately. The authors proposed a shape-remix network to recombine the part-wise latent vectors from different individuals which produces remixed face shape in the form of a label map. They also use a conditional label-to-face transformer to perform part-wise face editing while preserving the identity of the subject. Furthermore, it trains the system in an unsupervised manner by using a cyclic training strategy. The method can disentangle the shape features of different semantic parts correctly and achieve partial editing with realistic results.
5) Other Methods

Shen et al. [86] proposed a semantic face editing approach termed InterFaceGAN, which can synthesize high-fidelity image by semantic face editing in latent space. Portenier et al. [87] proposed a face image editing approach called FaceShop, and it can produce high quality and semantically consistent results.

In recent years, image editing using GANs has made great progress and achieve good results. Mask-guided image editing method is widely used and it can synthesize realistic with high quality. However, most of the methods proposed at present can only be used for the task of face portrait editing, and there will be artifacts and blurred results when performing whole-body editing.
SECTION VI.
Cartoon Generation
A. Cartoon Generation

The cartoon is popular with young people because of its interesting story. GANs have also attracted the interest of researchers in the field of cartoon generation, and they proposed a series of fresh and interesting cartoon generation methods.
1) CartoonGAN

Chen et al. [88] proposed a photo cartoonization solution called CartoonGAN, and it can transform a photo of a real-world scene into a cartoon style image. The architecture of CartoonGAN is shown in Fig. 38 .
FIGURE 38.

The architecture of the CartoonGAN [88] .

Show All

The objective loss function is: \begin{align*} (G^{\ast },D^{\ast })=&\arg \min \limits _{G} \max \limits _{D} L(G,D) \\=&L_{adv} (G,D)+\omega L_{con} (G,D)\tag{47}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} (G^{\ast },D^{\ast })=&\arg \min \limits _{G} \max \limits _{D} L(G,D) \\=&L_{adv} (G,D)+\omega L_{con} (G,D)\tag{47}\end{align*}

CartoonGAN is a cartoon generation method based on a generative adversarial network, which is easy to use by training with unpaired photos and cartoon images. The method is capable of generating high-quality cartoon images with clear edges and smooth color shading from real-world photos, following the style of specific artists. It copes with the substantial style variation between photos and cartoons by proposing a semantic content loss, which is formulated as a sparse regularization of high-level feature maps in the VGG network. CartoonGAN is able to preserve clear edges by proposing an edge-promoting adversarial loss. In addition, it is capable of improving the convergence of the network to the target manifold by introducing an initialization phase. The method can produce cartoon images with high-quality from real-world photos.
2) PI-REC

You et al. [89] proposed an image reconstruction approach called PI-REC, which can generate images from binary sparse edge and flat color domain. The architecture of PI-REC is shown in Fig. 39 .
FIGURE 39.

The network architecture of PI-REC [89] .

Show All

The loss function is calculated as below: \begin{align*} L_{G_{1}}=&\alpha L_{per-pixel}\! +\!\beta L_{GAN-G} \!+\!\gamma L_{feature} +\delta L_{style}\qquad ~~\tag{48}\\ L_{D_{1}}=&L_{GAN-D}\tag{49}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} L_{G_{1}}=&\alpha L_{per-pixel}\! +\!\beta L_{GAN-G} \!+\!\gamma L_{feature} +\delta L_{style}\qquad ~~\tag{48}\\ L_{D_{1}}=&L_{GAN-D}\tag{49}\end{align*}

PI-REC is able to reconstruct images by inputting binary sparse edge and flat color domain, which can not only control the content and style of generated images freely and accurately but also produce refined reconstruction results with high-quality. The method consists of three phases: Imitation Phase to initialize the networks, Generating Phase aims at reconstructing preliminary images, and Refinement Phase to fine-tune preliminary images and produce outputs with details. Besides, it can be used for hand-drawn draft translation tasks by utilizing parameter confusion operation, which obtains remarkable results. Furthermore, it is able to create anime characters by feeding the well-trained model with edge and color domain extracted from realistic photos, which improves the controllability and interpretability and generates abundant high-frequency details.
3) Internal Representation Collaging

Suzuki et al. [90] proposed an image synthesis strategy based on CNN, and it can manipulate the feature-space representation of the image in a trained GAN model to change the semantic information of an image over an arbitrary region. The algorithm of applying the feature-space collaging is shown in Fig. 40 .
FIGURE 40.

The algorithm of applying the feature-space collaging [90] .

Show All

The method can be used to edit artificial images or real images by using spatial conditional batch normalization (sCBN), which is a type of conditional batch normalization with user-specifiable spatial weight maps. It can modify the intermediate features directly and by using feature-blending in any GAN with conditional normalization layers. Besides, it is able to be used to edit anime face which can synthesize realistic results. However, the problem is that it may perform poorly in the transformation of a specific individual, and some information is bound to be lost in the process of projecting the target images to the restricted space of images.
4) U-GAT-IT

Kim et al. [91] proposed an image-to-image translation approach termed U-GAT-IT, and it can generate realistic anime images by using adaptive layer-instance normalization. The architecture of U-GAT-IT is shown in Fig. 41 .
FIGURE 41.

The architecture of U-GAT-IT [91] .

Show All

The objective function is: \begin{align*}&\min \limits _{G_{s\to t},G_{t\to s},\eta _{s},\eta _{t}} \max \limits _{D_{s},D_{t},\eta D_{s},\eta D_{t}} \lambda _{1} L_{gan} +\lambda _{2} L_{cycle} +\lambda _{3} L_{identity} \\& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad {+\,\lambda _{4} L_{cam} } \tag{50}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}&\min \limits _{G_{s\to t},G_{t\to s},\eta _{s},\eta _{t}} \max \limits _{D_{s},D_{t},\eta D_{s},\eta D_{t}} \lambda _{1} L_{gan} +\lambda _{2} L_{cycle} +\lambda _{3} L_{identity} \\& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad {+\,\lambda _{4} L_{cam} } \tag{50}\end{align*}

The U-GAT-IT is an unsupervised image-to-image translation approach, which can translate images with holistic or large shape changes by handling the geometric changes between domains. It incorporates a learnable normalization function and an attention module to distinguish between the source and target domains. The attention module is based on the attention map and obtained by the auxiliary classifier to guide the model to focus on more important regions. Furthermore, the attention-guided model is able to control the amount of change in shape and texture flexibly with the proposed Adaptive Layer-Instance Normalization (AdaLIN). Moreover, it is a method that can produce anime face with more visually pleasing results based on the attention module and AdaLIN.
5) Landmark Assisted CycleGAN

Wu et al. [92] proposed a cartoon face generation approach based on CycleGAN, and it trains with unpaired data between real faces and cartoon ones, the architecture is shown in Fig. 42 .
FIGURE 42.

The architecture of the cartoon-face landmark-assisted CycleGAN [92] .

Show All

The landmark consistency loss is: \begin{equation*} L_{c} (G_{(X,L)\to Y})=\left \|{ {R_{Y} (G_{(X,L)\to Y} (x,\ell))-\ell } }\right \|_{2}\tag{51}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{c} (G_{(X,L)\to Y})=\left \|{ {R_{Y} (G_{(X,L)\to Y} (x,\ell))-\ell } }\right \|_{2}\tag{51}\end{equation*}

The method is able to generate a cartoon face with high quality which captures the essential facial features of a person by proposing a landmark consistency loss and training a local discriminator in CycleGAN. It can produce cartoon faces with high-quality based on the conditional generator and discriminator, which enforces structural consistency in landmarks. Besides, it is a method guided by facial landmarks that can constrain the facial structure between two domains and can generate impressive high-quality cartoon faces according to the input human faces.
6) Other Methods

Taigman et al. [93] proposed an image generation approach called Domain Transfer Network (DTN), which can transfer from face photos to emoji. Jin et al. [94] proposed a method, which can generate facial images of anime characters with a promising result. Li [95] proposed an image translation method called TwinGAN, and it achieves unsupervised image translation from a human to anime characters. Ma et al. [96] proposed an instance-level image translation approach called DA-GAN, which can synthesize animation face from a human face. Hamada et al. [97] proposed an anime generation method called Progressive Structure-conditional Generative Adversarial Networks (PSGAN), and it is able to generate character images with full-body and high-resolution based on structural information. Gokaslan et al. [98] proposed an image-to-image translation approach termed the GANimorph, which can translate human faces to anime faces. Cao et al. [99] proposed a photo-to-caricature translation approach, and it is able to transfer from face photos to caricatures. Xiang and Li [100] proposed a Generative Adversarial Disentanglement Network, which can generate high-fidelity anime portraits.

Cartoon generation based on GANs mostly uses the normalization method. It can produce high-quality cartoon images from real-world photos. However, it sometimes generates images with artifacts. Besides, it may not perform well in the image translation of a particular individual.
SECTION VII.
Discussion

So far, we have discussed some of the applications of GANs in the field of image synthesis. Table 1 gives a summary and comparison between different methods based on GANs in terms of pros and cons.
TABLE 1 A brief summary of different GANs.

A. Open Questions

GANs can not only learn a highly nonlinear mapping from latent space to data space but also can utilize a large amount of unlabeled image data for deep representation learning. Compared with other frameworks, GANs tend to produce better results with realistic and clear images, which have attracted extensive attention. Due to the great potential and wide applicability of GANs, the researchers are constantly attracted to the research of GANs. However, there are still some problems that have not been completely solved in training and evaluating GANs, such as mode collapse, unstable training problem, and vanishing gradient problem. In addition, GANs are also faced with the problem of non-convergence and sensitivity to hyperparameters. At present, these problems remain to be solved, which need continuous research and efforts from the researchers, and many improved GAN-variants have emerged, including Least Square GAN (LSGAN) [101] , Wasserstein GAN (WGAN) [102] , WGAN-GP [103] , and Spectral Normalized GANs (SNGAN) [104] . These models not only greatly improved the quality and the stability of GANs, but also make it easy to converge and aim to solve the problem of unstable training. However, the problem of collapse during training and the mode collapse have not been completely solved due to the high dimensional characteristics of image data. By using maximum likelihood pre-training, with the help of adversarial fine-tuning is now an effective solution to deal with mode collapse. Other techniques that can be used to stabilize and improve GANs training performance like large batch sizes, dense rewards and discriminator regularization [105] . Recently, Liu et al. [106] proposed an approach called WGAN-QC, which can stabilize and speed up the training process based on the quadratic transport cost. Petroski Such et al. [107] proposed an approach called Generative Teaching Networks (GTNs), and it can stabilize and prevent mode collapse of GAN training.

On the other hand, how to evaluate the quality of generated images still lacks effective means. Generative Adversarial Networks are the most popular image generation methods today, but the way to evaluate and compare images produced by GANs is still an extremely challenging task. Many earlier studies on image synthesis based on GANs only used subjective visual assessments. Although it is very hard to quantify the quality of generated images, some studies of evaluating the GANs have begun to appear. For example, the Inception score (IS) [108] and Fréchet Inception Distance (FID) [109] are the most widely adopted evaluation metrics for quantitatively evaluating generated images. Besides, Bau et al. [110] proposed an approach to visualize and understand GANs at the scene-level. Zhou et al. [111] proposed a method called HUMAN EYE PERCEPTUAL EVALUATION (HYPE) to establish a gold standard human benchmark for generative realism. Grnarova et al. [112] propose an evaluation measure to monitor the training progress, which is able to detect failure modes, like unstable mode collapse and divergence. Other evaluation methods are studied as [113] , [114] .

With the successive development of methods for training and evaluation of GANs and the great progress that has been done on the GANs, the generative adversarial networks will be more and more widely used in various applications.
B. Future Opportunities

With the impetus of GANs, the research in the field of computer vision has been greatly developed in recent years, and various applications have emerged. Most of these applications involve image processing. Although there have been some studies involving video processing, such as video generation [115] , video colorization [116] , [117] , video inpainting [118] , motion transfer [119] , and facial animation synthesis [120] – [123] , the research on video using GANs is limited. In addition, although GANs have been applied to the generation and synthesis of 3D models, such as 3D colorization [124] , 3D face reconstruction [125] , [126] , 3D character animation [127] , and 3D textured object generation [128] , the results are far from perfect. At present, GANs are still based on large amounts of training data. It is an inevitable trend to reduce the use of data in the future. Although there is already some weakly supervised learning research [129] , [130] , these are still very limited, and the results are far from optimal.

Besides, GANs have great potential in data augmentation, due to its ability to synthesize high-quality images, especially in areas with data paucity, such as medical image analysis [131] . Frid-Adar et al. [132] presented a GAN-based method to generate synthetic medical images for data augmentation, which can improve the performance of medical problems with limited data. Han et al. [133] proposed a two-step GAN-based data augmentation method to minimize the number of annotated images required for the medical imaging tasks. Sandfort et al. [134] used a GAN-based method for data augmentation to improve the performance of tasks in medical imaging. Han et al. [135] proposed a data augmentation approach called Conditional Progressive Growing of GANs (CPGGANs) to minimize expert physicians’ annotation in medical applications. Schlegl et al. [136] presented an approach called fast AnoGAN (f-AnoGAN), which can identify anomalous images on a variety of biomedical data. Han et al. [137] proposed a data augmentation method called 3D Multi-Conditional GAN (MCGAN), and it can help to overcome medical data paucity.

Other directions that are equally noteworthy, such as in the modular [138] and game areas [139] , have rarely been studied. Recently, Lin et al. [140] proposed a novel method called Conditional Coordinate GAN (COCO-GAN), which uses the spatial coordinates as the condition to generate images by parts, and it achieves a high generation quality. Particularly, this approach can generate images larger than any training sample and can be used for large field-of-view generation. We conclude that there are opportunities for future research and application on GANs, especially in these areas.
SECTION VIII.
Conclusion

In this paper, we reviewed some basics of GANs and described some applications in the field of image synthesis based on GANs. The pros and cons of these GANs applications are also provided. Besides, we summarized the methods used in GANs applications which improved the performance of generated images. Although the research on GANs is becoming more and more mature, GANs are still faced with some challenges, such as unstable training and hard to evaluate, for which we introduced some methods for training and evaluating of GANs. We think there are some likely future research directions, such as video generation, facial animation synthesis, and 3D face reconstruction. The performance of GANs will continue to improve as various GAN-variants are proposed and GANs applications still need exploring. We expect more interesting applications based on GANs to appear in the future.

Authors
Figures
References
Citations
Keywords
Metrics
More Like This
Range Measurement by Computer Vision Systems Based on Invariant Moments

2021 25th International Computer Science and Engineering Conference (ICSEC)

Published: 2021
Image resolution improvement in digital holographic microscope with image restoration and PSF upscaling

Technical Digest of the Eighteenth Microoptics Conference

Published: 2013
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
