Mazin Hnewa and Hayder Radha

AUTONOMOUS DRIVING: PART 2

Object Detection Under Rainy Conditions for Autonomous Vehicles
A review of state-of-the-art and emerging techniques

Digital Object Identifier 10.1109/MSP.2020.2984801 Date of current version: 24 December 2020

©SHUTTERSTOCK.COM/MONOPOLY919

Advanced automotive active safety systems, in general, and autonomous vehicles, in particular, rely heavily on visual data to classify and localize objects, such as pedestrians, traffic signs and lights, and nearby cars, to help the corresponding vehicles maneuver safely in their environments. However, the performance of object detection methods could degrade rather significantly in challenging weather scenarios, including rainy conditions. Despite major advancements in the development of deraining approaches, the impact of rain on object detection has largely been understudied, especially in the context of autonomous driving.
Introduction
Visual data plays a critical role in enabling automotive advanced driver-assistance systems and autonomous vehicles to achieve high levels of safety while the cars and trucks maneuver in their environments. Hence, emerging autonomous vehicles are employing cameras and deep learningbased methods for object detection and classification [1]– [3]. These methods predict bounding boxes that surround detected objects and classify probabilities associated with each bounding box. In particular, convolutional neural network (CNN)-based approaches have shown very promising results in the detection of pedestrians, vehicles, and other objects [4]–[10]. These neural networks are usually trained using a large amount of visual data captured in favorable clear conditions. However, the performance of such systems in challenging weather, such as rainy conditions, has not been thoroughly surveyed or studied.
The quality of visual signals captured by autonomous vehicles can be impaired and distorted in adverse weather conditions, most notably in rain, snow, and fog. Such conditions minimize the scene contrast and visibility, and this could lead to a significant degradation in the ability of the vehicle to detect critical objects in the environment. Depending on the visual effect, adverse weather conditions can be classified as steady (such as fog, mist, and haze) or dynamic, which have more complex effects (such as rain and

1053-5888/21©2021IEEE

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

53

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

snow) [11]. In this article, we focus on rain because it is the highlighting the urgent need for developing new paradigms

most common dynamic challenging weather condition that for addressing the challenges of autonomous driving in severe

impacts virtually every populated region of the globe. Fur- weather conditions. Although generative model-based image

thermore, there has been a great number of recent efforts that translation and domain adaptation approaches do show some

attempt to mitigate the effect of rain in the context of visual promise, one overarching conclusion that we aim to convey

processing. While addressing the effect of other weather con- through this article is that current solutions do not adequately

ditions has been receiving some, yet minimal, attention, the mitigate the realistic challenges for autonomous driving in

volume of work regarding the mitigation of rain is far more diverse weather conditions. This overarching conclusion opens

prevalent and salient within different research communities. the door for the research community to pursue and explore new

It is worth noting that rain consists of countless drops frameworks that address this timely and crucial problem area.

that have a wide range of sizes and complex shapes, and rain The architectures highlighting the main parts of this article are

spreads quite randomly, with varying speeds when falling on highlighted in Figure 1.

roadways, pavement, vehicles, pedestrians, and other objects in

the scene. Moreover, raindrops naturally cause intensity varia- Object detection for autonomous vehicles

tions in images and video frames. In particular, every raindrop in clear and rainy conditions

blocks some of the light that is reflected by objects in a scene. The level of degradation in the performance of an object de-

In addition, rain streaks lead to low contrast and elevated lev- tection method, trained in certain conditions, is influenced

els of whiteness in visual data. Consequently, mitigating the heavily by 1) how different the training and testing domains

effect of rain on visual data is arguably one

are and 2) the type of deep learning-based

of the most challenging tasks that autonomous vehicles will have to perform, due to the fact that it is quite difficult to detect and

The impact of rain on object detection has

architecture used for object detection. Most recent object detectors are CNN-based networks, such as the single-shot multibox

isolate raindrops, and it is equally problem- largely been understudied, detector [9], region-based fully convolu-

atic to restore the information that is lost or occluded by rain.
Meanwhile, there has been noticeable

especially in the context of tional network [10], You Only Look Once

autonomous driving.

(YOLO) [8], RetinaNet [7], and Faster Regions With CNNs (R-CNNs) [6]. To that

progress in the development of advanced

end, we review two major classes of object

visual deraining algorithms [12]–[17]. Thus, one natural and detection frameworks that are both popular and representa-

intuitive solution for mitigating the effect of rain on active tive of deep learning-based approaches. As we see later in

safety systems and autonomous vehicles is to employ robust this tutorial, these two classes of architectures exhibit differ-

deraining algorithms and then apply the desired object detec- ent levels and forms of degradation in response to challeng-

tion approach to the resulting derained signal. State-of-the-art ing rainy conditions, and they also perform rather differently

deraining algorithms, however, are designed to remove the in conjunction with potential rain mitigation frameworks.

visual impairments caused by rain, while attempting to restore

In particular, we briefly describe the underlying architec-

the original signal with minimal distortion. Hence, the prima- tures for Faster R-CNN and YOLO as representatives of two

ry objective of these algorithms, in general, is to preserve the major classes of object detection algorithms. Faster R-CNN is

visual quality as measured by popular performance metrics, arguably the most popular of the object detection algorithms

such as the peak signal-to-noise-ratio and structure similar- that are based on a two-stage deep learning architecture; one

ity index (SSIM) [18]. These metrics, however, do not reflect stage is for identifying region proposals (RPs), and the second

a viable measure for analyzing the performance of the system is for refining and assigning class probabilities for the cor-

for more complex tasks, such as object detection.

responding regions. YOLO, on the other hand, is a represen-

The main objective of this article is to survey and pres- tative of detection frameworks that directly operate on the

ent a tutorial on state-of-the-art and emerging techniques whole image.

that are leading candidates for mitigating the influence

of rainy conditions on an autonomous vehicle’s ability to Deep learning-based methods for object detection

detect objects. In that context, our goal includes surveying The utility of CNNs for object detection was well established

and analyzing the performance of object detection methods prior to the introduction of the notion of RPs, commonly

that are representatives of state-of-the-art frameworks that known as R-CNN [4], where “R” stands for regions or RPs.

are being considered for integration into autonomous vehi- A fast version of R-CNN was later introduced [5], and then

cles’ artificial intelligence (AI) platforms. Furthermore, Ren et al. [6] presented the idea of the RP network (RPN) that

we survey and highlight the inherent limitations of leading shares convolutional layers with Fast R-CNN [5]. The RPN is

deraining algorithms, deep learning-based domain adapta- merged with the Fast R-CNN into one unified network that is

tion, and image translation frameworks in the context of known as Faster R-CNN to achieve more computationally ef-

rainy conditions.

ficient detection. Under Faster R-CNN, an input image is fed

While surveying a variety of relevant techniques in this to a feature extractor, such as the ZF model [19] or VGG-16

area, we present experimental results with the objective of [20], to produce a feature map. Then, the RPN utilizes this

54

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

feature map to predict RPs (regions in the image that could conditional class probabilities, given that it has an object. In

potentially contain objects of interest).

this case, there are potentially many wrongly predicted bound-

In that context, many RPs are quite overlapped with each ing boxes. To filter them out and provide the final detection

other, with significant numbers of pixels common among result, a threshold is used on the confidence scores of the pre-

multiple RPs. To filter out the substantial redundancy that dicted bounding boxes. Figure 2 illustrates the general archi-

might occur with such a framework, nonmaximum suppression tecture of YOLO.

(NMS) [21] is used to remove redundant regions while keeping the ones that have the highest prediction scores. Subsequently, each regional proposal that survives the NMS process is

Object detection performance for neural network architectures in clear and rainy conditions

used by a region-of-interest (RoI) pooling layer to crop the Here, we provide an insight into the level of degrada-

corresponding features from the feature map. This cropping tion caused by rainy conditions on the performance of the

process produces a feature vector that is fed to two fully two major deep learning architectures described previously.

connected layers: one predicts the offset

In particular, we focus on the following

values of a bounding box of an object with The quality of visual

fundamental question: how much degrada-

respect to the regional proposal, and the other predicts class probabilities for the predicted bounding box. Figure 2 shows a high-level architecture for Faster R-CNN.
On the other hand, Redmon et at. [8] proposed to treat object detection as a regres-

signals captured by autonomous vehicles can be impaired and distorted in adverse weather conditions.

tion a deep neural network that is trained in clear conditions will suffer when tested in rainy weather. In that context, we first describe the data set that we used for training and testing; this is followed by presenting some visual and numerical results. For

sion problem, and they developed a unified

the purpose of this tutorial, we needed a

neural network that is called YOLO to predict bounding boxes rich data set that was captured in diverse weather conditions.

and class probabilities directly from a full image in one evalua- Despite the fact that there are few notable data sets [22]–[24],

tion. Under YOLO, an input image is divided into a specific set which are quite popular among the computer vision and AI

of grid cells, and each cell is responsible for detecting objects research communities in terms of training deep NNs, there is

whose centers are located within that cell. To that end, each only one (arguably two [25], [26]) that is properly labeled and

cell predicts a certain number of bounding boxes, and it also annotated for our purpose and hence that could be used for

predicts the confidence scores for these boxes in terms of the training and testing for different weather conditions. In par-

likelihood that they contain an object. Furthermore, it predicts ticular, we use the Berkeley Deep Drive (BDD100K) data set

Input Image, Clear or Rainy

Clear Annotated Visual Data Training
Object-Detection Model
(a)

Clear Annotated Visual Data

Input Rainy Image

Image-to-Image Translation Training
Object-Detection Model
(c)

Detection Result
Detection Result

Input Rainy Image

Deraining Methods

Clear Annotated Visual Data Training
Object-Detection Model
(b)

Detection Result

Clear Annotated Rainy Unannotated

Visual Data

Visual Data

Input Rainy Image

Domain Adaptation
Training Object-Detection
Model
(d)

Detection Result

FIGURE 1. The architectures highlighting the main parts of the article. The (a) “Object Detection for Autonomous Vehicles in Clear and Rainy Conditions” section, (b) “Deraining in Conjunction With Object Detection” section, (c) “Unsupervised Image-to-Image Translation” section, and (d) “Domain Adaptation” section [25].

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

55

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

[25] because it contains image tagging for weather (i.e., each image in the data set is labeled with its weather condition, such as clear, rainy, foggy, and so on). Meanwhile, although some other data sets, such as nuScenes [26], might contain some visuals captured in rainy conditions, they do not have weather tagging. Hence, ­choosing the BDD100k data set was influenced by the fact that we could select images illustrating a specific weather condition.
Moreover, the BDD100K has 100,000 video clips captured in diverse geographic, environmental, and weather conditions. It is worth noting that only one selected frame from each video is annotated with object bounding boxes as well as image-level tagging. Examples of annotated frames in clear and rainy weather are shown in Figure 3. In this article, we consider the four classes (vehicle, pedestrian, traffic light, and traffic sign) that are labeled and provided as ground-truth objects within the BDD100K data set. Naturally, these four classes are among

the most critical objects for an autonomous vehicle. In this tutorial, we use images that are captured in clear weather from the designated training set of the BDD100K to form our underlying training data set. We refer to this training data as the train clear set, which we used consistently to train the detection methods for the different scenarios covered in this article. For testing, we use a collection of clear weather images from the testing set of the BDD100K. We refer to this latter group as the test clear set. Table 1 gives the number of annotated objects in the train clear and test clear data sets.
One approach to demonstrate the impact of rain on object detection methods that are trained in clear conditions is by rendering synthetic rain [27]–[29] within the images of the test clear set. Then, the synthetic rainy data can be used to test the already trained object detection methods. The benefit of this approach is that one would have the exact same underlying content in both testing data sets in terms of the

Input Image

CNN Feature Extractor

Feature Map

GRL

RPN

Rol Pooling Layer

FCs

Filter by Feature Vector

NMS

for Each RP

RPs

Bounding Box Parameters

x

y

FC

w h

pc1

FC

pc2

Detection

pcn

Result

Class Probabilities

Domain Adaptation

GRL

CNN Layers

Image-Level Domain Classifier

Consistency Regularization

Instance-Level Domain Classifier

FCs

Input Image

Grid Cells

(a)

CNN Layers

FCs

(b)

Parameters of m Bounding Boxes and Their Confidence Scores

x1 xm

y1 w1

.

.

.

ym wm

h1 hm

c1 cm

ppcc12

...

pcn
Conditional Class Probabilities

For Each Cell

Filter Based on Confidence Scores
Detection Result

FIGURE 2. The high-level architectures of the detection methods that are used in this tutorial. The (a) Faster R-CNN and (b) YOLO. The domain adaptation of Faster R-CNN is explained in the “Domain Adaptation” section [25]. GRL: gradient reversal layer.

56

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

Weather: Clear

Weather: Clear

Weather: Clear

Weather: Rainy Weather: Rainy

(a) Weather: Rainy
(b) Weather: Rainy

Weather: Rainy Weather: Rainy

(c)
FIGURE 3. The examples of the annotated images in the BDD100K data set [25]. Images in (a) are tagged as clear weather, and images in (b) and (c) are tagged as rainy weather. However, images in (c) are wrongly tagged; they actually show clear or cloudy weather.

objects within the scene, with one set representing the original clear-weather content when the data was captured and another set with the synthetic rain. This would plainly show the impact of rain, as the visual objects are the same in both tested sets (the test clear set and a test synthetic rain set). However, from our extensive experience in this area, we noticed that most well-known rain simulation methods do not render realistic rain that viably captures actual and true rainy weather conditions, especially for a driving vehicle. Thus, when comparing the two scenarios, this discrepancy between synthetic and natural (real) rainy conditions will lead to domain mismatch. As a result, we do not test the detection methods using synthetic rain in our study because those techniques will not demonstrate the impact of true natural rain on a driving vehicle.
Alternatively, we use images captured in real rainy conditions from the training and testing sets of the BDD100K data set to test the object detection methods. It is worth noting that several images in the data set are wrongly tagged as rainy weather when they actually show clear or cloudy conditions, such as the examples in Figure 3(c). To solve this problem, we manually selected the images that were truly captured in rainy weather to form what we refer to as the test rainy set. Equally important, we elected to have both the test clear

Table 1. The number of annotated objects in the training and testing sets that are used in our study.

Set
Train clear Test clear Test rainy

Vehicles
149,548 13,721 13,724

Pedestrians
16,777 2,397 2,347

Traffic Signs
43,866 3,548 3,551

Traffic Lights
26,002 4,239 4,246

and test rainy sets include approximately the same number of annotated objects, as shown in Table 1, to provide statistically comparable results.
It is important to make one final critical note regarding the currently available data sets for training NNs designed for object detection. The lack of data sets captured in diverse conditions, including rain, snow, fog, and other weather scenarios, represents one of the most challenging aspects of achieving a viable level of training for autonomous vehicles. Even for the BDD100K data set, which is one of very few publicly available data sets with properly annotated objects captured in different weather conditions, there is not a sufficient amount of annotated visual content that is truly viable for training in rainy weather. This fundamental issue with the lack of real training data for rainy and other conditions

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

57

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

has clearly become a major obstacle, to the extent that leading high-tech companies working in the area have begun a focused effort designated specifically for collecting data in rainy conditions. For example, Waymo recently announced plans to begin collecting data for autonomous driving in rainy conditions [30].
Performance metric
To evaluate the detection performance, we compute the mean average precision (mAP). This metric has been the most popular performance measure since the time when it was originally defined in the PASCAL Visual Object Classes Challenge 2012 for evaluating detection methods [31]. To determine the mAP, a precision/recall curve is first computed based on the prediction result against the ground truth. A prediction is considered a true positive if its bounding box has 1) an intersection-over-union value greater than 0.5 relative to the corresponding ground-truth bounding box and 2) the same class label as the ground truth. Then, the curve is updated by making the precision monotonically decrease. This is achieved by setting the precision for recall r to the maximum precision obtained for any recall r 2 r. The AP is the area under the updated precision/recall curve. It is computed by numerical integration. Finally, the mAP is the mean of the AP among all classes.
Results and discussion
We trained the detection methods (Faster R-CNN and YOLO) using the train clear set, which is described in the “Object Detection Performance for Neural Network Architectures in Clear and Rainy Conditions” section. We used the same training settings and hyperparameters that were employed in the original papers [6], [8]. Then, we tested the trained models by using the test clear and test rainy sets to illustrate the impact of rain. Table 2 presents the AP for each class as well as the mAP evaluated based on the AP values of the classes. From the table, we observe that the mAP clearly declines in rainy weather compared to clear weath-

er using both Faster R-CNN and YOLO. Consequently, these results undoubtedly illustrate that the performance of an object detection framework that is trained using clear visuals could significantly degrade in rainy weather conditions. The performance decreases due to the fact that rain covers and distorts important details of the underlying visual features, which are used by detection methods to classify and localize objects. Figure 4 provides examples when the detection methods fail to perceive most objects in rainy conditions.
Moreover, one can notice that in rainy conditions, the AP for the pedestrian and traffic light classes declines more significantly than the decrease in performance for vehicle and traffic sign classes. This discrepancy in performance degradation for different objects is due to a variety of factors. For example, vehicles usually occupy larger regions within an image frame than other types of objects; hence, even when raindrops or rain streaks cover a visual of a vehicle, there are still sufficient features that can be extracted by the detection method. Furthermore, traffic signs are normally made from materials that have high reflectivity, which makes it easier for an object detection method to achieve higher accuracy, even when a traffic sign visual is distorted by some rain. Overall, in both cases, the important features needed for reliable detection are still salient within the underlying deep NNs of the detection algorithms. Nevertheless, rain could still impact the detection of vehicle and traffic signs, as shown in the bottom three rows of Figure 4.
Deraining in conjunction with object detection
Deraining methods attempt to remove the effect of rain and restore an image of a scene that has been distorted by raindrops or rain streaks, while preserving important visual details. In this tutorial, we review three recently developed deraining ­algorithms [14]–[16] that employ deep learning frameworks for the removal of rain from a scene. The high-level architectures of these methods are illustrated in Figure 5. In the following, we briefly describe these three deraining methods and

Table 2. The AP for each class and mAP evaluated based on the AP values of the classes.

Mitigating Technique
None (clear conditions*) None (rainy conditions**) Deraining: DDN [14] Deraining: DeRaindrop [15] Deraining: PReNet [16] Image translation: UNIT [32] Domain adaptation [33]

Faster R-CNN

V-AP
72.61 67.84 67 64.37 63.69 68.47 67.36

P-AP
40.99 32.58 28.55 29.27 24.39 32.76 34.89

TL-AP
26.07 20.52 20.02 18.32 17.4 18.85 19.24

TS-AP
38.12 35.04 35.55 33.33 31.68 36.2 35.49

mAP
44.45 39 37.78 36.32 34.29 39.07 39.24

YOLO-V3

V-AP P-AP
76.57 37.12 74.15 32.07 73.07 29.89 70.77 30.16 70.83 27.36 74.14 34.19 Not applicable

TL-AP
46.22 41.07 40.05 37.7 35.49 41.18

TS-AP
50.56 50.27 48.74 48.03 43.78 48.41

mAP
52.62 49.39 47.94 46.66 44.36 49.48

V-AP: vehicle AP; P-AP: pedestrian AP; TL-AP: traffic light AP; TS-AP: traffic sign AP; DDN: deep-detail network. *The top row shows the performance in clear conditions (i.e., using the test clear set), while all other rows show the performance in rainy conditions (i.e., using the test rainy set). **Significant degradation in performance can be observed due to rainy conditions (text in red) relative to the performance in clear conditions (top row). Improvements in performance by mitigating the effect of rain can be observed using generative model-based image translation and/or domain adaptation (highlighted in bold). Meanwhile, deraining algorithms do not improve, and most of the time further degrade, the performance.

58

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

­highlight their limitations when employing them in conjunction with object detection methods.
Deep detail network
Fu et at. [14] proposed a deep detail network (DDN) to remove rain from a single image. They employed a CNN, which is a residual neural network (ResNet) [34], to predict the difference between clear and rainy images, and used this difference to remove rain from a scene. In particular, the DDN exploits only the rainy image’s high-frequency details, and it uses such details as input to the ResNet while ignoring the low-frequency background (interference) of the same underlying scene.
Attentive generative adversarial network
Qian et at. [15] proposed an attentive generative adversarial network (GAN) that is called DeRaindrop to remove raindrops from images. In this method, a GAN [35] with visual attention is employed to learn raindrop areas and their surroundings. The first part of the generative network, known

as the attentive recurrent network (ARN), produces an attention map to guide the next stage of the DeRaindrop framework. The ARN includes the ResNet, long short-term memory (LSTM) [36], and CNN layers. The second stage of DeRaindrop, which is known as the contextual autoencoder, operates on the attention map, and hence it focuses on (or pays more attention to) the raindrop areas. The overall process from the two stages is expected to clean images free of raindrops. The architecture also includes a discriminative network, which assesses the generated rain-free images to verify that they are similar to real ones that have been used during the training process.
Progressive image deraining network
Ren et at. [16] proposed a PReNet to recursively remove rain from a single image. At each iteration, some rain is removed, and the remaining rain can be progressively erased during subsequent iterations. Consequently, after a certain number of iterations, most of the rain should be removed, leading to a rain-free, quality image. In addition to several

(a)

(b)

(c)

FIGURE 4. The (a) ground truth shown with example detection results using (b) Faster R-CNN and (c) YOLO for different visual scenes from the test rainy set [25].

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

59

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

residual blocks of ResNet, the PReNet includes a CNN layer meaningful information and distinctive features of a scene

that operates on both the original rainy image and the cur- while attempting to remove the effect of rain.

rent output image. The PReNet also includes another CNN

In particular, it is rather easy to observe that state-of-the-

layer to generate the output image. Furthermore, a recur- art deraining algorithms smooth out the edges of objects in

rent layer is appended to exploit dependencies in the deep an image, which leads to a loss of critical information and

features across iterations via convolution-

features, which are essential for enabling

al LSTM. To train the PReNet, a single Relying purely on state-

the detection algorithms to classify and

negative SSIM [18] or mean-square-error loss is used.
Results and discussion
To demonstrate the performance of the de-

of-the-art deraining solutions does not represent a viable approach for mitigating

localize objects. The images in the top two rows of Figure 6, representing outputs of Faster R-CNN and YOLO, show some of the objects that are not detected after using the deraining methods but that are success-

raining methods outlined previously, we the impact of rain.

fully detected if rainy images are directly

apply the pretrained deraining models pro-

used as input into the detection algorithms.

vided by the corresponding authors to the test rainy set as a

A related critical issue to highlight for current derain-

prepossessing step. After applying the deraining algorithms, ing algorithms is their inability to remove natural raindrops

which are anticipated to remove the rain from the input vi- found in realistic scenes captured by moving vehicles. The

sual data and generate rain-free clear visuals, we feed root cause of this issue is the fact that deraining algo-

the derained images into the object detection methods. Table rithms have been largely designed and tested using synthetic

2 shows the performance of the detection methods after ap- rain visuals superimposed on the underlying scenes. What

plying the deraining approaches. It can be seen that the der- aggravates this issue is that, at least in some cases, the back-

aining algorithms actually degrade the detection performance ground environments employed to design and test deraining

when compared to directly using the rainy images as input algorithms are predominantly static scenes with a minimal

into the corresponding detection frameworks. This is true number of moving objects. Consequently, the salient dif-

for both Faster R-CNN and YOLO. One important factor for ferences between such synthetic scenarios and the realistic

this degradation in performance is that the deraining process environment encountered by a vehicle that is moving through

tends to smooth out the input image, and hence it distorts the natural rain represents a domain mismatch that is too

Low-Pass Filter

Base Layer

Input Image

– +
Detail Layer

ResNet

+

Negative Residual Clean Image/Rainy Image

(a)

Output Image

Input Image

ResNet

ARN LSTM

CNN Layers
Attention Map (b)

Contextual Autoencoder
CNN Layers With Skip Connections

Output Image

Input Image

CNN Layer

LSTM

ResNet

CNN Layer

(c)

Output Image

FIGURE 5. The general architectures of the used deraining methods, including (a) the DDN [14], (b) DeRaindrop [15], and (c) progressive recurrent ­network (PReNet) [16]. LSTM: long short-term memory; ARN: attentive recurrent network.

60

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

much to handle for current deraining algorithms, and this leads to the algorithms’ failure under realistic conditions for autonomous vehicles. Hence, overall, we believe that relying purely on state-of-the-art deraining solutions does not represent a viable approach for mitigating the impact of rain on object detection. The images in Figure 6(a) and (b), especially some of the cases in the bottom two rows, illustrate examples of the failure of deraining methods to improve the performance of object detection.

Alternative training approaches for deep learning-based object detection
The requirement that autonomous driving systems must work reliably in different weather conditions is at odds with the fact that the training data are usually collected in dry weather with good visibility. Thus, the performance of object detection algorithms degrades in challenging weather conditions, as we showed in the first “Results and Discussion” section.

Ground Truth

Without Deraining

DDN (a)

DeRainDrop

PReNet

Ground Truth

Without Deraining

DDN (b)

DeRainDrop

PReNet

FIGURE 6. The example detection results for different visual scenes where no deraining methods were employed and where deraining methods (DDN [14], DeRaindrop [15], and PReNet [16]) were used in conjunction with detection methods. Objects were perceived using (a) Faster R-CNN [6] and (b) YOLO [37].

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

61

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

One simple approach to address this problem is to train a given CNN for the detection of objects using images captured in real rainy weather. As we highlighted earlier, sufficient annotated data sets captured by moving vehicles in realistic urban environments in natural rainy conditions are not readily available. To that end, and in spite of the fact that some data sets are available, the very few data sets captured under real rainy conditions are not properly annotated [25]. Having such small data sets inherently makes them inadequate to reliably train deep learning architectures for objection detection. Furthermore, annotating the available data captured in real rainy conditions with accurate bounding boxes is an expensive and time-consuming process.
An alternative approach for addressing the lack-of-real-data issue is to train detection methods using synthetic rain data. However, and as we highlighted earlier, the trained methods generalize poorly on real data due to the domain shift between synthetic and natural rain. To solve this issue, we review approaches that can be employed for training the detection methods using annotated clear data in conjunction with unannotated rainy data. In particular, we review and survey two emerging frameworks for addressing this critical issue: image translation and domain adaptation.
Unsupervised image-to-image translation
Image-to-image translation (I2IT) is a well-known computer vision framework that translates images from one domain (e.g., captured in clear weather) to another domain (e.g., rainy conditions) while preserving the underlying and critical visual content of the input images. In short, I2IT attempts to learn a joint distribution of images in different domains. The learning process can be classified into a supervised setting, where the training data set consists of paired examples of the same scene captured in both domains (e.g., clear and rainy conditions), and an unsupervised setting, where different examples of both domains are used for training; hence, these examples do not have to be taken from the same corresponding scene.
The unsupervised case is inherently more challenging than supervised learning. More importantly, to address the main issue we face in the context of the lack of data needed to train object detection architectures in realistic conditions, we consequently need an unsupervised setting. In particular, the requirement of having a very large set of image pairs, where each pair of images must be of the same scene captured in different domains, renders supervised I2IT solutions virtu-

ally useless for our purpose. In fact, this requirement imposes more constraints than the lack-of-data issue that we are already trying to address. Hence, and despite the availability of wellknown supervised learning-based techniques in this area [38], [39], we have to resort to unsupervised solutions to address the problem at hand.
Recently, GANs [35] have been achieving very promising performance results in the area of image translation [32], [38]– [41]. In general, a GAN consists of a generator and a discriminator. The generator is trained to fool the discriminator, while the latter attempts to distinguish (or discriminate) between real natural images, on the one hand, and fake images, which are generated by the trained generator, on the other hand. By doing this, GANs align the distribution of translated images with real images in the target domain.
As mentioned earlier, data sets that have paired clear–rainy images in driving environments are not publicly available. As a result, we use unsupervised I2IT (UNIT) [32] to translate clear images to rainy ones since the training process for the UNIT framework does not require paired images of the same scene. In other words, UNIT training requires two independent sets of images, where one consists of images in one domain, and the other includes images in a different domain. The high-level architecture of the UNIT model is shown in Figure 7. First, the encoder network maps an input image to a shared latent code (a shared, compact representation of an image in both domains). Then, the generator network uses the shared latent code to generate an image in the desired domain.
To train the UNIT model that learns the mapping from clear images to rainy ones, we use the train clear set that consists of clear-weather annotated images as the source domain. For the target rainy domain, we extract a sufficiently large number of images from the rainy videos in the BDD100K data set. Subsequently, we apply images in the train clear set to the trained UNIT model to generate rainy images. We refer to the images that are generated by the UNIT model as the train-gen-rainy set. Examples of generated rainy images appear in Figure 8.
Eventually, we use the train-gen-rainy data set to train the detection methods. This is followed by using the test rain data set to evaluate the AP performance of the detection methods, which are now trained using the generated rainy set. We also calculate the mAP as we have done for other approaches. Table 2 shows the performance of detection methods that are trained using generated rainy images by the UNIT model.

Clear Image

Encoder Network

CNN Layers

ResNet

Shared Latent Code

Generator Network

ResNet

Transposed CNN Layers

Rainy Generated Image

FIGURE 7. The high-level architecture of the UNIT model [25] to generate images.

62

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

Domain adaptation
Domain adaptation is another potentially viable framework that could be considered to address the major challenges that we have been highlighting in this tutorial regarding 1) the salient mismatch between the two domains (clear and rainy weather conditions) and 2) the lack of annotated training data captured in rainy conditions. In particular, a domain a­ daptation ­framework [33] has been designed and developed specifically for Faster R-CNN due to the fact that it is among the most popular object detection approaches. (At this point, we are not aware of other domain adaptation frameworks that have been designed and developed for YOLO. Consequently, given the tutorial nature of this article, we review only domain adaptation that has already been developed for Faster R-CNN [33].) The framework developed in [33] adapts deep learning-based object detection to a target domain that is different from the training domain, without requiring any annotations in the target

domain. In particular, it employs the adversarial training strategy [35] to learn robust features that are domain invariant. In other words, it makes the distribution of features extracted from images in the two domains indistinguishable.
The architecture for the domain-adaptive Faster R-CNN model [33] is shown in Figure 2. There are two levels of domain adaptation that are employed. First, an image-level domain classifier is used. At this level, the global attributes (such as the image style, illumination, and so forth) of the input image are used to distinguish between the source and target domains. Thus, the (global) feature map resulting from the common CNN feature extractor of the Faster R-CNN detector is used as input toward the image-level domain classifier. Second, an instance-level domain classifier is employed. This classifier uses the specific features associated with a particular region to distinguish between the two domains. Hence, the instancelevel domain classifier uses the feature vector resulting from

(a)

(b)

FIGURE 8. The example images generated by the trained UNIT model. The (a) original clear images and (b) generated rainy images [25].

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

63

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

the FCs at the output of the RoI pooling layer of the Faster

To achieve these contradictory objectives, a GRL [42] is

R-CNN detector. The two classifiers, the image- and instance- employed. Thus, the GRL is a bidirectional operator that is

level ones, should naturally agree in terms

used to realize two different optimization

of their binary classification decision about whether the input image belongs to the source or target domain. Consequently, a consistency regularization stage combines the output of the two classifiers to promote

The lack of data, and especially annotated data, that captures the truly diverse nature of rainy

objectives. In the feed-forward direction, the GRL acts as an identity operator. This leads to the standard objective of minimizing the classification error when performing local backpropagation within the domain

consistency between their outcomes.

conditions for moving

adaptation network. On the other hand, for

While the two domain adaptation classifiers are optimized to differentiate between the source and target domains, the Faster R-CNN detector must be optimized such that it becomes domain-independent or domain

vehicles is arguably the most critical and fundamental issue in this area.

backpropagation toward the Faster R-CNN network, the GRL becomes a negative scalar. Hence, in this case, it leads to maximizing the binary classification error, and this maximization promotes the generation of a

invariant. In other words, the Faster R-CNN

domain invariant feature map by the Faster

detector must distinguish objects regardless of the input image R-CNN feature extractor.

domain (clear or rainy). Hence, the feature map resulting from

Consequently, for the purpose of this tutor ial, we devel-

the Faster R-CNN feature extractor must be domain invariant. To oped and employed a domain-adaptive Faster R-CNN

that end, this feature extractor should be trained and optimized to [33] for rainy conditions. To train this model, we prepare the

maximize the domain classification error achieved by the domain training data to include two sets: source data, which consists

adaptation stage. Thus, while both the image- and instance-level of images captured in clear weather (this set includes data

domain classifiers are designed to minimize the binary classi- annotations in terms of bounding box coordinates and object

fication error (between the source and target domains), the Faster categories), and target data, which includes only images cap-

R-CNN feature extractor is constructed to maximize the same tured in rainy conditions without any annotations. To validate

binary classification error.

the trained model using domain adaptation, we tested it using

(a)

(b)

(c)

FIGURE 9. The (a) ground truth, (b) original training, and (c) alternative training, where the example detection results using alternative training approaches for Faster R-CNN and YOLO are shown. The top-right image depicts the improvement in vehicle and traffic sign detection when images generated by I2IT (the UNIT model [25]) are used to train Faster R-CNN. The middle-right image shows the improvement in pedestrian detection when domain adaptation [33] is employed to train Faster R-CNN. The bottom-right image illustrates the improvement in pedestrian detection when images generated by I2IT (the UNIT model [25]) are harnessed to train YOLO.

64

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

the test rainy set. The performance of the detection method that a mitigating technique might not be able to recover the

(Faster R-CNN) that is trained by the domain adaptation salient features of these objects.

approach is shown in the bottom row of Table 2.

In summary, employing domain adaptation or generat-

Discussion

ing rainy weather visuals using UNIT translation, and then using these visuals for training, seems to narrow the gap

Based on the results in Table 2, we observe that while derain- in performance due to the domain mismatch between clear

ing algorithms degrade the AP performance when tested on and rainy weather conditions. This promising observation

scenes distorted by natural rain, improve-

becomes especially clear when consider-

ments can be achieved when employing Generative models could ing the disappointing performance of

I2IT and domain adaptation as mitigating techniques. Different cases are presented in Figure 9. In terms of the AP, and as an example, rainy conditions degrade the pedestrian detection capabilities for YOLO by more than 5% (from approximately 37% to

still play a crucial role in training object detection methods to be more robust and resilient in challenging conditions.

deraining algorithms. Nevertheless, it is also evident that there is still much room for improvement toward reaching the same level of performance in clear conditions. There are key challenges that need to be addressed, though, when designing

32%), but by using image translation, the

any new mitigating techniques for clos-

performance improves to an AP of more than 34%, conse- ing the aforementioned gap. These challenges include the

quently narrowing the gap between the clear- and rainy- broad and diverse scenarios for rainy conditions, especially

condition performances. Similarly, both image translation in driving environments.

and domain adaptation improve the traffic sign detection

These diverse cases and scenarios can’t be learned

performance for Faster R-CNN. Furthermore, image trans- in a viable way by using state-of-the-art approaches. For

lation seems to improve the vehicle detection performance example, raindrops have a wide range of possible appear-

under Faster R-CNN.

ances, and they come with various sizes and shapes,

In other cases, for example, the traffic light detection per- especially when falling on the windshield of a vehicle.

formance under Faster R-CNN, the domain adaptation and Another factor is the influence of windshield wipers on

image translation do not seem to perform well when tested on altering the amount of rain, and even the shapes and sizes

natural rainy images (even when using natural rainy images as of raindrops, between wipe cycles. Other external factors

the target domain for training these techniques). One potential include reflections from the surrounding wet pavement,

factor for this poor performance in some of these cases is the mist in the air, and splash effects. Current state-of-the-

fact that small objects, such as traffic lights, are quite chal- art image translation techniques and domain adaptation

lenging to detect to start with. This can be seen from the very are not robust enough to capture this wide variety of rain

low AP value, even in clear conditions, which is a mere 26%. effects. Figure 10 provides images from the test rainy set

Naturally, the impact of raindrops or rain streaks on such that illustrate several rainy weather scenarios and effects

small objects in the scene could be quite severe, to the extent for driving vehicles.

(a)

(b)

(c)

FIGURE 10. The images from the test rainy set that illustrate several rainy weather scenarios and effects for driving vehicles, with (a) no raindrops, (b) various raindrops, and (c) mist and pavement reflection [25].

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

65

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

Conclusions

ally represent, there will always be a need for capturing

Besides outlining state-of-the-art frameworks for object de­­

certain scenarios that are not included in a real data set. In

tection, deraining, I2IT, and domain adaptation, this tutorial

that context, generative models could be used to produce

highlighted crucial results and conclusions regarding current

data representing the scenarios that are missing from the

methods in terms of their performance in rainy weather

real data sets, and hence they could increase the coverage

conditions. In particular, we believe there is an overarching

and diversity of the cases that object detection methods

consistent message regarding the limitations of the surveyed

can handle.

techniques in handling and mitigating the impact of rain for 3) There is a need for novel deep learning architectures

visuals captured by moving vehicles. This consistent observation

and solutions that have adequate capacity for handling

has serious implications for autonomous

object detection under diverse conditions.

vehicles since the aforementioned limitations impact autonomous vehicles’ core safety capabilities. To address these issues, we recap some of our key findings and point out

There is a need for novel deep learning architectures and

Designing a neural network that performs quite well in one domain yet degrades in others is not a viable strategy for autonomous vehicles. In general, training the

potential directions.

solutions that have

leading object detection architectures

1) The lack of data, and especially annotated data, that captures the truly diverse nature of rainy conditions for moving vehicles is arguably the most critical

adequate capacity for handling object detection under diverse conditions.

through a diverse set of data does not necessarily improve the performance of these architectures relative to their results when trained on a narrow domain of

and fundamental issue in this area.

cases and scenarios. We believe that this

Major industry players are becoming more willing to tack-

issue represents an opportunity for researchers in the

le this problem and more open about addressing this issue

field to make key contributions.

publicly. Consequently, a few related efforts have just
been announced and actually commenced by high-tech Acknowledgments

companies. These efforts are specifically dedicated to This work has been supported by the Ford Motor Company

operating fleets of autonomous vehicles in challenging under the Ford–Michigan State University Alliance Program.

and diverse rainy weather conditions, explicitly for the We would like to acknowledge Ford’s advanced research en-

sake of collecting data under these conditions [30]. After gineers, Jonathan Diedrich and Mark Gehrke, for their invalu-

years of testing and millions of driven miles conducted able input and contributions throughout the collaborative effort

primarily in favorable and clear weather, there is a salient that resulted in this article.

admittance and willingness to divert important resources

toward data collection in challenging weather conditions Authors

that will be encountered by autonomous vehicles.

Mazin Hnewa (mazin@msu.edu) received his B.S. degree in

2) Despite the recent efforts to collect more diverse data, we computer engineering from the University of Baghdad, Iraq,

believe that generative models could still play a crucial in 2005 and his M.S. degree in electrical engineering from

role in training object detection methods to be more Michigan State University (MSU), East Lansing, in

robust and resilient in challenging conditions. In particu- 2012. He was a faculty member at the University of Karbala,

lar, we believe that novel and more advanced frameworks Iraq, in 2013–2017 and is currently working toward his

for UNIT could play a viable role for generating mean- Ph.D. degree at MSU, East Lansing, Michigan, USA. His

ingful data for training. Due to the fact that these frame- research interests include object detection in adverse weath-

works do not require annotated data, their underlying er conditions, deep learning, domain adaptation, and auton-

generative models could be useful in many ways. First, omous vehicles.

they could fill the gap that currently exists in terms of the

Hayder Radha (radha@egr.msu.edu) received his Ph.M.

lack of real annotated data in different weather condi- and Ph.D. degrees from Columbia University, New York, in

tions; hence, progress in terms of training and testing new 1991 and 1993, respectively. He is currently the Michigan

object detection methods could be achieved by using State University (MSU) Foundation Professor and director of

these generative models.

the Connected and Autonomous Networked Vehicles for

Second, even after a reasonable amount of annotated Active Safety program at MSU, East Lansing, Michigan,

data captured in natural rainy conditions becomes avail- USA. He was a fellow at Philips Research (1996–2000) and a

able, the generative models could still play a pivotal role distinguished member of the technical staff at Bell

in both the basic training and coverage of diverse scenari- Laboratories (1986–1996). He received the Amazon Research

os. In other words, UNIT models could always generate Award (2019), Semiconductor Research Consortium Award

more data that can compliment real data, and this, on its (2019), Google Faculty Research Award (2014 and 2015),

own, could be quite helpful to further the basic training of Microsoft Research Award, AT&T Bell Labs Ambassador and

object detection methods. Furthermore, despite the num- AT&T Circle-of-Excellence Awards, and William J. Beal

ber of various rainy condition scenarios that real data actu- Outstanding Faculty Award (2015). He is a Fellow of IEEE.

66

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

References
[1] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving: Learning affordance for direct perception in autonomous driving,” in Proc. IEEE Int. Conf. Computer Vision, 2015, pp. 2722–2730. doi: 10.1109/ICCV.2015.312.
[2] B. Wu, F. Iandola, P. H. Jin, and K. Keutzer, “Squeezedet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops, 2017, pp. 129–137.
[3] M. Teichmann, M. Weber, M. Zoellner, R. Cipolla, and R. Urtasun, “Multinet: Real-time joint semantic reasoning for autonomous driving,” in Proc. 2018 IEEE Intelligent Vehicles Symp. (IV), pp. 1013–1020. doi: 10.1109/IVS.2018. 8500504.
[4] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2014, pp. 580–587. doi: 10.1109/CVPR.2014.81.
[5] R. Girshick, “Fast R-CNN,” in Proc. IEEE Int. Conf. Computer Vision, 2015, pp. 1440–1448. doi: 10.1109/ICCV.2015.169.
[6] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time object detection with region proposal networks,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, June 2017. doi: 10.1109/TPAMI.2016. 2577031.
[7] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss for dense object detection,” in Proc. IEEE Int. Conf. Computer Vision (ICCV), Oct 2017, pp. 2999–3007.
[8] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), June 2016, pp. 779–788. doi: 10.1109/CVPR. 2016.91.
[9] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, “SSD: Single shot multibox detector,” in Proc. European Conf. Computer Vision, 2016, pp. 21–37. doi: 10.1007/978-3-319-46448-0_2.
[10] J. Dai, Y. Li, K. He, and J. Sun, “R-FCN: Object detection via region-based fully convolutional networks,” in Proc. Advances Neural Information Processing Systems, 2016, pp. 379–387.
[11] K. Garg and S. K. Nayar, “Detection and removal of rain from videos,” in Proc. 2004 IEEE Computer Society Conf. Computer Vision and Pattern Recognition, vol. 1, p. I. 10.1109/CVPR.2004.1315077.
[12] L. Kang, C. Lin, and Y. Fu, “Automatic single-image-based rain streaks removal via image decomposition,” IEEE Trans. Image Process., vol. 21, no. 4, pp. 1742– 1755, Apr. 2012. doi: 10.1109/TIP.2011.2179057.
[13] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint rain detection and removal from a single image,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2017, pp. 1357–1366. doi: 10.1109/CVPR.2017.183.
[14] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Paisley, “Removing rain from single images via a deep detail network,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2017, pp. 3855–3863. doi: 10.1109/ CVPR.2017.186.
[15] R. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu, “Attentive generative adversarial network for raindrop removal from a single image,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2018, pp. 2482–2491. doi: 10.1109/ CVPR.2018.00263.
[16] D. Ren, W. Zuo, Q. Hu, P. Zhu, and D. Meng, “Progressive image deraining networks: A better and simpler baseline,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2019, pp. 3937–3946. doi: 10.1109/CVPR.2019. 00406.
[17] T. Wang, X. Yang, K. Xu, S. Chen, Q. Zhang, and R. W. Lau, “Spatial attentive single-image deraining with a high quality real rain dataset,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2019, pp. 12270–12279. doi: 10.1109/ CVPR.2019.01255.
[18] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: From error visibility to structural similarity,” IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, 2004. doi: 10.1109/TIP.2003.819861.
[19] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional networks,” in Proc. European Conf. Computer Vision, 2014, pp. 818–833. doi: 10.1007/978-3-319-10590-1_53.
[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for largescale image recognition,” in Proc. 3rd Int. Conf. Learning Representations, ICLR 2015.

[21] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object detection with discriminatively trained part-based models,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, pp. 1627–1645, 2009. doi: 10.1109/TPAMI. 2009.167.

[22] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? The KITTI vision benchmark suite,” in Proc. Conf. Computer Vision and Pattern Recognition (CVPR), 2012, pp. 1–8. doi: 10.1109/CVPR.2012.6248074.

[23] M. Cordts et al., “The cityscapes dataset for semantic urban scene understanding,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2016, pp. 1–18. doi: 10.1109/CVPR.2016.350.

[24] G. Neuhold, T. Ollmann, S. Rota Bulò, and P. Kontschieder, “The mapillary vistas dataset for semantic understanding of street scenes,” in Proc. Int. Conf. Computer Vision (ICCV), pp. 4990–4999, 2017. [Online]. Available: https://www .mapillary.com/dataset/vistas

[25] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell, Bdd100k: A diverse driving video database with scalable annotation tooling. 2018. [Online]. Available: arXiv:1805.04687

[26] H. Caesar et al., Nuscenes: A multimodal dataset for autonomous driving. 2019. [Online]. Available: arXiv:1903.11027

[27] P. Rousseau, V. Jolivet, and D. Ghazanfarpour, “Realistic real-time rain rendering,” Comput. Graph., vol. 30, no. 4, pp. 507–518, 2006. doi: 10.1016/j.cag. 2006.03.013.

[28] K. Garg and S. K. Nayar, “Photorealistic rendering of rain streaks,” ACM Trans. Graph. (TOG), vol. 25, no. 3, pp. 996–1002, 2006. doi: 10.1145/1141911. 1141985.

[29] Adobe Inc., Sab Jose, CA. Cycore Rainfall Simulation, Adobe after Effects CC 2019. 2019. [Online]. Available: https://www.adobe.com/products/aftereffects.html

[30] K. Korosec, Waymo self-driving cars head to Florida for rainy season. Aug. 20, 2019. [Online]. Available: https://techcrunch.com/2019/08/20/waymo-self-driving -cars-head-to-florida-for-rainy-season/

[31] M. Everingham and J. Winn, “The PASCAL visual object classes challenge 2012 development kit,” in Pattern Analysis, Statistical Modelling and Computational Learning, Tech. Rep, 2011. [Online]. Available: http://host.robots .ox.ac.uk/pascal/VOC/voc2012/devkit_doc.pdf

[32] M.-Y. Liu, T. Breuel, and J. Kautz, “Unsupervised image-to-image translation networks,” in Proc. Advances Neural Information Processing Systems, 2017, pp. 700–708.

[33] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool, “Domain adaptive faster R-CNN for object detection in the wild,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2018, pp. 3339–3348. doi: 10.1109/CVPR.2018.00352.

[34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2016, pp. 770–778. doi: 10.1109/CVPR.2016.90.

[35] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in Proc. Advances Neural Information Processing Systems, 2014, pp. 2672–2680.

[36] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo, “Convolutional LSTM network: A machine learning approach for precipitation nowcasting,” in Proc. Advances Neural Information Processing Systems, 2015, pp. 802–810.

[37] J. Redmon and A. Farhadi, Yolov3: An incremental improvement. 2018. [Online]. Available: arXiv:1804.02767

[38] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2017, pp. 1125–1134. doi: 10.1109/CVPR.2017.632.

[39] C. Ledig et al., “Photo-realistic single image super-resolution using a generative adversarial network,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), July 2017, pp. 4681–4690. doi: 10.1109/CVPR.2017.19.

[40] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” in Proc. IEEE Int. Conf. Computer Vision, 2017, pp. 2223–2232. doi: 10.1109/ICCV.2017.244.

[41] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman, “Toward multimodal image-to-image translation,” in Proc. Advances Neural Information Processing Systems, 2017, pp. 465–476.

[42] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by backpropagation,” in Proc. 32nd Int. Conf. Machine Learning, 2015, vol. 37, pp. 1180–1189.



SP

IEEE SIGNAL PROCESSING MAGAZINE | January 2021 |

67

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 13,2022 at 11:13:42 UTC from IEEE Xplore. Restrictions apply.

