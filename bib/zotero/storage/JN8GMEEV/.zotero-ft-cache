The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)

On the Optimal Efﬁciency of A∗ with Dominance Pruning
A´ lvaro Torralba
Department of Computer Science, Aalborg University, Denmark alto@cs.aau.dk

Abstract A well known result is that, given a consistent heuristic and no other source of information, A∗ does expand a minimal number of nodes up to tie-breaking. We extend this analysis for A∗ with dominance pruning, which exploits a dominance relation to eliminate some nodes during the search. We show that the expansion order of A∗ is not necessarily optimally efﬁcient when considering dominance pruning with arbitrary dominance relations, but it remains optimally efﬁcient under certain restrictions for the heuristic and dominance relation.
Introduction
Heuristic best-ﬁrst search algorithms are a fundamental tool for problem solving whenever the problem can be modeled as ﬁnding paths in graphs. Heuristic functions guide the search towards the goal by estimating the distance from any given state to the goal. Whenever an optimal solution of minimum cost is required, A∗ search is often the algorithm of choice (Hart, Nilsson, and Raphael 1968). This is well supported by the well known result that, given a consistent heuristic h and no other source of information, A∗ does expand a minimal number of nodes up to tie-breaking among all algorithms that guarantee ﬁnding the optimal solution (Dechter and Pearl 1985).
Dominance pruning is a technique to eliminate nodes during the search if they can be proven to be dominated by another state (Hall et al. 2013; Torralba and Hoffmann 2015). This exploits an additional source of information in the form of a dominance relation , which compares two states to determine whether one can be proven to be as close to the goal as the other. This type of dominance appears naturally on problems that have to deal with resources, (i.e., removing states that have strictly less resources than another), and can also be applied on other kinds of problems (e.g., in gridworlds being at a central square can sometimes be proven better than being at a corner if the set of reachable squares in one step is strictly larger). This can be exploited by any search algorithm to reduce the number of nodes explored while retaining any solution optimality guarantees. This has been mainly used in the context of cost-optimal planning, as an enhancement for the A∗ algorithm. Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.

In this paper, we address the question of whether the expansion order of A∗ is good to minimize the number of expansions when dominance pruning is used. Prioritizing the expansion of states with lower f -value is not necessarily an obvious choice anymore, since states that are more promising according to the heuristic function are not necessarily better according to the dominance relation. Furthermore, previous results proving the optimal efﬁciency of A∗ are no longer valid due to having a new source of information.
Indeed, we show that there are cases where A∗ with dominance pruning is not optimally efﬁcient, and that different expansion orderings, or even expanding some states that could be pruned may lead to a globally higher number of expansions in some cases. However, this can be attributed to “inconsistencies” in the information provided by the heuristic function and the dominance relation. We extend the notion of consistent heuristics to consistent heuristic and dominance relation pairs, and prove that A∗ with dominance pruning is indeed optimally efﬁcient, meaning that there is a tie-breaking for A∗ that expands the lowest number of nodes among all admissible algorithms with dominance pruning.
We also analyze which tie-breaking strategies remain optimally efﬁcient up to the last f -layer, i.e., when we ignore the expansions of nodes with an f -value equal to the solution cost. This is relevant because when consistent heuristics are used, the choice of tie-breaking rule in A∗ is only relevant for the last layer, since all nodes with an f -value lower than the optimal solution cost must be expanded regardless of the expansion order. Therefore, most implementations of A∗ choose tie-breaking strategies in favor of nodes with lower h-value, which are expected to ﬁnd a solution faster in the last f -layer. We show that with dominance pruning this is no longer the case, as tie-breaking strategies in favor of nodes with lower g-value are preferable up to the last layer.
Background
A transition system (TS) is a tuple Θ = S, L, T, sI , SG where S is a ﬁnite set of states, L is a ﬁnite set of labels each associated with a label cost c(l) ∈ R+0 , T ⊆ S × L × S is a set of transitions, sI ∈ S is the start state, and SG ⊆ S is the set of goal states. We write s →−l t as a shorthand for (s, l, t) ∈ T . A plan for a state s is a path from s to any sG ∈ SG. We use h∗(s) to denote the cost of a cheapest

12007

plan for s, and g∗(s) to denote the cost of a cheapest path

from sI to s. A plan for s is optimal iff its cost equals h∗(s).

The sum f ∗(s) = g∗(s) + h∗(s) is the cost of an optimal

plan from sI passing through s. We denote F ∗ to the optimal

solution cost for sI , F ∗ = f ∗(sI ) = h∗(sI ). To deal with

tasks with 0-cost actions, we deﬁne a modiﬁed cost function

c so that all 0-cost actions are assigned a cost of , where

is a tiny constant such that the sum of arbitrarily many is

lower than any other action cost. We deﬁne g , h∗, f , etc as

the functions above under this new cost function.

A goal

heuristic h : distance. It

S is

a→dmRis+0s∪ib{le∞if}

is it

a function that estimates never overestimates the

real cost, i.e., h(s) ≤ h∗(s) for all s ∈ S, and consistent if

for any s →−l t it holds that h(s) ≤ h(t) + c(l). Best-ﬁrst search algorithms maintain an open and a closed
list with all nodes that have been seen so far. A search node ns characterizes a path from the initial state to the ﬁnal state of the path, s, where the g-value g(ns) is the cost of the path. We write ns →−l nt as a shorthand for s →−l t and g(nt) = g(ns) + c(l). The open list is initialized with the initial state that has a g-value of 0. At each step, a node is selected from the open list for expansion. When a node is expanded, it is removed from the open list and all the successors are generated and inserted into the open list. The closed list keeps all nodes that have been expanded to avoid duplicates so that a node is not expanded if another node with the same state and a lower or equal g-value has already been expanded. A∗ always selects for expansion a node with

minimum f -value where f (ns) = g(ns) + h(s). Since the behavior of A∗ is not uniquely deﬁned, we say that it is a family of algorithms, one per possible tie-breaking rule.

Optimal Efﬁciency of A∗
The seminal work by Dechter and Pearl (1985) analyzes the optimal efﬁciency of A∗ in great depth, considering several degrees of optimal efﬁciency. They consider the heuristic as part of the input to the algorithm, so a problem instance is a tuple Θ, h . An instance is consistent if it has a consistent heuristic h. An algorithm is admissible if it is guaranteed to return an optimal plan for Θ, whenever h is admissible.
To prove optimal efﬁciency of an algorithm, some assumptions about the considered algorithms are needed. As we are interested in admissible algorithms, we assume all families of algorithms considered in this paper to be admissible. In their paper, Dechter and Pearl deﬁne a family of algorithms that use only a few primitive functions, such as expansion and heuristic functions. Eckerle et al. (2017) reﬁne this by making explicit the assumption that all these functions are deterministic, and black box, deﬁning the family of Deterministic, Expansion-based, Black Box (DXBB) algorithms. We also assume that the transition relation can only be accessed in a forward manner, as a function that given a state returns its successors. If backward search is possible, A∗ does not guarantee optimal efﬁciency (Chen et al. 2017).
Deﬁnition 1 (UDXBB Algorithm). A algorithm is Unidirectional, Deterministic, Expansion-based, Black Box (UDXBB ) if it is deterministic and it has access to the state space Θ via exactly the following functions:

• Start: returns the initial state sI . • Is-goal: given a state s returns true iff s is a goal state. • Expand: given a state s returns a set of successor states
expand (s) = {t | s →−l t}. • Cost: given a state and a successor state returns the cost
of reaching it (cost(s, t) = minc(l) s →−l t ). Dechter and Pearl deﬁne a hierarchy with several degrees of optimality, based on comparing the sets of nodes expanded by different families of algorithms over a set of instances. Let N (A, I) be the set of expanded nodes by algorithm A on instance I. A family of algorithms A is Xoptimally efﬁcient over another B relative to an instance set I if: • Type 0: ∀I ∈ I, ∀B ∈ B, ∀A ∈ A, N (A, I) ⊆ N (B, I). • Type 1: ∀I ∈ I, ∀B ∈ B, ∃A ∈ A, N (A, I) ⊆ N (B, I). • Type 2: ∀I ∈ I, ∀B ∈ B, ∀A ∈ A, N (B, I) ⊂ N (A, I). • Type 3: ∀B ∈ B, ∀A ∈ A, (∃I1 ∈ I, N (A, I1) ⊆ N (B, I1)) =⇒ (∃I2 ∈ I, N (B, I2) ⊆ N (A, I2)) Among other results, Dechter and Pearl proved that, on consistent instances A∗ is 1-optimal, meaning that for any admissible UDXBB algorithm X, there exists a tie-breaking for A∗ that expands a subset of the nodes expanded by X. They also show that no family of algorithms can be 0optimal, meaning that there is no way to set the tie-breaking strategy to guarantee a minimal number of node expansions.
Dominance Pruning Dominance pruning is a technique that makes use of a dominance relation as an additional source of information. A relation ⊆ S × S is a dominance relation if, whenever s t, then h∗(t) ≤ h∗(s). We say that a node nt prunes another ns if ns = nt, g(nt) ≤ g(ns) and s t.
We deﬁne A∗ with dominance pruning (A∗pr ) as the vanilla A∗ algorithm with a simple modiﬁcation. Anytime that a node ns is selected for expansion, skip it if there exists another node nt in open or closed such that nt prunes ns. Nodes pruned this way are removed from the open list but they are neither expanded nor inserted into the closed list.1 Therefore, pruned nodes are “forgotten” and no node can be pruned due to being dominated by a previously pruned node. This is necessary to correctly handle the case where there are only two nodes that prune each other, since in that case any of the two nodes could be pruned, but at least one of them must be expanded to ﬁnd a solution.
In this work, we assume that the dominance relation is provided as an instance-dependent function. In practice, it can also be automatically obtained from a model of the problem, even though in this work we assume that the model is not available to the search algorithm. A common way to deﬁne a dominance relation is based on identifying resources (Hall et al. 2013), i.e. variables for which there exists a total order for their values such that larger values en-
1Nodes can also be pruned upon generation to avoid the overhead of computing h and open list insertion. But this does not affect the number of expanded states, which is what interests us.

12008

able more actions. Furthermore, there are other more advance methods that ﬁnd pre-orders on arbitrary abstract state spaces (Torralba and Hoffmann 2015). In both cases, the dominance relations that have been used in the literature are: • Pre-order relations: they are reﬂexive (s s for all s) and
transitive (s t ∧ t u =⇒ s u). • Cost-simulation relations: whenever s t, for all s →−l s ,
either s t or ∃t −→l t s.t. c(l ) ≤ c(l) and s t . Even though one can deﬁne dominance relations that do not satisfy these properties, they are naturally obtained in most cases. In particular, the property of cost-simulation is related to the way automatic methods prove that the obtained relation is a dominance relation without having access to h∗.
Deﬁnition of Optimal Efﬁciency
Following Dechter and Pearl, we are interested in the optimal efﬁciency of algorithms in regards of node expansions on concrete families of instances. In this section, we generalize their framework by considering the additional information of a dominance relation. This requires deﬁning what consistent instances are in this case, as well as deﬁning the different notions of optimal efﬁciency, and the families of algorithms that we will consider.
Consistent Instances A problem instance is a tuple Θ, h, , where Θ is a transition system, h is an admissible heuristic for Θ, and is a dominance relation for Θ. We say that an instance is consistent if both the heuristic and dominance relation are consistent on their own, and they are consistent with each other, meaning that they fulﬁll the following properties. Deﬁnition 2. An instance I = Θ, h, is consistent if: (i) h is consistent. (ii) is a transitive cost-simulation. (iii) is consistent with h: s t =⇒ h(t) ≤ h(s).
Condition (ii) ensures that the information provided by is consistent in two different ways. First, must be transitive, because if we do know that s t and t u, then h∗(u) ≤ h∗(t) ≤ h∗(s) so s u can be deduced. Second, for a dominance relation to be consistent, we require it to be a cost-simulation relation so that whenever nt prunes ns, then if ns or any of its successors would prune nu, then nt or some of its successors prune nu as well.
Condition (iii) requires and h to not contradict each other on their comparison for any two states s and t. Note that this does not render uninformative, since comparing states based on their heuristic value is no substitute for dominance analysis. In particular, even if always agrees with h, its role is to identify cases where the relative heuristic evaluation of both states is provably correct.
A question is how often these conditions happen in practice. The ﬁrst two conditions are indeed quite common: most heuristics that come from an optimal solution to a relaxation of the problem are indeed consistent; and typical approaches to compute dominance relations in planning are guaranteed

to return transitive cost-simulation relations (Torralba and Hoffmann 2015; Torralba 2017).
An analysis of whether heuristics are consistent with respect to a dominance relation is beyond the scope of this paper since that would require to consider concrete heuristic functions and dominance relations. In practice, it is reasonable to expect that most consistent heuristics will fulﬁll this property. For example, consider resource-based dominance relations that identify that states having more resources (fuel or money for example) are preferred. These are dominance relations because more resources can only enable more transitions in the state space; so heuristics that result from systematic (symmetric) relaxations of the problem will typically associate a lower heuristic value to states with more resources, everything else being equal. Indeed, for several families of heuristic functions in domain-independent planning, they have been shown to be consistent with symmetry equivalence relations (Shleyfman et al. 2015; Sievers et al. 2015), which are a special case of dominance relation. We conjecture that this holds as well for dominance relations based on comparing the values of sub-sets of variables, for heuristics that take into account the same subsets (e.g. we conjecture hmax and h+ are consistent with dominance relations over single variables, and pattern databases are consistent with dominance relations over subsets of the pattern).
Types of Optimality We extend the optimality criteria considered by Dechter and Pearl in several ways. Deﬁnition 3 (#-optimally efﬁcient). Let N (A, I) be the set of expanded nodes by algorithm A on instance I. A family of algorithms A is #-optimally efﬁcient over another B relative to an instance set I if for any algorithm B ∈ B and instance I ∈ I, there exists A ∈ A such that |N (A, I)| ≤ |N (B, I)|.
This deﬁnition of #-optimality is a relaxed variant of the 1-optimality deﬁnition by Dechter and Pearl, which requires the number of expansions by A to be lower or equal than that of B, instead of requiring it to be a subset (N (A, I) ⊆ N (B, I)). Our criteria is slightly weaker since it only requires having an overall minimum number of expansions, which implicitly assumes that all expansions are equally time consuming. We say that 1-optimality is strict if A is 1-optimally efﬁcient over B, but B is not over A. We say that #-optimality is strict if A is #-optimally efﬁcient over B, but B is not over A.
We also consider when A∗ is optimal up to the last layer, i.e., where only nodes with an f -value lower than the optimal solution cost are taken into account. That is, we replace N (X, I) by N (X, I) where N (X, I) = {n ∈ N (X, I) | f (n) < F ∗}. This is related to the notion of non-pathological instances introduced by Dechter and Pearl, which are those instances where A∗ does not expand any node n with f (n) = F ∗. However, paradoxically, nonpathological instances are very unlikely to occur in practice. For that reason, on the context of A∗ algorithms, we prefer to directly consider optimality up to the last layer, simply ignoring the effort that A∗ will make in the last f -layer, which

12009

1-opt, strict

UDXBB

UDXBB pr

A∗h < ,pr

1-opt
(Dechter and Pearl 1985)

#-opt

1-opt (last layer) strict

A∗ #-opt, strict A∗pr 1-opt (last layer) A∗g<,pr

Figure 1: Summary of optimal efﬁciency relationships. All results assume consistent instances.

most of the times strongly depends on the tie-breaking.
Families of Algorithms We introduce a new family of algorithms that extends UDXBB with dominance pruning. Deﬁnition 4 (UDXBB pr). UDXBB pr is a family of algorithms that extends UDXBB with the ability to perform dominance pruning, i.e., to discard any node ns if another node nt has been generated such that nt prunes ns.
Note that UDXBB pr algorithms cannot access the dominance relation directly or indirectly, i.e., they are not allowed to perform inference based on the fact that h∗(t) ≤ h∗(s) whenever s t. Our analysis focuses on dominance pruning, excluding other further uses of dominance relations.
Proposition 1. UDXBB pr is strictly 1-optimal over UDXBB on all instances.
Proof. 1-optimality follows trivially from the fact that UDXBB is contained in UDXBB pr, since UDXBB pr algorithms could choose not to prune any node if they desire so. To show this to be strict, it sufﬁces to show an instance where there are nodes ns, nt with f (nt) ≤ f (ns) ≤ F ∗ such that nt prunes ns. It is very easy to construct such example, e.g. see the instances in Figures 2, and 3.

Optimal Efﬁciency of A∗pr

Thorough the paper, we assume consistent instances, i.e.,

that the heuristic function and dominance relation are con-

sistent. Figure 1 summarizes our results. Our theoretical

analysis concludes that, in terms of node expansions using

dominance pruning is strictly better than not using domi-

nance. Our main result is that, on consistent instances, the

expansion order of A∗pr is #-optimally efﬁcient, meaning

that there exist imum number

soofmeexptiaen-bsiroenaks.inWg eofbAeg∗printhbayt

expands a minshowing some

counter-examples on instances that do not satisfy our consis-

tency criteria to highlight why consistency is required. Then

we discuss how to characterize the states that must be ex-

panded to ﬁnd a solution and prove it to be optimal; prove

our main result; and discuss what tie-breaking strategies are

more appropriate for A∗ with dominance pruning.

Counter-examples due to Inconsistencies The two things that characterize A∗pr algorithms from the sseutboofptUimDaXllBy Befpﬁrciaelngtoirnithinmcso,nasnisdtetnhtaitnmstaayncceasuasereA: ∗pr to be

1 C 1 C’ 1 . . .

I

1

B

1

B’

99

100
G

1 A 100 : B A, C B
(a) A∗: I, A, C, C , . . . , G opt: I, A, B, C, G

h=3 2

h=1
C

1

. . . 100 h=0

I

G

1

h=3
A

1

h=2
B

100

:C B

(b) A∗: I, C, . . . , A, B, G

opt: I, A, B, G

Fnoigtuorpeti2m:aClloyuenftﬁecrieexnatm, wplheesntphrautnsihnogwacccaosredsinwghtoerteheAd∗pormis-

inance relation below each ﬁgure. The “. . . ” region repre-

sents an arbitrarily large region of the state space that will

bperuenxinpganodredexbpyanAsi∗porn,

but could be avoided with a order strategies. In (a) h =

different 0 for all

states, in (b) each node is labeled with its h value.

1. A node is pruned whenever possible, and sometimes not pruning a node may lead to less overall expansions.

2. The expansion order of A∗ may not be optimally efﬁcient anymore when considering dominance pruning.

Figure 2 nodes than

snheocwesssaerxyamfoprlethsewsehetwreoAr∗perasdooness.

expand more The example

in Figure 2a illustrates a state space and dominance rela-

tion for which pruning a node whenever possible is not an

optimal strategy, independently of the expansion order (for

simplicity we set h = 0 for all states). After expanding the

initial state I, one can prune node B because it is dominated

by A. However, if B is pruned, B won’t be generated un-

der any expansion order so C and all its arbitrarily many

successors will be expanded.

Our second example, illustrated in Figure 2b, shows a

case where it is good to prune nodes whenever possible but

the expansion order of A∗ leads to a sub-optimal number of

expansions. The optimal expansion order is I, A, B, G . C

does not need to be expanded even though f (C) < F ∗ be-

cause C will be pruned (C B and g(B) ≤ g(C)). How-

ever, A∗pr will expand C after expanding the initial state I,

since f (C) < f (A) and B has not been generated yet.

All these scenarios can be attributed to “inconsistencies”

within the dominance relation or between and the

heuristic function h. In Figure 2b the dominance relation and

heuristic do not agree on the comparison between B and C.

The dominance relation proves that B is at least as close to

the goal as C, but the heuristic function estimates that C is

closer to the goal. In the case of Figure 2a, the dominance re-

lation is inconsistent because the information that A is closer

to the goal than B is lost after one expansion and neither A

nor any of its successors could be used to prune B or C .

Solution Sets We ﬁrst identify which states need to be expanded to prove optimality by any search that does not have access to any additional information, other than a heuristic function h and the ability to prune nodes. Traditionally, this is done by identifying must-expand states that must be expanded for every algorithm to prove optimality, or must-expand pairs as done

12010

in the bidirectional search setting (Eckerle et al. 2017). However, in our case there are many choices that can be made for dominance pruning, so now the difference between mustexpand nodes and the nodes that belong to any concrete solution is not restricted to the last f -layer.
We deﬁne instead solution sets, which take into consideration all nodes that must be expanded by any UDXBB pr algorithm to ﬁnd a solution, including the last f -layer. Let S be a set of nodes. We use [S] to denote the set extended with its immediate successors, i.e., [S] = S ∪ {ns | ns → ns , ns ∈ S}. The intuition is that, if S is the set of nodes that have been expanded at some point during the execution of a UDXBB algorithm, then [S] is the set of nodes that have been generated. In other words, if S represents the contents of the closed list, then [S] \ S contains the set of nodes in the open list and all pruned nodes. Deﬁnition 5 (UDXBB pr Solution Set). A set of nodes S is a UDXBB pr solution set for an instance I if: (a) ∀ns ∈ S \ {nsI }, ∃nt ∈ S, nt →−l ns. (b) ∃ns ∈ [S], s ∈ SG and g(ns) = F ∗, (c) ∀ns ∈ [S] \ S, f (ns) ≥ F ∗ or ∃nt ∈ S, nt prunes ns.
Condition (a) requires that every expanded node in S was generated by expanding one of its parents. Condition (b) requires that an optimal solution was found. Condition (c) ensures that the solution found is proven to be optimal, because all nodes in the open list after expanding S have a large enough f -value or are pruned by dominance.
Theorem 1. Let I be any admissible instance. Then, expanding a solution set is a necessary and sufﬁcient condition for admissible UDXBB pr algorithms, i.e., for any A in UDXBB pr, N (A, I) is a solution set.
Proof Sketch. Sufﬁcient: If (a), (b), and (c) hold, then an optimal solution has been found due to (a) and (b). The solution is provably optimal due to (c) since all nodes remaining in the open list have an f -value greater or equal to the incumbent solution. Necessary: If (a) does not hold, then a node has been expanded without being generated, which is impossible in UDXBB algorithms. If (b) does not hold, then no optimal solution has been found. If (c) does not hold, then there exists some ns in the open list that may lead to a solution with cost lower than F ∗, so the solution was not proven to be optimal.
We remark that the proof above relies on UDXBB pr algorithms not being allowed to use dominance relations for anything except dominance pruning. That is, (c) is only a necessary condition if we assume that there are no other pruning rules by which an algorithm could prove that ns is not part of an optimal solution. Otherwise, the criteria (c) of a solution set could be weakened, increasing the set of possible solution sets.
A∗pr is Optimally Efﬁcient on Consistent Instances Before proving our main result of this section, we analyze some properties that hold for consistent instances. An important one is that, whenever h and are consistent with

each other, nodes with larger f -value cannot prune nodes with lower f -value. Lemma 1. Let I be a consistent instance. Let ns, nt be any two nodes such that nt prunes ns. Then, f (nt) ≤ f (ns).
Proof. Since nt prunes ns, it holds that g(nt) ≤ g(ns) and s t. By consistency, h(t) ≤ h(s), so f (nt) ≤ f (ns).
Next, we show that pruning is transitive. Lemma 2. Let be a transitive relation. If nu prunes nt and nt prunes ns, then nu prunes ns.
Proof. By the assumption it follows that g(nu) ≤ g(nt) ≤ g(ns), and s t u. Therefore, g(nu) ≤ g(ns) and, by transitivity of , s u. So nu prunes ns.
Next, we show that all states in the smallest solution set must be expanded only with its optimal g-value. Lemma 3. Let I be a consistent instance. Then, there exists a solution set S for I of minimum size such that for all ns ∈ S, g(ns) = g∗(s).
Proof. Assume the contrary. Then, some ns has been expanded with a sub-optimal value, g∗(s) < g(ns). Therefore, a predecessor along the optimal path from sI to s has not been expanded. Let nt be the ﬁrst such predecessor. By consistency of h, we know that f -values monotonically increase along a path, so f (nt) ≤ f ∗(ns) < f (ns). As nt ∈ S, by condition (c) of a solution set, nt was pruned, i.e., ∃nu ∈ S s.t. nu prunes nt. As is a cost-simulation, then nu has some successor that would prune ns, so there must be a node in S that prunes ns. Therefore, S \ {ns} is also a solution set, contradicting the fact that S is of minimum size.
We next show that pruning a node whenever possible is an optimally efﬁcient strategy because there exists a solution set S of minimum size that does not contain any node that can be pruned by another node in [S], unless both nodes prune each other. To show this, we consider Algorithm 1.
Lemma 4. Let S0 be a solution set for a consistent instance. Let ns ∈ S0, nt ∈ [S0] such that nt prunes ns. Then, Algorithm 1 returns a solution set Sk such that: |Sk| ≤ |S0|; ns ∈ Sk; nt ∈ Sk; and If nt ∈ S0 then |Sk| < |S0|.

Algorithm 1: Replace

Input: S0,ns ∈ S0, nt ∈ [S0] where S0 is a solution set and nt prunes ns
Output: Solution set Si that does not contain ns 1 S1 := (S0 ∪ {nt}) \ {ns}; 2 i = 1;

3 while ∃nsi ∈ Si, ∃nui ∈ Si, nui →−l nsi do

4 Choose such an nsi with minimum g-value ;

5 Choose nti in [Si] such that nti prunes nsi ;

6

Si+1 := Si ∪ {nti } \ {nsi } ;

7 i=i+1;

8 return Si ;

12011

Proof Sketch. The size of the solution set cannot increase during the execution of Algorithm 1, i.e., |Si+1| ≤ |Si| because a node is removed at each iteration and at most one node is added. If nt ∈ S0 then |S1| = |S0| − 1, since ns was removed and no node was added, so in that case |Sk| ≤ |S1| < |S0|. Properties (b) and (c) of a solution set are preserved by all intermediate Si because nsi is replaced by nti such that nti prunes nsi , so by Lemma 1 and 2, nti can do anything nsi could. Property (a) holds when the algorithm terminates, since it is the stopping condition for the loop. The algorithm always terminates because all nodes nsi removed in the loop are descendants of ns which were present in S0, and there are only ﬁnitely many.
Finally, it remains to be proven that there always exists some nti in [Si] in line 4 such that nti prunes nsi . As nsi is a descendant of ns that has no parent in Si. Since all nodes in S0 have a parent, and all nti added along the way too, then the parent of nsi was some nsj removed in a previous iteration j < i, being replaced by ntj . Since is a cost-simulation relation, ntj must have a successor nti that prunes nsi .
Lemma 5. Let S be a solution set of minimum size for a consistent instance. Then there does not exist a pair of nodes ns, nt in S such that nt prunes ns.
Proof. Assume that nt prunes ns. By Lemma 4, using the procedure above we can construct another solution set S s.t. |S | < |S|, contradicting that S has minimum size.
Lemma 6. Let I be a consistent instance. Then, there exists a solution set S of minimum size for I such that there does not exist any ns ∈ S and nt ∈ [S] such that nt prunes ns and ns does not prune nt.
Proof Sketch. Assume the opposite, let S be a solution set such that there exist ns ∈ S and nt ∈ [S] where nt prunes ns and ns does not prune nt. By Lemma 5 nt ∈ S. By condition (c) of a solution set, we know that either f (nt) ≥ F ∗ or there exists nu ∈ S such that nu prunes nt.
Case 1: There exists nu ∈ S such that nu prunes nt. By transitivity, nu prunes ns, so one can construct a minimal solution set with Lemma 4 of smaller size, contradicting that S is a solution set of minimal size.
Case 2: f (nt) ≥ F ∗. Then, by Lemma 1, f (ns) ≥ f (nt) ≥ F ∗. If f (nt) > F ∗, we can remove ns and all its descendants from S0 to obtain a smaller solution set, contradicting the fact that it is a solution set of minimal size. Therefore, f (ns) = f (nt) = F ∗. Note that a solution set of minimum size only contains a node with f (ns) = F ∗ when ns is on the solution path returned by the algorithm. This path can be replaced by another of the same length and cost that goes through nt by repeatedly calling Algorithm 1.
Now we are ready to prove our main result. Theorem 2. A∗pr is #-optimal on consistent instances over UDXBB pr.
Proof. We show that there exists a solution set S of minimum size for which there exists a tie-breaking strategy under which A∗pr with h and expands exactly S. By Lemma 6,

we choose S so that there does not exist any ns ∈ S and nt ∈ [S] s.t. nt prunes ns and ns does not prune nt. Assume a tie-breaking that prefers expanding nodes in S over any

other node, and prefers pruning nodes not in S. Formally,

our tie-breaking strategy selects for expansion any node not

in S such that it can be pruned. If no such node exists, it se-

lects a node (with minimal f value) from S that cannot be

pruned. We prove that this tie-breaking always succeeds by

contradiction. Otherwise, assume that the tie-breaking fails.

Then, the open list does not contain any node with minimal

f value that is outside S and can be pruned or that it is in S

and cannot be pruned. Then, the node selected for expansion

either: (A) it is in S but can be pruned due to some node in

open or closed; (B) it is not in S and cannot be pruned.

Case (A). There exists nt that prunes some ns ∈ S. By

Lemma 5, we know that nt ∈ S. As nt is in the open

list after having expanded a subset of S, nt ∈ [S] and, by

our choice of solution set with Lemma 6, ns prunes nt. By

ALe∗mwmoaul1d,hfa(vnets)e≤lecfte(dnsn)t,

so with instead,

our tie-breaking strategy reaching a contradiction.

Case (B). Let ns be a node that is expanded by A∗pr but it

is not in S. If f (ns) = F ∗, then a node along the optimal

solution contained in S should have been chosen instead. If

f (ns) < F ∗, by condition (c) of a solution set, there exists nt ∈ S such that nt prunes ns. Again, if nt is in open or closed, ns will be pruned reaching a contradiction. Otherwise, there must be an ancestor along the path from sI to nt in open with its optimal g-value. Such an nu ∈ S, must have f (nu) ≤ f (nt) ≤ f (ns), so according to our tie-breaking nu would have been chosen for expansion instead of ns (nu cannot be pruned by the same argument as in case (A)).

Corollary 1. A∗pr is strictly #-optimal over A∗ on consistent instances.

Proof. This follows directly from the fact #-optimal over UDXBB pr and UDXBB pr

itshasttriAct∗plyr

is 1-

optimal over the family of UDXBB algorithms, which con-

tains all algorithms in the family of A∗ algorithms.

Optimal Tie-Breaking Strategies

For A∗ with consistent heuristics the tie-breaking strategy

is only relevant in the last f -layer. Ideally, once the min-

imum f -value in the open list is equal to F ∗, only nodes

on a path to the goal will be selected for expansion. Practi-

cal implementations often prefer expanding nodes with low-

est h-value, aiming to reduce the effort in the last layer. In

domain-independent planning, where a factored model of

the state space is available to offer additional information

to the algorithm, some other strategies have been suggested,

like using (possibly inadmissible) heuristic functions that

estimate plan length instead of plan cost (Asai and Fuku-

naga 2017; Correˆa, Pereira, and Ritt 2018). They showed

that tie-breaking can be quite signiﬁcant for the overall per-

formance, specially in domains with 0-cost actions.

breAak∗prin, ghsotwraetevgeyr,,

is more sensitive since it may matter

to the choice along previous

of tielayers.

This brings up the question of what is a good tie-breaking

12012

h=2 1

h=2
A1

1

h=1
A2

1

h=0
A3

1 h=0

I

1

h=1
B1

1

h=1
B2

1

h=0
B3

1

G

: B2 A2, B3 A3

AA∗h∗g < <

,pr ,pr

: :

I, B1, B2, B3, A1, A2, A3, G I, B1, A1, A2, A3, G

Figure 3: Counter-example for the 1-optimal efﬁciency of A∗h<,pr up to the last layer on consistent instances.

strategy favor of

sftoarteAs∗pwr .itWh me idneiﬁmnuemAg∗g-<v,aplruea.s

A∗pr

breaking

ties

in

Theorem 3. A∗g<,pr is 1-optimal efﬁcient up to the last layer over A∗pr on consistent instances.

Proof Sketch. be the subset

Let S be a of nodes in

solution solution

set set

for up

tAo ∗ptrh,e

and last

let S layer,

S = {ns ∈ S | f (ns) < F ∗}. We show that there is an expansion order compatible with A∗g<,pr that expands all

nodes in S before expanding any other node. For this, the

same proof from Theorem 2 applies up to case (B). For case

(B), we know that f (ns) < F ∗ and, by the same argument as in the proof of Theorem 2, some nu ∈ S must remain in the open list with f (nu) ≤ f (nt) ≤ f (ns). At this point the tie-breaking matters since whenever f (nu) = f (ns), the tiebreaking policy should allow selecting nu over ns. Since nu is an ancestor of nt, g(nu) ≤ g(nt), and since nt prunes ns, g(nu) ≤ g(nt) ≤ g(ns). Then, expanding nu instead of ns is still valid according to the g< tie-breaking strategy.

However, the same is not true for every tie-breaking strat-

egy for A∗ algorithms

. For with

aextiaem-bprelea,klientgAst∗hr< at,epgrybtehatht ealwfaamyislyproeffeArs∗pra

state with minimum h-value. As argued above this is the tie-

breaking preferred by most implementations of A∗ without

dominance pruning, but it cannot guarantee anymore that the

number of expansions up to the last layer will be minimal.

Theorem 4. A∗h<,pr is not optimally efﬁcient up to the last layer on consistent instances.

Proof Sketch. Figure 3 shows a counter-example of a con-

sistent instance where all tie-breaking strategies compatible

wAifttherAex∗h< pa,pnrdienxgpaInadnda Bno1d,ethtehaotpAen∗g<li,sptrcwonotualidnsntowtoexnpoadneds:.

B2 and A1, both with an f -value of 3. At this point, A2

has not been generated yet so B2 cannot be pruned. How-

eevnetirr,eAp∗hla<t,eparu

will of

expand B2 states with

and f=

B3 (and in general 3 underneath B2),

the be-

fore expanding A1. Note that this happens for nodes with

f = 3 < 4 = F ∗, i.e. nodes before the last f -layer.

Corollary 2. A∗g<,pr is strictly 1-optimally efﬁcient up to the last layer over A∗h<,pr on consistent instances.

Proof Sketch. 1-optimality follows directly from Theo-

rem 3, mality

issinsctreicAt ∗hfo<l,lporwiss

fcroonmtaTinheedorinemA4∗pr.

.

The

fact

that

opti-

Thus, there are two conﬂicting objectives. Up to the last layer, it is provably beneﬁcial to break ties in favor of lower g-value. On the last layer, empirical analysis show that it is better to break ties in favor of lower h-value. Which one is preferable depends on the particular domain, dominance relation and heuristic. Our preliminary experiments show that in common planning domains, it is often beneﬁcial to break ties in favor of lower h-value even with dominance pruning.

Conclusions

We analyzed the optimal efﬁciency of A∗ with dominance

pﬁrcuiennint,g,beAc∗paru.seAtshseurme inmgaya

consistent heuristic is not sufbe inconsistencies in the dom-

inance relation as well, unnecessary expansions.

which may We deﬁned

cause a new

cAri∗ptreritoonpoefrfcoornm-

sistency for heuristic and dominance relation pairs, which

ensures number

tohfaet xAp∗parndwedillnboedeosp. tWimealallysoefsﬁhcoiwentthiant

terms of the tie-breaking

in favor of nodes with lower g value is provably preferable to

minimize the number of expansions up to the last layer. This

contrasts with common strategies, which favor nodes with

lowest h-value to minimize expansions in the last layer. As in the optimal efﬁciency result for A∗, our analy-

sis is based only on the number of state expansions and

it ignores the actual runtime. There are of course other al-

gorithms which may outperform A∗ according to differ-

ent performance measures. For example, the IDA∗ algo-

rithm (Korf 1985) and other extensions like Budgeted Tree

Search (Helmert et al. 2019; Sturtevant and Helmert 2020)

outperform A∗ in terms of memory usage. EPEA∗ (Gold-

enberg et al. 2014) aims to minimize the number of nodes

generated, which is arguably more relevant to runtime than

expanded nodes, but it requires additional domain-speciﬁc knowledge. Finally, other algorithms may outperform A∗

in terms of runtime, e.g., when the beneﬁts of reducing

the number of node expansions does not compensate the

overhead of computing the heuristic or performing pruning,

which may require a quadratic cost in the number of gen-

erated states in the worst case. Nevertheless, for concrete

problems and/or dominance relations it may be possible to

perform the pruning more efﬁciently (e.g., dividing states in

classes so that each state needs to be compared only against

a small subset of alternatives), and one could extend ratio-

nal algorithms that reason about when it is worth to compute

the heuristic (Barley, Franco, and Riddle 2014; Karpas et al.

2018) to consider dominance as well.

Finally, in this work we extended the basic framework

with the ability of dominance pruning using a dominance

relation, but it could also be extended in other ways. For

example, if backward search is possible, there are a variety

of bidirectional heuristic search algorithms that can outper-

form A∗ in terms of node expansions (Eckerle et al. 2017;

Chen et al. 2017). One could consider several extensions of

this paradigm regarding different forms of dominance, e.g.,

introducing variants that make use of more general forms

of dominance (Torralba 2017), or alternative methods to ex-

ploit this information. This may open new avenues of re-

search on how to use dominance relations beyond domi-

nance pruning in order to make the most of them.

12013

Acknowledgments
A´ lvaro Torralba was employed by Saarland University and the CISPA Helmholtz Center for Information Security during part of the development of this paper. I would like to thank Nathan Sturtevant for his insights on DXBB algorithms. Thanks to the anonymous reviewers at SoCS’20, HSDIP’20, and AAAI’21 as well as to the Basel reading group for their comments that helped me to improve the paper.
References
Asai, M.; and Fukunaga, A. 2017. Tie-Breaking Strategies for Cost-Optimal Best First Search. Journal of Artiﬁcial Intelligence Research 58: 67–121. Barley, M. W.; Franco, S.; and Riddle, P. J. 2014. Overcoming the Utility Problem in Heuristic Generation: Why Time Matters. In Chien, S.; Do, M.; Fern, A.; and Ruml, W., eds., Proceedings of the 24th International Conference on Automated Planning and Scheduling (ICAPS’14). AAAI Press. Chen, J.; Holte, R. C.; Zilles, S.; and Sturtevant, N. R. 2017. Front-to-End Bidirectional Heuristic Search with Near-Optimal Node Expansions. In Sierra, C., ed., Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence (IJCAI’17), 489–495. AAAI Press/IJCAI. Correˆa, A. B.; Pereira, A. G.; and Ritt, M. 2018. Analyzing Tie-Breaking Strategies for the A* Algorithm. In Lang, J., ed., Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence (IJCAI’18), 4715–4721. ijcai.org. Dechter, R.; and Pearl, J. 1985. Generalized Best-First Search Strategies and the Optimality of A*. Journal of the Association for Computing Machinery 32(3): 505–536. Eckerle, J.; Chen, J.; Sturtevant, N. R.; Zilles, S.; and Holte, R. C. 2017. Sufﬁcient Conditions for Node Expansion in Bidirectional Heuristic Search. In Proceedings of the 27th International Conference on Automated Planning and Scheduling (ICAPS’17), 79–87. AAAI Press. Goldenberg, M.; Felner, A.; Stern, R.; Sharon, G.; Sturtevant, N. R.; Holte, R. C.; and Schaeffer, J. 2014. Enhanced Partial Expansion A*. Journal of Artiﬁcial Intelligence Research 50: 141–187. Hall, D.; Cohen, A.; Burkett, D.; and Klein, D. 2013. Faster Optimal Planning with Partial-Order Pruning. In Borrajo, D.; Fratini, S.; Kambhampati, S.; and Oddi, A., eds., Proceedings of the 23rd International Conference on Automated Planning and Scheduling (ICAPS’13). Rome, Italy: AAAI Press. Hart, P. E.; Nilsson, N. J.; and Raphael, B. 1968. A Formal Basis for the Heuristic Determination of Minimum Cost Paths. IEEE Transactions on Systems Science and Cybernetics 4(2): 100–107. Helmert, M.; Lattimore, T.; Lelis, L. H. S.; Orseau, L.; and Sturtevant, N. R. 2019. Iterative Budgeted Exponential Search. In Kraus, S., ed., Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence (IJCAI’19), 1249–1257. ijcai.org.

Karpas, E.; Betzalel, O.; Shimony, S. E.; Tolpin, D.; and Felner, A. 2018. Rational deployment of multiple heuristics in optimal state-space search. Artiﬁcial Intelligence 256: 181– 210.
Korf, R. E. 1985. Depth-First Iterative-Deepening: An Optimal Admissible Tree Search. Artiﬁcial Intelligence 27(1): 97–109.
Shleyfman, A.; Katz, M.; Helmert, M.; Sievers, S.; and Wehrle, M. 2015. Heuristics and Symmetries in Classical Planning. In Bonet, B.; and Koenig, S., eds., Proceedings of the 29th AAAI Conference on Artiﬁcial Intelligence (AAAI’15), 3371–3377. AAAI Press.
Sievers, S.; Wehrle, M.; Helmert, M.; Shleyfman, A.; and Katz, M. 2015. Factored Symmetries for Merge-and-Shrink Abstractions. In Bonet, B.; and Koenig, S., eds., Proceedings of the 29th AAAI Conference on Artiﬁcial Intelligence (AAAI’15), 3378–3385. AAAI Press.
Sturtevant, N.; and Helmert, M. 2020. A Guide to Budgeted Tree Search. In Harabor, D.; and Vallati, M., eds., Proceedings of the Thirteenth International Symposium on Combinatorial Search, SOCS’20, 75–81. AAAI Press. URL https://www.aaai.org/Library/SOCS/socs20contents.php. Torralba, A´ . 2017. From Qualitative to Quantitative Dominance Pruning for Optimal Planning. In Sierra, C., ed., Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence (IJCAI’17), 4426–4432. AAAI Press/IJCAI. Torralba, A´ .; and Hoffmann, J. 2015. Simulation-Based Admissible Dominance Pruning. In Yang, Q., ed., Proceedings of the 24th International Joint Conference on Artiﬁcial Intelligence (IJCAI’15), 1689–1695. AAAI Press/IJCAI.

12014

