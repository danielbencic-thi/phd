<!DOCTYPE html> <html class="js postmessage history draganddrop borderimage borderradius boxshadow textshadow cssgradients csstransforms csstransforms3d csstransitions generatedcontent localstorage sessionstorage" style lang=en-US><!--
 Page saved with SingleFile 
 url: https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654?arnumber=9007654 
 saved date: Wed Apr 20 2022 13:40:30 GMT+0200 (CEST)
--><meta charset=utf-8>
<meta name=Description id=meta-description content="Autonomous mobile robots are becoming more prominent in recent time because of their relevance and applications to the world today. Their ability to navigate in">
<link rel=canonical href="https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/?arnumber=9007654">
<meta name=viewport content="width=device-width, initial-scale=1.0">
<meta name=customerIndustry content=NA>
<title>A Review on Challenges of Autonomous Mobile Robot and Sensor Fusion Methods | IEEE Journals &amp; Magazine | IEEE Xplore</title>
<meta property=twitter:title content="A Review on Challenges of Autonomous Mobile Robot and Sensor Fusion Methods">
<meta property=og:title content="A Review on Challenges of Autonomous Mobile Robot and Sensor Fusion Methods">
<meta property=twitter:description content="Autonomous mobile robots are becoming more prominent in recent time because of their relevance and applications to the world today. Their ability to navigate in an environment without a need for physical or electro-mechanical guidance devices has made it more promising and useful. The use of autonomous mobile robots is emerging in different sectors such as companies, industries, hospital, institutions, agriculture and homes to improve services and daily activities. Due to technology advancement, the demand for mobile robot has increased due to the task they perform and services they render such as carrying heavy objects, monitoring, search and rescue missions, etc. Various studies have been carried out by researchers on the importance of mobile robot, its applications and challenges. This survey paper unravels the current literatures, the challenges mobile robot is being faced with. A comprehensive study on devices/sensors and prevalent sensor fusion techniques developed for tackling issues like localization, estimation and navigation in mobile robot are presented as well in which they are organised according to relevance, strengths and weaknesses. The study therefore gives good direction for further investigation on developing methods to deal with the discrepancies faced with autonomous mobile robot.">
<meta property=og:description content="Autonomous mobile robots are becoming more prominent in recent time because of their relevance and applications to the world today. Their ability to navigate in an environment without a need for physical or electro-mechanical guidance devices has made it more promising and useful. The use of autonomous mobile robots is emerging in different sectors such as companies, industries, hospital, institutions, agriculture and homes to improve services and daily activities. Due to technology advancement, the demand for mobile robot has increased due to the task they perform and services they render such as carrying heavy objects, monitoring, search and rescue missions, etc. Various studies have been carried out by researchers on the importance of mobile robot, its applications and challenges. This survey paper unravels the current literatures, the challenges mobile robot is being faced with. A comprehensive study on devices/sensors and prevalent sensor fusion techniques developed for tackling issues like localization, estimation and navigation in mobile robot are presented as well in which they are organised according to relevance, strengths and weaknesses. The study therefore gives good direction for further investigation on developing methods to deal with the discrepancies faced with autonomous mobile robot.">
<meta name=twitter:card content=summary_large_image>
<meta property=twitter:image content=https://ieeexplore-ieee-org.thi.idm.oclc.org/ielx7/6287639/8948470/9007654/graphical_abstract/access-gagraphic-2975643.jpg>
<meta property=og:image content=https://ieeexplore-ieee-org.thi.idm.oclc.org/ielx7/6287639/8948470/9007654/graphical_abstract/access-gagraphic-2975643.jpg>
<style>.cc-window{opacity:1;transition:opacity 1s ease}.cc-link{text-decoration:underline}.cc-window{position:fixed;overflow:hidden;box-sizing:border-box;font-family:Helvetica,Calibri,Arial,sans-serif;font-size:16px;line-height:1.5em;display:-ms-flexbox;display:flex;-ms-flex-wrap:nowrap;flex-wrap:nowrap;z-index:9999}.cc-window.cc-banner{padding:1em 1.8em;width:100%;-ms-flex-direction:row;flex-direction:row}.cc-btn,.cc-link{cursor:pointer}.cc-link{opacity:.8;display:inline-block;padding:.2em}.cc-link:hover{opacity:1}.cc-link:active,.cc-link:visited{color:initial}.cc-btn{display:block;padding:.4em .8em;font-size:.9em;font-weight:700;border-width:2px;border-style:solid;text-align:center;white-space:nowrap}.cc-banner .cc-btn:last-child{min-width:140px}.cc-window.cc-banner{-ms-flex-align:center;align-items:center}.cc-banner.cc-bottom{left:0;right:0;bottom:0}.cc-banner .cc-message{-ms-flex:1;flex:1}.cc-compliance{display:-ms-flexbox;display:flex;-ms-flex-align:center;align-items:center;-ms-flex-line-pack:justify;align-content:space-between}.cc-compliance>.cc-btn{-ms-flex:1;flex:1}@media screen and (max-width:900px){.cc-btn{white-space:normal}}@media screen and (max-width:414px) and (orientation:portrait),screen and (max-width:736px) and (orientation:landscape){.cc-window.cc-bottom{bottom:0}.cc-window.cc-banner{left:0;right:0}.cc-window.cc-banner{-ms-flex-direction:column;flex-direction:column}.cc-window.cc-banner .cc-compliance{-ms-flex:1;flex:1}.cc-window .cc-message{margin-bottom:1em}.cc-window.cc-banner{-ms-flex-align:unset;align-items:unset}}</style>
<meta http-equiv=X-UA-Compatible content="IE=Edge">
<meta http-equiv=origin-trial content="AxujKG9INjsZ8/gUq8+dTruNvk7RjZQ1oFhhgQbcTJKDnZfbzSTE81wvC2Hzaf3TW4avA76LTZEMdiedF1vIbA4AAABueyJvcmlnaW4iOiJodHRwczovL2ltYXNkay5nb29nbGVhcGlzLmNvbTo0NDMiLCJmZWF0dXJlIjoiVHJ1c3RUb2tlbnMiLCJleHBpcnkiOjE2NTI3NzQ0MDAsImlzVGhpcmRQYXJ0eSI6dHJ1ZX0="><meta http-equiv=origin-trial content=Azuce85ORtSnWe1MZDTv68qpaW3iHyfL9YbLRy0cwcCZwVnePnOmkUJlG8HGikmOwhZU22dElCcfrfX2HhrBPAkAAAB7eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiVHJ1c3RUb2tlbnMiLCJleHBpcnkiOjE2NTI3NzQ0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9><meta http-equiv=origin-trial content=A16nvcdeoOAqrJcmjLRpl1I6f3McDD8EfofAYTt/P/H4/AWwB99nxiPp6kA0fXoiZav908Z8etuL16laFPUdfQsAAACBeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiVHJ1c3RUb2tlbnMiLCJleHBpcnkiOjE2NTI3NzQ0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9><meta http-equiv=origin-trial content=AxBHdr0J44vFBQtZUqX9sjiqf5yWZ/OcHRcRMN3H9TH+t90V/j3ENW6C8+igBZFXMJ7G3Pr8Dd13632aLng42wgAAACBeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiVHJ1c3RUb2tlbnMiLCJleHBpcnkiOjE2NTI3NzQ0MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9><meta http-equiv=origin-trial content="A88BWHFjcawUfKU3lIejLoryXoyjooBXLgWmGh+hNcqMK44cugvsI5YZbNarYvi3roc1fYbHA1AVbhAtuHZflgEAAAB2eyJvcmlnaW4iOiJodHRwczovL2dvb2dsZS5jb206NDQzIiwiZmVhdHVyZSI6IlRydXN0VG9rZW5zIiwiZXhwaXJ5IjoxNjUyNzc0NDAwLCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content=A8FHS1NmdCwGqD9DwOicnHHY+y27kdWfxKa0YHSGDfv0CSpDKRHTQdQmZVPDUdaFWUsxdgVxlwAd6o+dhJykPA0AAACWeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiQ29udmVyc2lvbk1lYXN1cmVtZW50IiwiZXhwaXJ5IjoxNjQzMTU1MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlLCJ1c2FnZSI6InN1YnNldCJ9><meta http-equiv=origin-trial content=A8zdXi6dr1hwXEUjQrYiyYQGlU3557y5QWDnN0Lwgj9ePt66XMEvNkVWOEOWPd7TP9sBQ25X0Q15Lr1Nn4oGFQkAAACceyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiQ29udmVyc2lvbk1lYXN1cmVtZW50IiwiZXhwaXJ5IjoxNjQzMTU1MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlLCJ1c2FnZSI6InN1YnNldCJ9><meta http-equiv=origin-trial content=A4/Htern2udN9w3yJK9QgWQxQFruxOXsXL7cW60DyCl0EZFGCSme/J33Q/WzF7bBkVvhEWDlcBiUyZaim5CpFQwAAACceyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiQ29udmVyc2lvbk1lYXN1cmVtZW50IiwiZXhwaXJ5IjoxNjQzMTU1MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlLCJ1c2FnZSI6InN1YnNldCJ9><style>#MathJax_Message{position:fixed;left:1px;bottom:2px;background-color:#E6E6E6;border:1px solid #959595;margin:0px;padding:2px 8px;z-index:102;color:black;font-size:80%;width:auto;white-space:nowrap}.color-xplore-blue{color:#069}.color-gray-dark{color:#B7B7B7}@media only screen and (min-width:768px){.body-resp:not(.ea-only) .icon-size-md{font-size:25px}}@media only screen and (max-width:767px){.icon-size-md{font-size:20px}}@media only screen and (max-width:767px){.icon-size-md{font-size:20px}}@media only screen and (min-width:768px){.body-resp:not(.ea-only) .text-sm-md-lh{font-size:15px!important;line-height:20px!important}.body-resp:not(.ea-only) .text-base-md-lh{font-size:18px!important;line-height:30px!important}.body-resp:not(.ea-only) .text-2xl-md-lh{font-size:32px!important;line-height:38px!important}}span~i,span~span{padding-left:0.25rem}a~a{padding-left:0.25rem}.global-content-width-w-rr{width:calc(100% - 330px);flex:0 0 calc(100% - 330px);max-width:100%}.global-right-rail{box-sizing:border-box;max-width:330px!important;flex:0 0 330px}.global-ng-wrapper[_ngcontent-xlc-c451]{max-width:1680px;margin:0 auto;min-height:80vh;box-sizing:border-box}@media only screen and (min-width:768px){.global-ng-wrapper[_ngcontent-xlc-c451]{padding-right:15px;padding-left:15px}}.ng2-xplore-meta-nav #global-header-cart-count{padding-right:.5rem;border-right:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav #global-header-cart-count{padding-right:0}}.ng2-xplore-meta-nav .xplore-meta-nav{display:flex;width:100%;background-color:#17445a;box-sizing:border-box;padding:.35rem 15px;max-width:1680px}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links{width:auto;max-width:none}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links .meta-nav-menu{-webkit-padding-start:0;padding-inline-start:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links .meta-nav-item:last-child{padding-right:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links{width:auto;max-width:none;margin-left:auto}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .nav-right{width:100%}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .cart-container{margin-left:auto;display:flex}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .icons-panel{padding-right:0;padding-left:0}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links{flex:none;max-width:none;margin-left:0;width:100%;box-sizing:border-box}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item{list-style:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item{padding-right:0}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item:last-child{padding-right:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{list-style:none;font-size:14px;padding-right:.75rem}@media only screen and (max-width:991px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{padding-right:.5rem}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{padding-right:0}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:last-child){border-right:1px solid #fff}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:last-child){border-right:none}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:first-child){padding-left:.75rem}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:first-child){padding-left:0}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item>a{text-decoration:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item>a{padding-left:.75rem}}.ng2-xplore-meta-nav .xplore-meta-nav a{color:#fff}.ng2-xplore-meta-nav .xplore-meta-nav .ieee-xplore{color:#e4a42c}.ng2-xplore-meta-nav .xplore-meta-nav .personal-signin-container{position:relative;z-index:1049}.main-header[_ngcontent-xlc-c445]{position:relative;display:flex;flex-direction:column;background-color:#14303e}.search-bar-container[_ngcontent-xlc-c445]{z-index:20}.fill-background[_ngcontent-xlc-c445]{display:flex;justify-content:center;align-items:center;position:relative;background-color:#14303e;top:auto;left:auto;transform:translate(0);width:100%;min-height:115px}.menu-link[_ngcontent-xlc-c455]{padding:.25rem .5rem;white-space:nowrap}.primary-menu[_ngcontent-xlc-c455]{flex-grow:1;min-width:301px;max-width:400px;margin-top:5px}.primary-menu[_ngcontent-xlc-c455] ul[_ngcontent-xlc-c455]{display:flex;list-style-type:none;flex-grow:1;justify-content:space-around;margin:0;padding:0 1.5rem 0 .75rem}.primary-menu[_ngcontent-xlc-c455] a[_ngcontent-xlc-c455]{color:#fff}.hamburger-menu[_ngcontent-xlc-c455]{width:33%}.hamburger-menu[_ngcontent-xlc-c455] a[_ngcontent-xlc-c455]{font-size:2rem;display:flex;color:#fff}.institution-container[_ngcontent-xlc-c455]{flex-shrink:0}.institution-container.inst-logged-in[_ngcontent-xlc-c455]{margin-top:0}.right-side-container[_ngcontent-xlc-c455]{flex-basis:50%;flex-shrink:1;display:flex;flex-direction:row-reverse}.right-side-container.inst-logged-in[_ngcontent-xlc-c455]{width:auto;padding-left:1rem;margin-left:auto}@media only screen and (max-width:767px){.right-side-container[_ngcontent-xlc-c455]{width:33%}}.inst-logged-in[_ngcontent-xlc-c455] .left-side-container[_ngcontent-xlc-c455]{min-width:465px}.left-side-container[_ngcontent-xlc-c455]{flex-basis:50%;flex-shrink:1}@media only screen and (max-width:767px){.left-side-container[_ngcontent-xlc-c455]{width:33%}}.left-side-content[_ngcontent-xlc-c455]{display:flex;justify-content:flex-start}@media only screen and (max-width:767px){.left-side-content[_ngcontent-xlc-c455]{justify-content:center}}.left-side-content[_ngcontent-xlc-c455] .xplore-logo-wrapper[_ngcontent-xlc-c455]{margin-top:-5px}.navbar-container[_ngcontent-xlc-c455]{box-sizing:border-box;display:flex;flex-direction:column;flex-grow:1;z-index:1004;width:100%;max-width:1680px;padding:18px 15px}@media only screen and (max-width:767px){.navbar-container[_ngcontent-xlc-c455]{padding-top:0}}.inst-details-container[_ngcontent-xlc-c455]{flex-grow:1}.top-navbar[_ngcontent-xlc-c455]{display:flex}@media only screen and (max-width:767px){.top-navbar[_ngcontent-xlc-c455]{padding:1rem 1rem .5rem}}.bottom-navbar[_ngcontent-xlc-c455]{display:flex;justify-content:center;padding-top:calc(18px - 5px)}.navbar-container.not-homepage[_ngcontent-xlc-c455]{position:relative;margin:0 auto;padding-bottom:1.5rem}@media screen and (-ms-high-contrast:none){.navbar-container[_ngcontent-xlc-c455]{max-width:none;width:1460px}}[_nghost-xlc-c458]{width:100%}.not-homepage [_nghost-xlc-c458] .search-bar-wrapper[_ngcontent-xlc-c458]{margin-bottom:0}.not-homepage [_nghost-xlc-c458] .search-bar[_ngcontent-xlc-c458] .below-search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458]{justify-content:unset;margin-left:auto;margin-right:0}.not-homepage [_nghost-xlc-c458] .search-bar[_ngcontent-xlc-c458] .below-search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458] .advanced-search-div[_ngcontent-xlc-c458]{background-color:transparent;padding:0;margin:0;min-width:0}@media only screen and (max-width:767px){.not-homepage [_nghost-xlc-c458] .search-bar[_ngcontent-xlc-c458] .below-search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458] .advanced-search-div[_ngcontent-xlc-c458]{padding-right:.5rem}}.not-homepage [_nghost-xlc-c458] .search-bar[_ngcontent-xlc-c458] .below-search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458] .advanced-search-div[_ngcontent-xlc-c458] a[_ngcontent-xlc-c458]{font-size:12px}@media only screen and (max-width:767px){.not-homepage [_nghost-xlc-c458] .search-bar[_ngcontent-xlc-c458] .below-search-bar[_ngcontent-xlc-c458]{padding-left:.5rem}}.search-bar[_ngcontent-xlc-c458]{max-width:960px;margin:0 auto}.search-bar[_ngcontent-xlc-c458] .below-search-bar[_ngcontent-xlc-c458]{display:flex;justify-content:center;max-width:780px;flex-grow:1;margin:0 auto}.search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458]{display:flex;margin:0 32%}@media only screen and (max-width:767px){.search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458]{flex-direction:column;margin:.5rem auto 0;max-width:230px}}.search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458]>div[_ngcontent-xlc-c458]{text-align:center;max-width:200px;border-radius:.2rem}@media only screen and (max-width:767px){.search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458]>div[_ngcontent-xlc-c458]{margin:1rem 4% 1%}}.search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458]>div[_ngcontent-xlc-c458]>a[_ngcontent-xlc-c458]{color:#fff;font-family:Lato-Bold,Arial,sans-serif;font-weight:700}.search-bar[_ngcontent-xlc-c458] .advanced-search-wrapper[_ngcontent-xlc-c458]>div[_ngcontent-xlc-c458]>a[_ngcontent-xlc-c458]:hover{text-decoration:none}.search-bar-wrapper[_ngcontent-xlc-c458]{display:flex;max-width:780px;background-color:rgba(0,0,0,.1);padding:.5em;margin:0 auto 1%;justify-content:space-between}.search-bar-wrapper[_ngcontent-xlc-c458]>.drop-down[_ngcontent-xlc-c458]{flex-grow:0.08}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-xlc-c458]>.drop-down[_ngcontent-xlc-c458]{flex-grow:1;max-width:4rem}}.search-bar-wrapper[_ngcontent-xlc-c458]>.drop-down[_ngcontent-xlc-c458]>label[_ngcontent-xlc-c458]{position:relative}.search-bar-wrapper[_ngcontent-xlc-c458]>.drop-down[_ngcontent-xlc-c458]>label[_ngcontent-xlc-c458]:after{content:"\f0d7";font-family:Font Awesome\ 5 Pro;font-size:22px;font-weight:900;color:#e4a42c;right:4px;top:-6px;padding:0 2%;position:absolute;pointer-events:none;height:19px;width:15px}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-xlc-c458]>.drop-down[_ngcontent-xlc-c458]>label[_ngcontent-xlc-c458]:after{font-size:20px;right:5px;top:-4px;text-align:center}}.search-bar-wrapper[_ngcontent-xlc-c458]>.drop-down[_ngcontent-xlc-c458]>label[_ngcontent-xlc-c458]:before{content:"";right:4px;top:0;background:#fff;position:absolute;pointer-events:none;display:block}.search-bar-wrapper[_ngcontent-xlc-c458]>.drop-down[_ngcontent-xlc-c458]>label[_ngcontent-xlc-c458]>select[_ngcontent-xlc-c458]{padding:.35em .35em .35em .5rem;width:100%;height:100%;margin:0;color:#000;border:none;font-weight:700;background-color:#ddd;border-radius:0;-webkit-appearance:none;-moz-appearance:none}.search-bar-wrapper[_ngcontent-xlc-c458] .search-field[_ngcontent-xlc-c458]{display:flex;justify-content:space-evenly;flex-grow:1}.search-bar-wrapper[_ngcontent-xlc-c458] .search-field[_ngcontent-xlc-c458]>div[_ngcontent-xlc-c458]{flex-grow:1;padding-right:5%}.search-bar-wrapper[_ngcontent-xlc-c458] .search-field[_ngcontent-xlc-c458]>div[_ngcontent-xlc-c458]:last-child{padding-right:0}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-xlc-c458] .search-field[_ngcontent-xlc-c458]>div[_ngcontent-xlc-c458]{padding-right:0}}.search-bar-wrapper[_ngcontent-xlc-c458] .search-field[_ngcontent-xlc-c458] .global-search-bar[_ngcontent-xlc-c458]{flex-grow:1}.search-bar-wrapper[_ngcontent-xlc-c458] .search-field-icon-container[_ngcontent-xlc-c458]{display:flex}.search-bar-wrapper[_ngcontent-xlc-c458] .search-icon[_ngcontent-xlc-c458]{background-color:#e4a42c;width:3.5625rem;display:flex;align-items:center;justify-content:center;cursor:pointer}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-xlc-c458] .search-icon[_ngcontent-xlc-c458]{order:1}}.search-bar-wrapper[_ngcontent-xlc-c458] .search-icon[_ngcontent-xlc-c458]>.fa-search[_ngcontent-xlc-c458]{width:29px;height:25px;color:#000;text-align:center}.global-search-bar[_ngcontent-xlc-c458]{position:relative}.xplore-logo-container[_ngcontent-xlc-c441]{display:flex;flex-direction:column}.xplore-logo-container[_ngcontent-xlc-c441] a[_ngcontent-xlc-c441]{align-self:center}.xplore-logo-container[_ngcontent-xlc-c441] img.xplore-logo[_ngcontent-xlc-c441]{width:160px;height:40px}.ieee-logo-container[_ngcontent-xlc-c440]{display:flex;flex-direction:column}.ieee-logo-container[_ngcontent-xlc-c440] .ieee-logo[_ngcontent-xlc-c440]{width:100px;align-self:flex-end}.footer-new[_ngcontent-xlc-c450]{display:flex;background-color:#17445a;padding-top:3rem;color:#fff}@media only screen and (max-width:767px){.footer-new[_ngcontent-xlc-c450]{padding-top:.1rem}}.footer-new[_ngcontent-xlc-c450]>div[_ngcontent-xlc-c450]{padding-bottom:2rem}.footer-new[_ngcontent-xlc-c450] h3[_ngcontent-xlc-c450]{font-weight:800;font-family:"IBM Plex Serif",Arial,sans-serif}.footer-new[_ngcontent-xlc-c450] ul[_ngcontent-xlc-c450]{list-style-type:none;padding-left:0}.footer-new[_ngcontent-xlc-c450] li[_ngcontent-xlc-c450]{padding:.35rem 0;text-transform:uppercase}.footer-new[_ngcontent-xlc-c450] a[_ngcontent-xlc-c450]{width:100%;text-decoration:none;color:#fff}.footer-new[_ngcontent-xlc-c450] a[_ngcontent-xlc-c450]:hover{color:#e4a42c}.footer-new[_ngcontent-xlc-c450] .follow[_ngcontent-xlc-c450] ul[_ngcontent-xlc-c450]{display:flex}.footer-new[_ngcontent-xlc-c450] .follow[_ngcontent-xlc-c450] ul[_ngcontent-xlc-c450] li[_ngcontent-xlc-c450]{padding:0 .5rem}.footer-new[_ngcontent-xlc-c450] .follow[_ngcontent-xlc-c450] ul[_ngcontent-xlc-c450] li[_ngcontent-xlc-c450]:first-child{padding-left:0}.footer-new[_ngcontent-xlc-c450] .footer-wrapper[_ngcontent-xlc-c450]{flex-grow:1;max-width:1680px;margin:0 auto}.footer-new[_ngcontent-xlc-c450] .flexible-row-col[_ngcontent-xlc-c450]{display:flex;padding-bottom:3rem}@media only screen and (max-width:767px){.footer-new[_ngcontent-xlc-c450] .flexible-row-col[_ngcontent-xlc-c450]{padding-bottom:.1rem;flex-direction:column}}.footer-new[_ngcontent-xlc-c450] .footer-col[_ngcontent-xlc-c450]{flex-grow:1;padding-left:2rem}@media only screen and (max-width:767px){.footer-new[_ngcontent-xlc-c450] .footer-col[_ngcontent-xlc-c450]{padding-top:1rem}.footer-new[_ngcontent-xlc-c450] .footer-col[_ngcontent-xlc-c450]:first-child{padding-top:2rem}.footer-new[_ngcontent-xlc-c450] .footer-col[_ngcontent-xlc-c450]:last-child{padding-bottom:.25rem}}.footer-new[_ngcontent-xlc-c450] .footer-bottom-section[_ngcontent-xlc-c450] p[_ngcontent-xlc-c450]{margin:0;padding-left:2rem;padding-right:2rem}.footer-new[_ngcontent-xlc-c450] .footer-bottom-section[_ngcontent-xlc-c450] p[_ngcontent-xlc-c450]:last-child{padding-top:1rem}.footer-new[_ngcontent-xlc-c450] .footer-bottom-section[_ngcontent-xlc-c450] p[_ngcontent-xlc-c450] span[_ngcontent-xlc-c450]{padding-left:0}.footer-new[_ngcontent-xlc-c450] .footer-bottom-section[_ngcontent-xlc-c450] p[_ngcontent-xlc-c450] .ethics-reporting-link[_ngcontent-xlc-c450] i[_ngcontent-xlc-c450]{padding-left:.25rem}.footer-new[_ngcontent-xlc-c450] .nowrap[_ngcontent-xlc-c450]{white-space:nowrap}.signout[_ngcontent-xlc-c442]{font-size:.75rem;padding-left:1rem;padding-right:1.25rem;padding-top:.5rem}.signout[_ngcontent-xlc-c442] a[_ngcontent-xlc-c442]{display:block}.inst-detail[_ngcontent-xlc-c442]{display:flex;background:#fff;margin-top:-18px;border-radius:0 0 .4rem .4rem;height:calc(18px*2 + 35px)}@media only screen and (max-width:767px){.inst-detail[_ngcontent-xlc-c442]{height:auto;border-radius:0;justify-content:center;flex-wrap:wrap}}.inst-text-container[_ngcontent-xlc-c442]{padding-left:.5rem;padding-right:.5rem;margin-top:.25rem;display:flex;margin-bottom:1rem;border-right:1px solid #ddd}.inst-text-container.no-inst-logo[_ngcontent-xlc-c442]{max-width:160px}.access-text[_ngcontent-xlc-c442]{white-space:nowrap}.access-text[_ngcontent-xlc-c442],.inst-name[_ngcontent-xlc-c442]{font-size:12px}.Typeahead-input[_ngcontent-xlc-c54]{border-radius:0}.disqus-container[_ngcontent-xlc-c191]{padding:0 1em}@media only screen and (max-width:767px){.document-sidebar[_ngcontent-xlc-c191]{position:absolute;height:calc(100% - 129px);z-index:11000;right:0;padding:0;max-width:40vw;transform:translateX(40vw);transition:transform .25s ease-in-out 0s}}@media only screen and (max-width:575px){.document-sidebar[_ngcontent-xlc-c191]{max-width:80vw;transform:translateX(80vw)}}@media only screen and (max-width:767px){.document-sidebar-content[_ngcontent-xlc-c191]{max-height:calc(100% - 129px);width:100%;overflow:scroll;position:absolute}}.document-sidebar.top-spacing[_ngcontent-xlc-c191]{margin-top:35px}@media only screen and (max-width:767px){.document-sidebar.top-spacing[_ngcontent-xlc-c191]{margin-top:8.5rem}}@media only screen and (min-width:768px){.document-sidebar-rel-art[_ngcontent-xlc-c191]{padding:0 15px 35px}}.header-rel-art-toggle-mobile[_ngcontent-xlc-c191]{top:-2px}.document-title[_ngcontent-xlc-c144]{letter-spacing:normal}.document-title[_ngcontent-xlc-c144]{margin:0}.document-title-fix[_ngcontent-xlc-c144]{flex-grow:1;width:100%}.pdf-btn-container[_ngcontent-xlc-c144]{margin-left:28px}.document-header-inner-container[_ngcontent-xlc-c144]{max-width:100%;width:100%}.document-header-breadcrumbs-container[_ngcontent-xlc-c144]{padding:.4rem 1rem .8rem;margin:0;font-size:.8em}.document-header-breadcrumbs-container[_ngcontent-xlc-c144] #help[_ngcontent-xlc-c144]{font-size:14px}.document-header-metrics-banner[_ngcontent-xlc-c144]{padding:.4rem 1rem .8rem;width:100%}.document-header-metrics-banner.ccby-document[_ngcontent-xlc-c144]{padding-bottom:0}.document-header-title-container[_ngcontent-xlc-c144]{padding:.4rem 1rem .8rem;display:flex}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-xlc-c144]{flex-direction:column}}.document-header-title-container[_ngcontent-xlc-c144] .right-container[_ngcontent-xlc-c144]{margin-left:auto;display:flex;flex-direction:column}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-xlc-c144] .right-container[_ngcontent-xlc-c144]{margin-left:0;margin-right:auto}}.document-banner-access[_ngcontent-xlc-c144]{width:100%;display:flex}.mobile-serp-nav[_ngcontent-xlc-c144]{padding:.4rem 1rem .8rem}.breadcrumbs-separator[_ngcontent-xlc-c144]{padding:.4rem}.btn-container[_ngcontent-xlc-c144]{display:flex}.btn-container[_ngcontent-xlc-c144] .cite-this-related-btn-wrapper[_ngcontent-xlc-c144] .cite-this-btn[_ngcontent-xlc-c144]{padding:.5rem;border:2px solid #069;font-weight:700}.publisher-title-tooltip[_ngcontent-xlc-c144]{margin-top:.3em;padding-right:30px}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-xlc-c144]{position:relative}}.copyright-icon[_ngcontent-xlc-c146]{font-size:1.25rem}.doc-share-tool[_ngcontent-xlc-c140] i[_ngcontent-xlc-c140]{font-size:1.2rem}.doc-share-tool[_ngcontent-xlc-c140] i.fa-share-alt[_ngcontent-xlc-c140]{color:#069}[_nghost-xlc-c158]{width:100%}.ft-toc[_ngcontent-xlc-c158]{height:52px;border-top:1px solid #e5e5e5;border-bottom:1px solid #e5e5e5}.ft-toc[_ngcontent-xlc-c158]>div[_ngcontent-xlc-c158]{margin-top:13px}.ft-toc[_ngcontent-xlc-c158] a.toc-link[_ngcontent-xlc-c158]{font-size:1.2em;font-weight:700}.ft-toc[_ngcontent-xlc-c158] a.toc-link[_ngcontent-xlc-c158]:hover{text-decoration:none}.ft-toc[_ngcontent-xlc-c158] a.toc-link[_ngcontent-xlc-c158] img[_ngcontent-xlc-c158]{position:relative;top:-2px;margin-right:.3em}.full-text-toc-wrapper[_ngcontent-xlc-c158] .previous-next-nav-ctrl[_ngcontent-xlc-c158]{overflow:visible}.stats-document-container-fullTextSection[_ngcontent-xlc-c158]{padding:0 15px}.stats-document-container-rh[_ngcontent-xlc-c158]{padding-right:1em;padding-left:1em}.toc-container[_ngcontent-xlc-c158]{margin-bottom:.7em;font-weight:700}.toc-container[_ngcontent-xlc-c158] a[_ngcontent-xlc-c158]:hover{text-decoration:none}.hide-full-text[_ngcontent-xlc-c158]{max-height:0;overflow:hidden}.accordion-header[_ngcontent-xlc-c163]{color:#069;display:flex;align-items:center}.accordion-header[_ngcontent-xlc-c163] .accordion-chevron[_ngcontent-xlc-c163] .fa[_ngcontent-xlc-c163]{font-size:1.5rem}.accordion-header[_ngcontent-xlc-c163]:hover{color:#0081c1}.document-all-references[_ngcontent-xlc-c190]{position:fixed;top:0;right:0;width:30vw;min-width:450px;padding:1rem 1rem 3rem;box-shadow:3px 10px 10px #000;overflow:auto;height:100vh;box-sizing:border-box;z-index:99999;transform:translateX(100%);background-color:#fff;transition:transform .25s ease-in-out 0s}.header[_ngcontent-xlc-c190]{display:flex;align-items:center;padding:.5rem 0}.header[_ngcontent-xlc-c190] h1[_ngcontent-xlc-c190]{margin:0;font-weight:400;color:#333}.header[_ngcontent-xlc-c190] a[_ngcontent-xlc-c190]{margin-left:auto;font-size:1.25rem;color:#333}i.help-link[_ngcontent-xlc-c59]{color:#069}i.help-link[_ngcontent-xlc-c59]:hover{color:#0081c1}.help-link-icon[_ngcontent-xlc-c59]{font-size:1.2rem}.breadcrumb-help-link-icon[_ngcontent-xlc-c59]{font-size:1rem}a[_ngcontent-xlc-c59]:hover{text-decoration:none}.pdf-btn-link[_ngcontent-xlc-c113]{height:36px;background-color:#ff3500;padding:0 25px;display:flex;align-items:center;justify-content:center;border-radius:2px;color:#fff}.pdf-btn-link[_ngcontent-xlc-c113] .icon[_ngcontent-xlc-c113]{font-size:1.15rem;color:#fff;margin-right:.45rem}.pdf-btn-link[_ngcontent-xlc-c113]>span[_ngcontent-xlc-c113]{font-weight:700}.red-pdf[_ngcontent-xlc-c113]{font-size:1.3rem;margin-right:.15rem;font-style:normal}.red-pdf[_ngcontent-xlc-c113],.red-pdf[_ngcontent-xlc-c113]:after{color:#fc0d1b}.copyright-icon[_ngcontent-xlc-c143]{font-size:1.25rem}.document-authors-banner[_ngcontent-xlc-c137] .authors-container[_ngcontent-xlc-c137]{padding:0 0 0 1rem}.stats-document-authors-banner[_ngcontent-xlc-c137]{padding:.25rem 1rem .25rem 0}.authors-minimized[_ngcontent-xlc-c137]{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.toc-container[_ngcontent-xlc-c145]{padding:.6em .5em;border-bottom:1px solid #ddd}.toc-heading[_ngcontent-xlc-c145]{font-size:1em;padding-bottom:.5em}.toc-list[_ngcontent-xlc-c145]{list-style:none;padding:0}.toc-list-item[_ngcontent-xlc-c145]{padding:.5em 0}.toc-list-link[_ngcontent-xlc-c145]{display:flex;font-size:.9em}.toc-list-icon[_ngcontent-xlc-c145]{margin-right:5px}.toc-show-more-btn[_ngcontent-xlc-c145]{font-size:.85em;font-weight:700}.abstract-graphic-asset[_ngcontent-xlc-c186]{width:450px!important}.header-rel-art-pub[_ngcontent-xlc-c189]{margin:.25rem 0}.header-rel-art-pub[_ngcontent-xlc-c189]:last-child{margin:0 0 .5rem}.header-rel-art[_ngcontent-xlc-c189]{font-size:.9375rem;color:#333;border:1px solid #e5e5e5;border-top:5px solid #0081c1;background-color:#f8f8f8}@media only screen and (max-width:767px){.header-rel-art[_ngcontent-xlc-c189]{border-bottom:none}}.header-rel-art-action[_ngcontent-xlc-c189] a[_ngcontent-xlc-c189],.header-rel-art-title[_ngcontent-xlc-c189]{font-family:Helvetica-Nue-Bold,Arial,sans-serif}.header-rel-art-list[_ngcontent-xlc-c189]{font-family:Helvetica Regular,Arial,sans-serif}.cc-color-override-170793312.cc-window{color:rgb(255,255,255);background-color:rgb(0,0,0)}.cc-color-override-170793312 .cc-link,.cc-color-override-170793312 .cc-link:active,.cc-color-override-170793312 .cc-link:visited{color:rgb(255,255,255)}.cc-color-override-170793312 .cc-btn{color:rgb(0,0,0);border-color:transparent;background-color:rgb(255,255,255)}.cc-color-override-170793312 .cc-btn:hover,.cc-color-override-170793312 .cc-btn:focus{background-color:rgb(255,255,255)}</style><meta name=cToken content=eyJhbGciOiJIUzUxMiIsInppcCI6IkRFRiJ9.eNqqVkosKFCyUoooyMkvSlXSUcosLgZyK2Dc1AqgrKGZqYGJqbmpkSlQPrEEJmBiYWRaCwAAAP__.LOTR0APk5XvR9skAgZClsSCj3BMH_ARSumOe-Mfm2x_k8wzy1OQHs3OVBb2AdIjvhkLEidpRaBnZcoN9-DyXCw class=sf-hidden><link type=image/x-icon rel="shortcut icon" href="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="><style>.sf-hidden{display:none!important}</style><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:;"><body class="body-resp cmpl_embed_complete"><div role=dialog aria-live=polite aria-label=cookieconsent aria-describedby=cookieconsent:desc class="cc-window cc-banner cc-type-info cc-theme-block cc-bottom cc-color-override-170793312"><span id=cookieconsent:desc class=cc-message>IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our <a aria-label="learn more about cookies" role=button tabindex=0 class=cc-link href=https://www-ieee-org.thi.idm.oclc.org/about/help/security_privacy.html target=_blank>Privacy Policy.</a></span><div class=cc-compliance><a aria-label="dismiss cookie message" role=button tabindex=0 class="cc-btn cc-dismiss">Accept &amp; Close</a></div></div><div style=display:none id=lightningjs-usabilla_live></div><div id=MathJax_Message>Loading [MathJax]/extensions/MathMenu.js</div><g:compress>
 <style>--></style>
 <style media="screen, print">--></style>
 <style>--></style>
 <style media="screen, print">--></style>
</g:compress>
 
 
 
 
 
 
 <p class=JumpLink id=PageTop><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/# title="Click here to Skip to main content" accesskey=s>Skip to Main Content</a></p>
 <div id=global-notification class="row stats-global-notification">
 <div class="hide u-hide-important col Notification Notification--global Notification--fixed">
 <a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class="Notification-close js-close" aria-label="close message button"><i class="fa fa-close"></i></a>
 <div class=Notification-header></div>
 <div class=Notification-text></div>
 </div>
 </div>
 <div id=LayoutWrapper>
 <div class=container-fluid>
 <div class=row>
 <div class=col>
 
 
 
 
 
<div class=Header id=xplore-header data-service=true data-inst=true data-web=false style=display:none></div>
 <div id=global-alert-message></div>
 

<div class=ng2-app>
 
 
 
 
 <div class=global-content-wrapper>
 <xpl-root _nghost-xlc-c451 ng-version=11.2.14><xpl-meta-nav _ngcontent-xlc-c451><div class=ng2-xplore-meta-nav><div class="metanav-container u-flex-display-flex u-flex-justify-center"><div class="stats-metanav xplore-meta-nav"><div class="meta-nav-ieee-links hide-mobile text-sm-md-lh"><ul class="meta-nav-menu u-flex-display-flex u-m-0"><li class="meta-nav-item stats-extLink stats-Unav_exit_aaa"><a href=http://www.ieee.org.thi.idm.oclc.org/ id=u-home class=ieeeorg>IEEE.org</a><li class="meta-nav-item stats-extLink ieee-xplore">IEEE <em>Xplore</em><li class="meta-nav-item stats-extLink"><a href=http://standards.ieee.org.thi.idm.oclc.org/ id=u-standards class=exitstandardsorg>IEEE SA</a><li class="meta-nav-item stats-extLink"><a href=http://spectrum.ieee.org.thi.idm.oclc.org/ id=u-spectrum class=exitspectrum>IEEE Spectrum</a><li class="meta-nav-item stats-extLink"><a href=http://www.ieee.org.thi.idm.oclc.org/sitemap.html id=u-more class=exitmoreieeesites>More Sites</a></ul></div><div class="meta-nav-user-links u-flex-display-flex text-sm-md-lh"><ul class="u-flex-display-flex u-relative u-m-0 nav-right icons-panel"><div class="col-4 hide-desktop"></div><div class=cart-container><li id=global-header-cart-count class="meta-nav-item stats-mnEvLinks"><a title="View Cart" tabindex=0 class="cart stats-Unav_exit_Cart" style=white-space:nowrap href="https://www-ieee-org.thi.idm.oclc.org/cart/public/myCart/page.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore"><span id=cartCount>Cart&nbsp;</span></a><div id=mc_ieee-mini-cart-include_wrapper class="content-r cart-summary product-cart" style=display:none></div><li class="meta-nav-item stats-mnEvLinks hide-desktop"><a title="Create Account" class="create-account-new stats-Unav_CreateAcct hide-desktop" href="https://www-ieee-org.thi.idm.oclc.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&amp;sourceCode=xplore&amp;car=IEEE-Xplore&amp;autoSignin=Y&amp;signinurl=https%3A%2F%2Fieeexplore-ieee-org.thi.idm.oclc.org%2FXplore%2Flogin.jsp%3Furl%3D%2FXplore%2Fhome.jsp%26reason%3Dauthenticate&amp;url=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654?arnumber=9007654"><i class="fas fa-user-plus"></i></a><li class="meta-nav-item stats-mnEvLinks u-flex-display-flex u-ml-auto personal-signin-container hide-desktop"><a title="Sign In" class="stats-Unav_P_SignIn hide-desktop"><i aria-hidden=true class="fas fa-sign-in-alt"></i></a></li></div><div class="u-flex-display-flex text-sm-md-lh"><li class="meta-nav-item stats-mnEvLinks"><a title="Create Account" class="create-account-new stats-Unav_CreateAcct hide-mobile" href="https://www-ieee-org.thi.idm.oclc.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&amp;sourceCode=xplore&amp;car=IEEE-Xplore&amp;autoSignin=Y&amp;signinurl=https%3A%2F%2Fieeexplore-ieee-org.thi.idm.oclc.org%2FXplore%2Flogin.jsp%3Furl%3D%2FXplore%2Fhome.jsp%26reason%3Dauthenticate&amp;url=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654?arnumber=9007654">Create Account</a><li class="meta-nav-item stats-mnEvLinks u-flex-display-flex u-ml-auto personal-signin-container"><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ title="Sign In" class="stats-Unav_P_SignIn hide-mobile u-pr-05">Personal Sign In</a></li></div></ul></div></div></div></div></xpl-meta-nav><xpl-global-notification _ngcontent-xlc-c451 _nghost-xlc-c68></xpl-global-notification><xpl-header _ngcontent-xlc-c451 _nghost-xlc-c445><div _ngcontent-xlc-c445 class=main-header><xpl-navbar _ngcontent-xlc-c445 _nghost-xlc-c455><div _ngcontent-xlc-c455 class="navbar-container not-homepage inst-logged-in"><div _ngcontent-xlc-c455 class=top-navbar><div _ngcontent-xlc-c455 class="hamburger-menu hide-desktop"><a _ngcontent-xlc-c455><i _ngcontent-xlc-c455 aria-hidden=true class="fa fa-bars"></i></a></div><div _ngcontent-xlc-c455 class=left-side-container><div _ngcontent-xlc-c455 class=left-side-content><div _ngcontent-xlc-c455 class=xplore-logo-wrapper><xpl-xplore-logo _ngcontent-xlc-c455 _nghost-xlc-c441><div _ngcontent-xlc-c441 class=xplore-logo-container><a _ngcontent-xlc-c441 accesskey=1 title="Delivering full text access to the world's highest quality technical literature in engineering and technology" alt="IEEE Advancing Technology for Humanity" href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplore/home.jsp><img _ngcontent-xlc-c441 alt="IEEE Xplore logo - Link to home" class=xplore-logo src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></a></div></xpl-xplore-logo></div><div _ngcontent-xlc-c455 class="primary-menu hide-mobile text-base-md-lh"><ul _ngcontent-xlc-c455><li _ngcontent-xlc-c455><div _ngcontent-xlc-c455><a _ngcontent-xlc-c455 tabindex=0 class="menu-link stats-browse-book"> Browse <i _ngcontent-xlc-c455 aria-hidden=true class="fas fa-chevron-down"></i></a></div><li _ngcontent-xlc-c455><div _ngcontent-xlc-c455><a _ngcontent-xlc-c455 tabindex=0 class="menu-link stats-my-settings"> My Settings <i _ngcontent-xlc-c455 aria-hidden=true class="fas fa-chevron-down"></i></a></div><li _ngcontent-xlc-c455><div _ngcontent-xlc-c455><a _ngcontent-xlc-c455 tabindex=0 class="menu-link stats-get-help"> Help <i _ngcontent-xlc-c455 aria-hidden=true class="fas fa-chevron-down"></i></a></div></ul></div></div></div><div _ngcontent-xlc-c455 class="institution-container hide-mobile inst-logged-in"><div _ngcontent-xlc-c455><xpl-institution-details _ngcontent-xlc-c455 _nghost-xlc-c442><div _ngcontent-xlc-c442><div _ngcontent-xlc-c442 class=inst-detail><div _ngcontent-xlc-c442 class=u-flex-display-flex><div _ngcontent-xlc-c442 class="inst-text-container no-inst-logo"><span _ngcontent-xlc-c442 class=right-line><span _ngcontent-xlc-c442 class=access-text>Access provided by:</span><h4 _ngcontent-xlc-c442 class=inst-name>Technische Hochschule Ingolstadt</h4></span></div><div _ngcontent-xlc-c442 class=signout><a _ngcontent-xlc-c442 title="Sign Out" target=_self href="https://ieeexplore-ieee-org.thi.idm.oclc.org/servlet/Login?logout=/document/9007654/?arnumber=9007654">Sign Out</a></div></div></div></div></xpl-institution-details></div></div><div _ngcontent-xlc-c455 class="right-side-container inst-logged-in"><div _ngcontent-xlc-c455 class=row><xpl-ieee-logo _ngcontent-xlc-c455 _nghost-xlc-c440><div _ngcontent-xlc-c440 class="ieee-logo-container hide-mobile"><img _ngcontent-xlc-c440 alt="IEEE logo - Link to IEEE main site homepage" class=ieee-logo src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></div></xpl-ieee-logo></div></div></div><div _ngcontent-xlc-c455 class="bottom-navbar hide-desktop"><div _ngcontent-xlc-c455 class=inst-details-container><xpl-institution-details _ngcontent-xlc-c455 _nghost-xlc-c442><div _ngcontent-xlc-c442><div _ngcontent-xlc-c442 class=inst-detail><div _ngcontent-xlc-c442 class=u-flex-display-flex><div _ngcontent-xlc-c442 class="inst-text-container no-inst-logo"><span _ngcontent-xlc-c442 class=right-line><span _ngcontent-xlc-c442 class=access-text>Access provided by:</span><h4 _ngcontent-xlc-c442 class=inst-name>Technische Hochschule Ingolstadt</h4></span></div><div _ngcontent-xlc-c442 class=signout><a _ngcontent-xlc-c442 title="Sign Out" target=_self href="https://ieeexplore-ieee-org.thi.idm.oclc.org/servlet/Login?logout=/Xplore/guesthome.jsp">Sign Out</a></div></div></div></div></xpl-institution-details></div></div></div></xpl-navbar><div _ngcontent-xlc-c445><div _ngcontent-xlc-c445 class="search-bar-container fill-background not-homepage"><xpl-search-bar-migr _ngcontent-xlc-c445 _nghost-xlc-c458><div _ngcontent-xlc-c458 class=search-bar><form _ngcontent-xlc-c458 novalidate class="search-bar-wrapper ng-untouched ng-pristine ng-valid"><div _ngcontent-xlc-c458 class=drop-down><label _ngcontent-xlc-c458><select _ngcontent-xlc-c458 aria-label="content type dropdown"><option _ngcontent-xlc-c458 selected>All<option _ngcontent-xlc-c458>Books<option _ngcontent-xlc-c458>Conferences<option _ngcontent-xlc-c458>Courses<option _ngcontent-xlc-c458>Journals &amp; Magazines<option _ngcontent-xlc-c458>Standards<option _ngcontent-xlc-c458>Authors<option _ngcontent-xlc-c458>Citations</select></label></div><div _ngcontent-xlc-c458 class="search-field all"><div _ngcontent-xlc-c458 class=search-field-icon-container><div _ngcontent-xlc-c458 class=global-search-bar><xpl-typeahead-migr _ngcontent-xlc-c458 placeholder name=search-term ulclass="search-within-results ui-autocomplete ui-front ui-menu ui-widget ui-widget-content ui-corner-all" minchars=3 _nghost-xlc-c54><div _ngcontent-xlc-c54 class="Typeahead text-sm-md-lh"><input _ngcontent-xlc-c54 autocomplete=off aria-label="Enter search text" class="Typeahead-input ng-untouched ng-pristine ng-valid" placeholder value type=text></div></xpl-typeahead-migr></div><div _ngcontent-xlc-c458 class=search-icon><button _ngcontent-xlc-c458 type=submit aria-label=Search class="fa fa-search"></button></div></div></div></form><div _ngcontent-xlc-c458 class=below-search-bar><div _ngcontent-xlc-c458 class="advanced-search-wrapper text-sm-md-lh"><div _ngcontent-xlc-c458 class=advanced-search-div><a _ngcontent-xlc-c458 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/search/advanced target=_self><span _ngcontent-xlc-c458>ADVANCED SEARCH </span><i _ngcontent-xlc-c458 aria-hidden=true class="fas fa-caret-right adv-search-arrow sf-hidden"></i></a></div></div></div></div></xpl-search-bar-migr></div></div></div></xpl-header><div _ngcontent-xlc-c451 class=global-ng-wrapper><router-outlet _ngcontent-xlc-c451></router-outlet><xpl-document-details _nghost-xlc-c191><div _ngcontent-xlc-c191 class="row document ng-document stats-document"><div _ngcontent-xlc-c191 class="document-main global-content-width-w-rr"><section _ngcontent-xlc-c191 class="document-main-leaderboard-ad col-12"><xpl-leaderboard-ad _ngcontent-xlc-c191 class=hide-desktop _nghost-xlc-c124><div _ngcontent-xlc-c124 class="Ads-leaderboard ad-panel" style=display:none><div _ngcontent-xlc-c124 class="ad-leaderboard-ad-container sf-hidden"><div _ngcontent-xlc-c124 xplgoogleadmigr class="Ads-leaderBoardTablet sf-hidden"></div><div _ngcontent-xlc-c124 xplgoogleadmigr class="Ads-leaderBoardMobile sf-hidden"></div></div></div></xpl-leaderboard-ad></section><section _ngcontent-xlc-c191 class="document-main-header row"><div _ngcontent-xlc-c191 class=col-12><xpl-document-header _ngcontent-xlc-c191 _nghost-xlc-c144><section _ngcontent-xlc-c144 class="document-header row"><div _ngcontent-xlc-c144 class="document-header-breadcrumbs-container col-12"><div _ngcontent-xlc-c144 class="breadcrumbs col text-sm-md-lh"><span _ngcontent-xlc-c144><a _ngcontent-xlc-c144 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/browse/periodicals/title/>Journals &amp; Magazines</a><span _ngcontent-xlc-c144 class=breadcrumbs-separator> &gt;</span></span><span _ngcontent-xlc-c144><a _ngcontent-xlc-c144 href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a><span _ngcontent-xlc-c144 class=breadcrumbs-separator> &gt;</span></span><span _ngcontent-xlc-c144><a _ngcontent-xlc-c144 href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/tocresult.jsp?isnumber=8948470">Volume: 8</a><span _ngcontent-xlc-c144 class=breadcrumbs-separator></span></span><xpl-help-link _ngcontent-xlc-c144 id=help tooltiptype=breadcrumb _nghost-xlc-c59><a _ngcontent-xlc-c59 target=_blank tooltipclass=helplink-tooltip triggers=hover class="icon-size-md u-flex-display-inline" href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/ieee-xplore-training/working-with-documents#interactive-html><i _ngcontent-xlc-c59 class="fa fa-question-circle help-link breadcrumb-help-link-icon"></i></a></xpl-help-link></div></div><div _ngcontent-xlc-c144 class="document-header-inner-container row"><div _ngcontent-xlc-c144 class=col-12><div _ngcontent-xlc-c144 class="row stats-document-header"><div _ngcontent-xlc-c144 class="row document-title-fix"><div _ngcontent-xlc-c144 class="document-header-title-container col"><div _ngcontent-xlc-c144 class=left-container><h1 _ngcontent-xlc-c144 class="document-title text-2xl-md-lh"><span _ngcontent-xlc-c144>A Review on Challenges of Autonomous Mobile Robot and Sensor Fusion Methods</span></h1><div _ngcontent-xlc-c144 class="u-mb-1 u-mt-05 btn-container"><div _ngcontent-xlc-c144 class=publisher-title-tooltip><xpl-publisher _ngcontent-xlc-c144 tooltipplacement=right _nghost-xlc-c105><span _ngcontent-xlc-c105 class="text-base-md-lh publisher-info-container black-tooltip"><span _ngcontent-xlc-c105 xplhighlight><span _ngcontent-xlc-c105><span _ngcontent-xlc-c105 class=title>Publisher: </span><span _ngcontent-xlc-c105>IEEE</span></span></span></span></xpl-publisher></div><div _ngcontent-xlc-c144 class=cite-this-related-btn-wrapper><xpl-cite-this-modal _ngcontent-xlc-c144 _nghost-xlc-c125><div _ngcontent-xlc-c144><button _ngcontent-xlc-c144 placement=bottom class="layout-btn-white cite-this-btn">Cite This</button></div></xpl-cite-this-modal></div><div _ngcontent-xlc-c144 class="black-tooltip tool-tip-pdf-button"><div _ngcontent-xlc-c144 placement=bottom class="pdf-btn-container hide-mobile"><xpl-view-pdf _ngcontent-xlc-c144 placement=document-page-desktop _nghost-xlc-c113><div _ngcontent-xlc-c113><div _ngcontent-xlc-c113><a _ngcontent-xlc-c113 class="pdf-btn-link stats-document-lh-action-downloadPdf_2 pdf" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/stamp/stamp.jsp?tp=&amp;arnumber=9007654"><i _ngcontent-xlc-c113 class="icon-size-md icon red-pdf fas fa-file-pdf"></i><span _ngcontent-xlc-c113>PDF</span></a></div></div></xpl-view-pdf><xpl-login-modal-trigger _ngcontent-xlc-c144 _nghost-xlc-c135></xpl-login-modal-trigger></div></div></div></div><div _ngcontent-xlc-c144 class=right-container></div></div></div><div _ngcontent-xlc-c144 class="mobile-serp-nav col-18-24 hide-desktop"><span _ngcontent-xlc-c144 class=mobile-serp-nav-item> &nbsp;<a _ngcontent-xlc-c144 target=_self href="https://ieeexplore-ieee-org.thi.idm.oclc.org/search/searchresult.jsp?contentType=all&amp;queryText=9007654">&lt;&lt;&nbsp;Results&nbsp;</a>&nbsp; </span></div><div _ngcontent-xlc-c144 class=document-main-subheader><div _ngcontent-xlc-c144 class=document-main-author-banner><div _ngcontent-xlc-c144 class="document-authors-banner stats-document-authors-banner"><div _ngcontent-xlc-c144 class="row authors-banner-row u-flex-align-items-center u-flex-wrap-nowrap"><xpl-author-banner _ngcontent-xlc-c144 class=authors-banner-row-middle _nghost-xlc-c137><div _ngcontent-xlc-c137 class="document-authors-banner stats-document-authors-banner"><div _ngcontent-xlc-c137 class="row authors-banner-row u-flex-wrap-nowrap"><div _ngcontent-xlc-c137 class=authors-banner-row-middle><div _ngcontent-xlc-c137 class="authors-container stats-document-authors-banner-authorsContainer"><div _ngcontent-xlc-c137 class="authors-info-container overflow-ellipsis text-base-md-lh authors-minimized" id=indexTerms-container-1650454827174-0><span _ngcontent-xlc-c137 class=authors-info><span _ngcontent-xlc-c137 class=blue-tooltip><a _ngcontent-xlc-c137 placement=bottom triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37086218878><span _ngcontent-xlc-c137>Mary B. Alatise</span></a></span><span _ngcontent-xlc-c137 class=u-px-02><a _ngcontent-xlc-c137 target=_blank href=https://orcid.org/0000-0002-3868-6533><i _ngcontent-xlc-c137 class="icon icon-orcid"></i></a></span><span _ngcontent-xlc-c137>; </span></span><span _ngcontent-xlc-c137 class=authors-info><span _ngcontent-xlc-c137 class=blue-tooltip><a _ngcontent-xlc-c137 placement=bottom triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37284723700><span _ngcontent-xlc-c137>Gerhard P. Hancke</span></a></span><span _ngcontent-xlc-c137></span></span></div></div></div></div></div></xpl-author-banner><div _ngcontent-xlc-c144 class="u-flex-display-flex u-flex-align-items-center nowrap text-base-md-lh"><div _ngcontent-xlc-c144 class="authors-view-all-link-container hide-mobile"><a _ngcontent-xlc-c144 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class=text-base-md-lh>All Authors</a></div><div _ngcontent-xlc-c144 class="authors-mobile-view-all-container blue-tooltip hide-desktop"><a _ngcontent-xlc-c144 placement=bottom-right triggers=click:click class=authors-viewall-link><i _ngcontent-xlc-c144 class=authors-viewall-icon></i></a></div></div></div></div></div><div _ngcontent-xlc-c144 class="document-header-metrics-banner row ccby-document"><div _ngcontent-xlc-c144 class="document-banner col stats-document-banner"><xpl-login-modal-trigger _ngcontent-xlc-c144 _nghost-xlc-c135></xpl-login-modal-trigger><button _ngcontent-xlc-c144 class="sip-modal-button stats-document-banner-viewDocument"><div _ngcontent-xlc-c144 class=main-txt> View Document </div></button><div _ngcontent-xlc-c144 class="document-banner-metric-container row"><button _ngcontent-xlc-c144 class="document-banner-metric text-base-md-lh col"><div _ngcontent-xlc-c144 class=document-banner-metric-count>40</div><div _ngcontent-xlc-c144>Paper</div><div _ngcontent-xlc-c144>Citations</div></button><button _ngcontent-xlc-c144 class="document-banner-metric text-base-md-lh col"><div _ngcontent-xlc-c144 class=document-banner-metric-count>8826</div><div _ngcontent-xlc-c144><div _ngcontent-xlc-c144>Full</div><div _ngcontent-xlc-c144>Text Views</div></div></button></div><div _ngcontent-xlc-c144 class=document-banner-access><div _ngcontent-xlc-c144 class="document-access-container hide-mobile"><div _ngcontent-xlc-c144 class=document-access-icon><i _ngcontent-xlc-c144 class="icon-size-md u-mr-05 fas fa-lock-open-alt"></i><span _ngcontent-xlc-c144>Open Access</span></div></div><div _ngcontent-xlc-c144 class="document-disqus-anchor-container hide-mobile-imp"><a _ngcontent-xlc-c144 tabindex=0><i _ngcontent-xlc-c144 class="icon-size-md color-xplore-blue fas fa-comment u-mr-05"></i><span _ngcontent-xlc-c144>Comment(s)</span></a></div></div></div><div _ngcontent-xlc-c144 class=document-mobile-access-container><div _ngcontent-xlc-c144 class=document-access-icon><i _ngcontent-xlc-c144 class="icon-size-md u-mr-05 fas fa-lock-open-alt"></i></div></div><div _ngcontent-xlc-c144 class="document-disqus-anchor-container hide-desktop"><a _ngcontent-xlc-c144 tabindex=0><i _ngcontent-xlc-c144 class="icon-size-md color-xplore-blue fas fa-comment"></i></a></div><div _ngcontent-xlc-c144 class="col-7-24 black-tooltip hide-mobile"><xpl-document-toolbar _ngcontent-xlc-c144 _nghost-xlc-c143><div _ngcontent-xlc-c143 class="col-actions stats-document-container-lh u-printing-invisible-ie u-printing-invisible-ff"><div _ngcontent-xlc-c143 class=action-item-container><ul _ngcontent-xlc-c143 class="icon-size-md doc-actions doc-toolbar stats-document-lh-actions black-tooltip"><li _ngcontent-xlc-c143 placement=bottom class=doc-actions-item><a _ngcontent-xlc-c143 target=blank class="doc-actions-link stats_ReferencesView_Doc_Details_9007654" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/dwnldReferences?arnumber=9007654"><i _ngcontent-xlc-c143 class="icon-size-md color-xplore-blue fas fa-registered"></i></a><li _ngcontent-xlc-c143 placement=bottom class="doc-actions-item white-blue-border-tooltip"><xpl-document-social-media _ngcontent-xlc-c143 _nghost-xlc-c140><button _ngcontent-xlc-c140 triggers=click class=doc-share-tool><i _ngcontent-xlc-c140 aria-hidden=true class="fa fa-share-alt"></i></button></xpl-document-social-media><li _ngcontent-xlc-c143 placement=bottom class="stats-permission doc-actions-item disabled-look enable-hover"><a _ngcontent-xlc-c143 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class="doc-actions-link stats_Doc_Details_Copyright_9007654"><i _ngcontent-xlc-c143 class="color-xplore-blue copyright-icon far fa-copyright"></i></a><li _ngcontent-xlc-c143 placement=bottom class="doc-actions-item white-blue-border-tooltip save-to disabled-look enable-hover"><a _ngcontent-xlc-c143 placement=bottom-right triggers=click href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class=doc-save-tool><i _ngcontent-xlc-c143 class="icon-size-md color-xplore-blue fas fa-folder-open"></i></a><li _ngcontent-xlc-c143 placement=bottom class=doc-actions-item><xpl-manage-alerts _ngcontent-xlc-c143 class="white-blue-border-tooltip alerts-popover" _nghost-xlc-c141><a _ngcontent-xlc-c141 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ triggers=click:click class="doc-actions-link stats-document-lh-action-alerts hide-mobile"><i _ngcontent-xlc-c141 class="icon-size-md color-xplore-blue fas fa-bell"></i><span _ngcontent-xlc-c141 class=doc-actions-text>Alerts</span></a><div _ngcontent-xlc-c141 class="manage-alerts-popover-content hide-desktop"><h1 _ngcontent-xlc-c141 class=header>Alerts</h1><div _ngcontent-xlc-c141 class=manage-alerts-link><a _ngcontent-xlc-c141 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/alerts/citation> Manage Content Alerts <i _ngcontent-xlc-c141 class="icon icon-courses-chevron-blue"></i></a></div><div _ngcontent-xlc-c141 class=manage-alerts-link><a _ngcontent-xlc-c141 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/> Add to Citation Alerts <i _ngcontent-xlc-c141 class="icon icon-courses-chevron-blue"></i></a></div></div></xpl-manage-alerts></ul></div></div></xpl-document-toolbar></div></div><div _ngcontent-xlc-c144 class="ccby-indicator u-pl-2"> Under a <a _ngcontent-xlc-c144 target=_blank href=https://creativecommons.org/licenses/by/4.0/>Creative Commons License</a></div></div></div></div></div><hr _ngcontent-xlc-c144></section></xpl-document-header></div></section><div _ngcontent-xlc-c191 class="row document-main-body"><div _ngcontent-xlc-c191 class="document-main-left-trail col-5-24"><div _ngcontent-xlc-c191 class=col-24-24><div _ngcontent-xlc-c191 class=row><nav _ngcontent-xlc-c191 class="col-24-24 bg-ltgry tab-nav text-base-md-lh"><div _ngcontent-xlc-c191 id=document-tabs class="doc-tabs-list stats-document-tabs"><div _ngcontent-xlc-c191 routerlinkactive=active class="browse-pub-tab active"><a _ngcontent-xlc-c191 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654>Abstract</a></div><xpl-full-text-toc _ngcontent-xlc-c191 class=hide-mobile _nghost-xlc-c145><div _ngcontent-xlc-c145 class=toc-container><div _ngcontent-xlc-c145 class=toc-heading>Document Sections</div><ul _ngcontent-xlc-c145 class=toc-list><li _ngcontent-xlc-c145 class=toc-list-item><a _ngcontent-xlc-c145 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class=toc-list-link tabindex=0><div _ngcontent-xlc-c145 class=toc-list-icon>I.</div><div _ngcontent-xlc-c145 class=toc-list-icon></div><div _ngcontent-xlc-c145>Introduction</div></a><li _ngcontent-xlc-c145 class=toc-list-item><a _ngcontent-xlc-c145 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class=toc-list-link tabindex=0><div _ngcontent-xlc-c145 class=toc-list-icon>II.</div><div _ngcontent-xlc-c145 class=toc-list-icon></div><div _ngcontent-xlc-c145>Challenges of Autonomous Mobile Robot</div></a><li _ngcontent-xlc-c145 class=toc-list-item><a _ngcontent-xlc-c145 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class=toc-list-link tabindex=0><div _ngcontent-xlc-c145 class=toc-list-icon>III.</div><div _ngcontent-xlc-c145 class=toc-list-icon></div><div _ngcontent-xlc-c145>Sensors and Techniques in Mobile Robot Positioing</div></a><li _ngcontent-xlc-c145 class=toc-list-item><a _ngcontent-xlc-c145 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class=toc-list-link tabindex=0><div _ngcontent-xlc-c145 class=toc-list-icon>IV.</div><div _ngcontent-xlc-c145 class=toc-list-icon></div><div _ngcontent-xlc-c145>Object Recognition and Feature Matching</div></a><li _ngcontent-xlc-c145 class=toc-list-item><a _ngcontent-xlc-c145 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class=toc-list-link tabindex=0><div _ngcontent-xlc-c145 class=toc-list-icon>V.</div><div _ngcontent-xlc-c145 class=toc-list-icon></div><div _ngcontent-xlc-c145>Sensor Fusion Techniques</div></a></ul><button _ngcontent-xlc-c145 class=toc-show-more-btn><span _ngcontent-xlc-c145>Show Full Outline</span><i _ngcontent-xlc-c145 class="fa fa-caret-down"></i></button></div></xpl-full-text-toc><div _ngcontent-xlc-c191 routerlinkactive=active class=browse-pub-tab><a _ngcontent-xlc-c191 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/authors>Authors</a></div><div _ngcontent-xlc-c191 routerlinkactive=active class=browse-pub-tab><a _ngcontent-xlc-c191 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/figures>Figures</a></div><div _ngcontent-xlc-c191 routerlinkactive=active class=browse-pub-tab><a _ngcontent-xlc-c191 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/references>References</a></div><div _ngcontent-xlc-c191 routerlinkactive=active class=browse-pub-tab><a _ngcontent-xlc-c191 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/citations>Citations</a></div><div _ngcontent-xlc-c191 routerlinkactive=active class=browse-pub-tab><a _ngcontent-xlc-c191 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/keywords>Keywords</a></div><div _ngcontent-xlc-c191 routerlinkactive=active class=browse-pub-tab><a _ngcontent-xlc-c191 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/metrics>Metrics</a></div><div _ngcontent-xlc-c191 routerlinkactive=active class="browse-pub-tab similar"><a _ngcontent-xlc-c191 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/similar>More Like This</a></div></div></nav></div></div></div><div _ngcontent-xlc-c191 class=document-main-content-container><xpl-left-side-bar _ngcontent-xlc-c191 _nghost-xlc-c146><div _ngcontent-xlc-c146 xplscrollsnapmigr scrollreset=true offsetfrom=100 fromelementid=mobile-tab-pane tillelementid=full-text-footer offsetto=-800 cssclasstostick=document-mobile-leftrail-stick class="col-2 col-actions ng-col-actions hide-desktop stats-document-container-lh u-printing-invisible-ie u-printing-invisible-ff col-actions-mobile-closed ng-col-actions-mobile-closed"><div _ngcontent-xlc-c146 id=left-rail-container><div _ngcontent-xlc-c146 class=doc-actions-mobile-expand-button></div><ul _ngcontent-xlc-c146 class="doc-actions stats-document-lh-actions"><li _ngcontent-xlc-c146 class=doc-actions-item><xpl-view-pdf _ngcontent-xlc-c146 placement=document-page-mobile _nghost-xlc-c113><div _ngcontent-xlc-c113><div _ngcontent-xlc-c113><a _ngcontent-xlc-c113 target=_blank class="doc-actions-link stats-document-lh-action-downloadPdf_2 pdf" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/stamp/stamp.jsp?tp=&amp;arnumber=9007654"><i _ngcontent-xlc-c113 class="icon-size-md icon red-pdf fas fa-file-pdf"></i> Download PDF </a></div></div></xpl-view-pdf><xpl-login-modal-trigger _ngcontent-xlc-c146 _nghost-xlc-c135></xpl-login-modal-trigger><li _ngcontent-xlc-c146 class=doc-actions-item><a _ngcontent-xlc-c146 target=blank class="doc-actions-link stats_ReferencesView_Doc_Details_9007654" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/dwnldReferences?arnumber=9007654"><i _ngcontent-xlc-c146 class="icon-size-md color-xplore-blue fas fa-registered"></i> View References </a><li _ngcontent-xlc-c146 class="doc-actions-item white-blue-border-tooltip"><a _ngcontent-xlc-c146 class=doc-actions-link><xpl-document-social-media _ngcontent-xlc-c146 tooltipplacement=right placement=document-page-mobile _nghost-xlc-c140><button _ngcontent-xlc-c140 triggers=click class=doc-share-tool><i _ngcontent-xlc-c140 aria-hidden=true class="fa fa-share-alt"></i></button></xpl-document-social-media></a><li _ngcontent-xlc-c146 class="stats-permission doc-actions-item disabled-look black-tooltip"><a _ngcontent-xlc-c146 placement=right triggers=click class="doc-actions-link stats_Doc_Details_Copyright_9007654"><i _ngcontent-xlc-c146 class="copyright-icon far fa-copyright"></i> Request Permissions </a><li _ngcontent-xlc-c146 class="doc-actions-item disabled-look black-tooltip"><a _ngcontent-xlc-c146 placement=right triggers=click autoclose=outside class="doc-actions-link stats-document-lh-action-downloadPdf_3"><i _ngcontent-xlc-c146 class="icon-size-md color-xplore-blue fas fa-folder-open"></i> Save to </a><li _ngcontent-xlc-c146 class=doc-actions-item><a _ngcontent-xlc-c146 class="doc-actions-link stats-document-lh-action-alerts"><i _ngcontent-xlc-c146 class="icon-size-md color-xplore-blue fas fa-bell"></i> Alerts </a></ul></div></div></xpl-left-side-bar><section _ngcontent-xlc-c191 class="tab-pane col-24-24 u-printing-display-inline-ie u-printing-display-inline-ff"><div _ngcontent-xlc-c191 id=mobile-tab-pane></div><div _ngcontent-xlc-c191 class=document-main-left-trail-content><div _ngcontent-xlc-c191><router-outlet _ngcontent-xlc-c191></router-outlet><xpl-document-abstract _nghost-xlc-c186><section _ngcontent-xlc-c186 class="document-abstract document-tab"><div _ngcontent-xlc-c186 class="col-12 hide-desktop mobile-graphical-abstract"><div _ngcontent-xlc-c186 class=abstract-graphic><div _ngcontent-xlc-c186 class=mobile-graphic><img _ngcontent-xlc-c186 class=abstract-graphic-img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="Challenges of mobile robot."></div><div _ngcontent-xlc-c186 class=abstract-graphic-caption><span _ngcontent-xlc-c186 xplmathjax>Challenges of mobile robot.</span></div></div></div><div _ngcontent-xlc-c186 class="abstract-mobile-div hide-desktop"><div _ngcontent-xlc-c186 class=row><div _ngcontent-xlc-c186 class=mobile-col-12><div _ngcontent-xlc-c186 class=u-pb-1><strong _ngcontent-xlc-c186> Abstract:</strong><span _ngcontent-xlc-c186 xplmathjax>Autonomous mobile robots are becoming more prominent in recent time because of their relevance and applications to the world today. Their ability to navigate in an enviro...</span><span _ngcontent-xlc-c186><a _ngcontent-xlc-c186 class=mobile-toggle-btn>View more</a></span></div></div></div><div _ngcontent-xlc-c186 class="metadata-toggle-btn mobile-content"><strong _ngcontent-xlc-c186><i _ngcontent-xlc-c186 class=icon-caret-abstract></i><span _ngcontent-xlc-c186>Metadata</span></strong></div></div><div _ngcontent-xlc-c186 class="abstract-desktop-div hide-mobile text-base-md-lh"><div _ngcontent-xlc-c186 class="abstract-text row"><div _ngcontent-xlc-c186 class=col-12><div _ngcontent-xlc-c186 class=u-mb-1><strong _ngcontent-xlc-c186> Abstract:</strong><div _ngcontent-xlc-c186 xplmathjax>Autonomous mobile robots are becoming more prominent in recent time because of their relevance and applications to the world today. Their ability to navigate in an environment without a need for physical or electro-mechanical guidance devices has made it more promising and useful. The use of autonomous mobile robots is emerging in different sectors such as companies, industries, hospital, institutions, agriculture and homes to improve services and daily activities. Due to technology advancement, the demand for mobile robot has increased due to the task they perform and services they render such as carrying heavy objects, monitoring, search and rescue missions, etc. Various studies have been carried out by researchers on the importance of mobile robot, its applications and challenges. This survey paper unravels the current literatures, the challenges mobile robot is being faced with. A comprehensive study on devices/sensors and prevalent sensor fusion techniques developed for tackling issues like localization, estimation and navigation in mobile robot are presented as well in which they are organised according to relevance, strengths and weaknesses. The study therefore gives good direction for further investigation on developing methods to deal with the discrepancies faced with autonomous mobile robot.</div></div></div></div><div _ngcontent-xlc-c186 data-tealium_data='{"docType": "Journal"}' class="u-pb-1 stats-document-abstract-publishedIn"><strong _ngcontent-xlc-c186>Published in: </strong><a _ngcontent-xlc-c186 class=stats-document-abstract-publishedIn href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a><span _ngcontent-xlc-c186> ( <span _ngcontent-xlc-c186>Volume: 8</span>) </span></div><div _ngcontent-xlc-c186 class="row u-pt-1"><div _ngcontent-xlc-c186 class=col-6><div _ngcontent-xlc-c186 class=u-pb-1><strong _ngcontent-xlc-c186>Page(s): </strong> 39830 <span _ngcontent-xlc-c186>- 39846</span></div><div _ngcontent-xlc-c186 class="u-pb-1 doc-abstract-pubdate"><strong _ngcontent-xlc-c186>Date of Publication:</strong> 24 February 2020 <xpl-help-link _ngcontent-xlc-c186 arialabel="Get help with using Publication Dates" helplinktext="Help with using Publication Dates" helplink=http://ieeexplore.ieee.org.thi.idm.oclc.org/Xplorehelp/Help_Pubdates.html _nghost-xlc-c59><a _ngcontent-xlc-c59 target=_blank tooltipclass=helplink-tooltip triggers=hover class="icon-size-md u-flex-display-inline" href=http://ieeexplore.ieee.org.thi.idm.oclc.org/Xplorehelp/Help_Pubdates.html aria-label="Get help with using Publication Dates"><i _ngcontent-xlc-c59 class="fa fa-question-circle help-link help-link-icon"></i></a></xpl-help-link></div><div _ngcontent-xlc-c186 class=u-pb-1><div _ngcontent-xlc-c186><div _ngcontent-xlc-c186><strong _ngcontent-xlc-c186>Electronic ISSN:</strong> 2169-3536 </div></div></div></div><div _ngcontent-xlc-c186 class=col-6><div _ngcontent-xlc-c186 class=u-pb-1><strong _ngcontent-xlc-c186>INSPEC Accession Number: </strong> 19419238 </div><div _ngcontent-xlc-c186 class="u-pb-1 stats-document-abstract-doi"><strong _ngcontent-xlc-c186>DOI: </strong><a _ngcontent-xlc-c186 append-to-href="?src=document" target=_blank href=https://doi-org.thi.idm.oclc.org/10.1109/ACCESS.2020.2975643>10.1109/ACCESS.2020.2975643</a></div><div _ngcontent-xlc-c186 class="u-pb-1 doc-abstract-publisher"><xpl-publisher _ngcontent-xlc-c186 _nghost-xlc-c105><span _ngcontent-xlc-c105 class="text-base-md-lh publisher-info-container black-tooltip"><span _ngcontent-xlc-c105 xplhighlight><span _ngcontent-xlc-c105><span _ngcontent-xlc-c105 class=title>Publisher: </span><span _ngcontent-xlc-c105>IEEE</span></span></span></span></xpl-publisher></div></div></div><div _ngcontent-xlc-c186 class=row><div _ngcontent-xlc-c186 class=col-12><div _ngcontent-xlc-c186 class=abstract-graphic><div _ngcontent-xlc-c186 class=abstract-graphic-asset><img _ngcontent-xlc-c186 class=abstract-graphic-img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt="Challenges of mobile robot."></div><div _ngcontent-xlc-c186 class=abstract-graphic-caption><span _ngcontent-xlc-c186 xplmathjax>Challenges of mobile robot.</span></div></div></div><div _ngcontent-xlc-c186 class="show-full-abstract col-12"><a _ngcontent-xlc-c186 class=document-abstract-toggle-btn> Hide Full Abstract <i _ngcontent-xlc-c186 class="fa fa-angle-up"></i></a></div></div></div></section></xpl-document-abstract></div><xpl-leaderboard-middle-ad _ngcontent-xlc-c191 class=hide-desktop _nghost-xlc-c187><div _ngcontent-xlc-c187 class="Ads-leaderboard ad-panel"><div _ngcontent-xlc-c187 class="row u-flex-wrap-nowrap"><div _ngcontent-xlc-c187 class=ads-close-container><i _ngcontent-xlc-c187 aria-hidden=true class=ads-close-button></i></div></div><div _ngcontent-xlc-c187 class=ad-leaderboard-ad-container><div _ngcontent-xlc-c187 xplgoogleadmigr class=Ads-leaderBoardMiddleTablet><div id=div-gpt-ad-1606861708257-0 style="width:576px;height:71px;display:none;margin:0px auto;padding-bottom:0.5em"></div></div><div _ngcontent-xlc-c187 xplgoogleadmigr class=Ads-leaderBoardMiddleMobile><div id=div-gpt-ad-1606861708357-0 style="width:320px;height:50px;margin:0px auto;padding-bottom:0.5em" data-google-query-id=CPjY7K3HovcCFZWldwodV_0Ckg><div id=google_ads_iframe_/3890430/IEEEXplore/DocDetailsMiddle_1__container__ style="border:0pt none"></div></div></div></div></div></xpl-leaderboard-middle-ad><xpl-document-full-text _ngcontent-xlc-c191 _nghost-xlc-c158><section _ngcontent-xlc-c158><div _ngcontent-xlc-c158 id=toc-wrapper class="row full-text-toc-wrapper"><div _ngcontent-xlc-c158 xplscrollsnapmigr cssclasstostick=document-toc-stick fromelementid=toc-wrapper tillelementid=full-text-footer offsetfrom=150 offsetto=-800 scrollreset=true class="col-12 u-align-center ft-toc previous-next-nav-ctrl hide-desktop"><div _ngcontent-xlc-c158 class="toc-container hide-desktop"><a _ngcontent-xlc-c158 ngclass="{'disabled': !toc}" class="toc-link {'disabled': !toc}"><img _ngcontent-xlc-c158 src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="> Contents </a></div></div></div><hr _ngcontent-xlc-c158><div _ngcontent-xlc-c158 class="row document-full-text-content"><div _ngcontent-xlc-c158 id=full-text-section class="col col-text stats-document-container-fullTextSection u-printing-display-inline-ie u-printing-display-inline-ff" style=font-size:15px><span _ngcontent-xlc-c158 id=full-text-header></span><div _ngcontent-xlc-c158><div _ngcontent-xlc-c158 xplmathjax xplfulltextdomhandler xpllazyloadfigures class="document-text hide-full-text ng-non-bindable stats-document-dynamicFullTextOrSnippet-container">
<response><accesstype>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via <a class=vglnk href=https://creativecommons.org/licenses/by/4.0/ rel=nofollow><span>https</span><span>://</span><span>creativecommons</span><span>.</span><span>org</span><span>/</span><span>licenses</span><span>/</span><span>by</span><span>/</span><span>4</span><span>.</span><span>0</span><span>/</span></a> to obtain full-text articles and stipulations in the API documentation.</accesstype><div id=BodyWrapper class=ArticlePage xmlns:ieee=http://www.ieeexplore.ieee.org.thi.idm.oclc.org><div id=article>
<div class=section id=sec1><div class="header article-hdr"><div class=kicker>
 SECTION I.</div><h2>Introduction</h2></div><p>An autonomous mobile robot is a system that operates in an unpredictable and partially unknown environment. This means the robot must have the ability to navigate without disruption and having the capability to avoid any obstacle placed within the confinement of movement. Autonomous Mobile Robot (AMR) has little or no human intervention for its movement and it is designed in such a way to follow a predefined path be it in an indoor or outdoor environment. For indoor navigation, the mobile robot is based on floor plan, sonar sensing, Inertial Measurement Unit (IMU) etc. The first autonomous navigation was based on planar sensors such as laser range finder such that they navigate without human supervision. For an autonomous mobile robot to perform its task, it must have a range of environmental sensors. These sensors are either mounted on the robot or serve as an external sensor positioned somewhere in the environment. The number of different type of sensors mounted on the mobile robot to perform complex tasks such as estimation and localization makes the design of the overall system very tasking <a ref-type=bibr anchor=ref1 id=context_ref_1_1>[1]</a>–<a ref-type=bibr anchor=ref2 id=context_ref_2_1>[2]</a><a ref-type=bibr anchor=ref3 id=context_ref_3_1>[3]</a>. The basics of mobile robotics consist of locomotion, perception and navigation.<div class=section_2 id=sec1a><h3>A. Locomotion</h3><p>Locomotion system is an important aspect of mobile robot design which does not only rely on the medium in which the robot moves but also on other factors such as manoeuvrability, controllability, terrain conditions, efficiency, stability, and so on <a ref-type=bibr anchor=ref4 id=context_ref_4_1a>[4]</a>. The design of mobile robot is dependent on the service to be rendered; therefore, a mobile robot can be designed to walk, run, jump, fly etc. With the requirement of the designed robot, they are categorised into stationary and mobility: on land, water or air. Mobile robots especially autonomous are in high demand because of their ability and capacity to perform tasks that may seem difficult for humans. Examples of such designed mobile robots are wheeled, legged, walking or hybrid. Legged, wheeled, and articulated bodies are the main ways how mobile robot locomote <a ref-type=bibr anchor=ref5 id=context_ref_5_1a>[5]</a>. The wheeled robots are suited to ground either soft or hard ground while the legged and articulated bodies requires a certain degree of freedom and therefore greater mechanical complexity sets in <a ref-type=bibr anchor=ref6 id=context_ref_6_1a>[6]</a>. The wheel has been by far the most famous locomotion mechanism in mobile robotics and in vehicles in general. The advantages of wheel are efficiencies and simplicity. The use of wheels is easier and cheaper to build, design and program than its other counterparts. The control is less complex, and they cause minimum wear and tear on the surface where they move on. Another advantage is that they do not have issues with balancing because of its consistent contact with their mobility areas. The shortcoming of wheels is that they are not suitable at navigating over obstacles, such as stony terrain, unsmooth surfaces <a ref-type=bibr anchor=ref4 id=context_ref_4_1a>[4]</a>. To design and develop the locomotion system, the terrain type for the mobile robot must be identified. The types of terrain are: Uneven, Level Ground, Stair Up, Stair Down and Nontraversable <a ref-type=bibr anchor=ref5 id=context_ref_5_1a>[5]</a>. Another factor to consider when designing a mobile robot is stability. Stability is not usually a great problem in wheeled robot, because they are designed in such that all the wheels are always in contact with the ground. The use of four-wheeled is more stable than three, two and one because the Center of Gravity (COG) is located at the centre space of the wheels. In recent time, mobile robots are being designed to operate in two or more modes to improve performance. In <a ref-type=bibr anchor=ref7 id=context_ref_7_1a>[7]</a>, the author proposed a mechanism structure for the mobile robot with the advantage of adaptability using hybrid modes with active wheels. On a rough terrain the robot locomote using the leg mode while for smooth terrain it makes use of the wheeled locomotion by roller-skating using the passive wheels. The challenging part is that the wheels are usually very heavy and huge because they require driving actuators, steering and braking devices. Therefore, installation of the active wheels usually adds up to the whole weight of the vehicle which is already hefty enough limiting the versatility of the leg mechanism. To improve the localization of a mobile robot irrespective of the terrain, a technique has to be deployed. Dead reckoning has been already extended to the case of a mobile robot moving on uneven terrain. It gives information about positioning for mobile robots by directly computing the parameters such as position, velocity and orientation <a ref-type=bibr anchor=ref8 id=context_ref_8_1a>[8]</a>.</p></div><div class=section_2 id=sec1b><h3>B. Perception</h3><p>It is very important for an autonomous mobile robot to acquire information from its environment, sense objects around itself, or its relative position. Perception is an imperative aspect in mobile robot study. If a mobile robot is unable to observe its environment correctly and efficiently, to perform tasks such as estimating the position of an object accurately maybe an issue <a ref-type=bibr anchor=ref9 id=context_ref_9_1b>[9]</a>. To achieve this, information are perceived by the use of sensors and other related devices <a ref-type=bibr anchor=ref10 id=context_ref_10_1b>[10]</a>. Sensors make it possible to autonomously perform robot localization. They are also used for data collection, object identification, mapping and representation. Sensors used in the area of data collection is categorised into two major aspect; Proprioceptive/ exteroceptive sensors and active/passive sensors. Proprioceptive sensors measure values internally to the system (robot), e.g. battery level, wheel position, joint angle, motor speed etc. These sensors can be encoders, potentiometers, gyroscopes, compasses, etc. Exteroceptive sensors are used to extract information from the environments or objects. Sonar sensors, Infrared (IR) sensitive sensors, ultrasonic distance sensors are some examples of exteroceptive sensors. Active sensors emit their own energy into the environment, and then measure the environmental response. They often achieved a good performance due to their ability to manage interactions with the environment. Furthermore, an active sensor may suffer from interference between its signal and environment <a ref-type=bibr anchor=ref11 id=context_ref_11_1b>[11]</a>. Examples of active sensors include sonar sensors, radars etc. While passive sensors receive energy to make observation like camera such as Charge Coupled Device (CCD) or Complementary Metal Oxide Semiconductor (CMOS) cameras, temperature sensors, touch sensors etc. These sensors are most applicable in relation to specificity and achievement in the design of an autonomous mobile robot. <a ref-type=table anchor=table1 class=fulltext-link>Table 1</a> gives types of sensors used by an autonomous mobile robot.<div class="figure figure-full table" id=table1><div class=figcaption><b class=title>TABLE 1 </b>
Classification of Sensor System [11]</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck.t1-2975643-large.gif><img class="document-ft-image load-shimmer" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck.t1-2975643-small.gif data-alt="Table 1- 
Classification of Sensor System [11]"><div class=zoom title="View Larger Image"></div></a></div></div><p><p>Pp=Proprioceptive, Ep=Exteroceptive, A=Active,<p>P=Passive, A/P=Active and passive</p></div><div class=section_2 id=sec1c><h3>C. Navigation</h3><p>Navigation is a fundamental problem in robotics and other important technologies. In order for the mobile robot to autonomously navigate, the robot has to know where it is at present, where the destination is, and how it can reach the destination <a ref-type=bibr anchor=ref12 id=context_ref_12_1c>[12]</a>. The most important aspect in the design of a mobile robot is navigation abilities. The objective is for the robot to navigate from one destination to another either in a recognized or uncontrolled environment. Most of the time, the mobile robot cannot take the direct route from its starting point to the ending point, which means that motion planning techniques must be incorporated. This means that the robot must depend on other aspects, such as perception (valuable data acquired by the mobile robot through the use of sensors), localization (position and configuration to be determined by the robot), cognition (decision made by the mobile robot on how to achieve its goals), and motion control (the robot must estimate its input forces on the actuators to accomplish the anticipated trajectory).<p>In robotics, another area to consider is the use of computer vision applications to aid navigation and localization. In computer vision, object recognition and feature matching are a significant task to be performed for accurate positioning. Object recognition has long been adopted in mobile robot to detect or identify objects present in an image. The technique can either be used to determine coordinates of the object detected or calculate in relative to a proposed object identified in an image. Feature matching or image matching on the other hand performs the task of establishing correspondence between two images of the same scene/object.<p>Examples of features associated between the images could be points, edges or lines, and these features are often called keypoints features <a ref-type=bibr anchor=ref13 id=context_ref_13_1c>[13]</a>, <a ref-type=bibr anchor=ref14 id=context_ref_14_1c>[14]</a>. To perform the task of object recognition and feature matching, several algorithms were adopted and some of the algorithms were mentioned and discussed later in the paper.<p>Mobile robots attract attention more and more because of the increase in applications in various areas such as surveillance for security and monitoring home for health and entertainment, research and education etc., <a ref-type=bibr anchor=ref15 id=context_ref_15_1c>[15]</a>–<a ref-type=bibr anchor=ref16 id=context_ref_16_1c>[16]</a><a ref-type=bibr anchor=ref17 id=context_ref_17_1c>[17]</a>. Surveillance robots are now being installed in homes for domestic use, they are simple and easy to deploy, they are connected to Wi-Fi home network or smart environment to monitor and report activities going on in the environment. They have been designed further to engage in house cleaning, positioning objects where and when required. Recently, home robots are now being used by elderly people in a situation where emergency case arises. Therefore, these robots have helped to promote technology that aids to detect and react to events that demand immediate response <a ref-type=bibr anchor=ref18 id=context_ref_18_1c>[18]</a>. Another area where mobile robot is trending is the section of education. Educational robotics is primarily focused on creating a robot that will assist users to develop more practical, didactic, and cognitive skills. This approach is intended also to stimulate interest for research and science through set of different activities designed to support strengthening of specific areas of knowledge and skills. Introduction of mobile robot has increased not only on tertiary level and scientific research institutions, but also in lower grades such as secondary and primary schools <a ref-type=bibr anchor=ref19 id=context_ref_19_1c>[19]</a>. These have therefore improved the knowledge of people about mobile robot worldwide.<p>Furthermore, mobile robot is gaining more interest in the area of mining industry <a ref-type=bibr anchor=ref20 id=context_ref_20_1c>[20]</a>. The use of mobile robot has increased the efficiency and safety of miners. The robot assists in tracking people, robots and machines as well as monitor environmental conditions in mines. The mobile robotic platform is coupled with a set of range finders, thermal imaging sensors, and acoustic systems, all of which are functioned with neural networks. They navigate into different environments and identify potential risk areas before the workers go in. <a ref-type=fig anchor=fig1 class=fulltext-link>Figure 1</a> shows some applications of mobile robots but not limited to the areas mentioned. Furthermore, other applications includes firefighting, agriculture, museum and library guides, planetary exploration, patrolling, reconnaissance, petrochemical applications as well as for both domestic and industrial applications <a ref-type=bibr anchor=ref4 id=context_ref_4_1c>[4]</a> etc.
<div class="figure figure-full" id=fig1><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck1-2975643-large.gif data-fig-id=fig1><img class="document-ft-image load-shimmer" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck1-2975643-small.gif data-alt="FIGURE 1. - Applications of mobile robot."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 1. </b><fig><p>Applications of mobile robot.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/all-figures class=all>Show All</a></p></div><p><p>The other sections of this paper are as follows: <a ref-type=sec anchor=sec2 class=fulltext-link>Section 2</a> commences by presenting the challenges mobile robots are faced with. This is followed by sensors and technique used to determine the positioning of mobile robots such as to improve accuracy in <a ref-type=sec anchor=sec3 class=fulltext-link>Section 3</a>. <a ref-type=sec anchor=sec4 class=fulltext-link>Section 4</a> discusses the different types of methods used for object recognition and feature matching. Furthermore, related work on sensor fusion techniques were presented in <a ref-type=sec anchor=sec5 class=fulltext-link>Section 5</a>. <a ref-type=sec anchor=sec6 class=fulltext-link>Section 6</a> presented the classification of sensor fusion algorithms. <a ref-type=sec anchor=sec7 class=fulltext-link>Section 7</a> highlighted the importance of sensor fusion techniques while <a ref-type=sec anchor=sec8 class=fulltext-link>Section 8</a> discusses the areas where researchers can further investigate on the issues challenging mobile robot navigation and localization in both known and unknown environment and <a ref-type=sec anchor=sec9 class=fulltext-link>Section 9</a> concludes the paper.</p></div></div>
<div class=section id=sec2><div class="header article-hdr"><div class=kicker>
 SECTION II.</div><h2>Challenges of Autonomous Mobile Robot</h2></div><p>Autonomous mobile robots have proven to be a system that cannot be without as a result of increase in demand for diverse applications. Regardless, the potential and prospect, they are yet to attain optimal performance, this is because of inherent challenges that they are faced with. These challenges (see <a ref-type=fig anchor=fig2 class=fulltext-link>Figure 2</a>) have enabled more researchers to develop more interest in recent times. Some of the main challenges are navigation and path planning, localization and obstacle avoidance.
<div class="figure figure-full" id=fig2><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck2-2975643-large.gif data-fig-id=fig2><img class="document-ft-image load-shimmer" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck2-2975643-small.gif data-alt="FIGURE 2. - Challenges of mobile robot."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 2. </b><fig><p>Challenges of mobile robot.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/all-figures class=all>Show All</a></p></div><p><div class=section_2 id=sec2a><h3>A. Navigation and Path Planning</h3><p>As earlier said in <a ref-type=sec anchor=sec1 class=fulltext-link>Section I</a> that autonomous navigation of a mobile robot is an issue in robotics field. There are majorly two ways by which navigation problem is categorised into: local and global navigation. The local and the global navigation problem varies in terms of distances, scales and obstacle avoidance and inability for the goal state to be observed. For local navigation, occupancy grid of map is used to determine the navigation direction and for global navigation, landmark approach based on topological map is used. This have a compact representation of the environment and do not depend on the geometric accuracy. The limitation of this approach is that they are downgraded by the noise generated from the sensor. Mobile robot navigation systems depend on the level of abstraction of the environment representation. To accurately determine the position and orientation of the mobile robot, it is imperative for the environment to be modelled in a simple and understandable structure. Three main techniques for representing the environment are given as: geometric, topological and semantic <a ref-type=bibr anchor=ref21 id=context_ref_21_2a>[21]</a>.<div class=section_2 id=sec2a1><h4>1) Geometric</h4><p>This is used to describe robot environment by parameterizing primitive geometric object such as curves, lines and points. The geometric representation of the environment is closer to the sensor and actuator world and it is the best one to perform local navigation. In <a ref-type=bibr anchor=ref22 id=context_ref_22_2a1>[22]</a>, the author proposed the use of Principal Components Analysis (PCA) - Bayesian based method with grid map representation to compress images and reduce computational resources. The PCA was also use to reduce dimensionality and model the parameter of the environment by considering the pixels of an image as feature vectors of the data set <a ref-type=bibr anchor=ref23 id=context_ref_23_2a1>[23]</a>. In <a ref-type=bibr anchor=ref24 id=context_ref_24_2a1>[24]</a>, Markov localization method was proposed to provide accuracy and multimodality to represent probability distribution of diverse kind but require significant processing for update, hence it is impractical for large environment.</p></div><div class=section_2 id=sec2a2><h4>2) Topological</h4><p>This is considered by defining reference elements of the environment according to the distinct relations between them. A conventional method for modelling the robot’s environment is to discretize the environmental model by using a topological representation of the belief state, where each likely pose of the mobile robot is connected to a node in a topological map <a ref-type=bibr anchor=ref25 id=context_ref_25_2a2>[25]</a>. In <a ref-type=bibr anchor=ref26 id=context_ref_26_2a2>[26]</a>, the proposed approach uses visual features extracted from a pair of stereo images as landmarks. While the new landmarks are fused into the map and transient landmarks are removed from the map over time. Topological representation of the environment uses graphs to model the environment and it is used in large navigation tasks.</p></div><div class=section_2 id=sec2a3><h4>3) Semantic</h4><p>The current development in robotics is to alleviate from representation models that are closest to the robot’s hardware such as geometric models to those models closer to human reasoning, with which the robot will interact. It is proposed to relate model with the way robots represent the environment and the way humans do. Robots that are provided with semantic models of the environments where they operate have a larger decision autonomy, and become more robust and more efficient <a ref-type=bibr anchor=ref27 id=context_ref_27_2a3>[27]</a>.<p>An integrated approach for efficient online 3D semantic map building of urban environments and the subsequent extraction of qualitative spatial relationships between the different objects was presented, this enables efficient task planning <a ref-type=bibr anchor=ref28 id=context_ref_28_2a3>[28]</a>. Semantic information constitutes a better solution for interaction with humans <a ref-type=bibr anchor=ref29 id=context_ref_29_2a3>[29]</a>, the representation is the most abstract representation model and adds concepts such as utilities or meanings of the environment elements in the map representation. Semantic navigation is considered as a navigation system that considers semantic information to model that includes conceptual and physical representation of objects and places, utilities of the objects, and semantic relation among objects and places. This model allows the robot to manage the environment and to make queries about the environment in order to do plans for navigation tasks <a ref-type=bibr anchor=ref21 id=context_ref_21_2a3>[21]</a>. Environmental model requires improved representation to enable successful result, better accuracy and as well reduce the computational cost <a ref-type=bibr anchor=ref30 id=context_ref_30_2a3>[30]</a>. For this to prevail, the environment must be well represented, simple technique must be adopted and be incorporated in to the robot’s representation of its environment <a ref-type=bibr anchor=ref31 id=context_ref_31_2a3>[31]</a>.<p>Safe and efficient mobile robot navigation requests an efficient path planning technique since the quality of the generated path affects extremely the robotic applications <a ref-type=bibr anchor=ref32 id=context_ref_32_2a3>[32]</a>–<a ref-type=bibr anchor=ref33 id=context_ref_33_2a3>[33]</a><a ref-type=bibr anchor=ref34 id=context_ref_34_2a3>[34]</a>. In an environment with several obstacles, finding a path without collision with obstacles from the initial point to the final point becomes an issue such as shortness and simplicity of route are important criteria affecting the optimality of selected routes. Considering the length of the path travelled by the robot, energy consumption and its performance time, and an algorithm that finds the shortest possible route <a ref-type=bibr anchor=ref35 id=context_ref_35_2a3>[35]</a> is most appropriate. Basically, there are two types of environment: static and dynamic. While dynamic environment is divided into global and local path planning <a ref-type=bibr anchor=ref33 id=context_ref_33_2a3>[33]</a>. Global navigation strategy deals with a completely known environment while local navigation strategy deals with the unknown and partially known environment. <a ref-type=fig anchor=fig3 class=fulltext-link>Figure 3</a> shows the breakdown of path planning categories.
<div class="figure figure-full" id=fig3><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck3-2975643-large.gif data-fig-id=fig3><img class="document-ft-image load-shimmer" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck3-2975643-small.gif data-alt="FIGURE 3. - Classifications of mobile robot path planning methods [36]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 3. </b><fig><p>Classifications of mobile robot path planning methods <a ref-type=bibr anchor=ref36 id=context_ref_36_2a3>[36]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/all-figures class=all>Show All</a></p></div><p><p>Quite a number of studies have been investigated on path planning in dynamic environments. Authors in <a ref-type=bibr anchor=ref37 id=context_ref_37_2a3>[37]</a> proposed a new method to decide the optimum route of the mobile robot in an unknown dynamic environment, they used Ant Colony Optimization (ACO) algorithm to decide the optimal rule table of the fuzzy system. Other related algorithms are Bacterial Foraging Optimization (BFO) <a ref-type=bibr anchor=ref33 id=context_ref_33_2a3>[33]</a>, and Probabilistic Cell Decomposition (PCD) <a ref-type=bibr anchor=ref38 id=context_ref_38_2a3>[38]</a>.<p>A new mathematical method that is based on the concepts of 3D geometry is proposed to generate the route of the mobile robot. The mobile robot decides its path in real time to avoid randomly moving obstacles <a ref-type=bibr anchor=ref39 id=context_ref_39_2a3>[39]</a>. Other intelligent algorithms studied by researchers used by mobile robot to navigate in diverse environment are Differential Evolution (DE) algorithm <a ref-type=bibr anchor=ref40 id=context_ref_40_2a3>[40]</a>, <a ref-type=bibr anchor=ref41 id=context_ref_41_2a3>[41]</a>, Harmony Search (HS) algorithm <a ref-type=bibr anchor=ref42 id=context_ref_42_2a3>[42]</a>, Bat Algorithm (BA) <a ref-type=bibr anchor=ref43 id=context_ref_43_2a3>[43]</a>, and Invasive Weed Optimization (IWO) <a ref-type=bibr anchor=ref44 id=context_ref_44_2a3>[44]</a>.</p></div></div><div class=section_2 id=sec2b><h3>B. Localization</h3><p>Localization is another fundamental issue encountered in mobile robot which requires attention as well. The challenging part of localization is estimating the robot position and orientation of which this information can be acquired from sensors and other systems. So, to tackle the issue of localization, a good technique should be proposed to deal with errors, downgrading factors, improper measurement and estimations. The techniques are divided into two categories <a ref-type=bibr anchor=ref45 id=context_ref_45_2b>[45]</a>–<a ref-type=bibr anchor=ref46 id=context_ref_46_2b>[46]</a><a ref-type=bibr anchor=ref47 id=context_ref_47_2b>[47]</a><a ref-type=bibr anchor=ref48 id=context_ref_48_2b>[48]</a>: relative and absolute localization.<div class=section_2 id=sec2b1><h4>1) Relative Localization Techniques</h4><p>This method <b>e</b>stimate the position and orientation of the mobile robot by integrating information produced by diverse sensors through the combination of information presented by different sensors, usually encoder or inertial sensors. The integration starts from the initial position and continuously update in time. The relative positioning alone can be used only for a short period of time.</p></div><div class=section_2 id=sec2b2><h4>2) Absolute Localization Techniques</h4><p>This method permits the mobile robot to search its location directly from the mobile system environment. Their numerous methods usually depend on navigation beacons, active or passive landmarks, maps matching or satellite-based signals such as the Global Positioning System (GPS). For absolute localization, the error growth is alleviated when measurements are accessible. The position of the robot is externally determined, and its accuracy is usually time and location independent. In other words, integration of noisy data is not required and thus there is no aggregation of error with time or distance travelled. The limitation is that one cannot keep track of the robot for short distances.</p></div></div><div class=section_2 id=sec2c><h3>C. Obstacle Avoidance</h3><p>Obstacle avoidance is a vital task in the field of robotics, because it is important that the mobile robot get to its destination without being obstructed by any obstacle or an event of collision on its path. To this effect, collision free algorithm is a prerequisite of autonomous mobile robot, since it offers the safe trajectory and proves convergence <a ref-type=bibr anchor=ref49 id=context_ref_49_2c>[49]</a>. Some of the main algorithms that can be used for obstacle avoidance are discussed in this section. Bug algorithm <a ref-type=bibr anchor=ref50 id=context_ref_50_2c>[50]</a> is one of the earliest algorithms. It enables the robot to navigate the entire circumferences of the obstacle encountered and decide on the most appropriate point to leave towards the goal. The robot therefore moves to the best leaving position and later moves towards the object. The benefit of this algorithm is that it is easy to determine if an object is unreachable or not. However, the algorithm takes time to achieve its goal. Another algorithm is Vector Field Histogram (VFH) <a ref-type=bibr anchor=ref51 id=context_ref_51_2c>[51]</a> which is an improvement of the short coming of Virtual Force Field (VFF) algorithm <a ref-type=bibr anchor=ref52 id=context_ref_52_2c>[52]</a>. VFH allows detection of unknown obstacle and avoids collision while simultaneously piloting the mobile robot towards the target. This algorithm employs a 2-stage data reduction process in order to compute the desired control command for the robot. This ensure accurate computation of the robot path to the target, but it consumes more resources like memory, processor and power. Hybrid navigation algorithm with roaming trails (HNA) <a ref-type=bibr anchor=ref53 id=context_ref_53_2c>[53]</a> is an algorithm that is able to deal very efficiently with environments where obstacles are encountered by the robot during motion. During navigation the robot can deviate from its path to avoid obstacles on the basis of reactive navigation strategies, but it is never permitted to exit from the area. Since the robot is controlled to move within a convex area which includes the location of the target node, in presence of static obstacles it is guaranteed to reach the target by following a straight line. In some cases, the mobile robot has to either avoid the obstacles or simply stop in front of the obstacle. Another method that is similar to HNA is the New Hybrid Navigation Algorithms (NHNA) <a ref-type=bibr anchor=ref54 id=context_ref_54_2c>[54]</a>. The algorithm uses D-H bug algorithm (Distance Histogram bug) to avoid obstacle. It enables the robot to rotate freely at angle less than 90 degrees to avoid obstacle. If the rotation is 90 degrees or greater and it is required to avoid an obstacle, it acts as bug-2 algorithm <a ref-type=bibr anchor=ref50 id=context_ref_50_2c>[50]</a> and starting moving to destination when path is clear from obstacles. Conclusively, collision free algorithm is a requirement for autonomous mobile robot, since it provides safe trajectory.<p>In conclusion, challenges faced by mobile robot must be tackled to ensure effective performance. Navigation is one of the most important aspect to be considered when it comes mobile robot because it requires planning algorithms and appropriate information about robot’s location. This will navigate the robot through its pre-defined path. In as much as navigation is important so also is trajectory planning. This will determine the path the robot must follow in order to reach its destination. Therefore, a path must be planned accordingly to avoid collision and obstacles. Different algorithms are considered for obstacle avoidance depending on the goal to be achieved. Finally, the robot must know its position and direction per time. In this regard, an effective localization technique and reliable sensors are required to gather precise information.</p></div></div>
<div class=section id=sec3><div class="header article-hdr"><div class=kicker>
 SECTION III.</div><h2>Sensors and Techniques in Mobile Robot Positioing</h2></div><p>To ensure accuracy in localization, sensors and effective positioning system has to be considered. Objects positioning <a ref-type=bibr anchor=ref55 id=context_ref_55_3>[55]</a>, robotics, and Augmented Reality (AR) tracking <a ref-type=bibr anchor=ref56 id=context_ref_56_3>[56]</a> have been of interest in the literature of recent. This section will discuss the existing technologies that aim at determining mobile robot’s position within its environment.<div class=section_2 id=sec3a><h3>A. Inertial Sensors</h3><p>Inertial based sensor methods are also known as IMU (Inertial Measurement Units) which is a combination of accelerometers, gyroscopes and sometimes magnetometers. These sensors have become ubiquitous because many devices and system depend on them to serve a large sum of applications. They rely on measurement of acceleration, heading and angular rates, which can be acquired without external reference. Each of these sensors are deployed in robots, mobile devices and navigation systems <a ref-type=bibr anchor=ref57 id=context_ref_57_3a>[57]</a>–<a ref-type=bibr anchor=ref58 id=context_ref_58_3a>[58]</a><a ref-type=bibr anchor=ref59 id=context_ref_59_3a>[59]</a>. The benefits of using these sensors is solely to calculate the position and orientation of a device and/or object.<div class=section_2 id=sec3a1><h4>1) Accelerometer</h4><p>Accelerometer as a sensor measures the linear acceleration, which is the rate of change of velocity of an object. They measure in meters per second <inline-formula><tex-math notation=LaTeX>\$(m/s^{2})\$
</tex-math></inline-formula> or in gravity (g). They are useful for sensing vibration in system or for orientation in applications <a ref-type=bibr anchor=ref60 id=context_ref_60_3a1>[60]</a>. Velocity is determined from it if integrated once and for position, integration is done twice. Using a standalone sensor like accelerometer could be simple and of low cost as stated by the author in <a ref-type=bibr anchor=ref61 id=context_ref_61_3a1>[61]</a>, but the linear increasing error does not give a high-level of accuracy. The use of accelerometer alone may not be suitable because they suffer from extensive noise and accumulated drift. This can be complemented with the use of gyroscope.</p></div><div class=section_2 id=sec3a2><h4>2) Gyroscope</h4><p>Gyroscope sensor measures the angular velocity in degrees per second <inline-formula><tex-math notation=LaTeX>\$(^{\circ }/s)\$
</tex-math></inline-formula> or Revolution Per Second (RPS) and by integrating once, rotation angle can be calculated. Although gyroscope is small in size and inexpensive but run at a high rate in which they are able to track fast and abrupt movements. Another advantage of using gyroscope sensor is that it is not affected by illumination and visual occlusion <a ref-type=bibr anchor=ref55 id=context_ref_55_3a2>[55]</a>. However, their performances are degraded by accumulation of measurement errors for long periods. Consequently, the fusion of both accelerometer and gyroscope sensor is appropriate to determine the pose of an object and to make up for the weakness of one over the other.</p></div><div class=section_2 id=sec3a3><h4>3) Magnetometer</h4><p>Magnetometer is another sensor used to calculate the heading angle by sensing the earth magnetic field. They are combined with technologies to determine pose estimation <a ref-type=bibr anchor=ref62 id=context_ref_62_3a3>[62]</a>. However, magnetometer may not be so useful for indoor positioning because of the existence of metallic objects within the environment that could affect data collected through measurements <a ref-type=bibr anchor=ref55 id=context_ref_55_3a3>[55]</a>. Other methods that be used to determine indoor localization includes infrared, Wi-Fi, Ultra-Wideband (UWB), Bluetooth, Wide Local Area Network (WLAN), fingerprinting etc., <a ref-type=bibr anchor=ref63 id=context_ref_63_3a3>[63]</a>–<a ref-type=bibr anchor=ref64 id=context_ref_64_3a3>[64]</a><a ref-type=bibr anchor=ref65 id=context_ref_65_3a3>[65]</a><a ref-type=bibr anchor=ref66 id=context_ref_66_3a3>[66]</a>. However, these methods have their inadequacies, it is therefore necessary that two or more schemes be combined to attain accurate result.</p></div></div><div class=section_2 id=sec3b><h3>B. Monocular Vision Positioing System</h3><p>Monocular vision positioning uses a single camera to determine the pose estimation of a mobile device or static objects. Another type of vision positioning system is called binocular vision. Binocular stereo vision uses two cameras to estimate location of a mobile robot. Although it has the advantage of better performance in the regard of accuracy, but it is more expensive and complex to compute <a ref-type=bibr anchor=ref67 id=context_ref_67_3b>[67]</a>. While monocular vision on the other hand is simple to set-up and of low cost. Information collected from the environment captured by the camera can be in form of an image or video. This information is therefore processed to estimate the position and orientation of the robot per time. This poses a spatial relationship between the 2D image captured and the 3D points in the scene. According to Navab <a ref-type=bibr anchor=ref68 id=context_ref_68_3b>[68]</a>, the use of marker in augmented reality (AR) is very efficient in the environment. It increases robustness and reduces computational requirement. However, there are exceptional cases where markers are placed in the area and they need re-calibration from time to time. Therefore, the use of scene features for tracking in place of markers is reasonable especially when certain parts of the workplace do not change over time. Placing fiducial markers <a ref-type=bibr anchor=ref47 id=context_ref_47_3b>[47]</a> is a way to assist robot to navigate through its environments. In new environments, marker often need to be determined by the robot itself, using sensor data collected by IMU, sonar, laser and camera. Markers’ locations are known, but the robot position is unknown, and this is a challenge for tracking a mobile robot. From the sensor readings, the robot must be able to infer its most likely position in the environment. With monocular vision (one camera), a good solution in terms of scalability and accuracy is provided. The monocular vision is low in cost because only one camera is required, and this technique demands less calculation unlike stereo vision with high complexity. With the aid of other sensors such as ultrasonic sensor or barometric altimeter, the monocular vision can also provide the scale and in-depth information of the image frames. To calculate the pose of the mobile robot with respect to the camera based on the pinhole camera model. The monocular vision positioning system <a ref-type=bibr anchor=ref69 id=context_ref_69_3b>[69]</a>, can be use to estimate the 3D camera from 2D image plane <a ref-type=bibr anchor=ref70 id=context_ref_70_3b>[70]</a>. The relationship between a point in the world frame and its projection in the image plane can be expressed as:<disp-formula id=deqn1 class=display-formula><tex-math notation=LaTeX>\begin{equation*} \lambda p=MP\tag{1}\end{equation*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} \lambda p=MP\tag{1}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX>\$\lambda \$
</tex-math></inline-formula> is a scale factor, <inline-formula><tex-math notation=LaTeX>\$p=[u,v,1]^{T}\$
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX>\$P=[X_{w},Y_{w},Z_{w},1]^{T}\$
</tex-math></inline-formula> homogenous coordinates of <inline-formula><tex-math notation=LaTeX>\$p\$
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX>\$P\$
</tex-math></inline-formula>, and <inline-formula><tex-math notation=LaTeX>\$M\$
</tex-math></inline-formula> is a <inline-formula><tex-math notation=LaTeX>\$3\times 4\$
</tex-math></inline-formula> projection matrix.<p><a ref-type=disp-formula anchor=deqn1 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/#deqn1 class=fulltext-link>Equation (1)</a> can further be expressed as:<disp-formula id=deqn2 class=display-formula><tex-math notation=LaTeX>\begin{equation*} \lambda \left [{\begin{array}{c} {u} \\ {v} \\ {i} \end{array}}\right]=M\left ({R_{w c} t_{w c}}\right)\left [{\begin{array}{c} {X_{w}} \\ {Y_{w}} \\ {Z_{w}} \\ {1} \end{array}}\right]\tag{2}\end{equation*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} \lambda \left [{\begin{array}{c} {u} \\ {v} \\ {i} \end{array}}\right]=M\left ({R_{w c} t_{w c}}\right)\left [{\begin{array}{c} {X_{w}} \\ {Y_{w}} \\ {Z_{w}} \\ {1} \end{array}}\right]\tag{2}\end{equation*}
</span></span></disp-formula><p>The projection matrix depends on both camera intrinsic and extrinsic parameters. The intrinsic parameters contain five parameters: focal length <inline-formula><tex-math notation=LaTeX>\$f\$
</tex-math></inline-formula>, principal point <inline-formula><tex-math notation=LaTeX>\$u_{0},v_{0} \$
</tex-math></inline-formula> and the skew coefficient between <inline-formula><tex-math notation=LaTeX>\$x\$
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX>\$y\$
</tex-math></inline-formula> axis and is often zero.<disp-formula id=deqn3 class=display-formula><tex-math notation=LaTeX>\begin{equation*} M=\left [{\begin{array}{ccc} {a_{x}} &amp;\quad {\gamma } &amp;\quad {u_{0}} \\ {0} &amp;\quad {a_{y}} &amp;\quad {v_{0}} \\ {0} &amp;\quad {0} &amp;\quad {1} \end{array}}\right]\tag{3}\end{equation*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} M=\left [{\begin{array}{ccc} {a_{x}} &amp;\quad {\gamma } &amp;\quad {u_{0}} \\ {0} &amp;\quad {a_{y}} &amp;\quad {v_{0}} \\ {0} &amp;\quad {0} &amp;\quad {1} \end{array}}\right]\tag{3}\end{equation*}
</span></span></disp-formula> Extrinsic parameters: <inline-formula><tex-math notation=LaTeX>\$R,T\$
</tex-math></inline-formula> defines the position of camera center and the camera’s heading in world coordinates. Camera calibration is to obtain the intrinsic and extrinsic parameters. Therefore, the projection matrix of a world point in the image is expressed as:<disp-formula id=deqn4 class=display-formula><tex-math notation=LaTeX>\begin{equation*} C=-R^{-1}T=-R^{T}T\tag{4}\end{equation*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} C=-R^{-1}T=-R^{T}T\tag{4}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX>\$T\$
</tex-math></inline-formula> is the position of the origin of the world coordinate, and <inline-formula><tex-math notation=LaTeX>\$R\$
</tex-math></inline-formula> is the rotation matrix.</p></div><div class=section_2 id=sec3c><h3>C. Landmarks</h3><p>Landmark is the feature information recognized through robot’s sensors perception. For an autonomous robot, how to identify landmarks quickly and accurately plays an important role in localization and navigation. Robot navigation system based on landmarks research areas include landmark selection, landmark design, landmark detection, landmark navigation, environmental characterization and path planning, etc. Generally, landmarks are classified into two types: markerless (also known as natural landmark) and marker-based (also known as artificial landmark) <a ref-type=bibr anchor=ref71 id=context_ref_71_3c>[71]</a>.<p>Artificial Landmark: Artificial landmarks refer to the special designs of the objects or markers placed in an environment which can be detected by laser, infrared, sonar and vision sensors. The uniqueness of the marker is important with the features for quick recognition and high reliability, these landmarks can be identified accurately at various visual conditions <a ref-type=bibr anchor=ref71 id=context_ref_71_3c>[71]</a>, <a ref-type=bibr anchor=ref72 id=context_ref_72_3c>[72]</a>. Localization based on artificial landmarks is used more widely than other methods because the artificial landmarks are easy to detect and allowed to achieve high speed and precision. An artificial landmark could be any object whether static or mobile which could vary in size, shape, feature or color as long as it is placed in the environment with the purpose of robot localization. The author in <a ref-type=bibr anchor=ref73 id=context_ref_73_3c>[73]</a> use a sticker and LED array as an artificial landmark. These makers are easier to detect and describe because the details of the objects used are known in advance. These methods are used because of their simplicity and easy setup. However, they cannot be adopted in an extensive environment where large numbers of markers are deployed.<p>Natural Landmark: Natural landmarks are objects or features that are part of the environment and have a function other than robot navigation. Examples of natural landmarks are corridors, edges, doors, wall, ceiling light, lines, etc. The choice of features is vital because it will determine the complexity in the feature description, detection and matching <a ref-type=bibr anchor=ref55 id=context_ref_55_3c>[55]</a>. Although the natural landmarks have little influence on the environment, it is rarely used in the practical applications for its low stability and bad adaptability. Visual features are divided into three categories: point feature, line feature, block feature. Amongst the three categories, point feature is the easiest to extract, relatively stable and contain abundant information <a ref-type=bibr anchor=ref74 id=context_ref_74_3c>[74]</a>. Several work has dealt with the issue of using natural landmarks to extract feature that will aid robot localization using Scale-Invariance feature Transform (SIFT) features <a ref-type=bibr anchor=ref75 id=context_ref_75_3c>[75]</a> and Speeded Up Robust Feature (SURF) features <a ref-type=bibr anchor=ref76 id=context_ref_76_3c>[76]</a>, <a ref-type=bibr anchor=ref77 id=context_ref_77_3c>[77]</a>. <a ref-type=fig anchor=fig4 class=fulltext-link>Figure 4</a> shows an example of natural landmarks extracted using SURF algorithm.
<div class="figure figure-full" id=fig4><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck4-2975643-large.gif data-fig-id=fig4><img class="document-ft-image load-shimmer" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck4-2975643-small.gif data-alt="FIGURE 4. - SURF feature points from the scene image [77]."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 4. </b><fig><p>SURF feature points from the scene image <a ref-type=bibr anchor=ref77 id=context_ref_77_3c>[77]</a>.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9007654/all-figures class=all>Show All</a></p></div><p></p></div></div>
<div class=section id=sec4><div class="header article-hdr"><div class=kicker>
 SECTION IV.</div><h2>Object Recognition and Feature Matching</h2></div><p>In this section we presented the proposed method of object recognition and matching features. Object recognition under uncontrolled, real-world conditions is of vital importance in robotics.<p>It is an essential attribute for building object-based representations of the environment and for the manipulation of objects. Different methods of scale invariant descriptors and detectors are currently being adopted because of their affine transformations to detect, recognize and classify objects. Some of these methods are Oriented Fast and Rotated BRIEF (ORB), Binary Robust Invariant Scalable Keypoints (BRISK), Difference of Gaussians (DoG), FERNS <a ref-type=bibr anchor=ref78 id=context_ref_78_4>[78]</a> SIFT <a ref-type=bibr anchor=ref13 id=context_ref_13_4>[13]</a> and SURF <a ref-type=bibr anchor=ref76 id=context_ref_76_4>[76]</a>. More details of these method can be found in reference <a ref-type=bibr anchor=ref79 id=context_ref_79_4>[79]</a>. Object detection and recognition can be done using computer vision whereby an object will be detected in image or video sequence. The recognised object is used as a reference to determine the pose of a mobile device. Basically, object detection can be categorised into three aspects: appearance based, color based and features based. All these methods have their advantages and limitations <a ref-type=bibr anchor=ref80 id=context_ref_80_4>[80]</a>.<p>Appearance based objects are recognised based on the changes in color, size and shape. The techniques used are edge matching, divide and conquer search, greyscale matching, gradient matching etc. The color based techniques are based on the Red, Green and Blue (RGB) features to represent and match images. They provide cogent information for object recognition. While the feature-based technique finds the interest points of an object in image and matches them to the find object in another image of similar scene. Features extracted are surfaces, patches, corners and linear edges. The methods used to extract feature are interpretations trees, hypothesize and test, pose consistency, geometric hashing, SIFT, and SURF.<p>Mostly, finding the correspondences is a difficult image processing problem where two tasks have to be solved <a ref-type=bibr anchor=ref81 id=context_ref_81_4>[81]</a>. The first task consists of detecting the points of interest or features in the image. Features are distinct elements in the images, examples are corners, blobs, edges. The most widely used algorithm for detection includes the Harris corner detector <a ref-type=bibr anchor=ref82 id=context_ref_82_4>[82]</a>. It is based on the eigenvalues of the second moment matrix. Other types of detectors are correlation based: Kanade-Lucas-Tomasi tracker <a ref-type=bibr anchor=ref83 id=context_ref_83_4>[83]</a> and Laplace detector <a ref-type=bibr anchor=ref84 id=context_ref_84_4>[84]</a>. For feature matching, the two most popular methods for computing the geometric transformations are: Hough transform and Random Sample Consensus (RANSAC) algorithm <a ref-type=bibr anchor=ref79 id=context_ref_79_4>[79]</a>, <a ref-type=bibr anchor=ref85 id=context_ref_85_4>[85]</a>, <a ref-type=bibr anchor=ref76 id=context_ref_76_4>[76]</a>. They could estimate parameter with a high degree of accuracy even when a substantial number of outliers are present in the data set.<div class=section_2 id=sec4a><h3>A. Speeded-Up Robust Features (SURF)</h3><p>SURF was first introduced by Bay <i>et al.</i> <a ref-type=bibr anchor=ref76 id=context_ref_76_4a>[76]</a>. SURF outperforms formerly proposed scheme SIFT with respect to repeatability (reliability of a detector for finding the same physical interest points under different viewing conditions), distinctiveness, and robustness, yet can be computed much faster. The descriptors are used to find correspondent features in the image. SURF detect interest points (such as blob) using Hessian matrix because of its high level of accuracy (See <a ref-type=disp-formula anchor=deqn5-6 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/#deqn5-6 class=fulltext-link>equations 5 and 6</a>). This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based for the detector, and a distribution-based for the descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. SURF is used to detect key points and to generate its descriptors. Its feature vector is based on the Haar Wavelet response around the interested features <a ref-type=bibr anchor=ref80 id=context_ref_80_4a>[80]</a>. SURF is a scale-and rotation-invariant, that means, even with variations on the size and on the rotation of an image, SURF can find key points.<disp-formula id=deqn5-6 class=display-formula><tex-math notation=LaTeX>\begin{align*} I(X)=&amp;\sum \limits _{i=0}^{i\le x} {\sum \limits _{j=0}^{j\le y} {I(x,y)}} \tag{5}\\ H(x, \sigma)=&amp;\left [{\begin{array}{cccccccccccccccccccc} L_{x x}(x, \sigma) &amp;\quad L_{x y}(x, \sigma) \\ L_{x y}(x, \sigma) &amp;\quad L_{y y}(x, \sigma) \end{array}}\right]\tag{6}\end{align*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} I(X)=&amp;\sum \limits _{i=0}^{i\le x} {\sum \limits _{j=0}^{j\le y} {I(x,y)}} \tag{5}\\ H(x, \sigma)=&amp;\left [{\begin{array}{cccccccccccccccccccc} L_{x x}(x, \sigma) &amp;\quad L_{x y}(x, \sigma) \\ L_{x y}(x, \sigma) &amp;\quad L_{y y}(x, \sigma) \end{array}}\right]\tag{6}\end{align*}
</span></span></disp-formula><p><inline-formula><tex-math notation=LaTeX>\$X=(x,y)\$
</tex-math></inline-formula> is an image <inline-formula><tex-math notation=LaTeX>\$I\$
</tex-math></inline-formula>, Hessian matrix <inline-formula><tex-math notation=LaTeX>\$H=(x,\sigma)\$
</tex-math></inline-formula> in x at scale <inline-formula><tex-math notation=LaTeX>\$\sigma \$
</tex-math></inline-formula> is defined.<p>Where <inline-formula><tex-math notation=LaTeX>\$L_{xx} (x,\sigma)\$
</tex-math></inline-formula> is the convolution of the Gaussian second <inline-formula><tex-math notation=LaTeX>\$\frac {\partial }{\partial x^{2}}^{2}g(\sigma)\$
</tex-math></inline-formula> with the image in point <inline-formula><tex-math notation=LaTeX>\$x\$
</tex-math></inline-formula> and derivative for <inline-formula><tex-math notation=LaTeX>\$L_{xy} (x,\sigma)\$
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX>\$L_{yy} (x,\sigma)\$
</tex-math></inline-formula>.</p></div><div class=section_2 id=sec4b><h3>B. Random Sample Consensus (RANSAC)</h3><p>RANSAC is feature matcher which works well with SURF when matching detected objects in images. RANSAC was first published by Fischler and Bolles <a ref-type=bibr anchor=ref85 id=context_ref_85_4b>[85]</a> in 1981 which is also often used in computer vision. It simultaneously unravel the correspondence problem such as, fundamental matrix related to a pair of cameras, homograph estimation, motion estimation and image registration <a ref-type=bibr anchor=ref86 id=context_ref_86_4b>[86]</a>–<a ref-type=bibr anchor=ref87 id=context_ref_87_4b>[87]</a><a ref-type=bibr anchor=ref88 id=context_ref_88_4b>[88]</a><a ref-type=bibr anchor=ref89 id=context_ref_89_4b>[89]</a><a ref-type=bibr anchor=ref90 id=context_ref_90_4b>[90]</a><a ref-type=bibr anchor=ref91 id=context_ref_91_4b>[91]</a>. It is an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers. Standard RANSAC algorithm of this method is presented as follows:<p>Assuming a 2D image corresponds to a 3D scene point. <inline-formula><tex-math notation=LaTeX>\$(x_{i}, wX_{i})\$
</tex-math></inline-formula>. Assuming that some matches are wrong in the data. RANSAC uses the smallest set of possible correspondence and proceed iteratively to increase this set with consistent data.
<ul style=list-style-type:disc><li><p>draw a minimal number of randomly selected correspondences <inline-formula><tex-math notation=LaTeX>\$\text{S}_{\mathrm {k}}\$
</tex-math></inline-formula> (random sample)</p><li><p>compute the pose from these minimal set of point correspondences using () POSIT, DLT</p><li><p>determine the number <inline-formula><tex-math notation=LaTeX>\$\text{C}_{\mathrm {k}}\$
</tex-math></inline-formula> of points from the whole set of all correspondence that are consistent with the estimated parameters with a predefined tolerance. If <inline-formula><tex-math notation=LaTeX>\$\text{C}_{\mathrm {k}}&gt;\text{C}^\ast \$
</tex-math></inline-formula> then retain the randomly selected set of correspondences <inline-formula><tex-math notation=LaTeX>\$\text{S}_{\mathrm {k}}\$
</tex-math></inline-formula> as the best one: <inline-formula><tex-math notation=LaTeX>\$\text{S}^\ast \$
</tex-math></inline-formula> equal <inline-formula><tex-math notation=LaTeX>\$\text{S}_{\mathrm {k}}\$
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX>\$\text{C}^\ast \$
</tex-math></inline-formula> equal <inline-formula><tex-math notation=LaTeX>\$\text{C}_{\mathrm {k}}\$
</tex-math></inline-formula></p><li><p>repeat first step to third step.</p></ul> The correspondences that partakes to the consensus obtained from <inline-formula><tex-math notation=LaTeX>\$S^\ast \$
</tex-math></inline-formula> are the inliers and the outliers are the rest. It has to be noted that the number of iterations which ensures a probability <inline-formula><tex-math notation=LaTeX>\$p\$
</tex-math></inline-formula> that at least one sample with only inliers is drawn can be calculated. Let <inline-formula><tex-math notation=LaTeX>\$p \$
</tex-math></inline-formula> be the probability that the RANSAC algorithm selects only inliers from the input data set in some iteration. The number of iterations is denoted as <a ref-type=bibr anchor=ref92 id=context_ref_92_4b>[92]</a>–<a ref-type=bibr anchor=ref93 id=context_ref_93_4b>[93]</a><a ref-type=bibr anchor=ref94 id=context_ref_94_4b>[94]</a>:<disp-formula id=deqn7 class=display-formula><tex-math notation=LaTeX>\begin{equation*} k=\frac {\log (1-p)}{\log \left ({1-(1-w)^{n}}\right)}\tag{7}\end{equation*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} k=\frac {\log (1-p)}{\log \left ({1-(1-w)^{n}}\right)}\tag{7}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX>\$w\$
</tex-math></inline-formula> is the proportion of inliers and <inline-formula><tex-math notation=LaTeX>\$n\$
</tex-math></inline-formula> is the size of the minimal subset from which the model parameters are estimated.<p><p>Steps to detect and identify object in a scene:
<ul style=list-style-type:disc><li><p>Input training image</p><li><p>Convert the image to grayscale</p><li><p>Get rid of lens distortions from images</p><li><p>Initialise match object</p><li><p>Detect feature points using SURF</p><li><p>Check the image pixels</p><li><p>Extract feature descriptor</p><li><p>Use RANSAC algorithm to match query image with training image</p><li><p>If inliers &gt; threshold then</p><li><p>Compute Homography transform Box</p><li><p>Draw box on object and display.</p></ul><p></p></div></div>
<div class=section id=sec5><div class="header article-hdr"><div class=kicker>
 SECTION V.</div><h2>Sensor Fusion Techniques</h2></div><p>Several definitions of sensor fusion are given in the literature. Sensor fusion or data fusion as defined by Joint Directors of Laboratories (JDL) workshop <a ref-type=bibr anchor=ref95 id=context_ref_95_5>[95]</a> is a multi-level procedure dealing with the association, correlation, integration of data and information from single and multiple sources to attain distinguished position, determine estimates and complete timely assessments of situations, threats and their significance. Also, Hall and Llinas <a ref-type=bibr anchor=ref96 id=context_ref_96_5>[96]</a> presented the following well-known meaning of data fusion: “data fusion techniques combine data from multiple sensors and related information from associated databases to achieve improved accuracy and more specific inferences that could be achieved by the use of a single sensor alone”. According to the authors in <a ref-type=bibr anchor=ref97 id=context_ref_97_5>[97]</a> and <a ref-type=bibr anchor=ref98 id=context_ref_98_5>[98]</a>, sensor fusion was defined as the cooperative use of information provided by multiple sensors to aid on performing a function while several others authors <a ref-type=bibr anchor=ref99 id=context_ref_99_5>[99]</a>–<a ref-type=bibr anchor=ref100 id=context_ref_100_5>[100]</a><a ref-type=bibr anchor=ref101 id=context_ref_101_5>[101]</a> defined data fusion algorithms as the combination of data from multiple sources in order to enhance the performance of mobile robot. Regardless of different definition given, sensor fusion is the integration of information from multiple sources to improve accuracy and quality content, also with the aim to reduce cost. The technique finds wide application in many areas of robotics such as object recognition, environment mapping, and localization. Fusion techniques are therefore regarded as the most appropriate method to track objects and determine their locations. The advantages of sensor fusion are as follows: reduction in uncertainty, increase in accuracy and reduction of cost. It is therefore suggested by various researchers that to attain a level of accuracy, integration of more than one sensor is most suitable because the inadequacy of one sensor can be complemented by another. For example, the image captured by the camera was used to correct the abnormalities of inertial sensors <a ref-type=bibr anchor=ref102 id=context_ref_102_5>[102]</a>, <a ref-type=bibr anchor=ref103 id=context_ref_103_5>[103]</a>. The data fusion techniques deployed is influenced by the objective of applications in which it aids in building a more accurate world model for the robot to navigate and behave more successfully. The three fundamental ways of combining sensor data are the following <a ref-type=bibr anchor=ref99 id=context_ref_99_5>[99]</a>, <a ref-type=bibr anchor=ref104 id=context_ref_104_5>[104]</a>:<div class=section_2 id=sec5a><h3>A. Competitive</h3><p>The sensors are configured competitively to produce independent measurements of the same property. i.e. diverse kinds of sensors are used to measure same environment characteristic. This means data from different sensors can be fused or measurement from a single sensor taken at different periods can be fused. A special case of competitive sensor fusion is fault tolerance. Fault tolerance requires an exact requirement of the service and the failure modes of the system. This configuration therefore reduces the risk of incorrect indication that could be caused by one of the sensors. Most importantly, this might result in an increase in the reliability, accuracy or confidence of data measured by the sensors. This technique can also provide robustness to a system by combining redundant information <a ref-type=bibr anchor=ref105 id=context_ref_105_5a>[105]</a>, <a ref-type=bibr anchor=ref106 id=context_ref_106_5a>[106]</a>. However, the robust system provides a degraded level of service in the presence of faults while this graceful degradation is weaker than the accomplishment of fault tolerance. The method performs better in terms of resource need and work well with heterogenous data sources. Another name for competitive sensor configuration is also called a redundant configuration. An example of competitive is the reduction of noise by combining two overlaying camera images.</p></div><div class=section_2 id=sec5b><h3>B. Complementary</h3><p>This type of sensor configuration ensures that the sensors do not depend on each other but rather complement themselves with different measurements. This resolves the incompleteness of sensor data. This type is the most common for localization. Example is when vision is complemented by the short coming of accumulated errors in IMU. Another example of complementary configuration is the employment of several cameras each observing different area of the mobile robot surrounding to build up a picture of the environment. Generally, fusing complementary data is simple, since the data from independent sensors can be appended to each other, but the disadvantage is that under certain conditions the sensors maybe ineffective, such as when camera used in poor visibility <a ref-type=bibr anchor=ref107 id=context_ref_107_5b>[107]</a>.</p></div><div class=section_2 id=sec5c><h3>C. Cooperative</h3><p>This method uses the information made available by the two separate sensors to originate data that would not be obtainable from the single sensors. An example of a cooperative sensor configuration is stereoscopic vision by combining two dimensional images from two cameras at slightly dissimilar viewpoints in which 3D of the detected scene is derived. According <a ref-type=bibr anchor=ref107 id=context_ref_107_5c>[107]</a>, cooperative sensor configuration is the most difficult system to design due to their sensitivity to imprecisions in all individual participating sensors. Thus, in contrast to competitive fusion, cooperative sensor fusion generally decreases accuracy and reliability.<p>Conclusively, competitive fusion combinations increase the robustness of the perception, while cooperative and complementary fusion provide extended and more complete views. The methods particularly used in the fusion level is subject to the availability of components. Furthermore, these three combinations of sensor fusion are not mutually exclusive. Therefore, many applications implement aspects of more than one of the three types.</p></div></div>
<div class=section id=sec6><div class="header article-hdr"><div class=kicker>
 SECTION VI.</div><h2>Classification of Sensor Fusion Algorithms</h2></div><p>The sensor fusion algorithms are required to translate the diverse sensory inputs into reliable evaluations and environment models that can be used by other navigation subsystems. The methods usually implement iterative algorithms to deal with linear and non-linear models. In order to localize robot, many sensors have been adopted and fusion methods developed. These algorithms are a set of mathematical equations that provide competent computational means to estimates the state of a process. <a ref-type=table anchor=table2 class=fulltext-link>Table 2</a> also shows work based on the classification of sensor fusion method. Some of the sensor algorithms used are categorised into the following <a ref-type=bibr anchor=ref108 id=context_ref_108_6>[108]</a>:<div class="figure figure-full table" id=table2><div class=figcaption><b class=title>TABLE 2 </b>
Related Works of Different Sensor Fusion Algorithms</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck.t2-2975643-large.gif><img class="document-ft-image load-shimmer" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck.t2-2975643-small.gif data-alt="Table 2- 
Related Works of Different Sensor Fusion Algorithms"><div class=zoom title="View Larger Image"></div></a></div></div><p><div class=section_2 id=sec6a><h3>A. State Estimation Method</h3><p>The state estimation methods are used to ascertain the state of an anticipated system that is continuously changing given some observations or measurements. State estimation phase is a common step in data fusion algorithms since the target’s observation could come from different sensors or sources, and the final goal is to acquire a global target state from the observations. <a ref-type=table anchor=table3 class=fulltext-link>Table 3</a> shows related study carried primarily based on state estimate methods. The two major methods discussed are kalman filter and particle filter.<div class="figure figure-full table" id=table3><div class=figcaption><b class=title>TABLE 3 </b>
Related Works of State Estimate Sensor Fusion Algorithms</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck.t3-2975643-large.gif><img class="document-ft-image load-shimmer" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9007654/hanck.t3-2975643-small.gif data-alt="Table 3- 
Related Works of State Estimate Sensor Fusion Algorithms"><div class=zoom title="View Larger Image"></div></a></div></div><p><div class=section_2 id=sec6a1><h4>1) Kalman Filter</h4><p>Kalman filter (KF) is an efficient estimator used in various fields to estimate the unknown state of the system. Several applications were developed with the implementation of Kalman filter such applications include navigation, localization and object tracking. It involves using vision camera to perform real time image processing for robot tracking. Kalman filter is established to estimate the positions and velocities of vehicles or any moving object and provide tracking on such objects at a visible condition.<p>Kalman filter is an algorithm that estimates the state of a discrete time-controlled process described by the linear stochastic equation. It processes the state from the previous time step with the current measurement to calculate the estimate of the current state. Kalman filters are famous techniques in theory of stochastic dynamic systems, which can be used to improve the value of estimates of unknown quantities <a ref-type=bibr anchor=ref109 id=context_ref_109_6a1>[109]</a>. It is one of the most useful and common estimation techniques where it is easy to implement on linear systems. Equations for Kalman filter are given as follows <a ref-type=bibr anchor=ref110 id=context_ref_110_6a1>[110]</a>:<disp-formula id=deqn8-9 class=display-formula><tex-math notation=LaTeX>\begin{align*} \hat {x}_{k}=&amp;F_{k} \hat {x}_{k-1} +B_{k} u_{k} \,\, \tag{8}\\ P_{k}=&amp;F_{k} P_{k-1} F_{k}^{T}+Q_{k}\tag{9}\end{align*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} \hat {x}_{k}=&amp;F_{k} \hat {x}_{k-1} +B_{k} u_{k} \,\, \tag{8}\\ P_{k}=&amp;F_{k} P_{k-1} F_{k}^{T}+Q_{k}\tag{9}\end{align*}
</span></span></disp-formula> Vector <inline-formula><tex-math notation=LaTeX>\$\hat {x}_{k}\$
</tex-math></inline-formula> is the estimate state of the system <inline-formula><tex-math notation=LaTeX>\$x_{k}\$
</tex-math></inline-formula>. <inline-formula><tex-math notation=LaTeX>\$P_{k}\$
</tex-math></inline-formula> is the predicted covariance matrix. <inline-formula><tex-math notation=LaTeX>\$F\$
</tex-math></inline-formula> is the matrix that denote the dynamics of the system. <inline-formula><tex-math notation=LaTeX>\$B\$
</tex-math></inline-formula> is the control matrix and <inline-formula><tex-math notation=LaTeX>\$Q\$
</tex-math></inline-formula> is the noise covariance.<p>The Kalman filter equation are used to generate new estimates with the addition of an external unit for correction. The Kalman filter involve another stage to update the estimate. This is given by equations below:<disp-formula id=deqn10-11 class=display-formula><tex-math notation=LaTeX>\begin{align*} {\hat {x}}'_{k}=&amp;\hat {x}_{k} +{K}'(z_{k} -H_{k} \hat {x}_{k}) \tag{10}\\ {P}'_{k}=&amp;P_{k} -{k}'H_{k} P_{k}\tag{11}\end{align*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} {\hat {x}}'_{k}=&amp;\hat {x}_{k} +{K}'(z_{k} -H_{k} \hat {x}_{k}) \tag{10}\\ {P}'_{k}=&amp;P_{k} -{k}'H_{k} P_{k}\tag{11}\end{align*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX>\${K}'=P_{k} H_{k}^{T} (H_{k} P_{k} H_{k}^{T} +R_{k})^{-1}\$
</tex-math></inline-formula><p>From the above equations: <inline-formula><tex-math notation=LaTeX>\$z_{k} \$
</tex-math></inline-formula> is the measurement vector which is a reading from the sensors. <inline-formula><tex-math notation=LaTeX>\$H\$
</tex-math></inline-formula> is the transformation matrix, <inline-formula><tex-math notation=LaTeX>\$R\$
</tex-math></inline-formula> is the covariance matrix of the measurement noise and <inline-formula><tex-math notation=LaTeX>\$k\$
</tex-math></inline-formula> is the time interval. The Kalman gain (K) describes the amount of update needed at each recursive estimation which can be as the weighting factor that considers the relationship between the accuracy of the predicted estimate and the measurement noise. To analyze the statistical behavior of the measured values, KF is an optimal estimator that can be used. Most of the real time problem, the systems may not provide linear characteristic, so we use extended Kalman filter, which will linearize the system. The main benefit of Kalman filter is its computational competence but it can signify only unimodal distributions. So Kalman filters are best when the uncertainty is not too high. Other types of sensor fusion based on Kalman filter is EKF. The Extended Kalman Filter (EKF) is one of the most effective probabilistic solutions to simultaneously estimate the robot pose estimation based on sensor information.<p>Comparing Kalman filter to EKF, author <a ref-type=bibr anchor=ref111 id=context_ref_111_6a1>[111]</a> proves that that EKF algorithm is among the best method which ensures better performance and optimal result in determining robot localization. Another derivates of KF apart from EKF is Unscented Kalman filtering (UKF). According to the literature, it is stated that UKF delivers better results on data fusion compared to Kalman filter or EKF solutions <a ref-type=bibr anchor=ref112 id=context_ref_112_6a1>[112]</a>.</p></div><div class=section_2 id=sec6a2><h4>2) Particle Filter</h4><p>Particle Filter (PF), with the ability of approximating Probability Density Functions (PDFs) of any form, has received substantial attention among researchers. PF method is a Sequential Monte Carlo (SMC) technique for the solution of the state estimation problem, using the so-called Sequential Importance Sampling (SIS) algorithm and including a resampling step at each instant. This method builds the consequent density function using several random samples called particles. Particles are propagated over time with the integration of sampling and resampling steps. At each iteration, the sampling step is employed to reject some particles, increasing the significance of regions with advanced posterior probability. The particle algorithm is comprise of the following steps <a ref-type=bibr anchor=ref97 id=context_ref_97_6a2>[97]</a>, <a ref-type=bibr anchor=ref113 id=context_ref_113_6a2>[113]</a>–<a ref-type=bibr anchor=ref114 id=context_ref_114_6a2>[114]</a><a ref-type=bibr anchor=ref115 id=context_ref_115_6a2>[115]</a><a ref-type=bibr anchor=ref116 id=context_ref_116_6a2>[116]</a><a ref-type=bibr anchor=ref117 id=context_ref_117_6a2>[117]</a>:<p>Particle generation:<p>Generate <inline-formula><tex-math notation=LaTeX>\$N\{x_{1} (0),x_{2} (0),x_{3} (0),\ldots,x_{N} (0)\}\$
</tex-math></inline-formula> initial particles according to the initial probability density function (PDF) <inline-formula><tex-math notation=LaTeX>\$p(x(0))\$
</tex-math></inline-formula><p>Prediction:<p>For each particle <inline-formula><tex-math notation=LaTeX>\$x_{i} (k)\$
</tex-math></inline-formula>, propagate the <inline-formula><tex-math notation=LaTeX>\$x_{i} (k+1)\$
</tex-math></inline-formula> particle according to the transition PDF <inline-formula><tex-math notation=LaTeX>\$p(x(k+1)\,\vert x(k))\$
</tex-math></inline-formula>. Here, each particle accounts for the sum of the random noise to simulate the noise effect.<p>Sampling: For each particle <inline-formula><tex-math notation=LaTeX>\$x_{i} (k+1)\$
</tex-math></inline-formula>, generate <inline-formula><tex-math notation=LaTeX>\$w_{i} (k+1)=p[z(k+1)\vert x_{i} (k+1)]\$
</tex-math></inline-formula><p>Normalization and rejected sampling:<p>Weights of the particles are normalized. Particles with low weight are removed and particles with high weight are replicated such that each particle has the same weight.<p>PF is considered as an alternative for real-time applications, which are typically approached by model based traditional Kalman filter technique implementations. With the advantages of accuracy and stability, PF is currently being considered in the field of traffic control (car or people video monitoring), military field (radar tracking, air-to-ground passive tracking), mobile robot positioning and self-localization.</p></div></div><div class=section_2 id=sec6b><h3>B. Decision Fusion Method</h3><p>Decision fusion is one form of data fusion that combines the decisions of many classifiers into a mutual decision about the activity that happened. The fusion method reduces the level of uncertainty by maximizing a measure of evidence <a ref-type=bibr anchor=ref118 id=context_ref_118_6b>[118]</a>. These techniques frequently use symbolic information, and the fusion process requires to reason while accounting for the uncertainties and constraints. The two types of decision method discussed here are Bayesian Approach and Dempster-Shafer Approach.<div class=section_2 id=sec6b1><h4>1) Bayesian Approach</h4><p>Bayesian approach is a basic method to deal with conditional probability more precisely it relates the condition probability of more than two events. They are practically used for more complex relationship description <a ref-type=bibr anchor=ref119 id=context_ref_119_6b1>[119]</a>. The method provides a theoretical framework for dealing with this uncertainty using an underlying graphical structure. They are ideal for taking an event that happened and envisaging the likelihood that any one of numerous possible known causes was the contributing factor. Bayesian method can be mathematically presented as <a ref-type=bibr anchor=ref113 id=context_ref_113_6b1>[113]</a>:<disp-formula id=deqn12 class=display-formula><tex-math notation=LaTeX>\begin{equation*} P(C | D)=\frac {P(C | D) P(C)}{P(D)}\tag{12}\end{equation*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} P(C | D)=\frac {P(C | D) P(C)}{P(D)}\tag{12}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX>\$P(C)\$
</tex-math></inline-formula> is the probability of event <inline-formula><tex-math notation=LaTeX>\$C\$
</tex-math></inline-formula> without any effect of any other event. <inline-formula><tex-math notation=LaTeX>\$P(D)\$
</tex-math></inline-formula> is the probability of the event <inline-formula><tex-math notation=LaTeX>\$D\$
</tex-math></inline-formula> without any effect of any other event and <inline-formula><tex-math notation=LaTeX>\$P(D\vert C)\$
</tex-math></inline-formula> is the probability of event <inline-formula><tex-math notation=LaTeX>\$D\$
</tex-math></inline-formula> given that <inline-formula><tex-math notation=LaTeX>\$C\$
</tex-math></inline-formula> event is true. The result of <inline-formula><tex-math notation=LaTeX>\$P(C\vert D)\$
</tex-math></inline-formula> condition probability will be in range between zero and one [1 0]. Which means either the event <inline-formula><tex-math notation=LaTeX>\$P(C\vert D)\$
</tex-math></inline-formula> will occur. Bayesian method is computationally simpler, has higher probabilities for correct decision and it provides point estimates and posterior pdf <a ref-type=bibr anchor=ref120 id=context_ref_120_6b1>[120]</a>. However, they have the following demerits: difficulty in describing the uncertainty of decision, complexity when there are multiple potential hypothesis and a substantial number of events that depend on conditions, difficulty in establishing the value of a prior probabilities. Bayesian method is applicable to solve image fusion, where no prior knowledge in available. Also, it is applied in robotics learning by imitation. The approach enables the robot to study internal models of their environment through self-experience and employ the model for human intent recognition, skill acquisition from human observation.</p></div><div class=section_2 id=sec6b2><h4>2) Dempster-Shafer</h4><p>Dempster-Shafer (DS) has become very famous in which its application extends to pattern recognition methods which are widely used in signal solving and recognition. The method has a better adaptability of grasping unknown and uncertain problem when it is regarded as an uncertainty method. It also provides a vital formula which fuse diverse evident of different sources. Dempster-Shafter theory has been considered for a variety of perceptual activities including sensor fusion, scene interpretation, object target recognition, and object verification. In <a ref-type=bibr anchor=ref109 id=context_ref_109_6b2>[109]</a>, DS theory was successfully used in building occupancy map to improve reliability. D-S approach is more robust to perturbations such as noise and imprecise prior information <a ref-type=bibr anchor=ref120 id=context_ref_120_6b2>[120]</a>. The method is based on concept of combining information from different sources such as sensors. It uses belief and plausibility values to represent the evidence and corresponding uncertainty <a ref-type=bibr anchor=ref121 id=context_ref_121_6b2>[121]</a>, <a ref-type=bibr anchor=ref122 id=context_ref_122_6b2>[122]</a>. The method uses ‘belief’ rather than probability. Belief function is used to represent the uncertainty of the hypothesis <a ref-type=bibr anchor=ref123 id=context_ref_123_6b2>[123]</a>. The hypothesis is represented by a probability mass function <inline-formula><tex-math notation=LaTeX>\$m\$
</tex-math></inline-formula>. The amount of belief to a hypothesis <inline-formula><tex-math notation=LaTeX>\$(A)\$
</tex-math></inline-formula> is denoted by a belief function <a ref-type=bibr anchor=ref124 id=context_ref_124_6b2>[124]</a>:<disp-formula id=deqn13 class=display-formula><tex-math notation=LaTeX>\begin{equation*} B e l(A)=\sum _{B \subseteq A} m(B)\tag{13}\end{equation*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} B e l(A)=\sum _{B \subseteq A} m(B)\tag{13}\end{equation*}
</span></span></disp-formula> <a ref-type=disp-formula anchor=deqn13 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/#deqn13 class=fulltext-link>Equation (13)</a> is the sum of the mass probabilities assigned to all subsets of <inline-formula><tex-math notation=LaTeX>\$A\$
</tex-math></inline-formula> by <inline-formula><tex-math notation=LaTeX>\$m\$
</tex-math></inline-formula>. The availability of two or more evidence is integrated using the combination rule in equation below:<disp-formula id=deqn14 class=display-formula><tex-math notation=LaTeX>\begin{align*}&amp;\hspace {-2pc}\sum _{i, j} m_{1}\left ({B_{i}}\right) \cdot m_{2}\left ({C_{j}}\right) \\&amp;m(A)=\frac {B_{i} \cap C_{j}=A}{1-k}\tag{14}\end{align*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*}&amp;\hspace {-2pc}\sum _{i, j} m_{1}\left ({B_{i}}\right) \cdot m_{2}\left ({C_{j}}\right) \\&amp;m(A)=\frac {B_{i} \cap C_{j}=A}{1-k}\tag{14}\end{align*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX>\$1-k\$
</tex-math></inline-formula> is a normalization factor in which <inline-formula><tex-math notation=LaTeX>\$k\$
</tex-math></inline-formula> is the total of all non-zero values given to the null set hypothesis <inline-formula><tex-math notation=LaTeX>\$\varnothing \$
</tex-math></inline-formula>. The decision on the class of a feature can be decided based on a maximum belief decision rule, which is assigned a feature to a class <inline-formula><tex-math notation=LaTeX>\$A\$
</tex-math></inline-formula> if the total amount of belief supporting <inline-formula><tex-math notation=LaTeX>\$A\$
</tex-math></inline-formula> is more than that supporting its negation:<disp-formula id=deqn15 class=display-formula><tex-math notation=LaTeX>\begin{equation*} Bel\big)A\big)\ge Bel(\bar {A})\tag{15}\end{equation*}
</tex-math><span class=formula><span class=link>View Source</span><img class="document-ft-image load-shimmer" aria-describedby=qtip-0 style=display:inline title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-lazy=/assets/img/icon.support.gif data-alt="Right-click on figure for MathML and additional features." width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} Bel\big)A\big)\ge Bel(\bar {A})\tag{15}\end{equation*}
</span></span></disp-formula></p></div></div></div>
<div class=section id=sec7><div class="header article-hdr"><div class=kicker>
 SECTION VII.</div><h2>Importance of Sensor Fusion Techniques</h2></div><p>Techniques that employ sensor fusion methods has several advantages over single sensor systems. Combined information reduces the set of uncertain interpretations of the measured value. Expected benefits of sensor fusion techniques are presented as follows <a ref-type=bibr anchor=ref104 id=context_ref_104_7>[104]</a>:
<ul style=list-style-type:none><li><p><i>Reduction in Uncertainty</i>: Data provided by sensors is sometimes subjected to some level of uncertainty and discrepancy. Multi-sensor data fusion techniques reduce the uncertainty by combining data from numerous sources <a ref-type=bibr anchor=ref125 id=context_ref_125_7>[125]</a>. It is therefore imperative to compensate using other sensors by fusing their data together using data fusion algorithms. Authors in <a ref-type=bibr anchor=ref126 id=context_ref_126_7>[126]</a> was able to minimize uncertainty in robot localization based on EKF and PF. The measurement from the kinetic sensor was used to correct the error accumulated by odometry in order to estimate the pose of the mobile robot.</p><li><p><i>Increase in Accuracy and Reliability</i>: Integration of multiple sensor sources will enable the system to provide inherent information even in case of partial failure.</p><li><p><i>Extended Spatial and Temporal Coverage</i>: Area covered by one sensor may not be covered by the other sensor, therefore the coverage or measurement of one is dependent on the other and this complements each other. An example is inertial sensor such as accelerometer or gyroscope and vision. The coverage of a camera as vision sensor cannot be compared to the use of accelerometer which only takes measurement about the navigation route.</p><li><p><i>Improved Resolution</i>: The resolution resulting value of multiple independent measurements fused together is better than a singular sensor measurement.</p><li><p><i>Reduce System Complexity</i>: System where sensor data is preprocessed by fusion algorithms, the input to the controlling application can be standardized autonomously of the employed sensor kinds, consequently simplifying application implementation and providing the option of modifications in the sensor system concerning number and type of employed sensors without alterations of the application software.</p></ul><p></p></div>
<div class=section id=sec8><div class="header article-hdr"><div class=kicker>
 SECTION VIII.</div><h2>Future Research Areas</h2></div><p>Navigation and localization of a mobile robot in an arbitrary environment is a challenge due to the intricacy and diversity of environments, methods and sensors that are involved. It is therefore necessary to continue to research on new systems and new methods with the aim to unravel specific sensor fusion problems for robot navigation and localization. Several directions seem to call for further investigation, despite other related work carried out in the literature.
<ul style=list-style-type:none><li><p><i>3D Indoor Environmental Modelling</i>: 3D models of indoor environments are significant in many applications, but they usually exist only for newly constructed buildings <a ref-type=bibr anchor=ref127 id=context_ref_127_8>[127]</a>. For robot navigation purpose, 3D models are required in an indoor operation environment to ensure safe movement. The model is also expected to be used for recognition and location by robots. To develop a method to model 3D, simplicity and accuracy must first be put into consideration. A 3D model can convey more useful information than 2D maps used in many applications. For example, in an indoor environment where additional features are present and are also unresolved problems in modelling. This kind of environment requires more sophisticated models in order to determine the ability characteristics of the environment. Several methods are adopted in modelling the environment. Reference in <a ref-type=bibr anchor=ref132 id=context_ref_132_8>[132]</a> proposed a method of obtaining 3D models by a mobile robot with a laser scanner and a panoramic camera while Thrun <i>et al.</i> <a ref-type=bibr anchor=ref133 id=context_ref_133_8>[133]</a> proposed a multi-planar model from dense range data and image data using an improved Expectation-Maximization (EM) algorithm. Some authors worked with generation of precise 3D models using sufficient amount of data and expatiate statistical and geometrical estimation technique. Environment models are required for localization, object recognition/detection. Recently, 3D models are usually attained by hand-guided scanning which is very hard and time-demanding task for the human operator. Therefore, a robotic system to obtain 3D models of environment is highly beneficial <a ref-type=bibr anchor=ref134 id=context_ref_134_8>[134]</a>.</p><li><p><i>Landmarks and Feature Extraction</i>: Localization methods using vision are active research areas, especially in studies related with the identification of objects and the position and estimation of the recognized objects <a ref-type=bibr anchor=ref135 id=context_ref_135_8>[135]</a>. Another aspect to look into is the appearance changes of target objects over time; this also as a research area has gained much attention in the literature but with the limitation of robust detection algorithm.</p><li><p><i>Distinct Object</i>: To improve localization for a mobile robot in a structured or unstructured environment, it is suggested that distinct or specific objects are to be detected. Despite the work done, this is still an open problem.</p><li><p><i>Topological Modelling and Localization</i>: Several traditional localization approaches attempt to determine geometrically the position and the direction of the robot; new approaches are to be considered and compared. Recent approaches look for methods to build topological models once features and landmarks are detected and for topological estimation of the robot’s state.</p><li><p><i>Perception Planning and World Modelling</i>: Motion planning and path planning are factors that can also cause uncertainty in mobile robot. In a situation whereby the robot accidentally takes another route and misses it path, how such event is handled is an aspect that requires attention. Therefore, new techniques are suggested to determine motion plan for mobile robot. Also, a model of the environment is to be built for safe motion planning for the robot to operate in.</p></ul><p></p></div>
<div class=section id=sec9><div class="header article-hdr"><div class=kicker>
 SECTION IX.</div><h2>Conclusion</h2></div><p>Through the mobilization of autonomous mobile robot, businesses are increasing flexibility and diversifying applications. The new technologies have improved and ease the way of life of human beings in which their exposure and environmental dangers and hazard have been reduced to the minimum.<p>In this paper, we have been able to provide a background and identify the challenges of an autonomous mobile robot. These problems such as navigation and localization are what limit the performance of the robot. Therefore, some techniques have been presented in this paper on how to tackle the challenges. Such techniques are using sensors which are coupled on the mobile robot for effective performance. Using a single sensor to determine the pose of an object may not be reliable and accurate therefore, the use of multi-sensor is encouraged. Their objective is to integrate multiple data sources to produce more consistent, accurate, and useful information.<p>Methods used to extract information from environment using computer vison were also discussed. These methods are categorized into artificial and natural landmarks. They are used to detect/identify objects and match with the training image. The strength and weakness of these approaches were also presented. Exploring the conceptualizations and benefits, as well as existing methodologies, sensor are categorized into how to relate to one another, this is called sensor configuration. They are cooperative, complementary and competitive. The mostly used sensor configuration for autonomous mobile robot is complementary.<p>Also, benefits of using sensor fusion algorithms were identified in this paper. Finally, the paper highlighted some of the research areas that can be investigated for further work.</p></div>
</div></div></response>
</div><xpl-reference-pop-up _ngcontent-xlc-c158 _nghost-xlc-c149></xpl-reference-pop-up><span _ngcontent-xlc-c158 id=full-text-footer></span></div></div><div _ngcontent-xlc-c158 class="col-3-24 u-pr-1 u-pl-1 col-buttons stats-document-container-rh u-printing-invisible-ie u-printing-invisible-ff"><xpl-document-buttons _ngcontent-xlc-c158 _nghost-xlc-c157><div _ngcontent-xlc-c157 xplscrollsnapmigr cssclasstostick=document-side-menu-stick fromelementid=toc-wrapper tillelementid=full-text-footer offsetfrom=150 offsetto=-800 scrollreset=true id=scroll-snap-buttons-container class="document-doc-buttons stats-document-container-buttons"><ul _ngcontent-xlc-c157 class=tools><li _ngcontent-xlc-c157 id=search-popover xplpopoveranimateonscroll animateelementidposition=1 animatecontainerelementid=search-popover class="blue-tooltip increased-width special-left-tooltip"><a _ngcontent-xlc-c157 triggers=click placement=left><i _ngcontent-xlc-c157 class="fas fa-search icon-size-md color-gray-dark"></i></a><li _ngcontent-xlc-c157 id=resizer-popover xplpopoveranimateonscroll animateelementidposition=1 animatecontainerelementid=resizer-popover class="blue-tooltip special-left-tooltip"><a _ngcontent-xlc-c157 min-size=10 max-size=20 placement=left triggers=click><i _ngcontent-xlc-c157 class="far fa-text-size icon-size-md color-gray-dark"></i></a></ul><xpl-back-to-top-button _ngcontent-xlc-c157 section=full-text-header _nghost-xlc-c82><ul _ngcontent-xlc-c82 class="back-to-top sf-hidden"></ul></xpl-back-to-top-button></div></xpl-document-buttons></div></div></section></xpl-document-full-text><xpl-accordian-section _ngcontent-xlc-c191 _nghost-xlc-c183><div _ngcontent-xlc-c183 role=tablist class="document-accordion-section-container hide-mobile"><xpl-document-accordion _ngcontent-xlc-c183 class=accordion-panel-container _nghost-xlc-c163><div _ngcontent-xlc-c163><div _ngcontent-xlc-c163 role=tab class="accordion-header accordion-button" id=authors-header aria-expanded=false aria-disabled=false><a _ngcontent-xlc-c183 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ id=authors class="accordion-link text-base-md-lh">Authors</a><div _ngcontent-xlc-c163 class=accordion-chevron><i _ngcontent-xlc-c163 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-xlc-c163><div _ngcontent-xlc-c163 role=tab class="accordion-header accordion-button" id=figures-header aria-expanded=false aria-disabled=false><a _ngcontent-xlc-c183 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ id=figures class="accordion-link text-base-md-lh">Figures</a><div _ngcontent-xlc-c163 class=accordion-chevron><i _ngcontent-xlc-c163 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-xlc-c163><div _ngcontent-xlc-c163 role=tab class="accordion-header accordion-button" id=references-header aria-expanded=false aria-disabled=false><a _ngcontent-xlc-c183 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ id=references class="accordion-link text-base-md-lh">References</a><div _ngcontent-xlc-c163 class=accordion-chevron><i _ngcontent-xlc-c163 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-xlc-c163><div _ngcontent-xlc-c163 role=tab class="accordion-header accordion-button" id=citations-header aria-expanded=false aria-disabled=false><a _ngcontent-xlc-c183 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ id=citations class="accordion-link text-base-md-lh">Citations</a><div _ngcontent-xlc-c163 class=accordion-chevron><i _ngcontent-xlc-c163 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-xlc-c163><div _ngcontent-xlc-c163 role=tab class="accordion-header accordion-button" id=keywords-header aria-expanded=false aria-disabled=false><a _ngcontent-xlc-c183 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ id=keywords class="accordion-link text-base-md-lh">Keywords</a><div _ngcontent-xlc-c163 class=accordion-chevron><i _ngcontent-xlc-c163 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-xlc-c163><div _ngcontent-xlc-c163 role=tab class="accordion-header accordion-button" id=metrics-header aria-expanded=false aria-disabled=false><a _ngcontent-xlc-c183 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ id=metrics class="accordion-link text-base-md-lh">Metrics</a><div _ngcontent-xlc-c163 class=accordion-chevron><i _ngcontent-xlc-c163 class="fa fa-angle-down"></i></div></div></div></xpl-document-accordion></div></xpl-accordian-section></div></section></div><div _ngcontent-xlc-c191 class="document-disqus-container col-24-24"><div _ngcontent-xlc-c191 class=row><div _ngcontent-xlc-c191 class="col-12 disqus-container"><xpl-disqus-migr _ngcontent-xlc-c191 page=document _nghost-xlc-c188><div id=disqus_recommendations style=margin-bottom:12px></div><div _ngcontent-xlc-c188 id=disqus_thread></div></xpl-disqus-migr></div></div></div></div></div><div _ngcontent-xlc-c191 class="document-sidebar global-right-rail top-spacing"><div _ngcontent-xlc-c191 class=header-rel-art-toggle-mobile><i _ngcontent-xlc-c191 class=header-rel-art-toggle-icon></i></div><div _ngcontent-xlc-c191 class=document-sidebar-content><div _ngcontent-xlc-c191 class="document-sidebar-serp-nav hide-mobile"><span _ngcontent-xlc-c191> &nbsp;<a _ngcontent-xlc-c191 target=_self href="https://ieeexplore-ieee-org.thi.idm.oclc.org/search/searchresult.jsp?contentType=all&amp;queryText=9007654">&nbsp;Back to Results&nbsp;</a>&nbsp; </span></div><div _ngcontent-xlc-c191 class=hide-mobile><xpl-leaderboard-ad _ngcontent-xlc-c191 _nghost-xlc-c124><div _ngcontent-xlc-c124 class="Ads-leaderboard ad-panel"><div _ngcontent-xlc-c124 class="row u-flex-wrap-nowrap"></div><div _ngcontent-xlc-c124 class=ad-leaderboard-ad-container><div _ngcontent-xlc-c124 xplgoogleadmigr><div id=div-gpt-ad-1606861783116-0 style="width:300px;height:250px;display:none;margin:0px auto"></div></div></div></div></xpl-leaderboard-ad></div><div _ngcontent-xlc-c191 class=document-sidebar-rel-art><xpl-related-article-list _ngcontent-xlc-c191 _nghost-xlc-c189><div _ngcontent-xlc-c189 class=stats-document-header-relatedArticles><div _ngcontent-xlc-c189 class=header-rel-art><div _ngcontent-xlc-c189 class="header-rel-art-title text-base-md-lh"> More Like This </div><div _ngcontent-xlc-c189 class=header-rel-art-list><div _ngcontent-xlc-c189 class=header-rel-art-item><div _ngcontent-xlc-c189 class="row text-base-md-lh"><a _ngcontent-xlc-c189 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/4108730/><span _ngcontent-xlc-c189>A Sensor Fusion Method for Mobile Robot Navigation</span></a></div><p _ngcontent-xlc-c189 class="header-rel-art-pub text-sm-md-lh">2006 SICE-ICASE International Joint Conference<p _ngcontent-xlc-c189 class="header-rel-art-pub text-sm-md-lh">Published: 2006</p></div><div _ngcontent-xlc-c189 class=header-rel-art-item><div _ngcontent-xlc-c189 class="row text-base-md-lh"><a _ngcontent-xlc-c189 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/8740655/><span _ngcontent-xlc-c189>Research on autonomous navigation of mobile robot based on multi ultrasonic sensor fusion</span></a></div><p _ngcontent-xlc-c189 class="header-rel-art-pub text-sm-md-lh">2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)<p _ngcontent-xlc-c189 class="header-rel-art-pub text-sm-md-lh">Published: 2018</p></div></div><div _ngcontent-xlc-c189 class="header-rel-art-action text-base-md-lh"><a _ngcontent-xlc-c189 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/>Show More</a></div></div></div></xpl-related-article-list></div><div _ngcontent-xlc-c191 class=hide-mobile><xpl-leaderboard-middle-ad _ngcontent-xlc-c191 _nghost-xlc-c187><div _ngcontent-xlc-c187 class="Ads-leaderboard ad-panel"><div _ngcontent-xlc-c187 class="row u-flex-wrap-nowrap"></div><div _ngcontent-xlc-c187 class=ad-leaderboard-ad-container><div _ngcontent-xlc-c187 xplgoogleadmigr><div id=div-gpt-ad-1606861708157-0 style="width:300px;height:600px;display:none;margin:0px auto"></div></div></div></div></xpl-leaderboard-middle-ad></div></div></div><xpl-reference-panel _ngcontent-xlc-c191 _nghost-xlc-c190><section _ngcontent-xlc-c190 id=references-anchor class="document-all-references hide-mobile panel-closed"><div _ngcontent-xlc-c190 class=header><h1 _ngcontent-xlc-c190>References</h1><a _ngcontent-xlc-c190><i _ngcontent-xlc-c190 class="fas fa-times"></i></a></div><div _ngcontent-xlc-c190 id=references-section-container class=document-ft-section-container><div _ngcontent-xlc-c190><b _ngcontent-xlc-c190>References is not available for this document.</b></div></div></section></xpl-reference-panel></div></xpl-document-details></div><xpl-footer _ngcontent-xlc-c451 _nghost-xlc-c450><footer _ngcontent-xlc-c450 id=xplore-footer class="stats-footer footer-new"><div _ngcontent-xlc-c450 class=footer-wrapper><div _ngcontent-xlc-c450 class=flexible-row-col><div _ngcontent-xlc-c450 class=footer-col><h3 _ngcontent-xlc-c450 class=text-base-md-lh>IEEE Personal Account</h3><ul _ngcontent-xlc-c450 class=text-sm-md-lh><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_blank href="https://www-ieee-org.thi.idm.oclc.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Change username/password</a></ul></div><div _ngcontent-xlc-c450 class=footer-col><h3 _ngcontent-xlc-c450 class=text-base-md-lh>Purchase Details</h3><ul _ngcontent-xlc-c450 class=text-sm-md-lh><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_blank href="https://www-ieee-org.thi.idm.oclc.org/profile/payment/showPaymentHome.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_blank href=https://ieeexplore-ieee-org.thi.idm.oclc.org/articleSale/purchaseHistory.jsp>View Purchased Documents</a></ul></div><div _ngcontent-xlc-c450 class=footer-col><h3 _ngcontent-xlc-c450 class=text-base-md-lh>Profile Information</h3><ul _ngcontent-xlc-c450 class=text-sm-md-lh><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_blank href="https://www-ieee-org.thi.idm.oclc.org/ieee-privacyportal/app/ibp?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_blank href="https://www-ieee-org.thi.idm.oclc.org/profile/profedu/getProfEduInformation.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_blank href="https://www-ieee-org.thi.idm.oclc.org/profile/tips/getTipsInfo.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Technical interests</a></ul></div><div _ngcontent-xlc-c450 class="footer-col need-help"><h3 _ngcontent-xlc-c450 class=text-base-md-lh>Need Help?</h3><ul _ngcontent-xlc-c450 class=text-sm-md-lh><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 href=tel:+1-800-678-4333> US &amp; Canada: +1 800 678 4333 </a><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 href=tel:+1-732-981-0060> Worldwide: +1 732 981 0060 </a><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact> Contact &amp; Support </a></ul></div><div _ngcontent-xlc-c450 class="footer-col follow"><h3 _ngcontent-xlc-c450 class=text-base-md-lh>Follow</h3><ul _ngcontent-xlc-c450 class=icon-size-md><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_blank href=https://www.facebook.com/IEEEXploreDigitalLibrary/><i _ngcontent-xlc-c450 aria-hidden=true class="fab fa-facebook-f"></i></a><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_blank href=https://www.linkedin.com/showcase/ieee-xplore><i _ngcontent-xlc-c450 aria-hidden=true class="fab fa-linkedin-in"></i></a><li _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_blank href="https://twitter.com/IEEEXplore?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor"><i _ngcontent-xlc-c450 aria-hidden=true class="fab fa-twitter"></i></a></ul></div></div><div _ngcontent-xlc-c450 class=footer-bottom-section><p _ngcontent-xlc-c450 class=text-sm-md-lh><span _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/about-ieee-xplore.html>About IEEE <em _ngcontent-xlc-c450>Xplore</em></a></span> | <span _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact>Contact Us</a></span> | <span _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_start.html>Help</a></span> | <span _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/accessibility-statement.html>Accessibility</a></span> | <span _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_Terms_of_Use.html>Terms of Use</a></span> | <span _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_self href=http://www.ieee.org.thi.idm.oclc.org/web/aboutus/whatis/policies/p9-26.html>Nondiscrimination Policy</a></span> | <span _ngcontent-xlc-c450 class=ethics-reporting-link><a _ngcontent-xlc-c450 target=_blank href=http://www.ieee-ethics-reporting.org/>IEEE Ethics Reporting<i _ngcontent-xlc-c450 class="fa fa-external-link-alt"></i></a></span> | <span _ngcontent-xlc-c450><a _ngcontent-xlc-c450 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/overview-of-ieee-xplore/ieee-xplore-sitemap>Sitemap</a></span> | <span _ngcontent-xlc-c450 class=nowrap><a _ngcontent-xlc-c450 target=_self href=http://www.ieee.org.thi.idm.oclc.org/about/help/security_privacy.html>Privacy &amp; Opting Out of Cookies</a></span><p _ngcontent-xlc-c450> A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. <p _ngcontent-xlc-c450> © Copyright 2022 IEEE - All rights reserved. </p></div></div></footer></xpl-footer></xpl-root>
 </div>
 
 
 
</div>
 
<section id=xploreFooter class=hide-desktop>
 
 <div class="Footer stats-footer hide-mobile">
 <div class="pure-g Footer-sections">
 <div class=pure-u-1-4>
 <h3 class=Footer-header>IEEE Account</h3>
 <ul class=Footer-list>
 <li><a href="https://www-ieee-org.thi.idm.oclc.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Change Username/Password</a></li>
 <li><a href="https://www-ieee-org.thi.idm.oclc.org/profile/address/getAddrInfoPage.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Update Address</a></li>
 </ul>
 </div>
 <div class=pure-u-1-4>
 <h3 class=Footer-header>Purchase Details</h3>
 <ul class=Footer-list>
 <li><a href="https://www-ieee-org.thi.idm.oclc.org/profile/payment/showPaymentHome.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a></li>
 <li><a href="https://www-ieee-org.thi.idm.oclc.org/profile/vieworder/showOrderHistory.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Order History</a></li>
 <li><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/articleSale/purchaseHistory.jsp>View Purchased Documents</a></li>
 </ul>
 </div>
 <div class=pure-u-1-4>
 <h3 class=Footer-header>Profile Information</h3>
 <ul class=Footer-list>
 <li><a href="https://www-ieee-org.thi.idm.oclc.org/ieee-privacyportal/app/ibp?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a></li>
 <li><a href="https://www-ieee-org.thi.idm.oclc.org/profile/profedu/getProfEduInformation.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a></li>
 <li><a href="https://www-ieee-org.thi.idm.oclc.org/profile/tips/getTipsInfo.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Technical Interests</a></li>
 </ul>
 </div>
 <div class=pure-u-1-4>
 <h3 class=Footer-header>Need Help?</h3>
 <ul class=Footer-list>
 <li><strong>US &amp; Canada:</strong> +1 800 678 4333</li>
 <li><strong>Worldwide: </strong> +1 732 981 0060<br>
 </li>
 <li><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact>Contact &amp; Support</a></li>
 </ul>
 </div>
 </div>
 <div class=row>
 <div class="col-12 Footer-bottom">
 <div class="Footer-bottom-inner-div row">
 <div class=col>
 <ul class="Menu Menu--horizontal Menu--dividers u-mb-1">
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/about-ieee-xplore.html>About IEEE <em>Xplore</em></a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact>Contact Us</a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_start.html target=blank>Help</a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/accessibility-statement.html target=blank>Accessibility</a></li> 
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_Terms_of_Use.html target=_blank>Terms of Use</a></li>
 <li class=Menu-item><a href=http://www.ieee.org.thi.idm.oclc.org/web/aboutus/whatis/policies/p9-26.html>Nondiscrimination Policy</a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/sitemap.jsp>Sitemap</a></li>
 <li class=Menu-item><a href=http://www.ieee.org.thi.idm.oclc.org/about/help/security_privacy.html target=blank>Privacy &amp; Opting Out of Cookies</a></li>
 </ul>
 <p class=Footer-bottom-terms>
 A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.<br>© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
 </p>
 </div>
 <div><i class=logo-ieee-white></i></div>
 </div>
 
 </div>
 </div>
 </div>
 
 
</section>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<style>--></style>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div style=width:1263px id=popup_overlay></div>
<g:compress>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</g:compress>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 </div>
 </div>
 </div>
 
 
 
 </div>
 
<div id=cboxOverlay style=display:none></div><div id=colorbox role=dialog tabindex=-1 style=display:none></div><div class=usabilla_live_button_container id=usabilla_live_button_container_904426512 role=button tabindex=0 aria-label="Usabilla Feedback Button"><style nonce=e9930a118e08 class=sf-hidden>div.usabilla_live_button_container#usabilla_live_button_container_904426512[role="button"]{width:125px;height:50px;position:fixed;z-index:999;bottom:0;left:95%;margin-left:-62.5px}</style></div>