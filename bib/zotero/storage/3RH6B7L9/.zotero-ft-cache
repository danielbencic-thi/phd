Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)

SPAGAN: Shortest Path Graph Attention Network
Yiding Yang1 , Xinchao Wang1∗ , Mingli Song2 , Junsong Yuan3 and Dacheng Tao4 1Department of Computer Science, Stevens Institute of Technology 2College of Computer Science and Technology, Zhejiang University
3Department of Computer Science and Engineering, State University of New York at Buffalo 4UBTECH Sydney Artiﬁcal Intelligence Centre, University of Sydney
{yyang99, xwang135}@stevens.edu, brooksong@zju.edu.cn, jsyuan@buffalo.edu, dacheng.tao@sydney.edu.au.

Abstract
Graph convolutional networks (GCN) have recently demonstrated their potential in analyzing non-grid structure data that can be represented as graphs. The core idea is to encode the local topology of a graph, via convolutions, into the feature of a center node. In this paper, we propose a novel GCN model, which we term as Shortest Path Graph Attention Network (SPAGAN). Unlike conventional GCN models that carry out node-based attentions within each layer, the proposed SPAGAN conducts path-based attention that explicitly accounts for the inﬂuence of a sequence of nodes yielding the minimum cost, or shortest path, between the center node and its higher-order neighbors. SPAGAN therefore allows for a more informative and intact exploration of the graph structure and further a more effective aggregation of information from distant neighbors into the center node, as compared to node-based GCN methods. We test SPAGAN on the downstream classiﬁcation task on several standard datasets, and achieve performances superior to the state of the art.
1 Introduction
Convolutional neural networks (CNNs) have achieved a great success in many ﬁelds especially computer vision, where they can automatically learn both low- and high-level feature representations of objects within an image, thus beneﬁting the subsequent tasks like classiﬁcation and detection. The input to such CNNs are restricted to grid-like structures such as images, in which square-shaped ﬁlters can be applied.
However, there are many other types of data that do not take form of grid structures and instead represented using graphs. For example, in the case of social networks, each person is denoted as a node and each connection as an edge, and each node may be connected to a different number of edges. The features of such graphical data are therefore encoded in their topological structures, which cannot be extracted using conventional square or cubic ﬁlters tailored for handling gridstructured data.
∗Corresponding Author

Figure 1: Comparing neighbor attention and shortest path attention. Conventional approaches focus on aggregating information from ﬁrst-order neighbors within each layer, shown on the left, and often rely on stacking multiple layers to reach higher-order neighbors. The proposed SPAGAN shown on the right, on the other hand, explicitly conducts shortest-path-based attention that aggregates information from distant neighbors and explores the graph topology, all in a single shot.
To extract features and utilize the contextual information encoded in graphs, researchers have explored various possibilities. [Kipf and Welling, 2016] proposes a convolution operator deﬁned in the no-grid domain and build a graph convolutional network (GCN) based on this operator. In GCN, the feature of a center node will be updated by averaging the features of all its immediate neighbors using ﬁxed weights determined by the Laplacian matrix. This feature updating mechanism can be seen as a simpliﬁed polynomial ﬁltering of [Defferrard et al., 2016], which only considers ﬁrst-order neighbors. Instead of using all the immediate neighbors, GraphSAGE [Hamilton et al., 2017] suggests using only a fraction of them for the sake of computing complexity. A uniform sampling strategy is adopted to reconstruct graph and aggregate features. Recently, [Velicˇkovic´ et al., 2018] introduces an attention mechanism into the graph convolutional network, and proposes the graph attention network (GAT) by deﬁning an attention function between each pair of connected nodes.
All the above models, within a single layer, only look at immediate or ﬁrst-order neighboring nodes for aggregating the graph topological information and updating features of the center node. To account for indirect or higher-order neighbors, one can stack multiple layers so as to enlarge the size of receptive ﬁeld. However, results have demonstrated that, by doing so, the performances tend to drop dramatically [Kipf and Welling, 2016]. In fact, our experiments even demonstrate that simply stacking more layers using of the GAT model will often lead to the failure of convergence.
In this paper, we propose a novel and the ﬁrst dedi-

4099

Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
mates the path-based attentions for updating parameters. We test the proposed SPAGAN on several benchmarks for the downstream node classiﬁcation task, where SPAGAN consistently achieves results superior to the state of the art.

Figure 2: The overall training workﬂow of SPAGAN. Given a graph, for each center node we compute a set of shortest paths of varying lengths to its high-order neighbors denoted by P, and then extract their features as path features. Next, a shortest path attention mechanism is used to compute their attention coefﬁcients with respect to the center node. The features of each center node can therefore be updated according to the feature of paths and also their attention coefﬁcients to minimize the loss function. Afterwards, P will be regenerated according to these new attention coefﬁcients and used for the next training phase.
cated scheme, termed Shortest Path Graph Attention Network (SPAGAN), that allows us to, within a single layer, utilize path-based high-order attentions to explore the topological information of the graph and further update the features of the center node. At the heart of SPAGAN is a mechanism that ﬁnds the shortest paths between a center node and its higherorder neighbors, then computes a path-to-node attention for updating the node features and coefﬁcients, and iterates the two steps. In this way, each distant node inﬂuences the center node through a path connecting the two with minimum cost, providing a robust estimation and intact exploration of the graph structure.
We highlight in Fig. 1 the difference between SPAGAN and conventional neighbor-attention approaches. Conventional methods look at only immediate neighbors within a single layer. To propagate information from distant neighbors, multiple layers would have to be stacked, often yielding the aforementioned convergence or performance issues. By contrast, SPAGAN explicitly explores a sequence of high-order neighbors, achieved by shortest paths, to capture the more global graph topology into the path-to-node attetion mechanism.
The training workﬂow of SPAGAN is depicted in Fig. 2. For each center node, a set of shortest paths of different lengths denoted by P, are ﬁrst computed using the attention coefﬁcients of each pair of nodes that are all initialized to be the same. The features of P will then be generated. Afterwards, a path attention mechanism is applied to generate the new embedded feature for each node as well as the new attention coefﬁcients. The embedded features will be used for computing the loss, and the attention coefﬁcients, on the other hand, will be used to regenerate P for the next iteration.
Our contribution is therefore a novel high-order graph attention network, that explicitly conducts path-based attention within each layer, allowing for an effective and intact encoding of the graphical structure into the attention coefﬁcients and thus into the features of nodes. This is achieved by, during training, an iterative scheme that computes shortest paths between a center node and high-order neighbors, and esti-

2 Related Work

In this section, we review related methods on graph convolution and on graph attention mechanism, where the latter one can be considered as a variant of the former. The proposed SPAGAN model falls into the graph attention domain.

Graph Convolutional Networks. Due to the great success

of CNNs, many recent works focus on generating such frame-

work to make it possible to apply on non-grid data structure

[Gilmer et al., 2017; Monti et al., 2017a; Morris et al., 2018;

Velicˇkovic´ et al., 2018; Simonovsky and Komodakis, 2017;

Monti et al., 2017b; Li et al., 2018; Zhao et al., 2019;

Peng et al., 2018]. [Bruna et al., 2014] ﬁrst proposes a convo-

lution operator for graph data by deﬁning a function in spec-

tral domain. It ﬁrst calculates the eigenvalues and eigenvec-

tors of graph’s Laplacian matrix and then applies a function to

the eigenvalues. For each layer, the features of node will be a

different combination of weighted eigenvectors. [Defferrard

et al., 2016] deﬁnes polynomial ﬁlters on the eigenvalues of

the Laplacian matrix to reduce the computational complexity

by using the Chebyshev expansion. [Kipf and Welling, 2016]

further simpliﬁes the convolution operator by using only ﬁrst-

order neighbors. The adjacent matrix A is ﬁrst added self-

connections and normalized by the degree matrix D. Then,

graph convolution operation can be applied by just simple

matrix multiplication: H

=

D

−

1 2

AD−

1 2

H

Θ.

Under this

framework, the convolution operator is deﬁned by parameter

Θ which is not related to the graph structure. Instead of using

the full neighbors, GraphSAGE[Hamilton et al., 2017] uses a

ﬁxed-size set of neighbors for aggregation. This mechanism

makes the memory used and running time to be more pre-

dictable. [Abu-El-Haija et al., 2018] proposes a multi GCN

framework by using random walk. They generate multiple

adjacent matrices through random walk and feed them to the

GCN model.

Graph Attention Networks. Instead of using ﬁxed aggregation weights, [Velicˇkovic´ et al., 2018] brings attention mechanism into graph convolutional network and proposes GAT model based on that. One of the beneﬁts of attention is the ability to deal with input with variant sizes and make the model focus on parts which are most related to current tasks. In GAT model, the weight for aggregation is no longer equal or ﬁxed. Each edge in the adjacent matrix will be assigned with an attention coefﬁcient that represents how much one node affects the other one. The network is expected to learn to pay more attention to the important neighbors. [Zhang et al., 2018] proposes a gated attention aggregation mechanism which is based on GAT. They argue that not every attention head in GAT model is equally important for the feature embedding process. They propose a model that can learn to assign different weights to different attention head. [Liu et al., 2018] proposes an adaptive graph convolutional network by combining attention mechanism with LSTM model. They

4100

Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)

suggest using LSTM model can help to ﬁlter the information aggregated from neighbors of variant hops.
Difference to Existing Methods. Unlike all the methods described above, the proposed SPAGAN model explicitly conducts path-based high-order attention that explores the more global graph topology, within in one layer, to update the network parameters and features of the center nodes.

3 Preliminary
Before introducing our proposed method, we give a brief review of the ﬁrst-order attention mechanism.
Given a graph convolutional network, let h =
{h1, h2, h3, ..., hN } denotes a set of features of N nodes, where hi ∈ F with F being the feature dimension. Also, let A denotes the connection relationship, where Aij ∈ {0, 1} denotes whether there is an edge going from node i to node j. Typically, a linear transformation operation, W ∈ F ×F , is ﬁrst applied to the input features individually to transform them into a new space of F dimension. We write

(l)h j = lW (k)(l−1)hj ,

(1)

where lW (k) is deﬁned for each layer l and each attention head k, (l−1)hj is the feature of node j in (l − 1)-th layer, and
(l)h j is the projected feature for node j. Note that, layer zero represents the original feature of input.
A shared attention function is then applied to those pairs of connected nodes to obtain the attention coefﬁcients:

lαi(jk) =

exp(σ a, (l)h i (l)h j ) , exp(σ a, (l)h i (l)h j )

(2)

j∈Ni

where lαi(jk) represents the attention coefﬁcient from node j to node i, ·, · represents dot product of two vectors, Ni represents a set of 1-hop neighbors of node i, and σ can be any non-linear operation. Intuitively, the shared attention function controls how much the feature of one node affects that of the other node. By sharing the same vector a for all pairs, the number of parameters as well as the risk of over-ﬁtting can be reduced.
A forward process that updates the node features based on the obtained attentions, is then carried out:





 lhi = σ ΞKk=1 {

l αi(jk) (l−1) h

 j}

,

(3)



j∈Ni



where K is the number of attention head, Ξ denotes the operation that can be concatenation, max pooling, or mean pooling. For example, [Velicˇkovic´ et al., 2018] uses concatenation for the middle layers and mean pooling for the last layer.
The above forward process is totally differentiable and can be seen as one graph convolutional layer. Note that the above operation only considers one-hop neighborhoods within one layer. Although stacking multiple such layers can make farther nodes be accessible, it often leads to worse performance and in many cases convergence issues.

Figure 3: Aggregating feature for a center node using shortest path attention. For each center node d, shortest paths starting from it can be represented as a set of vectors with different sizes. These vectors will be mapped to the same dimension by the function φ. ∗ denotes the attention operator deﬁned in Eq. 8, which will take d as a query to conduct a hierarchical path aggregation.

4 Proposed Method
In this section, we will give more details of the Shortest Path Graph Attention Network (SPAGAN). The core idea of SPAGAN is to utilize shortest-path-based attention, which allows us to robustly and effectively explore the more global graph topology, to update the attention functions and the node features within one layer. The overall structure of SPAGAN is shown in Fig. 2 and can be optimized in an iterative manner. The network takes node features h and the shortest paths P to minimize the loss function and thus to update coefﬁcients and features, based on which the shortest paths are re-computed, and the process iterates.

4.1 Shortest Path Attention
We will discuss here how to aggregate the information of shortest paths to the center node, which we call shortest path attention. Different from graph attention network that only focuses on pairwise node attention deﬁned by the adjacent matrix, the proposed SPAGAN approach focuses on path-tonode attention. SPAGAN contains three steps, shortest path generation, path sampling, and hierarchical path aggregation, for which we give details as follows.

Shortest Path Generation

In the initial phase, the network takes the node features h and

the shortest paths P generated using the same edge weights

as input, to minimize the loss of a speciﬁc task, like cross

entropy loss for classiﬁcation. The attention function, once

trained, generates meaningful edge weights based on the

learned attention coefﬁcients. Shortest paths are then computed using Dijkstra’s algorithm [Dijkstra, 1959], where the

weights of edges are reversed ﬁrst and then transformed to

positive values using the approach of [Suurballe, 1974]. Note

that the paths here do not include the start node.

To get a robust representation of the weight of an edge, we

use the ﬁnal layer attention coefﬁcients and average those of

all attentions:

1 Wij = K

K

¯l αi(jk) ,

(4)

k=1

where ¯l denotes the last layer of network, K denotes the num-

ber of attention heads for that layer, αi(jk) denotes the attention coefﬁcient from node j to node i in k-th head, and Wij corresponds to the weight of the edge from node i to node j.

4101

Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)

Let pcij denotes the obtained shortest path from node i to node j with length c, and let P denote the set of all such paths. Since a path with length c will allow the center node to access nodes up to c-hop, we can therefore control the size of
receptive ﬁeld for a single layer by deﬁning the max value of c. We will give analysis about c in the experimental section. Also, we add the center nodes themselves to P.

Path Sampling
For shortest paths of the same length, those with smaller costs are heuristically more correlated with the center node, and vice versa. To highlight the contributions of the more correlated paths and meanwhile reduce the computational load, for a ﬁxed length c, we sample the top k paths with the lowest costs. We denote the set of all the sampled paths of length c centerd node i as ℵci . We write

ℵci = topk(Pc), k = degi ∗ r,

(5)

where Pc is the subset of P that contains all shortest paths of

length c, and k is determined by the degree of the center node,

because we want to make the embedded features from paths

of different lengths comparable. degi represents the degree of node i, and r is a hyper-parameter that controls the ratio
of the degree of the center node over the number of sampled

paths. We will also give analysis about this hyper-parameter in the experimental section.

Hierarchical Path Aggregation
Path aggregation is the core of our model. We expect the shortest paths of different lengths to encode richer topological information as compared to the ﬁrst-order neighbors do. To this end, we propose a hierarchical path aggregation mechanism that focuses on the paths of same length in the ﬁrst level (Eq. 6) and on different ones in the second level (Eq. 9).
The hierarchical path aggregation mechanism is shown in Fig. 3. In the ﬁrst level, given a center node i and its shortest path set ℵi, we aggregate features respect to each c. For the sake of simplicity, we omit l that represents layer in the following discussion. We write

c i

=

ΞKk=1

αi(jk)φ(pcij ) ,

(6)

pcij ∈ℵci

where ℵci is a set of shortest paths starting from node i with

length of c,

c i

is

the

aggregated

feature

for

node

i

respect

to ℵci , K is the number of path attention heads that is the

same for all c, Ξ can be concatenation, max pooling or mean

pooling operation. In our implementation, we use Ξ as a con-

catenation operation for all middle layers and mean pooling

operation for the ﬁnal layer. φ maps paths with variant sizes

to ﬁxed ones. In our implementation, mean pooling, an order-

invariant operator, is used for φ that computes the average of

all nodes’ features in a path. αi(jk) is the attention coefﬁcient between node i and path pcij, and is taken to be

αi(jk) = AT h i, φ(pcij )|aα ,

(7)

where aα is the parameter that deﬁnes the attention function AT , and h i donates the feature of node i after the linear

transformation. Note that when we set c to 2, the generated attention coefﬁcients will be equal to the node attention that can be used to update edges’ weights.
The attention function AT in Eq. 7 is taken to be

exp(σ θ, a b )

AT (a, b|θ) =

,

(8)

exp(σ θ, a b )

b∈ℵa

where θ is its parameter, ℵa denotes a set deﬁned on a, || represents concatenation. In the case of Eq. 7 or ﬁrst level aggregation, ℵa denotes ℵci , while in the case of Eq. 10 to be discussed below or second level aggregation, ℵa denotes a set of all i. The output of this function estimates the attention between a and b.
In the second level, we focus on aggregating features of
paths with different lengths and applying the attention mech-
anism to obtain the embedded features for the center node:

C

hi = σ

βc

c i

,

(9)

c=2

where

c i

is

the

aggregated

feature

for

node

i

from

paths

of

length c. C is the maximum-allowable path length and σ can

be any no-linear function. βc is the attention coefﬁcient for

c i

and

can

be

obtained

from

the

same

attention

mechanism

with respect to the center node i using an attention function

deﬁned by aβ:

βc = AT

h i,

c i

|aβ

.

(10)

Iterative Optimization The whole network then will be trained in an iterative manner that involves shortest paths P and all other parameters of the network. In the beginning, the network is trained based on h and P, where P is generated using equal edge weights. When the network converges, P will be regenerated according to the attention coefﬁcients in the ﬁnal layer for the next iteration. In practice, we ﬁnd two iterations is sufﬁcient to get a good performance, which we will give more discussions in the experimental section.

4.2 Discussion
Here, we will discuss some properties of SPAGAN.
Generalized GAT. SPAGAN can be seen as a generalized GAT, as it degenerates to GAT when we set the max value of c to 2 and the sample ratio r to 1.0.
Order Invariance. Order invariance refers to property that, the embedded feature for a node is independent of the order of its neighbors. It is thus a crucial property since we expect the feature for each node to depend only on the structure of the graph but not the order of the nodes. All our operations are based on pair-wise attention, which is order invariant.
Node Dependency. Node dependency refers to that for each node, the aggregation process is dependent on its local structure, so that every node will have its unique aggregation pattern. In SPAGAN, shortest paths starting from center node are expected to explore more global graph structure than ﬁrst-order mechanisms, which makes the feature aggregation process more node dependent.

4102

Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)

Relation to Random Walk. The random walk approach [Kashima et al., 2003] assumes that each node can transmit to its neighbors in equal probability that can be represent as the product of adjacent matrices. A k step random walk leads to a matrix of Ak. It suffers from tottering caused by walking through the same edge or node thus may get stuck at a local structure. Shortest path, by nature, can alleviate such tottering problem [Borgwardt and Kriegel, 2005].
5 Experimental Setup
In this section, we give the details of the datasets used for validation, the experimental setup, and the compared methods.
5.1 Experimental Datasets
We use three widely used semi-supervised graph datasets, Cora, Citeseer and Pubmed summarized in Tab. 1. Following the work of [Kipf and Welling, 2016; Velicˇkovic´ et al., 2018], for each dataset, we only use 20 nodes per class for training, 500 nodes for validating and 1000 nodes for testing. We allow all the algorithms to access the whole graph that includes the node features and edges. As done in [Kipf and Welling, 2016], during training, only part of the nodes are provided with ground truth labels; at test time, algorithm will be evaluated by their performance on the test nodes.
5.2 Experimental Setup
We implement SPAGAN under Pytorch framework [Paszke et al., 2017] and train it with Adam optimizer. We follow a similar learning rate and L2 regularization as GAT [Velicˇkovic´ et al., 2018]: For the Cora dataset, we set the learning rate to 0.005 and the weight of L2 regularization to 0.0005; for the Pubmed dataset, we set the learning rate to 0.01 and the weight of L2 regularization to 0.001. For the Citeseer dataset, however, we found that the original setting of parameters under the Pytorch framework yields to worse performances as compared to those reported in the GAT paper. Therefore, we set the learning rate to 0.0085 and the weight of L2 regularization to 0.002 to match the original GAT performance. For all datasets, we set a tolerance window and stop the training process if there is no lower validation loss within it. We use two graph convolutional layers for all datasets with different attention heads. For the ﬁrst layer, 8 attention heads for each c is used. Each attention head will compute 8 features. Then, an ELU [Clevert et al., 2015] function is applied. In the second layer, we use 8 attention heads for the Pumbed dataset and 1 attention head for the other two datasets. The outputs of second layer will be used for classiﬁcation which have the same dimension as the class number. Dropout is applied to the input of each layer and also to the attention coefﬁcients for each node, with a keep probability of 0.4.
For all datasets, we set the r to 1.0 which means the number of sampled paths is the same as the degree of each node. The max value of c is set to be three for the ﬁrst layer and two for the last layer for all datasets. The steps of iteration is set to two. For all the datasets, we use early stopping based on the cross-entropy loss on validation set. We run the model under the same data split for 10 times and report the mean accuracy as well as standard deviation. Experiments on the sensitivities of these three hyper-parameters will be provided.

Cora Citeseer Pubmed

#Nodes #Edges #Features #Classes

2,708 5,429 1,433
7

3,327 4,732 3,703
6

19,717 44,338
500 3

Table 1: Summary of the datasets used in our experiments. For each dataset, we use only 20 nodes per class for training, 500 nodes for validation, and 1000 nodes for testing.

Method

Cora

Citeseer

Pubmed

MLP DeepWalk Chebyshev GeniePath GCN MoNet GAT GAT3

55.1% 67.2% 81.2%
81.5% 81.7±0.5% 83.0±0.7% 81.2±0.6%

46.5% 43.2% 69.8%
70.3%
72.5±0.7% 68.8±0.7%

71.4% 65.3% 74.4% 78.5% 79.0% 78.8±0.3% 79.0±0.3% 78.5±0.4%

SPAGAN 83.6±0.5% 73.0±0.4% 79.6±0.4%

Table 2: Summary of results in terms of classiﬁcation accuracy. GAT3 denotes the three-layer GAT model with skip connections
(GAT model fails to converge when we simply stack three layers
without skip connections).

5.3 Comparison Methods
We compare our method with several state-of-the-art baselines, including MLP, DeepWalk [Perozzi et al., 2014], Chebyshev [Defferrard et al., 2016], GeniePath [Liu et al., 2018], GCN, MoNet [Monti et al., 2017a] and GAT. MLP only uses the node features. DeepWalk is a random walk base method. Chebyshev is a spectral method. GeniePath uses LSTM model to aggregate features from different layers on a GAT model. GCN can be seen as a special case of GAT model where αij is the same for each pair of nodes. MoNet is a ﬁrst-order spatial method, which uses Gaussian kernel to learn the weight for each neighbors. GAT is a ﬁrst-order attention framework which can be seen as a special case of SPAGAN when we set the max length of paths to 2 and the sampling ratio to 1.0. We will see the usefulness of path attention when compare to the naive ﬁrst-order attention model. We also compare SPAGAN with GAT model with the same size of receptive ﬁeld by increasing the number of layers by one. We add skip connections for each layer before ELU activation in three layers GAT model to make it converge.
6 Experimental Result
As we focus on node classiﬁcation, we use classiﬁcation accuracy to evaluate all methods. The results of our comparative experiments are summarized in Tab. 2, where we observe SPAGAN consistently achieves the best performance on all three datasets. This implies that SPAGAN, by exploring highorder neighbors and graph structure within one layer, indeed leads to more discriminant node features.
In addition, we provide a visualization on the learned embedded features as well as the shortest paths. The features

4103

Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)

(a) Results with different sampling ratios.

r

0.5

1.0

1.5

2.0

Acc 83.4±0.3% 83.6±0.5% 83.4±0.6% 83.3±0.5%

(b) Results with different steps of iterations.

I ter

1

2

3

4

5

Acc 83.0±0.7% 83.6±0.5% 83.6±0.6% 83.6±0.6% 83.5±0.6%

(c) Results with different lengths of paths.

C

2

3

4

5

Acc 83.0±0.7% 83.6±0.5% 83.5±0.7% 83.1±0.4%

Table 3: Sensitivity analysis of hyper-parameters. All these results are obtained on the Cora dataset.

Method GAT SPAGAN Fix SPAGAN

Cora 83.0±0.7% 83.2±0.4% 83.6±0.5%

Citeseer 72.5±0.7% 72.4±0.4% 73.0±0.4%

Pubmed 79.0±0.3% 79.2±0.4% 79.6±0.4%

Table 4: Classiﬁcation accuracy of SPAGAN with and without the hierarchical path aggregation. SPAGAN Fix refers to the model where we average the features coming from paths with different lengths (i.e., removing β from Eq. 9).

obtained from the hidden layers of a trained model on the Cora dataset are projected using t-SNE algorithm [Maaten and Hinton, 2008]. As shown in Fig. 4, we randomly sample 10 nodes and visualize four paths per node. Nodes with black surroundings indicate the starts of paths. For the correctly classiﬁed nodes, the paths are colored with cyan, otherwise the paths are colored with red. As can be observed, the shortest paths of most correctly classiﬁed nodes tend to reside within the same cluster. Another interesting observation is that, for nodes that lie in the wrong cluster, shortest paths can still lead them back to the right cluster and further produce the correct classiﬁcation results. For example, in the zoomed region, the node surrounded by the red dotted circle lies in blue cluster. However, since it is connected to shortest paths leading back to the green cluster, our network eventually predicts the correct label, green, to this node.
6.1 Sensitivity Analysis
There are three hyper-parameters in our model: the sampling rate of path r, the maximum depth of path C, and the number of iteration Iter. We change each of them to analyze the sensitive of our model respect to them. The results are shown in Tab. 3. All these results are obtained from the Cora dataset. For the sampling ratio, we ﬁx Iter to be 2 and C to 3 (for sampling ratio 0.5, we adopt a max operator over the number of sampled paths and one, to ensure that at least one path is selected); the best performance is obtained in 1.0, meaning that we use the same number of paths as the degree of each node. Then we ﬁx the sampling ratio to 1.0 and change the number of iterations. Results show that two iterations is sufﬁcient for a good performance. We also change the max depth of paths from 2 to 5. The best performance is obtained at 3.
6.2 Ablation Study
We conduct ablation study to validate the proposed hierarchical path attention mechanism described by Eq. 10. We compare the results between full SPAGAN model that has learnable attention coefﬁcients for paths of different lengths, and SPAGAN model without it, which we call SPAGAN Fix. The results are shown in Tab. 4, where the full model of SPAGAN improvement over. We can thus conclude that hierarchical attention mechanism indeed plays an important role in the feature aggregation process.

Figure 4: Visualization of features obtained from hidden layer of a trained SPAGAN model on the Cora dataset and paths starting from 10 random nodes (with black background). Color of nodes represents their labels. Paths starting from correctly classiﬁed nodes are colored with cyan, otherwise with red.
6.3 Computing Complexity
Many real-world graphs are sparse. For example, the average degree of the Cora dataset is only 2. A sparse graph can be represented by values and indexes in O(E) space complexity where E is the number of edges in the graph. The computed path matrix P is cutoff by C and further reduced by sampling. In our experiments, the space complexity of path attention layer is one or two times larger than that of the ﬁrst-order one, which is very friendly for GPU memory. Take Cora dataset as an example, GAT model takes about 600M GPU memory while our SPAGAN model only increases it by about 200M.
As for the running time, the shortest path algorithm we adopted, Dijkstra, runs at a time complexity of O(V ElogV ) where V is the number of nodes in a graph. The path attention mechanism is implemented using sparse operator [Fey et al., 2018] according to indices and values of P, which allows us to make full use of the GPU computing ability. The running time of one epoch with path attention on Pubmed dataset is 0.1s on a Nvidia 1080Ti GPU.
7 Conclusion
We propose in this paper an effective and the ﬁrst dedicated graph attention network, termed Shortest Path Graph Attention Network (SPAGAN), that allows us to explore high-order path-based attentions within one layer. This is achieved by computing the shortest paths of different lengths between a center node and its distant neighbors, then carrying path-tonode attention for updating the node features and attention coefﬁcients, and iterates. We test SPAGAN on several benchmarks, and demonstrate that it yields results superior to the state of the art on the downstream node-classiﬁcation task.

4104

Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)

References
[Abu-El-Haija et al., 2018] Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-gcn: Multi-scale graph convolution for semi-supervised node classiﬁcation. arXiv preprint arXiv:1802.08888, 2018.
[Borgwardt and Kriegel, 2005] Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In IEEE international conference on data mining, 2005.
[Bruna et al., 2014] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun. Spectral networks and locally connected networks on graphs. In International Conference on Learning Representations, 2014.
[Clevert et al., 2015] Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
[Defferrard et al., 2016] Michae¨l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing Systems, pages 3844–3852, 2016.
[Dijkstra, 1959] Edsger W Dijkstra. A note on two problems in connexion with graphs. Numerische mathematik, 1(1):269–271, 1959.
[Fey et al., 2018] Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich Mu¨ller. SplineCNN: Fast geometric deep learning with continuous B-spline kernels. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.
[Gilmer et al., 2017] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263– 1272. JMLR. org, 2017.
[Hamilton et al., 2017] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024–1034, 2017.
[Kashima et al., 2003] Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs. In International conference on machine learning, pages 321–328, 2003.
[Kipf and Welling, 2016] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.
[Li et al., 2018] Ruoyu Li, Sheng Wang, Feiyun Zhu, and Junzhou Huang. Adaptive graph convolutional neural networks. In AAAI Conference on Artiﬁcial Intelligence, 2018.
[Liu et al., 2018] Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan Qi. Geniepath: Graph neural networks with adaptive receptive paths. arXiv preprint arXiv:1802.00910, 2018.

[Maaten and Hinton, 2008] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–2605, 2008.
[Monti et al., 2017a] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In IEEE Conference on Computer Vision and Pattern Recognition, volume 1, page 3, 2017.
[Monti et al., 2017b] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5115–5124, 2017.
[Morris et al., 2018] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. arXiv preprint arXiv:1810.02244, 2018.
[Paszke et al., 2017] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
[Peng et al., 2018] Hao Peng, Jianxin Li, Qiran Gong, Yuanxing Ning, and Lihong Wang. Graph convolutional neural networks via motif-based attention. arXiv preprint arXiv:1811.08270, 2018.
[Perozzi et al., 2014] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In ACM SIGKDD international conference on Knowledge discovery and data mining, 2014.
[Simonovsky and Komodakis, 2017] Martin Simonovsky and Nikos Komodakis. Dynamic edge conditioned ﬁlters in convolutional neural networks on graphs. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[Suurballe, 1974] JW Suurballe. Disjoint paths in a network. Networks, 4(2):125–145, 1974.
[Velicˇkovic´ et al., 2018] Petar Velicˇkovic´, William Fedus, William L Hamilton, Pietro Lio`, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.
[Velicˇkovic´ et al., 2018] Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio`, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018.
[Zhang et al., 2018] Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint arXiv:1803.07294, 2018.
[Zhao et al., 2019] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dimitris N Metaxas. Semantic graph convolutional networks for 3d human pose regression. arXiv preprint arXiv:1904.03345, 2019.

4105

