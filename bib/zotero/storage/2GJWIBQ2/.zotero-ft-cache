IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Journals & Magazines > IEEE Access > Volume: 10
Quality-Oriented Hybrid Path Planning Based on A* and Q-Learning for Unmanned Aerial Vehicle
Publisher: IEEE
Cite This
PDF
Dongcheng Li ; Wangping Yin ; W. Eric Wong ; Mingyong Jian ; Matthew Chau
All Authors
327
Full
Text Views
Open Access
Comment(s)

    Alerts

Under a Creative Commons License
Abstract
Document Sections

    I.
    Introduction
    II.
    Modeling for UAV Path Planning
    III.
    Design of UAV Global-Local Path Planning Algorithm
    IV.
    Simulation Experiments
    V.
    Conclusion

Authors
Figures
References
Keywords
Metrics
Abstract:
Unmanned aerial vehicles (UAVs) are playing an increasingly important role in people’s daily lives due to their low cost of operation, low requirements for ground support, high maneuverability, high environmental adaptability, and high safety. Yet UAV path planning under various safety risks, such as crash and collision, is not an easy task, due to the complicated and dynamic nature of path environments. Therefore, developing an efficient and flexible algorithm for UAV path planning has become inevitable. Aimed at quality-oriented UAV path planning, this paper is designed to analyze UAV path planning from two aspects: global static planning and local dynamic hierarchical planning. Through a theoretical and mathematical approach, a three-dimensional UAV path planning model was established. Based on the A* algorithm, the search strategy, the step size, and the cost function were improved, and the OPEN set was simplified, thereby shortening the planning time and greatly improving the execution efficiency of the algorithm. Moreover, a dynamic exploration factor was added to the exploration mechanism of Q-learning to solve the exploration-exploitation dilemma of Q-learning to adapt to the local dynamic path adjustment for UAVs. The global-local hybrid UAV path planning algorithm was formed by combining the two. The simulation results indicate that the proposed planning model and algorithm can efficiently solve the problem of UAV path planning, improve the path quality, and can be a significant reference for solving other problems related to path planning, such as the reliability, security, and safety of UAV, when embedded into the heuristic function of the proposed algorithm.
Society Section: IEEE Reliability Society Section
Published in: IEEE Access ( Volume: 10 )
Page(s): 7664 - 7674
Date of Publication: 30 December 2021
Electronic ISSN: 2169-3536
INSPEC Accession Number: 21456260
DOI: 10.1109/ACCESS.2021.3139534
Publisher: IEEE
Funding Agency:
Quality-Oriented Global-local hybrid UAV path planning based on A Star and Q-Learning.
Quality-Oriented Global-local hybrid UAV path planning based on A Star and Q-Learning.
Hide Full Abstract
CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.
SECTION I.
Introduction

Unmanned aerial vehicles (UAVs) are aircraft that can be controlled by a ground station or via onboard electronic equipment and can be fully or partially autonomous. With high maneuverability and good concealment, UAVs are increasingly vital in people’s daily lives. At present, the typical uses of UAVs include surveillance, rescue, delivery, communication relay, and airborne early warning [1] .

According to whether the obstacle information is known, UAV path planning can be divided into two categories: static planning, in which the locations of all obstacles and threats are known before planning and whereby a reasonable path can be planned before UAV take-off [2] ; and dynamic planning, in which the UAV needs to deal with uncertain obstacles or unexpected threats by dynamically resolving the conflicts. Dynamic path planning involves more complex issues and can improve UAV flight efficiency [3] .

The existing methods for path planning can be divided into numerical optimization, potential field-based method, heuristics (classical heuristics and group intelligence algorithms), sampling-based method, and deep reinforcement learning [4] – [5] [6] . Table 1 illustrates the characteristics of all the above mentioned algorithms.
TABLE 1 Path Planning Methods
Table 1- Path Planning Methods

Given the status quo, UAV path planning mainly faces the following key technical challenges:

    Smart algorithms commonly used for UAV path planning often take a long time due to their high complexity [7] . For this reason, these algorithms are time-consuming and thus difficult to implement when solving large-scale schemes of UAV path planning.

    UAV path planning is subject to many constraints in practice. Restricted by the high complexity and time-consuming process of the model, the existing algorithms can be used only for experimental research based on simplified models [8] . Thus, this approach cannot truly reflect the actual needs of UAV path planning.

In this paper, through systematic investigation of the problems in UAV path planning, a global-local UAV path planning algorithm based on reinforcement learning was designed according to the simulated scenarios. On combining the constraints and targets, UAV path planning was achieved, and related experimental verification was performed.

The preceding sections of the paper are arranged as follows. Section 2 introduces the design and modeling considerations of UAV path planning. Section 3 presents the proposed solution and algorithmic approach for UAV global-local path planning. The experimental design and result analysis is explained in Section 4 . Finally, we conclude the paper and elaborate on potential future directions in Section 5 .
SECTION II.
Modeling for UAV Path Planning
A. Problem Description

The UAV path planning problem can be described as follows: There are N UAVs, M types of loads, and K static or dynamic obstacles, and the UAVs can perform tasks such as surveillance, rescue, and delivery. To satisfy the UAV kinematic constraints and various resource constraints, one should choose a path that passes through all target task nodes and can dynamically adjust in real time when encountering obstacles to shorten the task duration and improve the success rate. In this paper, the task nodes were abstracted as the path nodes on the UAV flight path, and the final planned result was a sequence of path nodes.

The path planning schemes are shown in Figure 1 .
FIGURE 1. - Global-local hybrid path planning scheme.
FIGURE 1.

Global-local hybrid path planning scheme.

Show All

For UAV path planning, the scheme designed in this paper consists of two levels: the global path planning based on modified A* and the local dynamic path planning based on modified Q-learning.
B. Constraints

1) Vertical maximum turning angle constraint

Let the present path node be P i ( x i , y i , z i ) and the subsequent one be P i + 1 ( x i + 1 , y i + 1 , z i + 1 ) . Then the maximum turning angle constraint in the vertical direction can be expressed as
tan − 1 ⎛ ⎝ ⎜ | z i + 1 − z i | ( x i + 1 − x i ) 2 + ( y i + 1 − y i ) 2 − − − − − − − − − − − − − − − − − − − − − √ ⎞ ⎠ ⎟ ≤ θ v , ( i = 1 , 2 , … , n ) (1)
View Source Right-click on figure for MathML and additional features. \begin{align*}& \tan ^{-1}\left ({\frac {\left |{ z_{i+1}-z_{i} }\right |}{\sqrt {\left ({x_{i+1}-x_{i} }\right)^{2}+\left ({y_{i+1}-y_{i} }\right)^{2}}} }\right)\le \theta _{v}, \\& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \left ({i=1,2,\ldots,n }\right)\tag{1}\end{align*} where θ v denotes the maximum turning angle of the UAV in the vertical direction.

2) Horizontal maximum turning angle constraint

Let the present path node be P i ( x i , y i , z i ) and the subsequent one be P i + 1 ( x i + 1 , y i + 1 , z i + 1 ) . Then the maximum turning angle constraint in the horizontal direction can be expressed as
tan − 1 ( ( y i + 1 − y i ) ( x i + 1 − x i ) ) ≤ θ l , ( i = 1 , 2 , … , n ) (2)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \tan ^{-1}\left ({\frac {\left ({y_{i+1}-y_{i} }\right)}{\left ({x_{i+1}-x_{i} }\right)} }\right)\le \theta _{l},\quad \left ({i=1,2,\ldots,n }\right)\tag{2}\end{equation*} where θ l denotes the maximum turning angle of the UAV in the horizontal direction.

3) Horizontal flight speed constraint

The horizontal flight speed constraint of the UAV can be expressed as
V l m i n ≤ V i ≤ V l m a x (3)
View Source Right-click on figure for MathML and additional features. \begin{equation*} V_{lmin}\le V_{i}\le V_{lmax}\tag{3}\end{equation*} where V l m i n denotes the minimum horizontal flight speed (excluding the starting phase) of the UAV, V i denotes the current horizontal flight speed of the UAV, and V l m a x denotes the maximum horizontal flight speed of the UAV.

4) Climbing speed constraint

The climbing speed constraint of the UAV can be expressed as
0 ≤ V i ≤ V v m a x (4)
View Source Right-click on figure for MathML and additional features. \begin{equation*} 0\le V_{i}\le V_{vmax}\tag{4}\end{equation*} where V i denotes the current climbing speed of the UAV and V v m a x denotes the maximum climbing speed of the UAV.

5) Minimum turning radius constraint

The minimum turning radius constraint can be expressed as
R i ≥ R m a x , ( i = 1 , 2 , … , n ) (5)
View Source Right-click on figure for MathML and additional features. \begin{equation*} R_{i}\ge R_{max},\quad \left ({i=1,2,\ldots,n }\right)\tag{5}\end{equation*} where R i denotes the turning radius at the i -th turn in the path planning result and R m a x denotes the maximum turning radius of the UAV. R m i n is calculated as
R m i n = V 2 m i n g × n 2 y m a x − 1 − − − − − − − − √ (6)
View Source Right-click on figure for MathML and additional features. \begin{equation*} R_{min}=\frac {V_{min}^{2}}{g\times \sqrt {n_{ymax}^{2}-1}}\tag{6}\end{equation*} where V 2 m i n denotes the minimum flight speed of the UAV and n 2 y m a x denotes the maximum normal phase overload of the UAV.

6) Farthest flight length constraint (maximum flight time constraint)

The farthest flight length constraint (the maximum flight time constraint) can be expressed as
Δ V ∗ T i ≤ L m a x ( T i < T m a x ) (7)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \Delta _{V}\ast T_{i}\le L_{max}\left ({T_{i}{< T}_{max} }\right)\tag{7}\end{equation*} where Δ V denotes the average speed of the UAV during flight, T i denotes the flight time, T m a x denotes the maximum flight time, and L m a x denotes the maximum allowable flight length.

7) Flight height constraint

The flight height constraint can be expressed as
H m i n ≤ H i ≤ H m a x , ( i = 1 , … , n ) (8)
View Source Right-click on figure for MathML and additional features. \begin{equation*} H_{min}\le H_{i}\le H_{max},\quad \left ({i=1,\ldots,n }\right)\tag{8}\end{equation*} where H m i n denotes the minimum flight height (excluding the take-off and landing phases) of the UAV, H i denotes the current UAV flight height, and H m a x denotes the maximum flight height of the UAV.

8) The total energy consumption of any UAV executing tasks shall not exceed its total energy and can be expressed as
E i ≤ E , ( i = 1 , 2 , … , n ) (9)
View Source Right-click on figure for MathML and additional features. \begin{equation*} E_{i}\le \textrm {E},\quad \left ({i=1,2,\ldots,n }\right)\tag{9}\end{equation*} where E i denotes the energy consumption of the i -th UAV and E denotes the total energy.

C. Cost Function
1) Energy Consumption Cost

The energy consumption cost of the UAV can be expressed as
C o s t e = ∑ N − 1 i = 1 k ∗ l i ( i ≥ 2 ) (10)
View Source Right-click on figure for MathML and additional features. \begin{equation*} {Cost}_{e}=\sum \nolimits _{i=1}^{N-1} {k\ast l_{i}} \left ({i\ge 2 }\right)\tag{10}\end{equation*} where C o s t e denotes the energy consumption cost, k denotes the ratio of energy consumption to flight length, N denotes the number of nodes in the resultant path of UAV path planning, and l i denotes the distance between the i -th node and the i + 1 node.

2) Threat Area Cost

There are two radii for the threats: One is the detection radius r 0 , and the other is the reaction radius r 1 . Then the UAV threat area cost can be expressed as
C o s t t ( x ) = ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ 0 , k 0 x − r 1 r 0 − r 1 , k 1 x r 1 , x > r 0 r 0 ≥ x ≥ r 1 x ≤ r 1 (11)
View Source Right-click on figure for MathML and additional features. \begin{align*} {Cost}_{t}\left ({x }\right)= \begin{cases} 0, & x>r_{0} \\[0.6pc] k_{0}\frac {x-r_{1}}{r_{0}-r_{1}}, & r_{0}\ge x\ge r_{1} \\[0.6pc] k_{1}\frac {x}{r_{1}}, & x\le r_{1} \\ \end{cases}\tag{11}\end{align*} where x denotes the distance between the UAV and the threat and C o s t t ( x ) denotes the threat cost. The surrounding area of the threat is divided according to the distance from the threat. That is, the closer, the more dangerous; the farther, the safer. However, according to the principle of high risk and high return, the closer the path to the threat area, the smaller the cost in the path.

3) Final Target Function

m i n : δ 1 C o s t e + δ 2 C o s t t (12)
View Source Right-click on figure for MathML and additional features. \begin{equation*} min:{\delta _{1}Cost}_{e}+{\delta _{2}Cost}_{t}\tag{12}\end{equation*} where C o s t e denotes the energy consumption cost of UAV path planning mentioned above, which is positively related to the length of the path planned; C o s t t denotes the threat area cost of the enemy threat environment to the UAV; δ 1 denotes the weight coefficient of the energy consumption cost; and δ 2 denotes the weight coefficient of the threat area cost. The target is to minimize the sum of these two cost functions.

SECTION III.
Design of UAV Global-Local Path Planning Algorithm
A. Global Path Planning Based on Modified A*
1) A* Algorithm

The greatest difference between the A* algorithm and other path planning algorithms is the composition of the heuristic function [9] . Let f ( n ) be the heuristic function of the A* algorithm, as shown in Equation 13 :
f ( n ) = g ( n ) + h ( n ) (13)
View Source Right-click on figure for MathML and additional features. \begin{equation*} f(n)=g(n)+h(n)\tag{13}\end{equation*} where n denotes the current node, g ( n ) denotes the actual cost value from the starting point to the current point n , and h ( n ) denotes the estimated cost value from the current node n to the end point. The design of the cost function in the heuristic function of the A* algorithm is directly related to the search performance of the A* algorithm.

2) Dynamic Weight Adjustment Based on Q-Learning

Because the actual cost information g ( n ) is not considered in the A* algorithm cost function, the path cost of the final planning result will not be the global minimum. In this paper, a cost function of dynamic weight adjustment based on Q-learning was proposed and can be calculated as
f ( n ) = g ( n ) + α h ( n ) (14)
View Source Right-click on figure for MathML and additional features. \begin{equation*} f(n)=g(n)+\alpha h(n)\tag{14}\end{equation*} where α is dynamically adjusted by the Q-learning algorithm to reduce the weight of the estimated cost and increase the weight of the actual cost, which ensures that the algorithm can obtain the path with the minimum comprehensive cost. Moreover, the guiding role of the heuristic factor is retained, which does not slow down the search speed for the path considerably and ensures planning efficiency.

The flow of the global path planning algorithm based on modified A* is shown in Figure 2 .
FIGURE 2. - Flow chart of modified A*.
FIGURE 2.

Flow chart of modified A*.

Show All

The pseudo code of modified A* is as follows:
Algorithm 1: Modified A*

search area, task node

path node

Q-learning elements, UAV information, constraint information, OPEN table (two-way), CLOSED table (two-way)

While forward search and reverse search have not met do

Place the forward search node into the forward open table

Place the reverse search node into the reverse open table

Place the node with the minimum cost from the forward open table into the forward closed table, delete it from the forward open table, and set the corresponding parent-child relationship

Place the node with the minimum cost from the reverse open table into the reverse closed table, delete it from the reverse open table, and set the corresponding parent-child relationship

if (forward and reverse search have not met) then {

continue;

} else {

break;

}

End While

B. Local Dynamic Path Planning Based on Q-Learning With Modified Exploration Mechanism
1) Three-Element Design of Modified Q-Learning Algorithm

    State space

The state space of modified Q-learning dynamically determines the state according to the UAV path meshing range.

    Action space

According to the Q-value table of the Q-learning algorithm, the corresponding action is selected to increase or decrease in the corresponding state to obtain the state of the next phase. The action space is defined as follows:
A = { a 1 , a 2 , … , a 17 } (15)
View Source Right-click on figure for MathML and additional features. \begin{equation*} A=\left \{{a_{1},a_{2},\ldots,a_{17} }\right \}\tag{15}\end{equation*}

    Reward mechanism

A reward function that meets the actual application scenario for the agent is designed by analyzing the state after the agent chooses an action. The reward function is calculated as follows:
r ( i ) = ⎧ ⎩ ⎨ C 1 , − C 1 , 0 , S i = S e d a b s = 0 o t h e r   s c e n a r i o (16)
View Source Right-click on figure for MathML and additional features. \begin{align*} r\left ({i }\right)= \begin{cases} C1,& S_{i}=S_{e} \\ -C1,& d_{abs}=0 \\ 0,& other~scenario \\ \end{cases}\tag{16}\end{align*} where i denotes the current iteration steps of the algorithm, r ( i ) denotes the reward function, S i denotes the current state of the agent, and S e denotes the target state. The distance between the agent and the nearest obstacle is denoted by d a b s . C1 is a constant that denotes the reward value obtained after the agent interacts with the environment. Such settings of reward function can be too simple. In most cases, the agent cannot get feedback from the environment and lacks key guidance. For this reason, the time it takes to complete tasks is greatly increased, and energy consumption also increases accordingly.

A new reward function was designed in this paper to solve the abovementioned problems, which categorizes the state–action pair of the agent and returns different reward values for different scenarios. The new reward function adds scenarios: close to the target position and far away from the target position. If the decision made by the agent keeps it away from the target position and no collision occurs, then a small negative feedback value is given to the agent; if the decision made by the agent brings it close to the target position and no collision occurs, then a small positive feedback value is given to the agent. The two newly added scenarios take into account the distance between the agent and the obstacles to dynamically set the reward and punishment function. The modified reward function is calculated as follows:
r ( i ) = ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ C 1 , − C 1 , C 2 ∗ d i D , − C 2 ∗ d i D , − C 3 ∗ d i D , S i = S e d a b s = 0 d i < d i − 1 ,   d a b s ≠ 0 d i > d i − 1 ,   d a b s ≠ 0 o t h e r (17)
View Source Right-click on figure for MathML and additional features. \begin{align*} r\left ({i }\right)= \begin{cases} C1,& S_{i}=S_{e} \\[0.6pc] -C1,& d_{abs}=0 \\[0.6pc] C2\ast \frac {d_{i}}{D},& d_{i}< d_{i-1},~d_{abs}\ne 0 \\[0.6pc] -C2\ast \frac {d_{i}}{D},& d_{i}>d_{i-1},~d_{abs}\ne 0 \\[0.6pc] -C3\ast \frac {d_{i}}{D},& other \\ \end{cases}\tag{17}\end{align*} where d i denotes the distance between the agent and the target location and d a b s denotes the distance between the agent and the closest obstacle. C1, C2, and C3 are all constants that denote the specific reward values obtained after the agent interacts with the external environment in different scenarios, C1 > C2 > C3.

2) Dynamic Exploration Factor

The exploration strategies commonly used in the classic Q-learning algorithm include greedy strategy and ε greedy strategy [10] – [11] [12] [13] [14] [15] [16] . The greedy strategy is to choose the action that maximizes the value for each step in each iteration of the algorithm, that is,
Q ( s i , a i ) = a r g m a x a i + 1 Q ( s i + 1 , a i + 1 ) (18)
View Source Right-click on figure for MathML and additional features. \begin{equation*} Q\left ({s_{i},a_{i} }\right)=arg{max}_{a_{i+1}}Q\left ({s_{i+1},a_{i+1} }\right)\tag{18}\end{equation*}

The ε greedy strategy sets an exploration factor ε to add a random strategy when selecting actions in the algorithm iteration process so that the agent has the probability of ε to choose the action that is most conducive to completing the tasks or maximizes the value. There is a probability of 1 − ε to randomly select an action from the action space, that is,
Q ( s i , a i ) { a r g m a x a i + 1 Q ( s i + 1 , a i + 1 ) , a i ϵ A x ≤ ε x > ε (19)
View Source Right-click on figure for MathML and additional features. \begin{align*} Q\left ({s_{i},a_{i} }\right) \begin{cases} arg{max}_{a_{i+1}}Q\left ({s_{i+1},a_{i+1} }\right),& x\le \varepsilon \\[0.5pc] a_{i}\epsilon A& x>\varepsilon \\ \end{cases}\tag{19}\end{align*}

However, the ε greedy strategy also has the problem of unbalanced exploration-exploitation because the value of ε is fixed. Therefore, an exploration method that dynamically adjusts the exploration factor was proposed, which can modify the exploration and exploitation process of the Q-learning algorithm and dynamically adjust in phases, that is,
ε = ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ε 1 ∗ ( k s t e p 1 ) , ε 1 + ε 2 ∗ ( k − s t e p 1 s t e p 2 ) ε = ε 2 + ε 3 ∗ ( k − s t e p 2 s t e p 3 ) k ≤ s t e p 1 s t e p 1 ≤ k ≤ s t e p 2 s t e p 2 ≤ k ≤ s t e p 3 (20)
View Source Right-click on figure for MathML and additional features. \begin{align*} \varepsilon = \begin{cases} \varepsilon _{1}\ast \left ({\frac {k}{step_{1}} }\right), & k\le {step}_{1} \\[0.8pc] {\varepsilon _{1}+\varepsilon }_{2}\ast \left ({\frac {k-{step}_{1}}{step_{2}} }\right) & {step}_{1}\le k\le {step}_{2} \\[0.8pc] \varepsilon ={\varepsilon _{2}+\varepsilon }_{3}\ast \left ({\frac {k-{step}_{2}}{step_{3}} }\right) & {step}_{2}\le k\le {step}_{3} \\ \end{cases} \\ {}\tag{20}\end{align*} where ε 1 denotes the value of the initial exploration factor ε in the exploration phase, ε 1 ∈ ( 0 , 0.5 ) ; ε 2 denotes the value of the initial exploration factor ε in the exploration-exploitation phase, ε 2 ∈ ( 0 , 1 ) ; ε 3 denotes the value of the initial exploration factor ε in the exploitation phase, ε 3 ∈ ( 0.5 , 1 ) ; k denotes the current number of iterations of the Q-learning algorithm; s t e p 1 denotes the maximum number of iterations of the algorithm in the exploration phase; s t e p 2 denotes the maximum number of iterations of the algorithm in the exploration-exploitation phase; and s t e p 3 denotes the maximum number of iterations of the algorithm in the exploitation phase.

k ≤ s t e p 1 indicates that the algorithm should be in the exploration phase of experience accumulation. In this phase, because the algorithm has just been iterated, the agent knows nothing about the information of the environment, as well as about how to complete the tasks or maximize the value. In this case, the agent’s first choice is to quickly explore the surrounding environment.

s t e p 1 ≤ k ≤ s t e p 2 indicates that the algorithm should be in the exploration and exploitation phase. In this phase, the agent makes appropriate use of the known environment and accumulated experience in the process of exploration to complete tasks faster or maximize the value. However, because the agent’s knowledge of the environment has not yet met the requirements, it is necessary to use existing experience while exploring the unfamiliar environment as much as possible.

The pseudo code of modified Q-learning is as follows:
Algorithm 2: Modified Q-Learning

search area, tasks node

path node

Q table, UAV information, constraint information, action space A = { a 1 , a 2 , … , a i } , action-value function Q ( s i , a i ) , where s i ∈ S , a i ∈ A

While S non-terminal state do

Initialize the state space S = { s 1 , s 2 , … , s i }

for each step in each round do # the direction of UAV that can be taken in three-dimensional space

Use the modified search strategy to select an action

Take the action selected in the previous step to obtain the feedback value r i and the new state s i + 1

( s i , a i ) = Q ( s i , a i ) + α ( r i + γ m a x a i + 1 Q ( s i + 1 , a i + 1 ) − Q ( s i , a i ) )

s i ← s i + 1

End For

End While

s t e p 2 ≤ k ≤ s t e p 3 indicates that the algorithm has accumulated enough experience and is in the phase of exploiting the experience. In this phase, the agent uses these experiences as much as possible to achieve the goal of completing tasks quickly or maximizing the value. At this time, the value of exploration factor ε becomes greater and approaches 1 as the number of iterations increases.

3) Flow of local dynamic path planning algorithm based on modified exploration mechanism as shown in Figure 3 .
FIGURE 3. - Flow chart of modified Q-learning.
FIGURE 3.

Flow chart of modified Q-learning.

Show All

SECTION IV.
Simulation Experiments

Python was used to perform simulations to verify the performance of the aforementioned global path planning based on modified A* and the local dynamic path planning algorithm based on Q-learning with a modified exploration mechanism. Simulation experiments of path planning were performed on the proposed fusion algorithm using Python, and the algorithm performance was compared before and after fusion. The system configurations for simulation experiments are shown in Table 2 .
TABLE 2 Configuration of the System
Table 2- Configuration of the System

A. Experimental Design and Analysis

Scheme 1:

The grid size divided by the grid method is 50*50*10, with a total of 25,000 path nodes. The number of static obstacles is randomly set to 1,250, accounting for 5% of the total nodes. The three-dimensional position coordinates of the starting point of the task are set as the origin S (0, 0, 0) of the grid, and the end point coordinates are set as E (50, 50, 10). The simulation results are shown in Figures 4 and 5 .
FIGURE 4. - Time consumption between algorithms in scheme 1.
FIGURE 4.

Time consumption between algorithms in scheme 1 .

Show All
FIGURE 5. - Number of path nodes between algorithms in scheme 1.
FIGURE 5.

Number of path nodes between algorithms in scheme 1 .

Show All

Scheme 2:

The grid size divided by the grid method is 50*50*50, with a total of 125,000 path nodes. The number of static obstacles is randomly set to 6,250, accounting for 5% of the total nodes. The three-dimensional position coordinates of the starting point of the task are set as the origin S (0, 0, 0) of the grid, and the end point coordinates are set as E (50, 50, 50). The simulation results are shown in Figures 6 and 7 .
FIGURE 6. - Time consumption between algorithms in scheme 2.
FIGURE 6.

Time consumption between algorithms in scheme 2 .

Show All
FIGURE 7. - Number of path nodes between algorithms in scheme 2.
FIGURE 7.

Number of path nodes between algorithms in scheme 2 .

Show All

Scheme 3:

The grid size divided by the grid method is 100*100*50, with a total of 50,000 path nodes. The number of static obstacles is randomly set to 25,000, accounting for 5% of the total nodes. The three-dimensional position coordinates of the starting point of the task are set as the origin S (0, 0, 0) of the grid, and the end point coordinates are set as E (100, 100, 50). The simulation results are shown in Figures 8 and 9 .
FIGURE 8. - Time consumption between algorithms in scheme 3.
FIGURE 8.

Time consumption between algorithms in scheme 3 .

Show All
FIGURE 9. - Number of path nodes between algorithms in scheme 3.
FIGURE 9.

Number of path nodes between algorithms in scheme 3 .

Show All

Path planning was simulated in the established terrain modeling environment. The path planning performances of the modified A* algorithm (BiRDA*), the classic A* algorithm, and the real-time adaptive A* (RTAA*) were compared. Table 3 presents the comparison of the experimental results of the global path planning based on the existing static obstacle information between the two algorithms, including the number of iterations, algorithm time consumption, number of path nodes, and path cost.
TABLE 3 Comparison of Experimental Results
Table 3- Comparison of Experimental Results

Table 3 presents the average values obtained through 50 random initializations of the map and path planning. As Table 3 suggests, compared with the classic A* algorithm and the anytime repairing sparse A* (ARA*) algorithm, the modified BiRDA* algorithm provides shorter planning duration, fewer path nodes, and smaller path cost calculated when the grid space divided by the grid method is large; however, when the scale is small, its performance is not as good as that of the ARA* algorithm.

Figure 4 indicates that some of the classic A* have shorter time consumption because when the map is small, the two-way search increases time consumption. ARA* is more suitable for small-scale situations than A*; therefore, ARA* has the best performance for the scale in Scheme 1 . Figure 5 indicates that some of the classic A* and ARA* have fewer path nodes because when the map is small, the modified search strategy with an angle removes some unqualified path nodes. According to Figures 6 – 9 , the overall performance of the algorithm is relatively stable, and the small range of fluctuations that occur are caused by the random initialization of obstacles. The data above shows that the modified A* algorithm has a better convergence speed, shorter path planned, better performance, and better path for path planning on a larger scale; however, when it is applied to a small scale, its time consumption may increase, and its performance is not as good as those of ARA* and classic A*.

The task node information and dynamic threat information are added after setting static obstacles for global path planning in the previous section. When the UAVs are executing tasks such as surveillance, rescue, and delivery, they may encounter interferences to their radar equipment, which are considered threats. For these threats, two radii are randomly initialized: One is the detection radius r 0 , and the other is the reaction radius r 1 , r 0 > r 1 > 0 .

Scheme 1:

The number of dynamic threats is set to 100, with random locations. The simulation results are presented in Figures 8 and 9 .

Scheme 2:

The number of dynamic threats is set to 500, with random locations. The simulation results are presented in Figures 8 and 9 .

Scheme 3:

The number of dynamic threats is set to 1,000. The simulation results are presented in Figures 8 and 9 .

The corresponding parameters of the Q-learning algorithm are generated by running dynamic path planning 1,000 times in an environment with the same batch of dynamic obstacles, and their optimal values are determined by the control variable method and the binary search method. Consequently, the parameters of the Q-learning algorithm are obtained, as shown in Table 4 , as well as the parameters of the dynamic exploration factor, as shown in Table 5 .
TABLE 4 Parameters of Q-Learning Algorithm
Table 4- Parameters of Q-Learning Algorithm
TABLE 5 Parameters of Dynamic Exploration Factor
Table 5- Parameters of Dynamic Exploration Factor

In this environment, the path planning simulation is executed, and it is compared with the algorithm based on Q-learning with a modified exploration mechanism (AQ-learning). Table 6 presents the comparison of the performance of the local path planning based on the existing dynamic threat information between the two algorithms, including the number of iterations, algorithm time consumption, and the number of path nodes, in the three schemes.
TABLE 6 Experimental Data Design
Table 6- Experimental Data Design

Table 6 presents the average values obtained through 50 random initializations of the map and 500 iterations of path planning. Because the dynamic adjustment occurs after the UAV global planning is completed, the adjustment extent is small, so the iterations are set to 500 times to take the average value. According to the results in the table, compared with the classic Q-learning algorithm and the Sarsa algorithm, the modified Q-learning algorithm has a shorter planning duration and fewer path nodes, but it is not as stable as Sarsa.

As shown in Figure 10 , Sarsa runs relatively stable when the scale is small, and the modified Q-learning takes the shortest time but is not as stable as Sarsa. Figure 11 indicates that the modified Q-learning has fewer path nodes. According to Figures 10 , 12 , and 14 , the time consumption of the modified Q-learning fluctuates greatly during dynamic path planning, compared with those of the classic Q-learning and Sarsa. The underlying reason is that although the exploration mechanism of Q-learning has been modified, there is still a certain probability for random selection, leading to great fluctuation. Sarsa performs better when the map is large because it is on-policy and infeasible actions are removed when selecting actions. Therefore, its time consumption is better than that of Q-learning at a large scale, but most modified algorithms have advantages. According to Figures 11 , 13 , and 15 , the modified Q-learning algorithm can reduce the number of path nodes to a certain extent. The above shows that the modified Q-learning algorithm has been well modified in response to the limitations of its exploration and exploitation, which can better balance the relationship between exploration and exploitation. When applied to UAV path planning, it also has faster convergence speed and better overall performance.
FIGURE 10. - Time consumption between algorithms in scheme 1.
FIGURE 10.

Time consumption between algorithms in scheme 1 .

Show All
FIGURE 11. - Number of path nodes between algorithms in scheme 1.
FIGURE 11.

Number of path nodes between algorithms in scheme 1 .

Show All
FIGURE 12. - Time consumption between algorithms in scheme 2.
FIGURE 12.

Time consumption between algorithms in scheme 2 .

Show All
FIGURE 13. - Number of path nodes between algorithms in scheme 2.
FIGURE 13.

Number of path nodes between algorithms in scheme 2 .

Show All
FIGURE 14. - Time consumption between algorithms in scheme 3.
FIGURE 14.

Time consumption between algorithms in scheme 3 .

Show All
FIGURE 15. - Number of path nodes between algorithms in scheme 3.
FIGURE 15.

Number of path nodes between algorithms in scheme 3 .

Show All

B. Analysis of Simulation Results

Figure 16 presents the planned path map based on the model in Chapter 2, which converts the latitude and longitude coordinates of the UAV into three-dimensional coordinates before rasterizing. Figure 19 presents the comparison of the path after the cubic B-spline curve is smoothed.
FIGURE 16. - Initial path.
FIGURE 16.

Initial path.

Show All
FIGURE 17. - Original path.
FIGURE 17.

Original path.

Show All
FIGURE 18. - Smoothed path.
FIGURE 18.

Smoothed path.

Show All
FIGURE 19. - Path before and after smoothing.
FIGURE 19.

Path before and after smoothing.

Show All

According to Figures 17 – 19 , the smoothed path has no conflicts with the obstacles, and the overall transition of the path is natural and significantly continuous without sharp corners, which meets the requirements of continuous changes in UAV speed and acceleration.
SECTION V.
Conclusion

In this paper, the research background and key technical problems of path planning were investigated. Through corresponding assumptions, constraints, cost function, evaluation indicators, environmental models, and simulation maps, a global-local UAV path planning model was established, and the effectiveness of the proposed model and algorithms was verified. The main conclusions drawn are as follows:

    The global path planning algorithm based on the modified A* and the local path planning algorithm based on the modified Q-learning have positive effects in reducing algorithm time consumption, the number of path nodes, and the path planning cost. Both are better than the classic A* and Q-learning algorithms, with certain guiding significance for UAV path planning.

    The proposed algorithm can deliver reasonable and stable path planning for the assigned tasks. Therefore, the proposed algorithm is feasible and effective for solving the problem of UAV path planning.

    From the perspectives of the UAV path planning model and corresponding constraint analysis, the proposed model and algorithms are superior to the classic two-dimensional model and the traditional static allocation algorithm (without considering the dynamic threats). Moreover, the UAV environment model considered is a three-dimensional space, and the constraint analysis is more realistic and comprehensive.

Although the proposed model of UAV path planning considers factors such as environment, constraint analysis, and cost function, it lacks a collaborative model of UAV path planning. It is crucial to propose a planning model with multi-UAV collaboration. In addition, the modified exploration mechanism method proposed in this paper adopts a design of phased adjustment and employs some settings for modification when implementing the dynamic exploration factors. One can incorporate better and more complex settings to enhance the effectiveness of the algorithm.

Authors
Figures
References
Keywords
Metrics
More Like This
Path Planning and Collision Avoidance with Artificial Intelligence for a Quadrotor UAV

2021 International Conference Automatics and Informatics (ICAI)

Published: 2021
Dynamic Collision Avoidance Path Planning for Mobile Robot Based on Multi-sensor Data Fusion by Support Vector Machine

2007 International Conference on Mechatronics and Automation

Published: 2007
Show More
References
1. C. Zhang and W. Fu, "Optimal model for patrols of UAVs in power grid under time constraints", Int. J. Performability Eng. , vol. 17, no. 1, pp. 103-113, Jan. 2021.
Show in Context Google Scholar
2. S. Al-Hasan and G. Vachtsevanos, "Intelligent route planning for fast autonomous vehicles operating in a large natural terrain", Robot. Auton. Syst. , vol. 40, no. 1, pp. 1-24, 2002.
Show in Context CrossRef Google Scholar
3. A. Richards and J. P. How, "Aircraft trajectory planning with collision avoidance using mixed integer linear programming", Proc. Amer. Control Conf. , vol. 3, pp. 1936-1941, May 2002.
Show in Context View Article Full Text: PDF (548) Google Scholar
4. J. Wang, W. Chi, C. Li, C. Wang and M. Q.-H. Meng, "Neural RRT*: Learning-based optimal path planning", IEEE Trans. Autom. Sci. Eng. , vol. 17, no. 4, pp. 1748-1758, Oct. 2020.
Show in Context View Article Full Text: PDF (2548) Google Scholar
5. N. Ozalp and O. K. Sahingoz, "Optimal UAV path planning in a 3D threat environment by using parallel evolutionary algorithms", Proc. Int. Conf. Unmanned Aircr. Syst. (ICUAS) , pp. 308-317, May 2013.
Show in Context View Article Full Text: PDF (1374) Google Scholar
6. U. Cekmez, M. Ozsiginan and O. K. Sahingoz, "Multi colony ant optimization for UAV path planning with obstacle avoidance", Proc. Int. Conf. Unmanned Aircr. Syst. (ICUAS) , pp. 47-52, Jun. 2016.
Show in Context View Article Full Text: PDF (997) Google Scholar
7. M. Radmanesh and M. Kumar, "Grey wolf optimization based sense and avoid algorithm for UAV path planning in uncertain environment using a Bayesian framework", Proc. Int. Conf. Unmanned Aircr. Syst. (ICUAS) , pp. 168-179, Jun. 2016.
Show in Context View Article Full Text: PDF (795) Google Scholar
8. I. Sung, B. Choi and P. Nielsen, "On the training of a neural network for online path planning with offline path planning algorithms", Int. J. Inf. Manage. , vol. 57, Apr. 2021.
Show in Context CrossRef Google Scholar
9. Y. Bengio, "Deep learning of representations: Looking forward", Proc. Int. Conf. Stat. Lang. Speech Process. , vol. 7978, pp. 1-37, Jul. 2013.
Show in Context CrossRef Google Scholar
10. R. S. Sutton and A. G. Barto, "Reinforcement learning: An introduction", IEEE Trans. Neural Netw. , vol. 9, no. 5, pp. 1054, Sep. 1998.
Show in Context View Article Full Text: PDF (8) Google Scholar
11. W. Feng and Y. Wu, "DDoS attack real-time defense mechanism using deep Q-learning network", Int. J. Performability Eng. , vol. 16, no. 9, pp. 1362-1373, Sep. 2020.
Show in Context Google Scholar
12. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu and J. Veness, "Human-level control through deep reinforcement learning", Nature , vol. 518, no. 7540, pp. 529-533, 2015.
Show in Context CrossRef Google Scholar
13. K. Arulkumaran, M. P. Deisenroth, M. Brundage and A. A. Bharath, "Deep reinforcement learning: A brief survey", IEEE Signal Process. Mag. , vol. 34, no. 6, pp. 26-38, Nov. 2017.
Show in Context View Article Full Text: PDF (3005) Google Scholar
14. G. Sartoretti, J. Kerr, Y. Shi, G. Wagner, T. K. S. Kumar, S. Koenig, et al., "PRIMAL: Pathfinding via reinforcement and imitation multi-agent learning", IEEE Robot. Autom. Lett. , vol. 4, no. 3, pp. 2378-2385, Jul. 2019.
Show in Context View Article Full Text: PDF (1626) Google Scholar
15. M. Duguleana and G. Mogan, "Neural networks based reinforcement learning for mobile robots obstacle avoidance", Expert Syst. Appl. , vol. 62, pp. 104-115, Nov. 2016.
Show in Context CrossRef Google Scholar
16. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, et al., "Mastering the game of Go with deep neural networks and tree search", Nature , vol. 529, no. 7587, pp. 484-489, 2016.
Show in Context CrossRef Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
