DYNAMIC PROGRAMMING

DYNAMIC PROGRAMMING
BY
RICHARD BELLMAN
PRINCETON UNIVERSITY PRESS PRINCETON, NEW JERSEY

A Rand Corporation Research Study Published, 1957, by Princeton University Press
ISBN 0-691-07951-X Sixth Printing, 1972
Printed in the United States of America

To Betty-Fo whose decision processes defy analysis

Preface
The purpose of this work is to provide an introduction to the mathematical theory of multi-stage decision processes. Since these constitute a somewhat formidable set of terms we have coined the term ‘“‘dynamic programming’ to describe the subject matter. Actually, as we shall see, the distinction involves more than nomenclature. Rather, it involves a certain conceptual framework which furnishes us a new and versatile mathematical tool for the treatment of many novel and interesting problems both in this new discipline and in various parts of classical analvsis. Before expanding upon this theme, let us present a brief discussion of what is meant by a multi-stage decision process.
Let us suppose that we have a physical system S whose state at any time ¢ is specified by a vector p, If we are in an optimistic frame of mind we can visualize the components of # to be quite definite quantities such as Cartesian coordinates, or position and momentum coordinates, or perhaps volume and temperature, or if we are considering an economic system, supply and demand, or stockpiles and production capacities. If our mood is pessimistic, the components of # may be supposed to be probability distributions for such quantities as position and momentum, or perhaps moments of a distribution.
In the course of time, this system is subject to changes of either deterministic or stochastic origin which, mathematically speaking, means that the variables describing the system undergo transformations. Assume nowthat in distinction to the above we have a processin which we have a choice of the transformations which may be applied to the system at any time. A process of this type we call a decision process, with a decision equivalent to a transformation. If we have to make a single decision, we call the process a single-stage process; if a sequence of decisions, than we use the term multi-stage decision process.
The distinction, of course, is not hard and fast. The choice of a point in three-dimensional space maybe considered to be a single-stage process wherein we choose (%, y, 2), or a multi-stage process where we choose first x, then y, and then z.
There are a number of multi-stage processes which are quite familiar to us. Perhaps the most commonare those occurring in card games, such
Vil

PREFACE
as the bidding system in contract bridge, or the raise-counter-raise system of poker with its delicate overtones of bluffing. On largerscale, we continually in our economic life engage in multi-stage decision processes in connection with investment programsand insurancepolicies. In the scientific world, control processes and the design of experiments furnish other examples.
The point we wish to make is that in modern life, in economic, industrial, scientific and even political spheres, we are continually surrounded by multi-stage decision processes. Some of these we treat on the basis of experience, some weresolve by rule-of-thumb, and some are too complex for anything but an educated guess and a prayer.
Unfortunately for the peace of mind of the economist, industrialist, and engineer, the problems that have arisen in recent years in the economic, industrial, and engineering fields are too vast in portent and extent to be treated in the haphazard fashion that was permissible in a more leisurely bygoneera. The price of tremendous expansion has become extreme precision.
These problems, although arising in a multitude of diverse fields, share a common property—they are exceedingly difficult. Whether they arise in the study of optimal inventory or stock control, or in an input-output analysis of a complex of interdependent industries, in the scheduling of patients through a medical clinic or the servicing of aircraft at an airfield, the study of logistics or investment policies, in the control of servo-mechanisms, or in sequential testing, they possess certain common thorny features which stretch the confines of conventional mathematical theory.
It follows that new methods must be devised to meet the challenge of these new problems, and to a mathematician nothing could be more pleasant. It is a characteristic of this species that its members are never so happy as when confronted by problems which cannot be solved—immediately. Although the day is long past when anyone seriously worried about the well of mathematical invention running dry, it is still nonetheless a source of great delight to see a vast untamed jungle of difficult and significant problems, such as those furnished by the theory of multi-stage decision processes, suddenly appear before us.
Having conjured up this preserve of problems, let us see what compass we shall use to chart our path into this new domain. The conventional approach we may label “enumerative.’’ Each decision may be thought of as a choice of a certain number of variables which determine the transformation to be employed; each sequenceof choices, or policy as we shall say, is a choice of a larger set of variables. By lumpingall these choices together, we “‘reduce’’ the problem to a classical problem of
V1

PREFACE
determining the maximum of a given function. This function, which arises in the course of measuring some quantitative property of the system, serves the purpose of evaluating policies.
At this point it 1s very easy for the mathematician to lose interest and let the computing machine take over. To maximize a reasonably well-behaved function seems a simple enough task; we take partial derivatives and solve the resulting system of equations for the maxtimizing point.
There are, however, some details to consider. In the first place, the effective analytic solution of a large number of even simple equations as, for example, linear equations, is a difficult affair. Lowering oursights, even a computational solution usually has a number of difficulties of both gross and subtle nature. Consequently, the determination of this maximum is quite definitely not routine when the numberof variables is large.
All this may be subsumed underthe heading “‘the curse of dimensionality.’ Since this is a curse which has hung over the head of the physicist and astronomer for many a year, there is no need to feel discouraged about the possibility of obtainingsignificant results despite it.
However, this is not the sole difficulty. A further characteristic of these problems, as weshall see in the ensuing pages, 1s that calculus is not always sufficient for our purposes, as a consequence of the perverse fact that quite frequently the solution is a boundary point of the region of variation. This is a manifestation of the fact that many decision processes embodycertain all-or-nothing characteristics. Very often then, we are reduced to determining the maximum of a function by a combination of analytic and “‘hunt and search’’ techniques.
Whatever the difficulties arising in the deterministic case which we have tacitly been assuming above, these difficulties are compoundedin the stochastic case, where the outcome of a decision, or tranformation, is a random variable. Here any crude lumping or enumerative technique is surely doomed by the extraordinary manner in which the numberof combinations of cases increases with the numberof cases.
Assume, however, that we have circumventedall these difficulties and have attained a certain computational nirvana. Withal, the mathematician has not discharged his responsibilities. The problem is not to be considered solved in the mathematical sense until the structure of the optimal policy 1s understood.
Interestingly enough, this concept of the mathematical solution is identical with the proper conceptof a solution in the physical, economic, or engineering sense. In order to make this point clear—and it is a most important point since in many waysit is the raison d’étre for mathe-
iX

PREFACE
matical physics, mathematical economics, and many similar hybrid fields—let us make a brief excursion into the philosophy of mathematical models.
The goal of the scientist is to comprehend the phenomena of the universe he observes around him. To prove that he understands, he must be able to predict, and to predict, one requires quantitative measurements. A qualitative prediction such as the occurrence of an eclipse or an earthquake or a depression sometime in the near future does not have the samesatisfying features as a similar prediction associated with a date and time, and perhaps backed up by the offer of a side wager.
To predict quantitatively one must have a mechanism for producing numbers, and this necessarily entails a mathematical model. It seems reasonable to suppose that the morerealistic this mathematical model, the more accurate the prediction.
There is, however, a point of diminishing returns. The actual world is extremely complicated, and as a matter of fact the more that one studies it the more oneis filled with wonder that we have even “‘order of magni-
tude’ explanations of the complicated phenomena that occur, much less fairly consistent “laws of nature.’ If we attempt to include too many features of reality in our mathematical model, wefind ourselves engulfed by complicated equations containing unknown parameters and unknown functions. The determination of these functions leads to even more complicated equations with even more unknownparameters and functions, and so on. Truly a tale that knows noend.
If, on the other hand, made timid by these prospects, we construct our modelin too simple a fashion, we soon find that it does not predict to suit our tastes.
It follows that the Scientist, like the Pilgrim, must wend a straight and narrow path between the Pitfalls of Oversimplification and the Morass of Overcomplication.
Knowing that no mathematical model can yield a complete description of reality, we must resign ourselves to the task of using a succession of models of greater and greater complexity in our efforts to understand. If we observe similar structural features possessed by the solutions of a sequence of models, then we may feel that we have an approximation to whatis called a “law of nature.”’
It follows that froma teleological point of view the particular numerical solution of anyparticular set of equations is of far less importance than the understanding of the nature of the solution, which is to say the influence of the physical properties of the system upon the form of the solution.
Nowlet us see how this idea guides us to a new formulation of these

PREFACE decision processes, and indeed of some other processes of analysis which
are not usually conceived of as decision processes. In the conventional formulation, we consider the entire multi-stage decision process as essentially one stage, at the expense of vastly increasing the dimension of the problem. Thus, if we have an N-stage process where M decisions are to be madeat each stage, the classical approach envisages an MNdimensional single-stage process. The fundamental problem that confronts us is: How can weavoid this multiplication of dimension which stifles analysis and greatly impedes computation ?
In order to answer this, let us turn to the previously enunciated principle that it is the structure of the policy which is essential. What does this mean precisely? It means that we wish to know the characteristics of the system which determine the decision to be made at any particular stage of the process. Put another way, in place of determining the optimal sequence of decisions from some fixed state of the system, we wish to determine the optimal decision to be made at any state of the system. Only 1f we know the latter, do we understand the intrinsic structure of the solution.
The mathematical advantage of this formulation les first of all in the fact that it reduces the dimension of the process to its proper level, namely the dimension of the decision which confronts one at any particular stage. This makes the problem analytically more tractable ‘and computationally vastly simpler. Secondly, as we shall see, it furnishes us with a type of approximation which has a unique mathematical property, that of monotonicity of convergence, and is well suited to applications, namely, “approximation in policy space’’.
The conceptual advantage of thinking in terms of policies is very great. It affords us a means of thinking about and treating problems which cannot be profitably discussed in any other terms. If we were to hazard a guess as to which direction of research would achievethe greatest success in the future of multi-dimensional processes, we would unhesitatingly choose this one.
The theme of this volume will be the application of this concept of a solution to a number of processes of varied type which weshall discuss below.
The title is also derived in this way. The problems wetreat are programming problems, to use a terminology now popular. The adjective ‘“‘dynamic,”’ however, indicates that we are interested in processes in which time plays a significant role, and in which the order of operations may be crucial. However, an essential feature of our approach will be the reinterpretation of many static processes as dynamic processes in which time can be artificially introduced.
x1

PREFACE
Let us now turn to a discussion of the contents. In the first chapter we consider a multi-stage allocation process of deterministic type which is a prototype of a general class of problems encountered in various phases of logistics, in multi-stage investment processes, in the study of optimal purchasing policies, and in the treatment of many other economic processes. From the mathematical point of view, the problem is related to multi-dimensional maximization problems, and ultimately, as will be indicated below, to the calculus of variations. Weshall first discuss the process in the conventional manner and observe the dimensional difficulties which arise from the discussion of even very simple processes. Then we shall introduce the fundamental technique of the theory, the conversion of the original maximization problem into the problem of determining the solution of a functional equation. The functional equations whicharise in this way are of a novel type, completely different from any of the functional equations encountered in classical analysis. The particular one we shall employ for purposes of discussion in this chapter is
(1) f(*) = Max [g(y) +A (x—y) +f (ay+ (x—y))]. O<sysu
where g and / are known functions and a and b are known constants, satisfying the condition 0< a,b <1.
After establishing an existence and uniqueness theorem, we shall derive some simple properties of the optimal policy which can be deduced from simple functional properties of g and h. In particular, we shall present the explicit solution of some equations where g and h have various special forms.
The advantage of obtaining these solutions lies in the fact that they can be utilized to obtain approximationsto the solutions of more complhicated equations, and, what is more important, approximations to the associated optimal policies. The subject of approximation leads us to the concept of approximation in policy space, of importance and utility in both theoretical and practical discussion, and to the discussion of the question of the stability of f under changes in g and A.
In the second chapter we consider a multi-stage decision process of stochastic type in the guise of a gold-mining venture with a delicate gold-mining machine. Here we encounter the equation

(2)

I (%, ¥)

== Max

A:
FB

py(rx%
pelrav

+f((l1—n) x,y)]
+f (x, (1 —?r,) oo

XI

PREFACE

In addition to pursuing an investigation similar to that given in Chapter I, we actually obtain a solution to this equation, and some of its generalizations. The solution has a particularly simple and intuitive form, and introduces the useful idea of “decision regions.”’
We show, however, that some other generalizations do not have as simple a structure, and, indeed, pose as yet unresolved problems. An attempt to obtain approximate solutions to these problems for a particular region of parameter space will lead us to the continuous versions treated in Chapter VIII.
Chapter IIT is devoted to a synthesis of these processes which seem so different at first glance. In this chapter we analyze the commonfeatures of the two processes treated in the preceding chapters, and then proceed to formulate general versions of these processes. In this way we obtain the functional equation

(3)

f (p) = Max [g (6, g) + A(b, a) F(T (6, O)),

q

which includes both of the preceding, and a number of equations of still more general type.
Also in this chapter we explicitly state the “principle of optimality”’ whose mathematical transliteration in the case of any specific process yields the functional equation governing the process. The concept of “approximation in policy space’ is also discussed in more detail.
In the following chapter, Chapter IV, a number of existence and uniqueness theorems are established for several frequently occurring classes of equations having the above form. Our proofs hinge upon a simple lemma which enables us to compare twosolutions of the equation in (3). Although these equations are highly non-linear, in many ways they constitute a natural generalization of linear equations. For this reason alone, aside from their applications, they merit study.
In Chapter V, we discuss a functional equation derived from a problem of much economic interest at the current time, the “optimal inventory”’ problem. Here we show that the various techniques we have discussed in the preceding chaptersyield the solutions of someinteresting particular cases. In particular, we show that the method of successive approximations is an efficient analytic tool for the discovery of properties of the solution and the policy, rather than merely a humdrum meansof obtaining existence and uniqueness theorems. There are many different versions of the optimal inventory problem and werestrict ourselves to a discussion of the mathematical modelfirst proposed by Arrow, Harris,and Marschak, and treated also by Dvoretzky, Kiefer, and Wolfowitz.

X11

PREFACE
A particular equation of the type we shall consider is
(4) f (x) = Min (g (y — x) + at{{2 (s -~ y) dG (s) +f (0) [a (s)
+ [Fo —s ac ish

Wethen turn to a study of what wecall “‘bottleneck processes.’’ These

we define as processes where a numberof interdependent activities are

to be combined for one common purpose, with the level of this principal

activity dependent upon the minimumlevelof activity of the components.

Two chapters are devoted to these problems, the first, Chapter VI,

of theoretical nature, and the second, Chapter VII, given over to the

actual details of the complete solution of one particular process.

The problems that we encounter are particular cases of the general

problem, apparently not treated before in any mathematical detail, of

determining the maximum over z of the inner product (x (7), a), where

x and z are connected by meansof the vector-matrix equation

(5)

dx|dt = Ax + Bz,x(0) =c,

and where there is a constraint of the form Cz + Dx <f. Here x, z, c and f are vectors and A, B, C and D are matrices. The linearity of the operators and functionals constitutes the principal difficulty.
We might observe parenthetically that it is often thought that linearizing a problem facilitates its solution. On occasion, however, particularly in variational problems, it frequently complicates affairs to an enormous degree, since this linearization renders classical variational techniques largely inapplicable. In return, however, the computational solution of particular cases may often be obtained by routine procedures.
In Chapter VIII, we return to the gold-mining process, and consider a continuous version. There are many problems, someof a quite recondite nature, associated with the formulation of continuous stochastic decision processes. In the processes at hand, we are fortunate in being able to sidestep these difficulties. In the continuous version, combining the classical variational approach with the techniques employed in previous chapters, we are able to solve completely the continuous versions of a numberof problems that were resolutely intractable in the discrete case.
Wenowturn to the calculus of variations in Chapter IX, and show that various characteristic problems may be viewed as dynamic programming processes of continuous and deterministic type.
In geometric terms, the classical formulation is equivalent to considering an extremal curve as a locus of points, while the dynamic

X1V

PREFACE

programming formulation conceives of the extremal as the envelope of tangents.
Taking this latter point of view, we are able to obtain a new formulation of some parts of the classical theory. In particular, we show how to obtain partial differential equations, in terms of suitably introduced state variables, for the principal eigen-value of the differential equation

(6)

u" + 2? (t) u =: 0, u (0) = u (1) = 0.

Furthermore, we provide a new computational approach to variational problems with constraints.
In Chapter X, we consider dynamic programmingprocesses involving two decision-makers, essentially opposed to each other in their interests, This leads to the discussion of multi-stage games, and, in particular, to the very interesting class of games called “games of survival.’’ With the aid of some heuristic reasoning, we are able to obtain a new rationale for non-zero sum games, as a by-product.
The functional equations encountered in this domain have the general form

(7)

f(b, p’) = Max Min [|| te (6, 2,9, 9’) +

G

G’

hO,P'7.47) F111 (0,097), Te(b, i. @ 7] 4G (g) dG’(9’)].

They may be treated by means of the same general methods used in Chapter IV to discuss the equation in (3) above.
In the final chapter, we considera class of continuous decision processes which lead to non-linear differential equations of the form
(8) a AX: Max[ 2v i (¢; g) x7 + Bi (g)], x2 (0) = C1, 4; =1,2,..., N, q j=
together with the corresponding equations derived from the discrete process.
These equations possess amusing connections with some classical non-linear equations, as we indicate.
In addition to a numberof exercises inserted for pedagogical purposes, we have included a cross-section of problems designed to indicate the scope of the application of the methods of dynamic programming.
There may be some whowill frown upon someof the less than profound subjects which are occasionally discussed in the exercises, and used to illustrate various types of processes. We are prepared to defend ourselves against the charges of lése majesté in a number of ways, but we prefer the two following. In thefirst place, interesting mathematics is where

XV

PREFACE
you find it, sometimes in a puzzle concerning the bridges of Koenigsberg, sometimes in a problem concerning the coloring of maps, or perhaps the seating of schoolgirls, perhaps in the determining of winning play in games of chance, perhaps in an unexpected regularity in the distribution of primes. In the second place, all thought 1s abstract, and mathematical thought especially so. Consequently, whether we introduce our mathematical entities under the respectable sobriquets of A and B, or by the more charming Alice and Betty, or whether we speak of stochastic processes, or the art of gaming, it is the mathematical analysis that counts. Any mathematical study, such as this, must be judged, ultimately upon its intrinsic content, and not by the density of high-sounding pseudo-abstractions with which a text may so easily be salted.
This completes our synopsis of the volume. Since the processes we consider, the functional equations which arise, and the techniques we employ are in the main novel and therefore unfamiliar, we have restricted ourselves to a moderate mathematical level in order to emphasize the principles involved, untrammeled by purely analytic details. Consistent with this purpose we have not penetrated too deeply into any one domain of application of the theory from either the mathematical, economic, or physical side.
In every chapter we have attempted to avoid any discussion of deeper results requiring either more advanced training on the part of the reader or more high-powered analytic argumentation. Occasionally, as in Chapter VI and Chapter IX, we have not hesitated to waive rigorous discussion and proceed in a frankly heuristic manner.
In a contemplated second volume on a higher mathematical level, we propose to rectify some of these omissions, and present a number of topics of a more advanced character which we have either not mentioned at all here, mentioned in passing, or sketched in bold outline. It will be apparent from the text how much remains to be done.
In this connection it is worth indicating a huge, important, and relatively undeveloped area into which this entire volume represents merely a small excursion. This is the general study of the computational solution of multi-dimensional variational problems. Specifically we may pose the general problem as follows: Given a process with an associated variational problem, how do weutilize the special features of the process to construct a computational algorithm for sclving the variational problem ?
Dynamic programming is designed to treat multi-stage processes possessing certain invariant aspects. The theory of linear programming is designed to treat processes possessing certain features of linearity, and the elegant “simplex method” of G. Dantzig to a large extent solves XVI

PREFACE
the problem for these processes. For certain classes of scheduling processes, there are a variety of iterative and relaxation methods. In particular, let us note the methods of Hitchcock, Koopmans, and Flood for the Hitchcock-Koopmans transportation problem, and the “flooding technique” of A. Boldyreff for railway nets. Furthermore, there is the recent theory of non-linear programming of H. Kuhn and A. W. Tucker and E. Beale. The study of computational techniques is, however, in its infancy.
Let us now discuss briefly some pedagogical aspects of the book. We have taken as our audienceall those interested in variational problems, including mathematicians,statisticians, economists, engineers, operations analysts, systems engineers, and so forth. Since the interests of various members of this audience overlap to only a slight degree, some partsof the book will be of greater interest to one group than another.
As a mathematics text the volume is suitable for a course on the advanced calculus level, either within the mathematics department proper, or in conjunction with engineering or economics departments, in connection with courses in applied mathematics or operationsresearch.
For first courses, or first readings, we suggest the following programs:

Mathematician: Economist: Statistician: Engineer:
Operations Analyst:

Chapters I, II, HI, 1V, 1X, X Chapters I, II, III, V, [x Chapters I, II, III, 1X, X, XI Chapters I, II, III, 1X
Chapters I, II, III, V, 1X, X

Finally, before ending this prologue, it is a pleasure to acknowledge my indebtedness to a number of sources: First, to the von Neumann theory of games as developed by J. von Neumann, O. Morgenstern, and others, a theory which shows how to treat by mathematical analysis vast classes of problems formerly far out of the reach of the mathematician—andrelegated, therefore, to the limbo of imponderables—and, simultaneously, to the Wald theory of sequential analysis, as developed by A. Wald, D. Blackwell, A. Girshick, J. Wolfowitz, and others, a theory which shows the vast economyofeffort that may be effected by the proper consideration of multi-stage testing processes; second, to a numberof colleagues and friends who have discussed various aspects of the theory with me and contributedto its clarification and growth.
Many of the results in this volume were obtained in collaboration with fellow mathematicians. The formulation of games of survival was obtained in conjunction with J. P. LaSalle; the results on the optimal inventory equation were obtained together with I. Glicksberg and O.
XVI

PREFACE
Gross; the results on the continuous gold-mining process in Chapter VIII and the results in Chapter VII concerning specific bottleneck processes were obtained together with S. Lehman; a number of results obtained with H. Osborn on the connection between characteristics and Euler equations, and on the convergence of discrete gold-mining processes to the continuous versions will not appear in this volume. Nor shall we include a study of the actual computational solution of many of the processes discussed below,in which we have been engaging in conjunction with S. Dreyfus.
I should particularly like to thank I. Glicksberg, O. Gross and A. Boldyreff who read the final manuscript through with great care and made a number of useful suggestions and corrections, and S. Karlin and H. N. Shapiro who have done much valuable work in this field and from whose many stimulating conversations I have greatly benefited.
Finally, I should like to record a special debt of gratitude to O. Helmer and E. W. Paxson who early appreciated the importance of multi-stage processes and who, in addition to furnishing a number of fascinating problemsarising naturally in various important applications, constantly encouraged me in my researches.
A special note should be madehere of the fact that most of the mathematicians cited above are either colleagues at The RAND Corporation, or are consultants. Our work has been conducted under a broad research program for the United States Air Force.

Santa Monica, Caltfornia

RICHARD BELLMAN

XVll

Contents

PREFACE .

vill

Ne) NOP

CHAPTER I

A MULTI-STAGE ALLOCATION PROCESS

SECTION

1.1 Introduction .

,

] A multi-stage allocation process.

] Discussion .

;

] Functional equation approach

] Discussion .

] A multi-dimensional maximization problem.

] A “smoothing” problem .

10

]

Infinite stage approximation .

;

1]

] Existence and uniqueness theorems .

12

] Successive approximations.

16

] Approximation in policy space

.

16

] Properties of the solution—I: Convexity.

19

] Properties of the solution—II: Concavity

20

]. Properties of the solution—III: Concavity .

22

] An “‘ornery’’ example .

29

] A particular example—I.

26

] A particular example—II

28

] Approximation and stability .

29

] Time-dependent processes

30

] Multi-activity processes

,

;

dl

] Multi-dimensional structure theorems ;

,

33

] Locating the unique maximum of a concave function

34

] Continuity and memory .

37

] Stochastic allocation processes

38

] Functional equations

39

] Stieltjes integrals .

,

40

Exercises and research problems

40

Bibliography and comments

o9

XIX

CONTENTS

CHAPTER II

A STOCHASTIC MULTI-STAGE DECISION PROCESS

SECTION

2.1 Introduction. ..... Soe ee ee ee ee ee ee) «66M

2.2 Stochastic gold-mining. .... tee eee ee ee ee) «661

2.3 Enumerative treatment .........2... 2... 62

2.4 Functional equation approach .........4.2... «63

2.5 Infinite stage approximation ............. . 63

2.6 Existence and uniqueness ............... 64

2.7 Approximation in policy space and monotone convergence . 60

2.8 Thesolution ..... 6)

2.9 Discussion... .. 1. 1. ee ee ee eee ee ee «69

2.10 Some generalizations ...........4.. ... 69

2.11 The form of f(x,y)... 2...

tee ee ee 6

2.12 The problem for a finite number of stages toe ee ee ew ew TD

2.13 A three-choice problem ...... 2... 6 ee eee 74

2.14 Astability theorem ..............0424. . 16

Exercises and research problems ........ wee 47

Bibhography and comments .......... wee. 19

CHAPTER III

THE STRUCTURE OF DYNAMIC PROGRAMMING PROCESSES

3.1 Introduction... .

Done eee ee ee ew we) «BI

3.2 Discussion of the two preceding processes ........ 81

3.3 The principle of optimality. ....

.. . 883

3.4 Mathematical formulation—I. A discrete deterministic process 83

3.5 Mathematical formulation—II. A discrete stochastic process 85

3.6 Mathematical formulation—III. A continuous deterministic

process ...

Soe ee ew ee ee ew ew 8B

3.7 Continuous stochastic processes. toe ee ee eee we BT

3.8 Generalizations...

Doe ee ee ee . . 87

3.9 Causality and optimality . Done eee eee ee ee ee BT

3.10 Approximation in policy space ............. 88

Exercises and research problems ............ 90

Bibliography and comments ............. . 115

XX

CONTENTS
CHAPTER IV
EXISTENCE AND UNIQUENESS THEOREMS SECTION
4.1 Introduction. ..............2.2.0 084 116 4.2 <A fundamental inequality Boe ee ee ee 117 4.3 Equations of typeone...............084 119 4.4 Equations of typetwo. ............. 20084 121 4.5 Monotone convergence. ......... 26.28.2800. 122 4.6 Stability theorems. ............ 50.0888. 123 4.7 Some directions of generalization ........4.... 124 4.8 An equation of the third type. ..........4... 125 4.9 An ‘optimal inventory” equation. ...... ww ee . 129
Exercises and research problems ............ 132 Bibliography and comments .........2..0442-. 151

CHAPTER V

THE OPTIMAL INVENTORY EQUATION

5.1 Introduction... .... 1. ee ee ee 152

5.2 Formulation of the general problem. .......... 153

A. Finite total time period. . ..........2.42.. 154

B. Unbounded time period—discounted cost... .... 156

C. Unbounded time period—partially expendable items . . 156

D. Unbounded time period—oneperiod lag in supply . . . 156

E. Unbounded time period—two period lag ...... . 157

5.3 Acsimple observation .........2.4.. 2... . 157

5.4 Constant stock level—preliminary discussion... .. . . 158

5.5 Proportional cost—one-dimensional case. . ....... 159

5.6 Proportional cost—multi-dimensional case ........ 164

o.7 Finite time period. . .... 1... ee ee ee ee 166

5.8 Finite trme—multi-dimensional case. . ......... 169

5.9 Non-proportional penalty cost—red tape. ........ 169

5.10 Particular cases. 2. 2 1

ee ee 171

5.11 The form of the general solution Se ee ee ee ee 171

9.12 Fixed costs. 2... 2.

ee ee 172

5.13 Preliminaries to a discussion of more complicated policies . 173

5.14 Unbounded process—one period time lag. . ....... 173

9.15 Convex cost function—unbounded process ........ 176

Exercises and research problems .........2... 178

Bibliography and comments ............-. 182

XXI

CONTENTS CHAPTER VI BOTTLENECK PROBLEMS IN MULTI-STAGE PRODUCTION PROCESSES SECTION 6.1 Introduction. .................08.8. 183 6.2 A general class of multi-stage production problems ... . 184 6.3 Discussion of the preceding model. ........2.2.. 187 6.4 Functionalequations ........2.2.2.2...8848 188 6.5 Acontinuous version ...........2.2 880848 189 6.6 Notation. . 2... 1... ee ee ee ee 191 6.7 Dynamic programming formulation. .......... 192 6.8 The basic functional equation. ............. 192 6.9 The resultant nonlinear partial differential equation. . . . 193 6.10 Application of the partial differential equation ...... 193 6.11 A particularexample ..............2048-4. 194 6.12 Adual problem ................2. 0408-4 197 6.13 Verification of the solution givenin§10......2... 200 6.14 Computational solution ...........2.2.4.2.2.. 202 6.15 Nonlinear problems... ..........2.20884 203 Exercises and research problems ............ 204 Bibliography and comments ..........2..246+4 205
CHAPTER VII BOTTLENECK PROBLEMS: EXAMPLES Introduction... .. 1... ee ee ee ee ee 207 Preliminaries. . .. 2... 0... ee ee we ee es 209 Delta-functions. . .. 1... ee ee ee 211 The solution... . 1... ee ee ee ee ee 211 The modified w solution. ......4.2.2.2.22..20e848.8 214 The equilibrium solution. . ........... 846-4 215 A short-time w solution ........2.2.2.24288848 217 Description of solution and proof. .......4.2.4.. 218 Bibliography and comments ..........2.2.2.. 221
CHAPTER VIII A CONTINUOUS STOCHASTIC DECISION PROCESS Introduction... 1... 1 1 we ee ee ee es 222 Continuous versions—I: A differential approach’. .... 223 Continuous versions—II: An integral approach. ..... 224

NQWN NNN AA COND OTR & DS =

CONTENTS
SECTION 8.4 Preliminary discussion. . ...... 1... ee ee ee 224 8.5 Mixing atapoint..............2 0.020084 226 8.6 Reformulation of the gold--mining process ........ 227 8.7 Derivation of the differential equations ......... 227 8.8 The variational procedure ...........2..2.48. 228 8.9 The behavior of Ki... .. 1... 2. eee ee 229 8.10 The solution forT =co............2048. . 230 8.11 Solution for finite total time ..........4.2.. . 231 8.12 The three-choice problem ...........2.4.. . 233 8.13 Some lemmas and preliminary results .......... 234 8.14 Mixed policies . ..........-22.24. wee 285 8.15 The solution for infinite time, D >0O0 .......2.2.. 236 8.16 D<0........0..224. Loe ee ee 240 8.17 The caserzs =r, . 2. 1 ee ee ee ee ee 241 8.18 Nonlinear utility—two-choice problem. ......... 242 Bibliography and comments ............4.-. 244

OO wo

Oo

Co ©

CO

mow Oo OO OW DM OH WO © CO

CHAPTER IX

Oo OF me © BDO =

A NEW FORMALISM’'IN THE CALCULUS OF VARIATIONS Introduction... 2... . 1 ee ee ee 245 Anew approach ....... 2... ee ee ee ee 246

Mayx |J,O F(x,y)dt 2... 0 ee ee ee ee ee 248

Discussion. .......4.4.424. we ee ee ee 251

The two,-dimensional case . ...... 4. ee ee ee 251 Mayx |JO F(x, y)dt 2... ee ee ee 252

.7 Mayx |JOT F (x, y) dt under the constraintO<y<in ... . 253

.8 Computational solution .......2..2... . ee « 254

.9 Discussion... . 1. 1 ee ee ee ee ee 255

.10 Anexample ..... 1... ee ee ee ee ee 256

.11 A discrete version. . . ... 1. ee ee ee ee ee 258

.12 A convergence proof. . . . 2... 6 6 eee ee es 260

13

Mayx

|T JO

F(x,y,f)dt

0... 263

.14 Generalization and discussion. . ......... . . . 264

.15 Integral constraints . . 2... 2 1 ee eee ee ee 265

.16 Further remarks concerning numerical solution. . . .. . 266

.17 Eigenvalue problems ......... 2-22-2486. 267

XX111

Woe OO © OO CO

CONTENTS
SECTION
9.18 The first formulation .19 An approximate solution . 20 Second formulation . .21 Discrete approximations. 22 Successive approximation .23 Monotone approximation. 24 Uniqueness of solution . 20 Minimum maximum deviation Exercises and research problems Bibhography and comments

CHAPTER X

MULTI-STAGE GAMES

10.1 Introduction .

10.2 A single-stage discrete g§ ame

10.3 The min-max theorem .

10.4 Continuous games.

10.5 Finite resources.

10.6 Games of survival .

10.7 Pursuit games

10.8 General formulation .

10.9 The principle of optimality and functional equations.

10.10 More general process

10.11 A basic lemma .

,

10.12 Existence and uniqueness

10.138 Proof of results .

.

10.14 Alternate proof of existence

10.15 Successive approximations in general

10.16 Effectiveness of solution .

10.17 Further results .

10.18 One-sided min-max .

Lo

10.19 Existence and uniqueness for games of survival |

10.20 An approximation.

Lo

;

10.21 Non-zero-sum games — games of survival

10.22 An approximate solution .

Loe

10.23 Proof of the extended min-max theorem .

10.24 A rationale for non-zero sum games.

Exercises and research problems

Bibhography and comments

XX1V

. 268 . 269 . 270 . 270 . 27) . 272 . 273 . 273 . 274 . 282 . 283 . 284 . 285 . 286 . 287 . 287 . 288 . 289 . 291 . 292 . 294 . 295 . 297 . 299 . 300 . 300 . 302 . 302 . 303 . 306 . 307 . 308 . 3808 . 3809 . 310 . 315

CONTENTS

CHAPTER XI

MARKOVIAN DECISION PROCESSES

SECTION

11.1 Introduction. ..........2..204.2.

11.2 Markovian decision processes . .

11.3 Notation.

11.4 A lemma.

toe ee eee

11.5 Existence and uniqueness—I . .

11.6 Existence and uniqueness—II

11.7 Existence and uniqueness—III .

11.8 The Riccati equation

Lo

11.9 Approximation in policy space .

11.10 Discrete versions .

11.11 The recurrence relation

11.12 Min-max. ..............8.0848

11.13 Generalization of a Von Neumann result. .....

Exercises and research problems

Bibliography and comments

Index of applications

Name and subject index .

. 3l7 . d3l7 . . 319 . 320 . . dal . d24 . 325 . . 325 . . 326 . . 328 . . 330 . . 332 . 332 . dd4 . . 336 . 338 . 339

AXV

DYNAMIC PROGRAMMING

CHAPTER I
A Multi-Stage Allocation Process
§ 1. Introduction In this chapter we wish to introduce the reader to a representative class of problems lying within the domain of dynamic programming and to the basic approach weshall employ throughout the subsequent pages. To begin the discussion we shall consider a multi-stage allocation process of rather simple structure which possesses manyof the elements common to a variety of processes that occur in mathematical analysis, in such fields as ordinary calculus and the calculus of variations, and in such applied fields as mathematical economics, and in the study of the control of engineering systems. We shall first formulate the problem in classical terms in order to illustrate some of the difficulties of this straightforward approach. To circumvent these difficulties, we shall then introduce the fundamental approach used throughout the remainderof the book, an approach hased upon the idea of imbedding any particular problem within a family of similar problems. This will permit us to replace the original multidimensional maximization problem by the problem of solving a system of recurrence relations involving functions of much smaller dimension. As an approximation to the solution of this system of functional equations we are lead to a single functional equation, the equation
(1) f(x) = Max [g(y) +h(x—y) + flay + 6 (x —y))]. Osy<a2
This equation will be discussed in some detail as far as existence and uniqueness of the solution, properties of the solution, and particular solutions are concerned.
Turning to processes of more complicated type, encompassing a greater range of applications, we shall first discuss time-dependent processes and then derive some multi-dimensional analogues of (1), arising from multi-stage processes requiring a number of decisions at each stage. These multi-dimensional equations give rise to some difficult, and as yet unresolved, questions in computational analysis.
In the concluding portion of the chapter we consider somestochastic 3

A MULTI-STAGE ALLOCATION PROCESS
versions of these allocation processes. As we shall see, the same analytic methods suffice for the treatment of both stochastic and deterministic processes.

§2. A multi-stage allocation process Let us now proceed to describe a multi-stage allocation process of simple but important type. Assume that we have a quantity x which we divide into two nonnegative parts, y and x —y, obtaining from the first quantity y a return of g (y) and from the second a return of h (x — y).1 If we wish to perform this division in such a way as to maximizethetotal return we are led to the analytic problem of determining the maximum of the function

(1)

R, (x, vy) = gly) +h (x—y)

for all y in the interval [0, x]. Let us assume that g and / are continuous functions of x for all finite x > 0 so that this maximum will alwaysexist.
Consider now a two-stage process. Suppose that as a price for obtaining the return zg (y), the original quantity y is reduced to ay, where a is a constant between 0 and 1, O<.a <1, and similarly x — y 1s reduced to b(x — y), O<b <1, as the cost of obtaining (x — y). With the remaining total, ay + b(x —~y), the process is now repeated. We set

(2)

ay + b(x—y) = % = ¥,4+ (41 —N),

for 0 < y, < %,, and obtain as a result of this new allocation the return g (vi) + h(x,—y,) at the second stage. The total return for the twostage process is then

(3)

Re (%, ¥, V1) = gly) + h(x —y) + 8 (v1) + 4 (41 — 41)

and the maximum return is obtained by maximizing this function of y and y, over the two-dimensional region determined by the inequalities

(4)

a. 0O<y<x

b O<y,< x

Let us turn our attention now to the N-stage process where we repeat

1 The units of the return are, in this case, different from the units of x. Thus, for example, + may bein dollars, and g (y) may be man-hoursof service from machines purchased with the y dollars. In other cases, occurring in multi-stage investment problems, or multi-stage production problems, this will not be so, in that the units of the return will be the sameas that of the resources, or a mixture of both situations will occur. We are considering the simplest case here.
4

A MULTI-STAGE ALLOCATION PROCESS

the above operation of allocation N times in succession. The total return from the N-stage process will then be

(5)

Ry (%*,¥, Vare++, n-1) = &(¥) + A(x —y) + 8 (Nn)

+ h(%,y— 41) +... + g (yw—-1) + A (xn -1— yw -1),

where the quantities available for subsequent allocation at the end of the first, second, ..., (NW — l)st stage are given by

(6) % Xe

=ay+b(x—y),0<yxx, = ay, + b(%,—), OS WH,

XN-1 = Ayn—-2+ 0 (xn-~2—Yn-2),
O< yn-2<%Nn-2, O<yn-1< %N-1
The maximum return will be obtained by maximizing the function Ry over the N-dimensional region in the space of the variables y, y,,..., yn -1, described by the relations in (6).
§ 3. Discussion In setting out to solve this problem, the temptation is, quite naturally, to use calculus. If the absolute maximum occursinside the region, which is to say if all the y; satisfy the strict inequalities 0 < yi < x, and if the functions g(x) and A(x) possess derivatives, we obtain for the determination of the maximizing y; the system of equations, (1) g’ (vw-1) —M (4n-1 — yn-1) = 0
g’ (yw ~2) — fh’ (4n~2 — Vn -2) + (a — Ob) hh’ (xn -1— yn -1) = 0

g’ (vy) + A’ (x — y) + (a@— Ob) h(4, — yi) +... = 0,7
upon taking partial derivatives. However, in the absence of this knowledge, since we are interested not in Jocal maxima, but in the absolute maximum, we must also test the boundary values y; = 0. and x:, and all combinations of boundary values and internal maxima. Furthermore, if the solution of the equationsin (1) is not unique, we must run through a set of conditions sufficient to ensure our having a maximum and not
D

A MULTI-STAGE ALLOCATION PROCESS
a minimum or a mere local maximum.It is evident that for problems of large dimension, whichis to say for processes involving a large number of stages, a systematic procedure for carrying out this program 1s urgently required to keep the problem from getting out of hand.
Suppose that we abdicate as an analyst in the face of this apparently formidable task and adopt a defeatist attitude. Turning to the succor of modern computing machines, let us renounce all analytic tools. Consider, as a specific example, the problem posed by a 10-stage process. Then, if we wish to go about the determination of the maximum in a rudimentary fashion by computing the value of the function Rig = Rio (V, V1, - ++, Vo) at Suitably chosen lattice points, we may proceed to divide all the intervals of interest, O< y<ix,0<y,<%,..., O< Vo < %o, into, say, ten parts, and compute the value of Ry) at each of the 10° points obtained in this manner. 10!° is, however, a number that commandsrespect. Even the fastest machine available today or in the near future, will still require an appreciable time to determine the solution in this manner.
To give someidea of the magnitude of 10'°, note that if the machine took one second for the calculation of R,, at a lattice point, storage and comparison with other values, the computation of 101° values would require 2.77 million hours; if one millisecond, then 2.77 thousand hours; if one
micro-second, then 2.77 hours. This last seems fairly reasonable. Observe, however, that 1f we consider a 20-stage process, we must multiply any such value by 10? 1e., 102° = 10!°- 1019,
Needless to say, there are various ingenious techniques that can be employed to cut this time down. Nonetheless, the method sketched above is still an unwieldy and inelegant method of attack.
Furthermore, it should be realized that if we are sufficiently interested in the solution of the above decision process to engage in computations, we will, in general, wish to compute the answernot only for one particular value of x, but for a range of values, not only for one set of values of a and 6 but for a set of values, and not only for one set of functions g and h, but for a class of functions. In other words, we will perform a sensitivity analysts or stability analysis of the solution. Any suchsensitivity analysis attempted by the above methods will run into fairly large computing times.
One of the aspects of the situation viewed in these terms which is really disheartening is that this problem is, after all, only the consequence of a very, almost absurdly, simple version of an applied problem. It is clear that any modification of the problem in the direction of realism, say subdivision of x into more than two parts, which is to say an increase in the number of activities we can engage in, or an increase
6

A MULTI-STAGE ALLOCATION PROCESS
in-the types of resources, will increase the computing time at an exponential rate.
Furthermore, as we have pointed out in the Preface, we must realize that the essential purpose in formulating many of these mathematical models of the universe, economic, physical, biologic, or otherwise, is not so much to calculate numbers, which are in manycases of dubious value because of the lack of knowledge of some of the basic constants and functions involved, but rather to determine the stvucture of the solution. Concepts are, in many processes, more important than constants.
The two, however, in general go hand-in-hand. If we have a thorough understanding of the process, we have means, through approximation techniques of various sorts, of determining the constants we require. Furthermore, in the processes occurring in applications, of such enormous complexity that trial and error computation is fruitless, it is only by having an initial toe-hold on the solution that we can hope to use computing machines effectively.
Going back to the idea of the intrinsic structure of a solution, we may ask whatit is that we really wish to know if we are studying a process of this type. Naturally, we would like to obtain the point(y, yi, ..., yn) at which the maximum occurs, and any solution must furnish this. But from the point of view of a person carrying out the process, all that is really required at any particular stage of the process is the value of y in terms of x, the resources available, and N, the number of stages ahead; that is to say, the allocation to be made when the quantity available is x and the numberof stages of the process remaining is N. Viewed as a multi-stage process, at each stage a one-dimensional choice is made, a choice of y in the interval [0, x]. It follows 2 that there should be a formulation of the problem which preserves this dimensionality and saves us from becoming bogged down in the complexities of multidimensional analysis.
§ 4. Functional equation approach
Taking this as our goal, namely the preservation of one-dimensionality, let us proceed as follows. We first observe that the maximum total return over an N-stage process depends only upon WN and the initial quantity x. Let us then define the function,
(1) fw (x) = the maximum return obtained from an N-stage process starting with an initial quantity x, for N=1,2,..., and x > 0.
2 As an application of the useful principle of wishful thinking.

A MULTI-STAGE ALLOCATION PROCESS

We have

(2) with

Jw (x) = Max Ry(x, y, ..., yn-1), N = 2, 3, .. {y, vs}

(3)

fi(%) = Max [g(y) + A(x —y)]j.

O<y<r

Ourfirst objective is to obtain an equation for f, (x) in termsof f, (x).
Considering the two-stage process, we see that the total return will be the return from the first stage plus the return from the second stage, at which stage we have an amount ay + b (x — y) left to allocate. It is clear that whatever the value of y choseninitially, this remaining amount, ay + b(x—vyy), must be used in the best possible manner for the re-
maining stage, if we wish to obtain a two-stage allocation which maximizes.
This observation, simple as it is, is the key to all of our subsequent mathematical analysis. It is worthwhile for the reader to pause here a moment and make sure that he really agrees with this observation, which has the deceptive simplicity of a half-truth.
It follows that as a result of an initial allocation of y we will obtain a total return of f, (ay + b (x —y)) from the second stage of our two stage process, if y, is chosen optimally. Consequently, for the total return from the two stage process resulting from the initial allocation of y, we have the expression

(4)

Rz (x, ¥, V1) = B(y) th(x—y) + fi (ay + 0 (x — y)).

Since y is to be chosen to yield the maximum of this expression, we derive the recurrence relation

(5) fe (x) = O<Myasxe [g(y) + 4(*—y) + fi (ay + 6 (x — y))],

connecting the functions f, (x) and f,(x). Using precisely the same argumentation for the N-stage process, we obtain the basic functional equation

(6) fiw (x) = Max [g (y) + 4(%—y) + fw-1 (ay + 6 (x — y))] O<sys2
for N > 2, with f, (x) defined as in (3) above. Starting with f, (x), as determined by (3), we use (6) to compute f/f,(x),
which, in turn, repeating the process, yields /; (x), and so on. At each step of the computation, we obtain, not only fx (x), but also yz (x), the optimal allocation to be made at the beginning of a k-stage process, starting with an amount x.

8

A MULTI-STAGE ALLOCATION PROCESS
The solution, then, consists of a tabulation of the sequence of functions
{yx (x)} and { fx (*)} forx >0,k = 1,2,....

Given the sequence of functions {y; (x)}, the solution of a specific problem, involving a given N and a given x has the form

Se] SS] SS]

(7)

yN (x),

1 yN- 1(ay + b (x — ¥)),

2

yN- 2 (ay, + 6 (x4 — V1)),

YN-1 = 491 (ayn—2 + b(xn-~2—¥ n-2)), where (¥, V1, -.-, Yn -1) is a set of allocations which maximizesthetotal N-stage return.
A digital computer may be programmedto print out the sequence of values Y, V1, -.., Yn -1, in addition to tabulating the sequences { f; (x)}
and {yx (x)}.
§ 5. Discussion The important fact to observe is that we have attempted to solve a maximization problem involving a particular value of x and a particular value of N by first solving the general problem involving an arbitrary value of x and an arbitrary value of N. In other words, as we promised in the first section, we have imbedded the original problem within a family of similar problems. Weshall exploit this basic method of mathematical analysis throughout the book. Whatare the advantages of this approach? In thefirst place, we have reduced a single N-dimensional problem to a sequence of N onedimensional problems. The computational advantagesof this formulation are obvious, and weshall proceed in the next sections to show that there are analytic advantages as well, as might be suspected. As weshall see, we will be able to obtain explicit solutions for large classes of functions g and h, which can be used for approximation purposes. This point will be discussed again below. Furthermore, we will be able to determine many important structural features of the solution even in those cases where we cannot solve completely. The utilization of structural properties of the solution and the reduction in dimension combine to furnish computing techniques which greatly reduce the time required to solve the original problem. Weshall return to this point in connection with some multi-dimensional versions.

A MULTI-STAGE ALLOCATION PROCESS § 6. A multi-dimensional maximization problem Before proceeding to a more detailed theory of the processes described above, let us digress for a moment and briefly present two further applications of the general method. For the first application, consider the problem of determining the maximum of the function

(1)

F

(x1,

%2,

...,%Nn)

N
== i=2Xl gi

(xi)

,

over the region defined by

(2)

(a) %, +X. t+... + XN =C,

(b) x; > 0.

Each function g; (x) 1s assumed to be continuousfor all x > 0. Since the maximum of F depends only upon c and N, let us define the sequence of functions

(3)

fn (c) = Max F (x1, %2,..., XN),

{ti}

for c >0 and N=1,2,.... Then, arguing as above, we have the recurrence relation

(4)

fn (c) = Max [gn (x) + fv-1(¢ —)],

O<a<e

for N = 2,3,..., with

(5)

Si (¢) = & (c).

§7. A “smoothing” problem As the second application, let us consider the problem of determining the sequence {xx} which minimizes the function

I Ms
I Mo

(1)

F (%1, %2, ..+, 4N) =
k

1Ba (Xe -— 1) + k

ht (Xx — Xe -1). I

Here {7x} is a given sequence, %» = c a given constant, and we assume that the functions gz (x) and A; (x) are continuous for all finite x, and that gx (x), Ax (x) > co as | x | 0.
The genesis of this problem, explaining its name, will be discussed in the exercises.

10

A MULTI-STAGE ALLOCATION PROCESS

Let us define the sequence {fr (c)}, R = 1, 2,..., N, by the property that fr (c) is the minimum overall pr, ¥r+1, ..., *n of the function

N

N

(2)

Fr= k=2R ge (XE — 7x) + ka =R Ni (Xk — Xe -1),

where *rR-1 = C.

We have

(3)

fx (c) == Min [gn (x — rw) + hy (x — )],

and

(4)

fr (c) = Min [gr (x —rr) + he(x—c) + frsi(%)),

for R=1, 2,...,N—1.

§ 8. Infinite stage approximation

Let us now return to the allocation process. The treatment we present here serves as a prototype for the discussion of a number of multi-stage processes, of diverse origin, but similar analytic structure.
If N is large, it is reasonable to consider as an approximation to the N-stage process, the infinite stage process defined by the requirement that the process continue indefinitely. Although an unbounded process is always a physical fiction,? as a mathematical process it has many attractive features. One immediate advantage of this approximation lies in the fact that in place of the sequence of equations given by (4.6), we now havethe single equation

(1)

f(x) = O<Mya<xuz[g(y) +A(x—y) + flay + 6 (x —y))]

satisfied by f(x), the total return of the process, with a single allocation function y = y (x), determined by the equation.
To balance this, we encounter manyof the usualdifficulties associated with infinite processes. It is, first of all, no longer clear that a maximum exists rather than a supremum. This is to say, there may be noallocation policy which actually yields the total return f(x). Furthermore, if we wish to employ (1) in an unrestricted fashion to determine properties of the infinite process, we must show that it possesses no extraneous solutions. In other words, we must establish existence and uniqueness theorems if this equation is to serve a useful purpose.

3 We shall occasionally use the word “‘physical’’ to describe the ‘‘real’’ world. It should be interpreted to mean economic, biological, engineering, etc., depending upon the background and interests of the reader.
1]

A MULTI-STAGE ALLOCATION PROCESS
§ 9. Existence and uniqueness theorems The result we obtain in this section is actually a special case of a more general result we shall derive in a later chapter. Repetition, however, no matter how dismaying as a social or literary attribute, is no great mathematical sin, and it is important to present the simpler case first, enabling the basic ideas to appear unimpeded bytechnicalities of lesser import.
Let us now demonstrate

THEOREM 1. Let us assume that
(1) a. g(x) and h(x) are continuous functions of x for x > 0, g (0) = A(0) = 0.

b. If m(x) = Max Max (/g(y) |, |A(y) |), and O<sy<z c = Max (a, b), then & m(c® x) < co forallx > 0. n=0
c Ox<a<l1O<db<l.

Under these assumptions, there is a unique solution to (8.1) which ts continuous at x = 0, and has the value 0 at this point; moreover, this function 1s continuous.
Before proceeding to the proof, let us digress for a moment and consider the important special case where g and / are both non-negative. The sequence {fw (x)} as given by (4.6) is a monotoneincreasing sequence, with boundedness a consequence of condition (1b), as we shall show below in a moment. Consequently, for all x > 0, fw (x) converges to a function f(x) as N -> oo.
Let us show that this function satisfies the equation

(2)

f(x) = Sup [g(y) + h(x—y) + flay + 0 (x — y))].

O<y<2

To simplify our notation, let us set

(3)

T(fiy)=ely) +h(x—y) + flay + b(x—y)).

The basic recurrence relation is then

(4)

fu +1(%) = Max T (fn, 9).

O<y<z

12

A MULTI-STAGE ALLOCATION PROCESS

From (4) we obtain as a consequence of the monotonicity in N>

(5)

f(x) 2 Max T(fy,y).

O<y<2z

For any y in the interval (0, x], this means that the inequality

(6)

f (x) = (fy, y)

holds. Letting N —oo, this yields

(7)

f(x) =2T(fy)

for all y in [0, x], which, in turn, leads to the result

(8)

f(x) = Sup T(f,y).

Osy<2

We cannot write Max since we have no guarantee that the
O<ysz
limit function f (x) is actually continuousas a function of x. On the other hand, from (4) we also obtain

(9)

fu+i(%)< Sup T(f,y),

Osy<cz

for all N, and thus

(10)

f(*)< Sup T(f,y).

OsyS2

Comparing (8) and (10), we obtain (2). One of the defects of this proof based solely upon monotonicity is
that it does not yield the continuity of the limit function, a result which implies the existence of an optimal policy. This optimal policy is a function y (x) which yields the maximum in

(11)

f(x) = OSMyasxz T(f,y),

when the maximum exists. The existence of an optimal policy for the infinite process is directly
of no particular importance computationally, or as far as applications are concerned. It is, however, of great importance in connection with the determination of the structure of optimal policies for the infinite process. Thus, indirectly, the question of the existence of continuous solutions is significant as far as numerical results are concerned, since the solution of the infinite process can be used as an approximation to the solution of the finite.
In order to establish the existence and uniqueness of a ‘continuous

13

A MULTI-STAGE ALLOCATION PROCESS
solution of (11), we shall employ a technique that is applicable to a large class of equations of this type, the method of successive approximations. Weshall, however, encounter monotonicity arguments again in later chapters.
Turning to the recurrencerelations in (4), let us begin with the observation that f; (x) is continuousfor all x > 0 by virtue of the assumptions made concerning g(x) and A(x). It follows, inductively, that each element of the sequence { fy (x)} is continuous. It is worth pointing out, however, that the location of the maximizing y need not depend continuously upon x. In other words, the policy is not necessarily a continuous function of x. An example of this is given in § 15.
Let yn (x) be a value of y which yields the maximum in (4). It is a matter of indifference as to which value of y we choose, if there is more than one value producing the maximum. Then we have

(12)

fy +1 (%) = T (fy, yy),

Jn +2 (x) =m T (fw +1, YN +1),

and, as a consequence of the maximum property of the yy, the further inequalities

(13)

fy+1(*) = T (fy, yn) > T (fy, yn +1)

In +2(%) = T (fne3, Yve+1) > T (fn 41, yy).

These, in turn, yield

(14)

fn +1 (x) —fnw+2 (x) > T (fn, vv 41) — T (fn +1, yn 4-1)

<T (fn, yu) — T (fw +1, Vn)

The two inequalities combined yield the important estimate

(15) | fv +1 (x) —f +2 (%) |< Max [|T (fw, yw +1) —T (fy+1, yn +1) |, |T (fx, yw) — T (fy +1, yw) |].

Turning to the definition of TJ (f, y) given in (3), we see that

(16)

| T (fx, yw) — T (fw +1, yw)|

= | fv (ayn + 0 (x — yw)) — fu +1 (ayn + 6 (x — yy)) |

Let us now define

(17)

un (x) = Max fw (z) —fv+i(z)|,N=1,2,...

O<z<z

14

A MULTI-STAGE ALLOCATION PROCESS

Since ay + b (x — y) <cx for all y in (0, x], the relation in (16) yields

(18)

Un +1(%) < un (cx).

It remains to estimate u, (x). We have, referring to the equations for f, (x) and f, (x), the relation

(19)

| fx (*) — fe (x) |< Max [| fi (@y. + 6 (% —y,) |,

fi (42 + 6 (% — ye) |] < m (cx),

using the definition of m (x) given in (1b). Hence we see that , (x) < m (cx), and thus, using (18), that uy (x)
<m(c¥ +1 x). By virtue of our assumption concerning m (x) it follows

thatN=21uy (x) converges for all x, and what is important, uniformly in any finite interval. The limit function f(x) = lm fy (x),in consequence,
N— oo
exists and is continuousfor all x. Furthermore, the uniformity of convergence ensures that f(x) is a solution of (8.1).
It remains to establish the uniqueness of the solution. Let F (x) be any other solution which exists for all x and is continuous at x = 0, with F (0) = 0.
In the equation

(20)

f(x) = Max T(f,y),

O<y<2

let y = vy (x) bea value of y which yields the maximum, andlet w = w (x) play the similar role in

(21)

F (x) = Max T(F,y).

Osyxsrz

Then, as above, we obtain the two inequalities,

(22)

f(*)=T(fy) 2 (fw)

F(x) =T(F,w) =T(F, y),

and, as before, this leads to

(23) | f(x) —F (x) |< Max[|T(f,y)—T(F,y) |, |T(fe)—T(F, 2)|). < Max[|f(ay + 6(%—y)) —F (ay + b(x—y)) |,

|f (aw + b(x—w)) —F (aw + b(x —w)) |].

Let us now define

(24)

u(x) = Sup |f(z)—F() |.

O<z<2

15

A MULTI-STAGE ALLOCATION PROCESS

Since f(x) 1s continuousfor all x > 0 and F (x) is, by assumption, continuous at x = 0, we see that u(x) is continuous at x = O and has the value 0 there.
From (23) we obtain

(25) whence, by iteration,

u(x) <u (cx),

(26)

u(x) << u(cN x),

for all N > 1. Since u(x) is continuous at x = 0 and uw (0) = 0, upon letting N — oo we obtain uw (x) < 0, and thus that f(x) = F (x). This completes the proof of the existence and uniqueness of a solution of the functional equation associated with the infinite process.

§ 10. Successive approximations

In considering the equation

(1)

f(x) = Max T(f,y),

O<sy<cz

we have shownthat a particular sequence of successive approximations

converged to the unique solution which is continuous at x = 0 and

zero there. It is important for both analytic and computational purposes

to know that actually any sequence whose initial function satisfies

certain simple requirements converges.

The methods we have used above may also be employed to prove the

following

THEOREM 2. Let f, (x) satisfy the following conditions:

(1)

a. fo (x) 18 continuous for x > 0.

b. fo (0) = 0.

Then, tf the conditions of Theorem I are fulfilled, the sequence defined by

(2)

fn+1(%) = Max T(fn,y),N =0,1,...,

Osy<z

converges to the solution f (x) obtained above, uniformly in any finite interval.

§ 11. Approximation in policy space

We have employed above the classical technique of successive approximations in order to obtain a solution to the nonlinear functional equation

(1)

f(x) = Max T(f,y).

O<y<2z

16

A MULTI-STAGE ALLOCATION PROCESS
We now wish to exploit a certain duality which is present in these decision processes to show that we can choosethe initial approximation in such a way that we can always ensure this approximation being monotone. This means that we have uniformly better convergence with each iteration.
Let us begin by introducing some terminology. Weshall call a sequence of allocations; 1.e., a sequence of admissible choices of y, a policy, and a policy which yields f(x) an optimal policy.
The duality that exists in the theory of dynamic programmingarises from the interconnection between the functions f(x) which measure the maximum return and the policies which yield these maximum returns. Actually a policy is a function, since a policy is a determination of y as a function of x. It is worthwhile nonetheless to preserve this terminology since it possesses certain advantages derived from intuition. If the policy is not unique, y will not be a single-valued function of x.
It follows from the functional equation that a knowledge of f (x) yields y (x), and conversely any y (x) determines f(x), iteratively by means of the functional equation

(2)

f(4)=T (fy (x).

Thus, for example, if the optimal policy consisted of the choice y = 0 continually, f(x) would satisfy the functional equation

(3)

f(x) = h(x) + f(x),

which would yield the result

(4)

f(x) = 2 Alo»x).

As we have mentioned above, the purpose of ourinvestigation is not so much to determine f(x), which is really a by-product, but more importantly, to determine the structure of the optimal policy, which is to say to determine y (x).
This leads to an important anduseful idea. Just as we can approximate in the space of the functions f(x), so we can approximate in the space of policies, y (x). Furthermore, in many ways, this is a more natural and simpler form of approximation. The advantage of this type of approximation analytically is that it always leads to monotone approximations. From the standpoint of applications, it 1s by far the more natural approximation since it is usually the one part of the problem about which a certain amount is known as a result of experience.
Let yo (x) be an initial guess for an optimal policy and let f, (x) be 17

A MULTI-STAGE ALLOCATION PROCESS

the return function derived from this policy function, which is to say that f. (x) satisfies the functional equation

(5)

fo (x) = L (fo, Vo (%)),

an equation which wesolve iteratively. To improve yo (x), we determine y,(x) as a function of x which maximizes T (fo, y) for O<y< x. Assume for the momentthat y, (x) is itself continuousin x, (which need not necessarily be the case), and that the return function /, (x) computed using this policy is also continuous. This will always be the case, as we point out again below, under the assumptions we have made. We now continue in this way, generating a sequence of policies, {yn (x)}, and a sequence of return function, { fw (x)}.
It is easy to show, utilizing the methods described in the foregoing sections, that under the assumptions we have made the sequence { fy (x)} is monotone increasing. A rigorous proof of the existence and convergence of the sequences {yy (x)} and { fw (x)} described above seems difficult to obtain. Consequently, we compromise for the following.

THEOREM 3. Let fy (x) be the result of an initial approximation in policy
space, that 1s,

(6)

fo (*) = T (for vo (x),

where Vo (x) 1s any continuous function of x satisfying the conditions

(7)

OS(*%)< *.

Under the assumptions of Theorem 1, the sequence defined by

(8)

in+1(*) = Max T(fy,y),N =0,1,2,...,

Osy<ux

converges uniformly to the solution f (x) obtained, and this convergence ts monotone.

PRroor. Let us demonstrate the monotonicity, which is the essential feature, first. We have

(9)

fi(*) = Max T(fo,¥).

O<sy<z2z

Comparing the definition of /, given in (5) with the definition of f, above, we see that f, > fo for all values of x. From this it follows inductively that fyv+1 > fw for all values of x > 0.
It remains to prove the continuity of the function f, (x) for x > 0.

18

A MULTI-STAGE ALLOCATION PROCESS

The conditions upon g and / which we have imposed above show that the formal series for fo (x)

(10)

fo (x) = 8 (Yo) HA(*—Yo) + .--,

obtained iteratively, converges uniformly in any finite interval and represents a continuous function of x for x > 0, if yy (x) is a continuous function of x.

§ 12. Properties of the solution—I: Convexity Let us now show that we can derive certain structural properties of the optimal policy from various simple structural properties of the functions g and h. Thestructure of the optimal policy y (x) and that of the return function f(x) turn out to be intimately entwined. Our first result in this direction is

THEOREM 4. If, in addition to the assumptions in Theorem 1, we wmpose
the conditions that g and h be convex functions of x, then f(x) will be a convex function, and for each value of x, y will equal 0 or x.

ProoFr. The proof will be inductive. Since

(1)

fi (*) = Max (g (y) + h(x — y))

Osy<2z

and g(y) +h(x—vy) is convex as a function of y for O< y< x, it follows that

(2)

fi (*) = Max (g (x), A(x) ),

since the maximum of a convex function must occur at one of the end-points. As the maximum of two convex functions, /, (%) is convex.
Since g(y) + A(x — y) +f, (av + 6 (x —y)) is a convex function of y for y in [0, x] it follows by repetition of the above argument that

(3)

Fs (x) = Max (g (x) + fi (ax), A(x) + fi (0%)),

is a convex function of x. Wesee then, inductively that fy (x) 1s convex, and thus that the limit function f(x) 1s convex.
Turning to the equation f(x) = Max T(f,y), the convexity of f
O<y<2z
reduces this to the simpler equation ‘

(4)

f (x) = Max (g (x) + f (ax), A (x) + f (6x),

showing that y = 0 or x for each value of x. This equations is, sur-

19

A MULTI-STAGE ALLOCATION PROCESS
prisingly, still a difficult equation to solve in general. We shall consider a particular case of it below.

§ 13. Properties of the solution—II: Concavity
Let us now demonstrate that an analogous result holds for the case where g and A are both strictly concave functions of x for x > 0.

THEOREM 5. If, 1n addition, to the assumptions in Theorem 1, we impose
the conditions that g and h be strictly concave functions of x, then f (x) will be a strictly concave function of x.
In this case, the optimal policy will be unique.

Proor. Let us consider the one-stage case first, and perform some simple calculations which will show us why the result should be true, before proceeding to a rigorous proof using a different and more general technique.
We have

(1)

fi(*) = Max [g(y) +4(x—y)].

O<sy<z

Since g and / arestrictly concave functions, the function g (y) + h (« — y) is a strictly concave function of y. There is, in consequence, a single maximum, which may, nonetheless, occur at an end point y = 0 or y = x. Let us suppose for the momentthat it occurs at an interior point, and that g and / possess second derivatives. Then,

(2)

fi (*) = g (y) + A(x —y)

where y is determined as a function of x by meansof therelation

(3)

g’ (y) = hi’ (x —y).

Differentiation of (2) yields

(4) fi’ (&) = (6 (vy) — A’ (x — y)) dy|dx +h’ (x —y) =H (x —y),

and thus

(5)

fi" (%) = h" (% — y) (I — dy/dx).

Differentiating the relation in (3), we obtain

(6)

g" (y) dy/dx = h" (x — y) (1 — dy/dx),

which yields

(7)

dy/dx = h" (x — y)/(g" (y) + h" (x —y)).

20

A MULTI-STAGE ALLOCATION PROCESS
This shows that 1 > dy/dx > 0, and thus,returningto (5), thatf,” (x) <0.
If the maximumis not actually inside, we can force it to be by various modifications of the functions g and # which prevent the maximum from ever being at y = Oory = x; e.g., by addition of a term « log y (x — y), where € is a Small positive quantity. We can then proceed induc-
tively and establish the same result for all the members of the sequence fw (x). This is, however, a rather clumsy method which does not extend without pain to multi-dimensional problems. We shall therefore use a more elegant and simple method.

LemMA 1. IfG (x, y) ts a concave function 4 of x and y for x, y > 0, then f(x) as defined by

(8)

f(x) = Max (x,y)

O<ysz

ts a concave function of x for x > 0.

Proor. We have, for O<A< l,

(9)

f(Ax+(1—A)z)= — Max

G(Ax + (1—A)z,¥).

O<y<Art+(l—A)z

We mayreplace y by the quantity y = Ay, + (1 —A) v2 where y, and Vj, range independently over the intervals O< y,< 4,0<y,< 2. Then

(10) f(Ax + (L—A)2) = Max G(Ax + (1—A)z,dy. + (L—A)y2). O<4y<2 O<y¥ <2
Since G (x, y) 1s concave in x and y, we have

(11) G(Ax + (1—A) 2,Ay1 + (1—A) ve) SAG (x, yx) + (L—A) G (2, y2)

Hence

(12) f(Ax+(1—A)2) > Max [AG (x, 91) + (1—A) G (2, ¥2)]

O<y% <2 O<¥, <2

> A Max G(x,y1) +(1—A) Max G (z, y2)

O<y, <2

O<¥3 <2

= Af (x) + (L—A) f (2).

Let us now apply this lemma to prove Theorem 5. It is easily verified that g (vy) + A(x —y) is a concave function of x and y if g and A are concave functions. This shows immediately that /f, («) is concave. Similarly, since f, (ay + 6(x—y)) is a concave function of x andy, /, (x)

4 Concavity in both x and y means the G (A #, + (1 —A) %, 491 + (1—A) yq) SA G (1, V1) + (1—A) G (#5, y_), for 0 SA <1.

21

A MULTI-STAGE ALLOCATION PROCESS
as defined by the basic recurrence relation is a concave function. We thus proceed inductively and show that each function in the sequence, {fn (x)}, is a strictly concave function, and hence that the limit function is concave. That it is strictly concave follows from the strict concavity of g and A, using Lemma 1 upon the functional equation for f (x).
Once we have established the strict concavity of f(x), the uniqueness of the maximizing y and thus of the optimal policy follows immediately. This completes the proof of Theorem 5.

§ 14. Properties of the solution—III: Concavity Let us now show that the assumption of concavity enables ustotell quite a bit more about the nature of the solution.

THEOREM 6. Let us assume that (1) a. g(x) and h(x) are both strictly concave for x > 0, monotone in-
creasing with continuous derivatives and that g (0) = h(0) = 0. b. g’ (0)/(L — a) > A’ (0)/(1 — b), W’ (0) > g’ (co), b > a,
Then the optimal policy has the following form:
(2) a. y=x forO<x<x, where x ts the root of h’ (0) = g’ (x) + (6 — a) g’ (ax) + (b—a) ag’ (a? x) +...
b. y = y (x) for x > x where y(x) 1s a function satisfying the inequalities O << y (x) <x, and y (x) ts the solution of
3) ag’ (y)A(x—y) + (@—4)f'(ay + b(x —y))= 0.
Remark. We have given the solution for only one of the possible combinations of inequalities connecting g’ (0), h’ (0), 6 and a. It will be easily seen from the procedure below, that corresponding results hold for the other cases. Furthermore, the number of cases can be halved by the observation that the interchange of y and x —y results in an interchange of a and 0.

Proor. Let us employ the method of successive approximations. Set

(4)

fi(x) = Max [g(y) + 4 (*—9)].

sysuz

Since, by assumption, g’ (0) >’ (0), for small x, we have g’ (y) — h' (x —y) > 0, for y in the interval [0, x]. Hence g(y) + h(x —y) is monotone increasing in 0< y< x and the maximum occurs at y = x. As % increases, the equation g’ (y) —h’ (x —y) =0 will ultimately

22

A MULTI-STAGE ALLOCATION PROCESS

have a root at y = x, and then as ~x increases further a root inside the interval [0, x]. The critical value of x is given as the solution of g’ (x) — h' (0) = 0. This equation has precisely one solution, which wecall %,. For x > x, let y, = y, (x) be the unique solution of g’ (y) = h’ (x — y). The uniquenessof solution is a consequenceof the concavity assumptions concerning g and A, and the existence of a solution is a consequence of the continuity of g’ and h’.
Thus we have

(5)

fi (*) = g (),

O<x< Mm,

= g (vi) + A(x —y,),

x > %y.

and

(6) fi’ (~) = 8" (x),

O<% <x,

= [g’ (vi) — Ff (x — y,)] dyifdx + Wh (% — 91) = h' (x — yy),

for x > %*.

Since y; (x,) = 1, we see that /,’ (x) is continuous at x = x,, and hence, for all values of x > 0. Furthermore f/f, (x) is a concave function of x; cf. the analysis of § 11.
Now let us turn to the second approximation

(7) fo(x) = Max [g(y) +A4(x*—y) + fi (ay + 6 (x —y)))]. O<y<ur

Tb+h(exg'—c(y0r))it)i(caa(la——bf8)un)c.>t#iS'oin(n0c)ies g[’n{(o(10w)——aDA)’(y()(10=)++ga’f(—yy’5))(—0/)h(W(1a’ ——(xd8—))y=})1g'+](0f)>l’—0h(,a'v w(+0e)
see that D (y) 1s again positive for all y in [0, x] for small x. Hence the maximum occurs in (7) at y = x for small x. As x increases, there will be a first value of x where D (x) = 0. This value, x., is determined by the equation g’ (x) = A’ (0) + (6 — a) fi’ (ax). Comparing the two equations

(8)

g’ (x) = h(0)

g’ (x) = h’ (0) + (6 —a) fi’ (ax),

we see that 0 <x, < x.

Hence the equation for x, has the simple form

(9)

g’ (x) = h (0) + (6 — a) g(ax).

Thus y = x for O<x%< x, in (7) and y = y,(x) for x > %,, where
v2 (x) 1s the unique solution of

(10)

ge’ (vy) = A(x — y) + (8 —a) fi’ (ay + b(x — y)).

23

A MULTI-STAGE ALLOCATION PROCESS

Furthermore

(1) fo (@)=e'(*), ON*cx,

= h' (x — ye) + bfi’ (av_ + 5 (x — 2),

x Xe,

and f,’ (x) 1s continuous at « = %». Comparing (10) with the equation g’ (y) = h’ (x — y) defining y,, we
see that y. (x) < y, (x). In order to carry out the induction and obtain the corresponding results for all members of the sequence {fn}, defined recurrently by the relation

fo+1= Max [g(y) + h(e—y) + fn lay + (x —y))], O<y<sz

we require the essential inequality /f,’ (x) >,’ (x). There are three

intervals [0, x2], [%2, x1], [%1, co], to examine, each one requiring a separate argument. Using (10) and (11) we have

(12)

fat(x) = Sa)=ah(9s)

for x > x,. Combining (6) and the equation for y, we have

(13)

Si’

(%)

_

58"

(ys)

— ah’ (%&
b—a

—

yn)

The function [bg’ (y) — ah’ (x — y) ]/( — a) is monotone decreasing in y forO< y< x. Since y, < y, wesee that f,’ (x) > /f,’ (x). This completes the proof for the interval [%,, co]. The interval [0, x,] yields equality. The remaining interval is [%2, x,]. In this interval, we have

(14)

fy’ (x) = g' (x)

fl (x) = bg’ (Ye) — ah’ (x — ye) b—a

Hence in this interval, since O< y,< x,

(15)

fi,t(2) = EMO) be’ (x) — ah' (0 S o,ry,

since g’ (x) > h’ (0) is a consequence of g’ (y) > hh’ (x — y) forO< y<x and 0<x< %,. This completes the proof that /,’ (x) >/f\’ (x).
We now haveall the ingredients of an inductive proof which shows that

(16)

a. X%, > X%, >... Xn >... > O

b. fi’ (*#) < fh’ (x) ow. fh’ (XS...

C. V1 (%) > ye (x) >...

24

A MULTI-STAGE ALLOCATION PROCESS
Since fa (x) converges to f(x), fn’ (x) to f” (x), yn (x) to y (x) and xn to %,
we see that the solution has the indicated form.
§ 15. An “ornery’” example Having imposed successively the conditions that g and h be both convex or both concave, let us now show by means of an example that the solution can be exceedingly complicated if we allow more general functions possessing points of inflection. Let us consider the equation
(1) f(x) = Max [e-t/y + e-1/(t-) + f(.8y +.9(x—y))]. O<syx<z2z
The function e~¢/* is used since it is one of the simplest possessing a point of inflection. Determining f(x) by means of the method of successive approximations, we obtain a well-behaved curve

E(x) O9T 8 7 6 5 4
3 2 I

%

o

30

40

50

Figure 1
Note, however, the strange behavior of y (x)!
20--

y (x)

KN

0

1

I

}

20

30

40

50

Figure 2 25

A MULTI-STAGE ALLOCATION PROCESS
As soon as we allow changesof sign on the part of g” (x) and h’ (x), we seem to encounter functional equations which defy precise analysis.

§ 16. A particular example—I Figures 1 and 2 show the difficulties that can be encountered in the pursuit of general solutions. Let us then consider some simpler equations which can be used for approximation purposes.

THEOREM 7. The continuous solution of
(1) f(x) = Max [ext + f (ax), ex? + f (bx)], £0) =0,

subject to

(2) ts given by
(3)

a O<a,b<l1;¢,d,e,g>0, bO<d<g,
I(*) = pcx= OS 4 S%_,

Ff (%) = ext + f (bx), x > x,

®

i= [iam ~ — fe/(1— a4) 1/(9—4)

Since 0 <b <1, f(x) may be found explicitly in the intervals [w, x/b], ... [x/b", x/b™ +4] ..., forn =0,1,2,...
PRooF. Let us represent by A the operation of choosing cx4 + f (ax), and by B theoperation of choosing ex# + f (bx). A solution corresponding to an optimal sequence of choices, S may then be represented symbolically by

(5)

S = A% By Aw Be... ,

where a; and are positive integers or zero, and A*% means the choice A repeated a; times, with B% having a similar meaning.

Let us assume for the momentthat the solution does have the indicated form and show howto calculate x. At the point x either an A or B 26

A MULTI-STAGE ALLOCATION PROCESS

decision is optimal, while below x only an A decision is optimal. Consequently, symbolically, x is the point where

(6)

BA? = A®.,

To compute A®™ we write

(7)

f(x) = cx4 + f (ax) = cx4 +c (ax)4+c(atxja+ ...

= cx4/(1 — a4),

Similarly BA® yields

(8)

f (x) = ex + ch4xa/(1 — a4),

Equating the two expressions, we find that x has the stated value. It remains to prove that the solution has the desired form. Let us
begin by showing that A is always used when x is small. To do this it is sufficient to show that f(x) = cx4/(1 — a4) is a solution for small x, and then to invoke the uniqueness theorem. § We must assure ourselves that

(9)

1—aé cx = Max i —_c—x——- exog +. — Tcho_ a gx— 4s

for small x. This, however, is clear if g >d>0Oand0<0d< 1. We now proceed inductively. Let z be the smallest value of x for which a B-choice is optimal. At this point BA® = A®. This means that z = x. Let us now consider the interval x > x, and begin by asking for the point where AB and BA are equally effective as a set of first two choices.
We have, using an obvious notation,

(10)

fap (x) = ex? + easxo + f (abx)

fea (x) = ex9 + ch4x4 + f (abx).

Hence the required point # is given by

(11)

p = [c (1 — b4)/e (1 — as).

Since g > 4d, we see that p < x. It follows then from the fact that fag (x) < fsa (x) for x > p that
for x > x, AB plus an optimal continuation is inferior to BA plus an optimal continuation. From this we see that A cannot be used for x > x

5 Strictly speaking, we haven’t established this uniqueness theorem yet. However, it is easy to see that the method used to establish Theorem 1 works equally well in this case.
27

A MULTI-STAGE ALLOCATION PROCESS
unless followed by A, which we knowis also impossible. This completes the proof.
§ 17. A particular example—II Another interesting case is that where g and A are quadratic in x. We leave as an exercise the following result:

THEOREM 8. Let c,d >0 and 0O<b<a<l. Let

(1) f(x) = Max [cy — y? + a (x — y) — (x—y)*+ f (ay +b (x—y))], Sys72 f (0) = 0.
Then, tn the interval * 0 < x < Min (c/2, d/2), f (x) has the following form, which depends on the sign of c/(1 — a) —a/(1 — 6):

Case I: c/(l1— a) = a](1— dD).

(2)

f(x) = 1—(b¢6c+—(d)ba—tad)a ~ 1—[(aa@—+b)(al+—bad}? )’

where

(3)

a= 1 +3“($=* + y/1 + i(f== =5"

, r= (£5) »-(25) Case II: c/(l —a) < d/(1 —b).

for 0< x < Min (A, c/2, d/2), where

(5)

A=

(1

+

6)

[d

(1 — a) —c 2 (1 — ab)

(1

—

8)]

When i < Min (c/2, d/2) use of (1) as a recursion formula enables one to obtain f (x) over the entive interval of interest.

Case III: ¢/(1 —a) > d/(1 — 3).

(6)

fe) =(72,)2-(-4)

for 0< *< Min (u, c/2, d/2) where

(7)

p= (1+ a) [e(1—b) —d (1 —a) | 2(1 —ad).

¢ This is the maximum interval over which the g and A functions are both increasing.

28

A MULTI-STAGE ALLOCATION PROCESS
§ 18. Approximation and stability

It is, of course, interesting to have the explicit solutions of as many equations as possible available. However, the true importance of the explicit solutions of simple equations lies in the use of these solutions
as approximatesolutions to more obdurate equations, and in furnishing clues to the nature of optimal policies for more complicated processes.
In the above sections we have derived explicit solutions for the case where g and A have monomial forms cx¢, and for the case where they are quadratic. Note that approximation to g(x) by means of cx@ is equivalent to an approximation to log g(e*) by meansof log ¢ + dx, qa straight line, which is readily accomplished.
Observe that as x changes, we may change our approximating curves
so as to obtain betterfits if we wish closer approximations. Furthermore, let us point out that in general the approximation is most useful as an approximation in policy space rather than in function space.
In order to use approximation techniques, we require an estimate for the difference between the solutions’ of the two equations

(1)

f(x) = Max [u(x,y)+ flay+b(x—y))], f(0) =9, O<y<z

F(x) = OM<ayx<z [v(%,y) +F(ay+b(x—y))], F(0)=9,

in terms of the difference between u (x, y) and v (x, y). This is a stability theorem in the classical sense.

Let us prove

THEOREM 9. Let f(x) and F (x) be the continuous solutions of the above equations under the assumptions that u(x, y) and v (x,y) are continuous

in x and y for all x,y >0, withO <a,b <1, and that XY m(c*z) < co

n=0
where m(z) = Max [Max Max {| u(x, y) |, | v(x, y) | }.

O<a2<z O<y<2

If

(2)

Max {Max |w(x,y)—v(x,y)|}=D(2),

O<2<z O<y<z2

and &co D (c®z) < oo, c = Max (a, b), then

n=0

co

(3)

If (x) —F (x) [|< 2 D (cnx).

n=

7 The existence and uniqueness of these solutions is assured by the natural modification of the proof of Theorem 1. When we speak of the solution, we shall mean the continuous solution, or, generally, the solution furnished by the existence theorem.

29

A MULTI-STAGE ALLOCATION PROCESS

ProoF. Define

(4)

fi (*) = Max wu (x, ¥)

fn +1 (%) = Max (x, } + fr (ay + 6 (x —y))]

Fi (x) = Max . (x, ¥)

Fy +1 (2) = Max [u (x, ¥) + Fy (ay + 6 (x —y))].

Weknow, using the methods given previously that fy (x) converges to f(x), and Fy (x) converges to F(x) as N > oo.

Let us estimate the difference between /, and F,. Clearly,

(5)

fi (4) —Fi (x) |< Max |u(x,y)—v(%,y) |< D2).

O<y<2z

Proceeding, as in § 7, we have

(6)

| fv + 1 (*) — Fr +1 (2) — Max | fy (ay + b (x —y))

—Fry(ay + b(x—y)) | + | Max | u (x,y) — v(x, 9)|

It now follows inductively that

N

(7)

| fy +1 (4) —Fw 41 (*) |< 2 D (crx).

n=0

Letting N — co, we obtain (3).

§ 19. Time-dependent processes We havetacitly assumed in the foregoing pages that the processes under consideration were time-independent in that the total return depended only upon the initial quantity x and the duration of the process N, and not upon the time at which the process were initiated. Let us now see how wecan handle situations in which this is not the case. Let us assume that as a result of the division of x into y and (x — y) at the kt” stage, we receive a return gx (x,y) and are left with a quantity ax (x, y). It is required to determine the allocation policy which maximizes the total N-stage return. We shall assume that gx (x, y) is continuous in x and y for * >0 and 0O<_y< x and that ax (x, y) is hkewise continuous in this region 30

A MULTI-STAGE ALLOCATION PROCESS

andsatisfies the inequality 0< az (x, y) < ax,a <1, fork =1,2,... Define

(1) fx, w (¥) = total N-stage return obtained starting with a quantity x at stage k and employing an optimalpolicy.

We have

(2)

fi, 1 (*) = Max ge (x,y),

Osy<2z

and for N > 2, arguing as in the preceding pages,

(3)

fi, n(x) = Max (gu (4,9) + fre+a, w -1 (ax (%, 9) -

O<y<2

Since the double subscriptis distressing both analytically, esthetically, and aboveall, computationally, let us see whether or not we canrestore the single subscript relation. Having made up our mind that weare interested in an N-stage process starting at stage 1, let us define

(4) fx (x) = total return obtained starting with a quantity x at stage k
akn=d1,e2n,d.in.g.,aNt. stage N, employing an optimal policy, Then

(5) fi (x) = ax bw (x, ¥)
fu (x) = Max[gs (x, y) + fest (ae (x, y))],& =1,2,...,.N—1.

This simplification is essential if we are interested in computational solutions, since the difference betweentheeffort involved in the tabulation of functions of one variable and functions of two variables is enormous, while that between the tabulation of functions of two variables and functions of three variables may be the difference between a feasible and unfeasible approach.
The case of unbounded processes, i.e., N = oo, yields the set of functional equations

(6)

f(x) = Max [gx (x,y) + fr+1 (ax (x, y))].

O<sy<z

It is not difficult to obtain the analogues of Theorem 1 for these systems.

§ 20. Multi-activity processes The process we have been using for expository purposesis the simplest of its category since we allow only one type of resource, and require only
Jl

A MULTI-STAGE ALLOCATION PROCESS

one allocation at each stage. Let us now discuss the formulation of more general and more realistic processes.
Let there be M different kinds of resources, in quantities %,, %2,...,
xm respectively. At each stage, a quantity xi of the 2'® resource is utilized to produce an additional quantity of the jt? resource. Hence we have the equations, relating the resources at the (k + 1)s* stage to the resources at the kstage,
(1) x4 (R + 1) = x4 (R) — 2M° x45 (A) + gi (x10 (A), x20 (R), ..., xe (h)), j=1

for 1 = 1, 2, ..., M, where

(2)

(a) x(k) =O,

M
(b)j&m ie (A) < x4 (R),

and the production functions, g;, are assumed known, together with the initial quantities, x; (0) = cy.
The x(k) are to be chosen so as to maximize some pre-assigned function

(3)

Ry = (x, (N), x, (N), ...,%m (N)),

of the final resources. In manycases, as we shall see in Chapter 6, there are other constraints
in addition to those of (2), If we set

(4)

Jn (Cx, Cg, ++, Cm) = er Ry,

we obtain, as before, the recurrence relations

M

(5)

Tn (C1, Cg,

...,Cm)

= Max fy—1(€y — Lip + 83 (Virs Var

{y,3}

jj==1

«++, Vmi),-+-)

for N > 2, where the yi are restricted by the relations

(6)

(a) yy > O

M

(b) jo=s1 Vic, 1 = 1, 2, ...,M,

and

(7)

Fi (Cr, Co, ..., Cm) = F (cy, Cg, ..., Cm).

32

A MULTI-STAGE ALLOCATION PROCESS
Existence and uniqueness theorems covering the unbounded versions of these general processes will be given in Chapter IV, in conjunction with a better notation. We shall encounter a particular example of this equation further along in connection with the bottleneck processes of Chapter VI. In the present chapter weshall discuss briefly some of the difficult computational problems raised in maximizing over a multidimensional domain.

§ 21. Multi-dimensional structure theorems

It is not difficult to extend the results we obtained in the onedimensional case concerning convexity and concavity of the solutions of the functional equation of (8.1) to the multi-dimensional equations of § 20.
Let G(x) be a scalar function of a vector variable x. It is said to be convex if

(1)

G (Ax + (1 —A) y) SAG (x) + (1 —A) G (y)

for all A in the range O <1 A < 1. Thefunction is concave if the inequality goes the other way.
The multi-dimensional analogue of Lemma 1, proved in § 13, is valid and the proof is precisely the same. Using the lemma, we can establish the result below.
Before stating the result, let us introduce a more convenient notation.
Let x denote the vector whose components are x;, and y) denote the vector whose components are yj, for 1< 1,7 < M. Then, in terms of the process described above, we have

(2)

(a) x = Ly),

(b) yO >0,

where the notation y > 0 signifies that all components of y are nonnegative. Let D (x, y) denote the domain defined by (2). THEOREM 10. If 7 (x, y) and a (x, y) are continuous concave functions of x and y for all x,y >0, and r (x, y), a(x, y) ave monotone increasing in the components of x, then the functions { fn (x)} defined by the equations

(2)

fi (x) = Maxr (x,y),

D(x, y)

fv +1 (%) = DM(ax,xy) [r(x, y) + fw (a (2, 9))]

are all concave functions of x for x > 0.

33

A MULTI-STAGE ALLOCATION PROCESS

This implies a unique optimal policy for each N, wf r (x, y) ts stroctly concave.
The importance of this result resides in the following. If we have an N-stage process where #& decisions must be made at each stage, the functional equation approach reduces the Nk-dimensional maximization problem to a set of N k-dimensional problems. Although this is an essential reduction, the k-dimensional maximization problems themselves possess thorny features.
If, however, the function of k variables we are maximizingis strictly concave, we know that it possesses a unique relative maximum which is the absolute maximum. Given this additional information that the function under investigation has a unique relative maximum, we should be able to determine a search procedure for the location of this maximum which is far more efficient than the search procedure we would employ for a general function.

§ 22. Locating the unique maximum of a concave function

The determination of optimal search procedures ® for the location of the maximum of a concave function or, conversely, for the minimum of a convex function, is an extremely important and difficult problem which has not been solved to date. The solution has, however, been obtained in the one-dimensional case for the more general situation where the function is unimodal, whichis to say possessesa single relative maximum.
Let us pose the problem in the following terms. The function y = f (x) is a strictly unimodal function defined on the interval [0, Ln]. We wish to determine the maximum Ly, with the property that we can always locate the maximum of y = f(x) on a sub-interval of unit length by calculating at most » values of the function f(x). Since the maximum may not exist, it is safer to begin by setting

(1)

Fn = Sup Ln

We then have the following result THEOREM II. Fp ts the n*” Fibonacci number; i.e., Fo = F, = 1 and

(2)

F,rp=Fn-1+Fha-2e

for n> 2.

Proor. The definition of F, is a matter of convention, on the other

hand the value of F, is determined by the process.

8 It is actually not easy to specify precisely what we mean by an optimal search procedure. It clearly depends upon the type of equipment we have, the type of operations we permit, the ‘“‘cost’’ of these operations, and so on. Consequently, there are a variety of problems of the above type which may be posed. The subject has not been explored to any extent.

34

A MULTI-STAGE ALLOCATION PROCESS

Let us now proceed inductively. Fix m and calculate the values Vi =f (%1), Ve =f (%2) where 0 < x, < 4%, < Ln. If y,; > ye, the maximum occurs on (0, x2) since f(x) is strictly unimodal. If y, > y,, the maximum is on (%,, Ln). If vy; = ye, choose either of the above intervals, even though we know the maximum occurs on (x, x2). Thus, at each stage after the first computation weare left with a subinterval and the value of f(x) at some interior point x. Since values at the ends of an interval furnish no information per se, we restrict our attention to the interior points.
For” = 2,Ln = 2— €,x, = 1—e,x, = 1, forarbitrarily small e > 0. From the preceding argumentit follows that FP, = 2 =F, + Fy.
Consider the case where n > 2 and assume that Fy, = Fy-1+ Fr-2 for k = 2,...,n—1. Let us begin by showing that

(3)

Fa<Fn-1+ Fn-2.

For if we calculate f(x) at x, and x, on (0, Ln) we have

O

X|

X2

Ln

Figure 3

If y, > ye, we obtain the new picture

yy

0

x

Figure 4

Xo
3D

A MULTI-STAGE ALLOCATION PROCESS

In this case x, < Fn-—, since we have only (m — 2) additional choices with x, a first choice, for the case k = » — 1. Moreover, x, < Fn-, since the maximum could occur on [0, x,], with two choices of x already used.
Similarly if yz > y,, we have Ln —2x, < FPn-y1 Thus in all cases Ln < Fn 1+ Frn-—,, which yields (3). Now chose Ln, x1, %, arbitrarily close to their respective upper bounds Fn —~ ; + Frn- a, F,-, and Fn- , respectively. Then Fn = Fn-,-+ Fa -.. This yields the proof of Theorem 11. Furthermore, it yields the optimal policy, since each x; is either discarded or is the optimal first choice for the remaining subinterval. The sequence {F,} has as its first few terms

(4)

1, 1, 2, 3, 5, 8, 13, 21, 34, 55, ...,

with F4, > 10,000. Hence the maximum of a strictly unimodal function can always be located within 10-4 of the original interval length with at most 20 calculations of the value of the function.
It is easy to obtain an explicit representation for fn, namely

5
©) where
(6)

(7. — 1)

(1

71)

Fp =

vr

7n

(72 —7;) : T Gn) °

1+v5

6 +2

=~ 1.61

1—v5

a

%, =

9

= 7”

From this we see that Fa+,/Fn > 7,2 1.61 as n-> oo. Thus, for large », a uniform approximate procedure is to choose the two first values at distances L/r, from either end, where L is the length of the interval. This is a useful technique for machine computation.
Consider now the related problem where the ‘unimodal function is defined only for discrete values of x. Let Kn be the maximum number of points such that the maximum of the function can always be identified in » computations. The same type of proof as aboveestablishes.

THEOREM 12. Ko, = 1, K, = 1, K, = 2, K, = 4, and

(7)

Kn =14+ Fa, n > 3.

36

A MULTI-STAGE ALLOCATION PROCESS

§ 23. Continuity and memory

Let us suppose that we have a function of two variables, f(x, y), depending continuously on x and y for x >0 and O< y< x. Define the function

(1)

g(x) = Max f(x,y).

Osy<z

It is clear that g(x) will be continuous, but the function y = y (x) yielding the maximum need not be continuous. We havealready seen an example of this in connection with the functional equation of § 15.
Suppose, however, that we restrict f(x, y) to be a strictly concave function of y for all y in [0, x], for x > 0.

f (x,y)

O

y

x

Figure 5

It is clear that as x varies, the maximizing y will now be a continuous function of x.
Let us see how we canutilize this information to simplify the memory problem for computing machines. Consider the equations
(2) fu +1(x%) = Max [g(y) + h(x—y) + fy (ay + b(x—y))], O<y<z N =1,2,....
If we have no information concerning the location of a maximizing y, we must have available all values of fy (z) for 0< z< ax in order to determine fy +1 (x). Suppose, however, we take g(x) and h(x) to be strictly concave as well as continuous. In this case, fy (x) 1s strictly concave for each N and the function g(y) +h(x—vy) + fw (ay + b (x — y)) is strictly concave for 0 < y < x, and what is most important the function yy (x) which yields the maximum in (2) is unique and continuous as a function of x.
It follows than that if we are using an x-grid of values 0,4,24,..., to compute f(x), the complete set of values of fy (z) for O< z< ax 1s
37

A MULTI-STAGE ALLOCATION PROCESS
not required to compute fy +1(x), but only the values of fy (z) in a relatively small neighborhood of z = yy (x — J).
The same idea extended to multi-dimensional equations can result in a considerable saving of memory space in computing machines. Reciprocally, we will be able to solve problems using existing machines which might otherwise escape them. In any case, a great saving in running time will result, once again increasing the feasibility of a solution by these means.
§ 24. Stochastic allocation processes
In the preceding pages of the chapter, we have considered, in greater and lesser detail, various multi-stage allocation processes characterized by the property that the outcomeof any decision was uniquely determined by the choice of this decision. Processes of this type wecall deterministic.
Not all multi-stage processes, however, possess this property, and, as a matter of fact, many of the most interesting are quite definitely not of this type. Let us consider here one important class of non-deterministic processes in which theeffect of a decision is to determine a distribution of outcomes in the sense of probability theory. Processes of this type we shall call stochasttc.
Weshall limit ourselves in this book to processes of these two types. The discussion of the origin of processes of more complicated nature, and their treatment, we shall defer to another place.
From the mathematical point of view, stochastic processes furnish varied classes of fascinating analytic problems, and throw unexpected light upon many processes of supposedly deterministic nature. Applications of the theory are furnished by scores of processes drawn from biologic, economic, engineering, and physical fields.
Returning to our domain of decision processes, a fundamental problem confronting us is that of defining what we mean by an optimalpolicy in the face of uncertain outcomes. What is crystal clear, but so often overlooked in a posteriori comment, is the fact that a lack of complete control over a process effectively prevents a guarantee of a maximum return.
On the other hand, despite this Damoclean sword of uncertainty, there must exist some means of comparing policies, taking into account the possible fluctuation of outcomes.
What causes a major difficulty in applications is not that it is hard to find such a measure, but rather thatis is hard to find a unique measure. In short, it must be emphasized that there is no one method which can have any pretensions to thetitle of ‘‘best.’’ Whatever method is used dependsto a large extent upon various analytic and arithmetic aspects
38

A MULTI-STAGE ALLOCATION PROCESS
of the process, and, it must be confessed, upon the philosophical and psychological attitudes of the decision-makers.
Having thus dwelt upon the dismal side of the matter, to assuage our consciences, let us now proceed more constructively.
The general idea, and this is fairly unanimously accepted, is to use some average of the possible outcomes as a measure of the value of a policy. It is in the choice of this average that the difficulties arise.
Let us point out in passing that there is a definite lack of unanimity concerning the use of averages in determining policies for stochastic processes which may be carried through once, or at best, only a few times. In some cases, “‘distribution-free’’ policies can be obtained. In general, however, there seems to be no other approach to these questions than the usual one we present here.
The first average, or criterion, we shall employ is the commonarithmetic weighted average, or expected value. Due to the linearity of this average, it possesses a most important invariant property whichgreatly simplifies the functional equations which describe the process. This property enables the future decisions to be based solely upon the present state of the system, independently of the past history of the process.
The second criterion, whichis far less frequently used,is the probability of achieving at least a certain level of return. This also possesses the proper invariant structure as far as multi-stage processes are concerned. Wewill discuss this criterion in greater detail in a subsequent chapter.
§ 25. Functional equations Let us now consider a simple stochastic version of the deterministic process considered in § 2, and show that the same functional equation technique is applicable. In place of assuming that the outcomeof a division of x into y and x—vyisa return of g(y) + h(x — y), leaving a new quantity x, = ay + b(x—y), let us assume that with probability #, there is a return of g,(y) + Ay (x —y) and a remaining quantity a, y + 6, (x —y), and with probability , = 1— 9, a return of g. (y) + h,(* —y) and a new quantity a,y + b, (x — y) Let us define (1) fw (x) = the expected total return of an N-stage process, obtained
using an optimal policy, starting with an initial quantity x. Then, as before, we obtain the equations
(2) fi, (*) = Max [pi (g1 (vy) + 4c (%—¥)) + Be (Ba lv) + M(x —y))], O<ysz 39

A MULTI-STAGE ALLOCATION PROCESS
fu +1 (x) = Max [61 [gi (y) + Ay (x —y) + fn (ary + 8, (% —y))] +
O<y<z
Pele (v) + he (x —v) + fr (ey + b2(x — y))]], for N > 1.
The equations have the same analytic structure as those obtained from the deterministic process. By agreeing to use the “expected value”’ as the measureof the value of a policy, we have eliminated the stochastic aspects of the process, at least as far as the analysis is concerned.
§ 26. Stieltjes integrals For those who are familiar with the Riemann-Stieltjes integral, there is a much more compact way of writing the above equations. Let
(1) dG (u, v; x, y) = distribution function of a return of uw and a remaining quantity of v, starting with an initial quantity x and making an allocation of y.
Taking fw (x) to be defined as above, we obtain the equations

(2)

fi («) = Max [ uac (u,v; x, ¥),

Osy<r

fv +1 (*) = Max | + fy (v)] dG (u, v; %, y) O<y<z

It is much simpler to describe the processes, to establish existence and uniqueness theoremsfor the resultant functional equations, and to derive analytic properties of the solution, using this short-hand notation. The basic mathematical ideas are, however, the same.
Equations of this type will be discussed again in Chapter III within a more general framework.

Exercises and Research Problems for Chapter I
1. Let us define the function fn (a) = MaRx [x, x, ... xn]
where F is the region determined by the conditions ax +x,+t... +an=—a,a>Q0. b. x: > 0.
40

A MULTI-STAGE ALLOCATION PROCESS
Prove that fw (a) satisfies the recurrence relation
fy (a) = Max xfv-i1(a—x),N>2,
O<z<a
with f; (2) = a.
9. Show inductively that fy (a) = a%/N%, and hence establish the arithmetic-geometric mean inequality,

(Fat ok sel

N

=> XX, ... XN,

for xi > 0, with equality only if x,=x*,—=... = xn.

3. Let us define the function

N
fn (a) = MRini=21 xi?,p > 0,
where FR is the region defined by

N
a. i&=1 x1 >a,a>Q0.

b. Xi => QO.

Show that fw (a) satisfies the recurrence relation
f(a) = Min [2 + fv-1(a—x],N>2, O<2z<a
with f, (a) = a?.

4. Show that fy (2) = a? cy, where cy depends only upon N and 4, and thus that
cn = Min [xP + (1 — x) Pen -1].
0O<a2<1
Determine cy for the ranges O< f < 1,1 < 4,respectively.
5. Consider the problem of minimizing the function N
F (%4, %g, ...,%n) = iL=1 pi sil(si + xi),
where the #; and s; are parameters subject to the conditions f; > 0, pi Ni = 1, s; > 0, and the x; range over the region defined by x; > 0,
aX = a. i= ]
41

A MULTI-STAGE ALLOCATION PROCESS

Obtain the corresponding recurrence relations and show that the solution is of the form
xj=90, Ox<7 <1,

xj >0, t+1<7<N

under a suitable reordering of the x;’s.

6. Consider the problem of maximizing the function

N

F

(x1,

%2,

...,4n) N

= i2=X1(xi),

subject to the constraints x; > 0,i2=1x; = c. Show that the maximum

is m(c), under the assumption that (x) is convex.

7. Consider the case where (x) is a monotonically increasing function which is strictly concave. Show that the solution of the corresponding functional equation,

fn (c) = Max [gy (y) +fy-i(e—y)],N >2, O<syce
fi (c) = @(c),
has the form yn =0,0<ec< en,

= 2n,C > tn,

where zy 1s the unique solution of

Y(y) = fv-1(¢— 9),
for N > 2, and show how to determine the sequence {cy}.

8, Obtain explicit recurrence relations, and the analytic form of the sequence for the case where
9 (y) = y — by?, b> 0,
and c is restricted to the range O<c< 1/20.

9. What are the analogues of these result for the case where the function

F has the form © gi (xi), where each function q; (x) satisfies the same

conditions as above?

10. Carry through the corresNponding analysis for the prNoblem of minimizing F (x1, %2,...,%*w) = 2 (xi), subject to x; > 0, Y x; = a in the

1i=1

{= 1

42

A MULTI-STAGE ALLOCATION PROCESS
case where @ (x) 1s a non-negative monotonically increasing function which is strictly convex. Consider, in particular, the case where
gy (x) = x+ bx?,b>0.

11. Consider the problem of maximizing

N

subject to

F (x3, Ne, eee, XN; Vi» Vor. ety YN) — a YP (xi, Vi), 1=1

N

N

Ki, Vi OO, LY He = Cy, L Vi = Cz,

i=1

7=1

where @ (x, y) is a Strictly concave function, monotone increasing in x and

Show that the corresponding functional equation fn (C1, Co) = Max [p (x,y) + fw -1 (C1—*, C2—Yy)],
Oeyen
possesses for each N > 2 solution of the form

Figure 6
and show how to determine the boundary curves. Consider, in particular, the case where
p (x,¥) = U,x + ,y + uy x? + Qu, xy + Uy, Y?,
12. Under the assumption that » (x) isa monotonically increasing strictly coNncave function, determine the maximum of F (%,, %9,...,%N) = i»=1 (xi) over the region determined by
43

A MULTI-STAGE ALLOCATION PROCESS
N ai»=l %<01,% >0
N i=l b. oy XiP< Co, for p > 1 and p < respectively.
13. ObtaNin the recurrence relations arising from the problem of minimizing i2=1qi (xi) subject to the restrictions
a OS M<N, N
b. 2 wi (xi) >a,
7~=1
under the assumptions that eNach y; (x)x) is a non-negative monotone in-
creasing function of x, with 2 yi (ri) > a. t=1
14. Consixder the corresponding multi-dimensional problem of minimizing i2=1 qi (xi, yi) subject to the constraints
a ON S H<nN,0KSyW< ss,
b. a Yi (vi, vi) Da, ~=1
under appropriate assumptions concerning the sequence {yi}.
15. Determine the maximumof the function x, x, ... xn over the region defined by
N a 2 x =1, % > 0,
i=1
b. bx4<. %n41,0>1,k = 1,2,...,N—1.
N
Consider the same problem for the functioni2=1 ~;?, for different ranges of p.
16. Consider the recurrencerelations
fi, (x) =) Max [g(y) + h(x — y)], Sys
Jw +1 (%) = Max [g (y) + A(x —y) + fw (ay + 6 (x —y))], O<sy<z
4.4

A MULTI-STAGE ALLOCATION PROCESS
whereg (y) = cyy4, h (y) = cyy4, with cy, c,,d >0. Show that fy (x) =
unx%, where

u, = Max [c,v4 + c, (1 — v)4], 0O<rv<l

un+1 = Max [c,v4 + c,(1 —v)4 + uy (av + 5b (1 — v))4], O<v<il
Show that

lim wy = Max

N — oo

O<v<l

| c, v4 + c,(1—v)4 1— (av + b(1—v))4]40

17. Consider the process described in § 2 under the assumptionthatit is not required to useall the resources available at each stage. Show that the functional equation obtained in this way has the form

f(x) = Max [g(y.) + h (yo) +f (avi + bye + % —¥i — y2)]. W+¥2 <2
Does this equation have a solution if g (x) and A(x) are both concave functions of x? Does it have a solution if they are both convex ? Under what conditions upon g (x) and h (x) does it have a solution witha corresponding optimal policy? 18. Show that if there is a solution with y, + vy, < %, ¥1, V2 > 0, then g’ (y,)/(1 — a) = hr’ (y,)/(1 — 6) under suitable assumptions concerning g and h. Whatis the interpretation of this solution ? 19. Consider the process described in § 2 under the assumption that additional resources are added at each stage, either externally or from the conversion of all or part of the return g (y) + 4 (x — y) into resources, and obtain the corresponding recurrencerelations. 20. Consider the process described in § 2. Define gy (z) as the minimum cost required to obtain a total return of z at the end of N stages. Show that
gi(z2)= Min [(l—a)y, + (1—}) 9,], g (Hi) + (ys) =z

gv+1(2) = Min [1 —a) 91 + (L— ) ya + gw (@ — 8 (x) — A (y2))] YY. =
21. There are N different types of items, with the z‘» item having weight w; and a value v;. It 1s desired to load a ship having a total capacity of w pounds with a cargo of greatest possible value. Show that this problem leads to the problNem of determining the maximum over the ; of the linear form L = i2=1 ni v;, subject to the constraints, m; = 0, 1,2,...,N,
45

A MULTI-STAGE ALLOCATION PROCESS
& mm wi < w, and thus that this problem leads to the recurrence relations i=1
fi (w) = v, [w/w,], ([a] denotes the greatest integer containedin a)
fnuei(w) = Max [xun +1 + fn (Ww — xwn +1)],
w <a],
N+1
where x can assumeonly zero or integral values.
22. Suppose that we have a herd of cattle and the prerogative, at the end of the year, of sending one part of the herd to market, and retaining the other part for breeding purposes. Assume that the dollar value of y cattle sent to market is @ (y), and that z retained for breeding purposes yield az, a > 1, at the beginning of the next year.
Show that the problem of determining a breeding policy which maximizes the total return over an N-year period leads to the recurrence relation
fi (x) = Max oy)
O<y<ur
fw (x) = Max [@ (y) + fy-1(a (x —y))}. Osy<z
23. Determinethe structure of the optimal policies in the following cases:
a. p(y) =ky,k >0
b. q (y) is quadratic in y
c. p(y) is strictly convex d gy ((yy)) is strictly concave
24. Formulate the equations under the additional restriction that cattle must be 2 years old before they can be sold. Take into account feeding cost and mortality rates. 25. Consider the case in which there are probability distributions for the price and demand. 26. In problem 22, let @ (x) = cx4,c,d > 0. Show that fw (x) = cenx4, where cy = c and cyi1= Max ([7v#+cya¢(1—r)4, N=1,2,....
O<r<l
Determine the asymptotic behaviorof cy +1/cw and ry +1/rn.
27. Suppose that we have a quantity x of money, and that portionsof this money can be used for commongoods, invested in bonds, or invested in stocks, The return from y dollars invested in bondsis ay dollars, a > 1, over a period of one year; the return from z dollars invested in stocksis 46

A MULTI-STAGE ALLOCATION PROCESS
bz dollars, 6 > 1, over a period of one year. The utility of w dollars spent is y (w). How should the capital be utilized so as to derive a maximum utility over an N year period?
28. Consider the same problem under the assumption that the return from stocksis a stochastic quantity.
29. A sophomorehasthreegirl friends, a blonde, a brunette, and a redhead. If he takes one of the three to the Saturday night dance, the other two take umbrage, with the result that the probability that they will refuse an invitation to next week’s dance increases. Furthermore, as a result of his invitation, there is a certain probability that the young lady of his choice will be more willing to accept another invitation and a certain probability that the young lady will be less willing.
Assuming that feminine memories do not extend back beyond one week, what dating policy maximizes the expected numberof dances the sophomore attends—with a date?
30. Obtain a sequence of recurrence relatNions equivalent to determining the minimum of the linear form L = i2=1 x%;, subject to the constraints x4 > 0, Hi + Xi41 D> ait = 1,2,...,N—1. Thus, or otherwise, show that Min L = Maxai, granted that one a;is positive.
31. Solve the corresponding problem for the case where the constraints are Xi + %i41 + X%i4+2 > M%,41=—1,2,...,N—2.
32. DNetermine the recurrence relations for the problem of minimizing L = i2=}1 ¢; Xi, Ci > 0, subject to the constraints
xi > 0, bi x; +- di Xi41 > 4,t= l, 2, ...,N—1.
33. Solve the problem formulated above in (32) for the case where the constraints are
Xi + M41 > 4,1—1,2,...,N —1, xy > an, or Xi t+ M41 > a,t=—1,2,...,N—1,%, >a, xn > ay, or C. Xi + x%i41.+- X42 >a1,1=—1,2,...,N—2,%xn-1+ 4n > an-1,
XN > an.
plus the usual constraint x; > 0. 34. Show how to approximate to f(x) in the interval[a, 6] by means of a
47

A MULTI-STAGE ALLOCATION PROCESS
linear function ux + v according to the following measures of deviation

a. [- FG) — x — ue dx a

b. Max | f(x) —ux—v
a<axz<ob

35. Suppose that it is necessary to traverse a distance x. If we travel at a

speed v there is a probability # (v) ds of being stopped in the interval

(s, s + ds) and incurring a delay of d time units. At what fixed speed

should we travel in order to minimize the expected time required to cover

a distance x?

(Greenspan)

36. Under the same conditions as those of Problem 35, at what speed should we travel in order to minimize the probability of requiring more than a time T to coverthe distance x ?

37, Assume that there is a penalty of # dollars when stopped and that actual travelling time costs c dollars per unit time. How do weproceed to minimize expected cost ?

38. Obtain a recurrence relaNtion equivalent to the problem of minimizing

the quadratic form Qy = & (xx — Xx- ,)? over all sets of values for the

af

k=1

xx for whichk/=1x,2 = 1,%, =.

39. We are informed that a particle is in either of two states, which we shall call S and 7, and are given the initial probability x that it is in state T. If we use an operation A we reduce this probability to ax, where ais a positive constant less than 1, whereas operation L, which consists of observing the particle, will tell us definitely which state it is in. It is desired to transform the particle into state S in a minimum time, with certainty.
If f (x) is defined to be the expected numberof operations required to achieve this goal, show that f(x) satisfies the equation
P(x) = Mi(n Las. 1L +4fxf(a(x))|,0<x<1, f (0) =0.

40. Show that there is a number%, in the interval (0,1) with the property that
f(x =I1l+4+xf(l),0<*<%,
=1+f(ax),1lo>x>Xo.

ee?

48

Show that

A MULTI-STAGE ALLOCATION PROCESS

x= (O l—a) o fl) (lL—a)(R+ 1)’
for the minimizing value of .
41. At each stage of a sequence of actions, we are allowed our choice of one of two actions. Thefirst has associated a probability #, of gaining one unit, a probability ~, of gaining two units, and a probability #; of gaining nothing and terminating the process. The second has a similar set of probabilities #,', p.’, f;’. We wish to determine a sequence of choices which maximizes the probability of attaining at least ~ units before the process is terminated.
Let ~ (n) denote the this probability for m = 1, 2,3, .... Showthat p (n) satisfies the equation
p(n) = Max Pi p (n — 1) scorn
bi p(n —1) + pp’ p(n — 2)]” forn = 2,3, 4, ..., with p (0) = 1, and
p (1) = Max (Ay, py’).
42. With reference to § 7, show that if g (x) and / (x) are quadratic in x, then fy (c) = an + Bue + yne? where ay, Bn, yn are independentofc. 43. Show that there exist recurrence relations of the form
an+1 = R,(an, By, yy),
Bn +1 = R, (an, Bn, VN), YN +1 = R; (an, By, YN);
where the R; are rational functions. 44, Treat in a similar way the problem of minimizing the function
N
F(X, %2,+ ++, XN) =+ Ie (xn —fr) + h (%% — %e - 1)
+ m (%4— 2Xe-1 + Xe - 2)],
where g (x), 2 (x) and m (x) are quadratic. 49

A MULTI-STAGE ALLOCATION PROCESS

45. Suppose that we have a machine whose outputperunit timeis 7 (¢) as a function of ¢, its age measured in the same units. Its upkeep cost per unit timeis # (¢) and its trade-in value at any time¢ is s(¢). The purchase price of a new machine is # >s(0). At each of the times ¢ = 0,1,2,..., we have the option of keeping the machine, or purchasing a new one. Consider an unbounded process where the return one stage awayis discounted by a factor a, 0<a<_l. Let f(t) represent the total overall return obtained using an optimalpolicy.
Show that f(t) satisfies the equation

f (t) = Max

r(t)—u(t) + af (t+ 1), ypc aw + af (1)

46. Using the fact that an optimal policy, starting with a new machine, is to retain the machine for a certain number of time periods, and then purchase another one, determinethe solution of the above equation.

47. Is it uniformly true that, if given an over-age machine, the optimal policy is to turn it in immediately for a new one?

48. How does one formulate the problem to take into account technological improvement in machines and operating procedures?

49. A secretary is looking for a single piece of correspondence, ordinarily a carbon on thin paper. She usually has 6 places she can look

Three folders of about 30 sheets each One folder of about 50 sheets One folder of about 100 sheets Elsewhere

Folder Number k
1,2,3 4 5 6

Theinitial probabilities of the letter being in the various placesare usually

k

Pr

1-2;

tie

Probability of

Probability of

Time for one

letter in folder

being found on

examination

one examination

if in folder

1

11

2

1]

3

11

4

. 20

5

JOT

6

.10

.99

1

.99

1

.95

1

.85

2

10

3

.10

100

50

A MULTI-STAGE ALLOCATION PROCESS
Howshall the secretary look through the folders so as to a. Minimize the expected time required to find a particularletter ? b. Maximize the probability of finding it in a given time? (F. Mosteller)
50. Let the function a (x) satisfy the constraint a (x) < d < 1 forall x. Show that the solution of the equation
u = Max [b (x) + a(x) uw],
if it exists, 1s unique, and is given by the expression u = Max b (x)/(1 — a (x)).
Under what conditions does the solution exist ? If a(x) does not satisfy the above condition, show that the number
of solutions is either 0, 1, 2 or a continuum, and give examplesof each occurrence. 51. We are given a quantity x > 0 that is to be utilized to perform a certain task. If an amount y, 0< y < ¥%, is used on anysingle attempt, the probability of success is a (y). If the task is not accomplished on the first try, we continue with the remaining quantity x — y. Show that if f (x) represents the over-all probability of success using an optimalpolicy, then f (x) satisfies the functional equation
f(x) = Sup [a(y) + (l—al(y))f(x—y)]. O<y<z
52. Derive the corresponding equation for 1 — f(x), the probability of failure. 53. Consider the two cases where a (y) is convex or concave, and obtain the explicit solutions for these cases. Observe that in one case thereis no optimal policy. 54. Consider the process discussed in § 2 under the assumption that the total return from an N-stage processis
R'v=g(y) +h(x—y) + e(y1) +h (4%: —v1) +...
+ g (yn -1) +h (xn -1— yn -1) + (xy), where & (x) is a given function.
55. Consider the functional equation
f(x) = Max [g (y) + h(x —y) + flay + b (x — y))], O<y<zr ol

A MULTI-STAGE ALLOCATION PROCESS

under the assumption that

a. or
b.

gly) ~ ce, y4,h(y) ~ cy y4, Cy, Cz, d > 0, as y > 00 gly) ~ cy, hy) ~ coy%, Cy, Cg, dy, dz > O0as yoo.

In both cases, determine the asymptotic behavior of f(x) as x > 0.

56. Determine a recurrence relation for

Min | as ga ‘maa, an |.

r,>0 ¥2 + %3 %3 + Xq

Xn + Xy Xy + Xe

with the introduction of suitable additional parameters.

57. Consider the problem of determining the minimum ofthe function

N

N

k=&1 x (%e,%e+1) + k&=1 he (re),

where 7n +1 = 7, and the 7, are subject to the constraint

a O< re < dx,
N b 2 Dx (7x) = C,
k=1
with each x (x) a known monotoneincreasing function of x, wx (0) = 0. Introduce the auxiliary problem: Minimize

g (u, 72) + g (72, 7s) + eee + g (rn - 1, ny) + g (rN, v) N
+ k=22 he (re),
with 72, 73, ..., 7w subject to the constraints
a. O< ry <i by
N
b. k=XY2 pe(re) Sc. Show that if we designate the above minimum byF (uw, v, c), then the minimum in the original problem is given by
Min F (7%, 7, ¢ —qy,(7,)).
O<71<)
52

A MULTI-STAGE ALLOCATION PROCESS

58. Introduce the sequence of functions, R = 2, 3,...,N—1,

Fp (u, v, c) = Min [g (u, rr) + g(7R,7R+1) +... + 8 (rN - 1, YN)

Th

N

+ g(rv,v) + k2=&R he(re)),

with

Fy (u,v, c) = Min [g (uw, rw) + g (rn, v) + hn (rN) ]. "N
FNor each R, admit only c-values satisfying the restriction k&=Rx (bx) > c, where the b, are fixed positive constants.

Show that we havethe recurrence relation

Fp (u, v, c) = Min [g (u, rr) + Ar (rr) + Fr+i(re, v, C—pr(rr))], "R
where 7p varies over the interval defined by

a. O<rr< dp,
N
b. k=a R+1 Vk (dx) > C—pr(rr).
59. Consider in a similar fashion the problem of minimizing a function such as

Ry = 8 (11, %2, 13) + & (Ye, %a, Ya) + ++ +8 (%N- 1, 7N, 11)

+ g (rN, ‘1; 10).
60. Suppose that we have a quantity of capital x, and a choice of the production in varying quantities of N different products. Assumeinitially that there is an unlimited supply of labor and machinesfor the production of any items we choose, in any quantities we wish. —
If we decide to produce a quantity x; of the 2¢” item, we incurthefollowing costs:
a. a; = unit cost of raw materials required for the 2¢” item b. 06; = unit cost of machine production of 2¢* item c. ci = unit cost of labor required for 2#” item. d. C; =a fixed cost, independent of the amount produced
of the 2#” item, if x; > 0. The cost of producing a quantity x; of the z#” item is then

gi (xe) = (ae + Oe + ci) mi + Ci,
= Q,

“4 > 0
xi = 0.

53

A MULTI-STAGE ALLOCATION PROCESS

Let ~; be the selling price per unit of the z#* item. The problem is to choose the x; so as to maximize thetotal profit

subject to the constraints N
(a) & gi (mi) <x, i=1

N Py= 2 Pi %,
i=1

(b) x >0.

Let Jn (x) = Max Py.

Show that Fi(*) = pi (x —Cy)/(Qitbite,), %* SC,

and

= 0,

0 <= x <= Ci,

fn (x) = Max [pn xn + fw -1(% — gn (xy))].
z.>0
Uy (ty) <2

Show that xv > 0 can be replaced by

xn

> fv

-

1

(%)

—fn -
Pn

1

(¥

—Cy) .

61. Assume that the demand for each item is stochastic. Let Gx (z) represent the cumulant function for the demand z for the k¢” item. Show that the expected return from the manufacture of x; of the k** item is

bi [*2aGe (2) + pe[ x dG (2) Hie

= px {* zAGx (2) + pe xe (1 —Ge (xx)),
and obtain the recurrence relation corresponding to the problem of maximizing the total expected return. 62. Consider the problem of maximizing the probability that the return exceed 7. 63. Consider the above problem in the deterministic and stochastic versions when there are restrictions upon the quantity of machines available and the labor supply. 54

co AO OP

A MULTI-STAGE ALLOCATION PROCESS
64. Obtain the recurrence relations corresponding to the case where we have ‘‘complementarity’’ constraints such as
a. XX, = 0, %7%ge =O, Xo Xp X11 = O,*~ and so on, or
b. %:x*141=0, 1 =1,2,..., N—I1. 65. Suppose that we have a complicated mechanism consisting of N interacting parts. Let the z¢” part have weight Wi, size S;, and let us assume that we knowthe probability distribution for the length of time that any particular part will go without a breakdown, necessitating a new
part. Assumealso that we knowthe time and cost required for replacement, and the cost of a breakdown. Assuming that there are weight and size limitations on the total quantity of spare parts we are allowed to stock, how do we stock so as to minimize
the expected time lost due to breakdowns, the expected cost of breakdowns, a given function of the two, time andcost, . the probability that the time lost due to breakdownswill exceed 7, the probability that the cost due to breakdownswill exceed C? 66. Determine the possible modes of asymptotic behaviorof the sequence {un} determined by the recurrencerelation
Un+1 = Max [aun + 0, cun + d], and generally by the recurrencerelation
Un+1 = Max [aiun + di], 7+ =1,2,...,R. (cf. Problem 50). 67. Determine the minimum of
F (x1, %, ..., XN) = i 1 gi (xi) + Max (xy, %2, ..., XN), subject to the constraints x; > 0. 68. Suppose that we have N different activities in which to invest capital. Let gi (xi) be the return from the 2*® activity due to an investment of %;. Given an initial quantity of capital x, we are required to invest in at most & activities so as to maximize the total return.
Denote the maximum return by fr, n(x). Show that we have the recurrence relation
DD

I MSs

A MULTI-STAGE ALLOCATION PROCESS

fr, w (x) = Max for L=< k= N—1.

[| Max [gn(y) + fe -1," -1, (x —y)] >

O<y<z

,

| fe, nw —1 (%)

69. Two corporations, with interlocking directorate, are forbidden by anti-monopoly statutes from investing in the same enterprise. The first corporation has capital x to invest, the second capital y, with known returns g; (z) from an investment of a quantity of capital z in the 2of N different enterprises.
Show that if the directors wish to maximize the total return from the two corporations, they must maximize

AV

N

Fr (xi, yt) = i2=X1 gi(xi) + i&=1gi ly),

subject to the constraints N
a. & Xi = xX, Xi > O, i=1 N
b. ia=1YVi=V,Vi > 0.
C. xi Vi = Oz.

Let

Show that

Fn (%, y) = {Misa4x} Fr (%:, i)

ww (X,

—M

Max [gn (yw) + fw -1 (x, y — yn) ] |
Osyn sy

Ie) el Max [gn (xv) + fn -1(% — xn, Y) )

|O<%@y <2

Consider the case where the different corporations derive different returns from the sameenterprise.

70. It 1s decided to employ a policy of replacing all light bulbs in an office building at one time. Assume that the cost of replacing the bulbs is a, and that g(x) represents the cost due to lack of lighting if a time interval x elapses between replacements. Over a time interval 7, it is decided to make replacements at times %1, %1 + %2,...,%1 + xe+...4+ + x, =T, where x is to be determined.

56

A MULTI-STAGE ALLOCATION PROCESS

The efficiency of the program is to be measured by the average loss sustained

E (a +g (x) )
F (x1, x2, wee, Xn) = i T

What is the optimal policy?

(I. R. Savage)

71. Let the functions gi (x) be such that the maximum of

N

Fry (x1, x2, N

...,*w)

=i=21 gi

(xi)

over the region x; > 0, i2=’1x; = c may be obtained by use of a Lagrange

multiplier A, considering the expression

N

N

Gy = i2=1gi (xi) —AKi=S1X.

On the other hand, let fw (c) = Max Fy. Show that
{7}

A = fw’ (¢)

N

Obtain the corresponding result for the maximum ofi»=1g; (%:, yi)

subject to

N

N

ax = C1, L Vi= Co, xi, yi > 0.

t=1

i=l

72. Let

M, (%1, x2, ..., Xn) = the rt” largest of the quantities %1, x2, ..., xn,

Nr (%1, x2, ..., ¥N) = the vt” smallest of the quantities %1, x2, ..., xn,

fory = 1, 2, ..., N. Obtain recurrence relations connecting the members of the sequences

{My (x1, 2, ..., Xn)}, {N+ (%1, x2, ..., Xn)}, 7=1,2,...,. N
73. Consider the problem of maximizing i2=127%; N
subject to the constraints x; >0, i2=l1/(1 + «:) < x. (J. V. Whittaker)

57

A MULTI-STAGE ALLOCATION PROCESS
74, A gambler has a capital of x dollars and wishes to bet on the outcomes of N different events. There is a probability px that he can predict the kth outcome correctly. The only constraint on the total amount that he bets is the condition that he be able to pay off his losses.
Show that the problem of maximizing his expected return may be converted into the problem of maximizing
x Ly (x) = k2=lpe xx subject to the constraints (a) xi > 0,
x (b) io=e1MK + 4,7 = 1, 2, Ju, NV,
75, Consider the problem of maximizing N
Lyn (x) = ka =1Pr Xk subject to the constraints
(a) x; > 0
(b) SN xui<u+ x; i= 1

I Me

(c) a

1 Ni < U.

Define fy (wu, v) = Max Ln (x). Show that

In (u, v) == Max [pn XN + fw -1 (u —xy, Min (v —— XN, u) ) ]

76. The problem of designing an efficient water distillation plant for

heavy water production involves the minimization of

Vu = (ai) + g(a) | elas) |8 (am ,

ay

ayag

Aa, a2... Am —1

where the a; are subject to the constraints

(a) ai =_ 1
(b) @1 dg... Am = X.

Show that this may be reduced to the functional equation

fe +1 (x) = Min |g (a1)+—1fe(—)x

a, >1

ay

ay

,

and find the solution in the case where g(y) = y®, b > 0.

58

A MULTI-STAGE ALLOCATION PROCESS

77. Consider the case where

Vn = g1 (a1) + G2 (42) ot

&m (am)

ay

A, a2... Am —_1

(E. Cerri, M. Silvestri and S. Villan, ‘‘The Cascading Problem in a Water Distillation plant and Heavy Water Production,” Z. Naturforschg., 1la, 694 (1956).)

78. Consider the problem of allocating resources to N different activities, leading to the problem of maximizing a function & gi(xi) subject to the constraints L' x; = c, x; > 0. Show that the function fy (c) obtained via the usual recurrence relations does not depend upon the way in which the activities are numbered.

Bibliography and Comments for Chapter I
§ 1. A fairly complete bibliography of papers up to 1954 plus some remarks which complement the text may be found in R. Bellman, ‘“‘The Theory of Dynamic Programming,” Bull. Amey. Math. Soc., vol. 60 (1954), pp. 503-516.
§ 2. This process was first discussed in Econometrica, vol. 22 (1954), pp. 37-48.
§ 7. Further discussion of this problem may be found in R. Bellman, “A Class of Variational Problems,” Quart. of Appl. Math., 1956. An interesting discussion of general ‘‘smoothing”’ problems may be found in I. J. Schoenberg, “‘On Smoothing Functions and their Generating Functions,”’ Bull. Amer. Math. Soc., vol. 59 (1953), pp. 199-230, where a number of further references may be found.
§ 11. The importance of the concept of approximation in policy space was stressed in R. Bellman, “On Computational Problems in the Theory of Dynamic Programming,’”’ Symposium on Numerical Methods, Amer. Math. Soc. Santa Monica, 1953.
§ 12. The elegant proof of Lemma 1 was found independently by I. Ghicksberg and W. Fleming to whom the author posed the problem of finding a better proof than that given in the opening lines of the section.
§ 17. The results in this section were derived by D. Anderson. § 18. A more complete discussion of the conceptof the stability of solutions of functional equations may be found in R. Bellman, Stability Theory of Differential Equations, McGraw-Hill, 1954. § 19. The reduction of the sequence {/fx, ~ (¥)} to a sequence {f; (¥)} is an important piece of mathematical legerdemain as far as computational solutions are concerned; cf also § 6 and § 7. The limited storage capacity of computing machines makes one quite stingy with subscripts and parameters.
59

A MULTI-STAGE ALLOCATION PROCESS § 22. The proof in the text follows a paper of S. Johnson, ‘‘Optimal Search is Fibonaccian,’’ 1955 (to appear). An equivalent result was found earlier by J. Kiefer, unbeknownst to Johnson, using a much more difficult argument: J. Kiefer, “Sequential Minimax Search for a Maximum,’’ Proc. Amer. Math. Soc., vol. 4 (1953), pp. 502-6. The problem of determining a corresponding result for higher dimensions seems extraordinarily difficult, and nothing is known in this direction at
the present time. § 24. An excellent introduction to the study of stochastic processes is
given in the book by W. Feller, Probability Theory, John Wiley and Sons, 1948. A number of important physical processes are discussed in the book by M. S. Bartlett, An introduction to stochastic processes with special reference to methods and applications, Cambridge, 1955. Exercise 76. See R. Bellman, Nuclear Engineering, 1957
60

CHAPTER II
A Stochastic Multi-Stage Decision Process
§ 1. Introduction In the preceding chapter we considered in some detail a multi-stage decision process in both deterministic and stochastic guises. In this chapter weshall discuss a stochastic multistage decision process of an entirely different type which possesses a numberof interesting features. In particular, in obtaining the solution of some simple versions of processes of this type, we shall encounter the important concept of “‘decision regions. Weshall follow essentially the same lines pursued in the previous chapter, first a statement of the problem, then a brief discussion in classical terms. Following this, the problem will be formulated in termsof a functional equation, the required existence and uniqueness theorems will be proved, and then the remainderof the chapter devoted to a discussion of various properties of the solution, such as stability and analytic structure. For the simple process used as our model, we are fortunate enough to obtain a solution which has a very interesting interpretation. Equally fortunately as far as the mathematical interest of the problem is concerned, this solution does not extend to more general processes of the same type. This forces us to employ techniques of an entirely different type which weshall discuss in a later chapter, Chapter 8. The failure of the elementary solution is not due solely to the inadequacy of the analysis. A counter-example has been constructed showing that the solution of a multi-stage decision process of this class cannot always have the simple form of the solution given in § 8 below. Another proof of this fact is furnished by Lemma 8 of Chapter8. A number of interesting results which we do not wish to discuss in detail are given as exercises at the end of the chapter.
§ 2. Stochastic gold-mining Weshall cast the problem in the mold of a gold-mining process. Suppose that we are fortunate enough to own two gold mines, Anaconda and Bonanza, the first of which possesses within its depths an
61

STOCHASTIC MULTI-STAGE DECISION PROCESS

amount of gold x, and the second an amount of gold y. In addition, we have a single, rather delicate, gold-mining machine with the property that if used to mine gold in Anaconda, there is a probability #, that it will minea fraction 7, of the gold there and remain in working order, and a probability (1 — ,) that it will mine no gold and be damaged beyond repair. Similarly, Bonanzahas associated the correspondingprobabilities p, and 1 — fz, and fraction 7,.
Webegin the process by using the machine in either the Anacondaor Bonanza mine. If the machine is undamaged after its initial operation, we again make a choice of using the machinein either of the two mines, and continue in this way making a choice before each operation, until the machine is damaged. Once the machine is damaged, the operation terminates, which meansthat no furthergold is obtained from either mine.
What sequence of choices maximizes the amount of gold mined before the machine is damaged ?

§ 3. Enumerative treatment

Since we are dealing with a stochastic process, it 1s not possible to talk

about the return from a policy, a point we have already discussed in § 24

of the previous chapter, nor can we choose a policy which guarantees a

maximum return. We must console ourselves with measuring the value

of a policy by means of some averageof the possible returns, and choosing

an optimal policy on this basis. As before, the simplest such average is

the expected value.

Let us then agree that we are interested in the policies (since there may

be many) which maximize the expected amount of gold mined before the

machine is damaged. A policy here will consist of a choice of A’s and B’s,

A for Anaconda and B for Bonanza. However, any such sequence such as

(1)

S = AABBBABB...

must be read: A first, then A again if the machine is undamaged, then B if the machineis still undamaged, and so on.
Let us initially, to avoid the conceptual difficulties inherent in unbounded processes, consider only mining operations which terminate automatically after N steps regardless of whether the machineis undamaged or not. In this case it is quite easy, in theory, to list all feasible policies, and to compute all possible returns.? It is possible to use this idea to some extent in certain problems. However,in general, this procedure is rather limited in application, unrevealing as to the structure of an
optimal policy, and, as a brute force method, a betrayal of one’s mathematical birthright.

2 To quote numbers again, a 10-stage policy would require the listing of 21° = 1024 possible policies; if three choices at each stage, then 59,049 different policies.

62

STOCHASTIC MULTI-STAGE DECISION PROCESS

§ 4. Functional equation approach

In place of the above enumerative approach, weshall once again employ the functional equation approach. Let us define

(1)

fw (x, y) = expected amountof gold mined before the machine

is damaged when A has x, B has y and an optimal

policy which can last at most N stages is employed.

Considering the one-stage process, we see that an A-choice yields an expected amount #, 7, x, while a B-choiceyields p, 7, y. Hence

(2)

fi (x, y) = Max [pi71%, Pore).

Let us now consider the general (N + 1)-stage process. Whatever choice is made first, the continuation over the remaining N stages must be optimal if we wish to obtain an optimal (N + 1)-stage policy. Hence the total expected return from an A-choice is

(3)

fa(%, vy) = pi (7%, % + fw (1 —7,) x, y)),

and the total expected return from a B-choice is

(4)

f(x, ¥) = bate y + fr (x, (1 — 72) y))-

Since we wish to maximize our total (NV + 1)-stage return, we obtain the basic recurrencerelation

(5) fu +1 (%, y) = Max[fa (x, y), fa (*, y) J,

= Max [p (1, x + fu ((L—1) *, 9), be Tay +
fr (x, (1 — 72) y))].

§ 5. Infinite stage approximation
The same argumentation shows that the return from the unbounded process, which wecall f(x, y), assuming that it exists, satisfies the functional equation
(1) f(x,y) = Max [Ai (1. * + f((l—n) *, ¥)), Ba 2 ¥ + f(x, (1 —72)9))]-
Once again, the infinite process is to be considered as an approximation to a finite process with large N. In return for the advantage of having only a single function to consider, we face the necessity of establishing the existence and uniquenessof a solution of the equation in (1). This we proceed to do in the nextsection.
63

STOCHASTIC MULTI-STAGE DECISION PROCESS § 6. Existence and uniqueness

Let us now provethe followingresult:

THEOREM 1. Assumethat

(1)

a.

b

[Pi |, |e) <1?
O<"1,7r. <1.

Then there is a unique solution to (5.1) which ts bounded in any rectangle Ox 4a X,0Sy<lY.
This solution f (x, y) ts continuous in any finite part of the region x, y > 0.

ProoF: Let us, to simplify the notation, set

(2)

Ti(f) =pilnx +f((l—n) x, y)I,

T2(f) = pelrey +f (x, (1 — 72) y)].

Then the functional equation in (5.1) has the form

(3)

f(x, ¥) = Max [T, (f), T2(f)]-

Define the sequenceof functions

(4)

fi (%, ¥) = Max [p11 %, pare ¥),

fn +1 (%, y) = Max [T, (fy), Ts (fy)],

= Max [Ti (fy)]

precisely as in the recurrencerelation of (4.5). Let 1 =1(N) =12(N, x, y) be an index which yields the maximum in
the expression iM=a1,x2 [T:(/fw)], for N = 1, 2,... Then we have,

(5)

Ju +1 (%, ¥) = Tt wy (fr) = Ti wey (fy)

In +2(*, ¥) = Ti ww) (fv 41) > Ti wy) (fv 41),

using the same device we employed in the course of the existence and uniqueness proof for the solution of the functional equation in (8.1) of Chapter 1.

2 In the equation arising from the process described above, the p,; are nonnegative. The proof we give covers the more general equation as well. 64

STOCHASTIC MULTI-STAGE DECISION PROCESS Hence
(6) | fr +1 (%, ¥) — fv +2 (%, y) | << Max [| Te (wy (fv) — Ti cy(fn +1)|, | Ti ww +a) (fw) — Ti ww +1) (fw +1)|]
< Max[ | Ti (fv) — Ti (fw +1) |]

< Max [| A, | fy (1 — 1) x, y) — fv +1((l —1) x, y) |,
| Pa | | fv (x, (1 — re) v) — fiw +1 (x, (1 — 72) y) |].
Let us now define

(7)

S518) un (x,y) = Max fn (s, ¢) —fw+1(s, 2) |

From (6) we obtain

(8)

un +1(%, ¥) <q Un (X,Y),

where g = Max(|4, |, | p2|). Since O< q <1, we see that the series

N&=1wun (x,_y) converges uniformly in any boundedrectangle O< x+< X, O<vy< Y. Hence fn (x, y) converges uniformly to a function f(x, y) whichsatisfies the relation (5.1), and which is continuous in any bounded rectangle in the (x, y)-plane.
The uniqueness proof follows the samelines as the proof of Theorem 1 of Chapter 1 andis left as an exercise for the reader.
Aswesee from the aboveproof, the choiceof f, (x, y) is arbitrary provided only that it be bounded in any finite rectangle. It is interesting to note that the limit function will be continuous evenif the initial function is not, as a consequence of the uniquenessof the solution.

§ 7. Approximation in policy space and monotone convergence
Asbefore, it is easily seen that we can ensure monotone convergence by approximation in policy space, in the case where #,, 2. > 0. The two simplest approximations are those corresponding to A® and B”,? From the first policy we obtain the expected return

(1)

fa (x,y) = fin x/(1—p~,(1—n)),

and from the second, the return

(2)

fa (%,¥) = pote y/(l — p2(1 —7%)).

3 It is interesting to observe the following difference between the process and the functional equation obtained from it. The sequence A© is conditional as far as the process is concerned, but deterministic as far as the equation is concerned.

65

STOCHASTIC MULTI-STAGE DECISION PROCESS
As weShall see below in § 8 and § 9, we actually possess a far more sophisticated technique for obtaininga first approximationin the discussion of more complicated processes, at the expense, of course, of the above simplicity of expression. The guiding principle is, however, quite simple.
§ 8. The solution Let us now turn to the solution of the equation in (5.1) for the case where#, and #, are real numberssatisfying the inequality 0 < ~,, f. <1. It is intuitively clear that an A-choice is made when x/y > 1 anda B-choice is made when y/x > 1 4. It is also easily seen that the choice at each stage depends only on the ratio x/y, since f (kx, ky) = kf (x, y) for k > 0. Perhaps the quickest way to provethis is to invoke the uniqueness theorem, althoughit is intuitively clear from the description of the process. It follows then that if we examine the positive (x, y)-quadrant, and divide it into an A-set and a B-set, which is to say those valuesof x and y at which an A-decision is the optimalfirst choice and those at whichthe B-decision is optimal, then (x, y) in the A-set implies that (kx, ky) is in the A-set for all k > 0, and similarly for the B-set. If these sets are well-behaved, it follows that their boundaries must be straight lines,
Figure 1
as conceivably in the figure above. The regions where A and B are used are called decision regions.
Let us now boldly conjecture that there are only two regions, as in Figure 2, and see if we can determine the boundaryline, L, if this is the case.
4 The notation a > 1 signifies that a is very large comparedto 1. 66

STOCHASTIC MULTI-STAGE DECISION PROCESS L
y

Figure 2

Whatis the essential feature of the boundaryline which will enable us to determineits equation? It is this: it is the line on which A or B choices are equally optimal.
If we use A at a point (x, y), with an optimal continuation from the first stage on, we have

(1)

fa(%,¥) =pPinx + pif ((lL—n) %, 9),

while similarly B at (x, y), and an optimal continuation, yield

(2)

f(x,y) = Petey + bef (x, (1 —7”) y).

Equating these two expressions we obtain the equation for L. Unfortunately, this equation as it standsis of little use since it involves the unknown function /.
In order to complete the analysis successfully we must make a further observation. When at a point on L we employ A, we decrease x while keeping y constant and hence enter the B region; similarly, if we use B at a point on L weenterthe A region (see Figure 2 above). It follows that for a point on L an initial first choice of A is equivalent to an initial first and second choice of A and then B, while, conversely, an initial first choice of B is equivalent to an initial first and second choice of B and then A.
If we use A and then B and continue optimally, we have

(3) fab (x, ¥) = Pim % + Pi fetey + pi bef ((1—1) x, (1 —7re) ¥), and similarly

(4) faa(%,¥) = Pete¥ + Pi beri % + pi bef ((l—n) x, (1 —r2) y). Equating fas and fga, the unknown function f disappears® and we
obtain the equation
5 The meaning of this is that having survived both an A choice and a B choice, it is no longer of any importance in the continuation of the process as to the original order of these choices.
67

STOCHASTIC MULTI-STAGE DECISION PROCESS

(5)

£11, x/(1 — pi) = pore y/(1 — po),

for L. It remains to establish this equation rigorously. Let us begin by proving
that there is a region near the x-axis where A is always the optimalfirst choice.
If y = 0, we have

(6)

f(x, 0) = Max p1e /1 e a1ti 1— rs1) x, 0]

=— Pix + 61, f (1 — 1) Xx, 0).

Since f(x, y) is continuousin y, it follows that

(7)

F(%,¥) > pa(tay +f (x, (L—7) y)),

for 0 < y < kx, where k is some small positive constant, since thestrict inequality holds for y = 0.
Thus we have a region in which A is usedfirst, shown below in Figure 3.

Figure 3

Let us now take a point P = P (x, y), in the region between L and y y = kx, with the property that (x, (1 — 7,) y) is in the shadedregion. In other words, use of B at P mustresult in an A-choice next, provided that machine is undamaged. (This proviso is necessary when discussing the process, but not when discussing the equation, as we have noted above.)
If B is optimal at P, we obtain

(8)

f (%, y) = faa (x, y),

68

STOCHASTIC MULTI-STAGE DECISION PROCESS
as given by (4). However, we know that below L, fra (x, y) < fap (x, ). Hence B cannot be optimal at P. Proceeding inductively in this fashion we extend the shaded region up to L. Since precisely the same argument shows that the region between L and the y-axis is a B-region, we have completed the proof of
THEOREM 2. Consider the equation
(9) (9) _= Max fbeinlyx ++ff(((al(—L7—)nx), 9)y)],|. xy 20,
whereO< 1,0. <1,0<4,7,<01. The solution 1s given by

(10)

f(%, 9) = pilx + f((l— 1) x, y)], for

Prt x/(1 — Pi) > P22 ¥/(1 — Po)
= pelray + f(x, (lL—712) y)], for

Pits x/(1l — pi) < pot2y/(1 — po). For p17, x/(1 — pi) = pete y/(1 — pe) erther choice is optimal.

§ 9. Discussion The solution has a very interesting interpretation. We may consider p17, x to be the immediate expected gain and (1 — #,) to be the immediate expected loss. The theorem then asserts that the solution consists of making the decision which at each instant maximizes theratio of immediate expected gain to immediate expected loss. As we shall see, this intriguing criterion occurs from time to time throughout the theory of dynamic programming.

§ 10. Some generalizations The same methodssuffice to prove the tworesults below. THEOREM 3. Consider the equation

A: 2N pielerx+f(c’'x x, y)],

(1)

f (x, ¥) = Max

k=1
y

B: = ge [dx y + f (x, a’x y)]

where x, y > O and

N

N

(2)

(a) pbx > 0, gx > 0, XY pe, 2 ge <1,

k=1 k=1

(b) l>cpdeg>Ocet+ecrH=@retdzk=il.

69

STOCHASTIC MULTI-STAGE DECISION PROCESS

The optimal choice of operations 1s the following: If

N

N

& Peer

& Qa

k=1

k=1

(3)

N x>

vn»

1— » px k=1

1— » ax k=1

choose A; tf the reverse inequality holds, choose B. In case of equality, etther choice 1s optimal.

THEOREM 4. Consider the functional equation

K (4) f (x1, Xe, -- +, %w) = Mai x [okDX=1pixlCin xi +f(1, Xe, .. 6, Cin Xi, «.~,Xn) |]

where x; > O and

(5)

(a)

K
pi 0,kY=1pir <1,i1=1,2,...,n.

(b) l>cu > 0, cin + cor’ = 1.

The decision functions are

K

k=1 Pik Cik

D; (x) = —— K

Xi

1— » pir
k=1

in the sense that the index which yrelds the maximum ofDi (x) fort = 1,2,..., n is the index to be chosen in (4). In case of equality, it is a matter of indrfference as to which 1s used.
It is clear that we can combine Theorems3 and 4 into one more comprehensive result, which in turn can be generalized by the use of the Stieltjes integral. Thus a version of (1) arising from a continuous distribution of outcomesis

[er +F(A—2) xdG, f(x, vy) = Max |) toy +4, —w) 9] a (w).

Weleave the derivation of the extensions of Theorems 3 and 4, and the statements and proof of the corresponding existence and uniqueness theorem, as exercises for the reader. 10

STOCHASTIC MULTI-STAGE DECISION PROCESS
$11. The form of f (x, y) Having obtained a very simple characterization of the optimal policy, let us now turn ourattention to the function f(x, y). In general, no simple analvtic representation will exist. If, however, we consider the equation

a,x+t+a

C

(1)

f(x, vy) = Max i1X7+ bee v +e gofe (x, d] e y)

we can show that if c, and d, are connected by relation of the type c,” = d,", with m and positive integers, a piece-wise linear representation for f (x, y) may be obtained.
It is sufficient, in orderto illustrate the technique, to consider the simplest case where the relation is cz = dy. Let (x, y) be a point in the A-region. If A is applied to (x, y), this point is transformed into (c, x, y), which maybe in either an A- or a B-region. Let L, be theline that is transformed into L* when (x, y) goes into (c. x, ¥),
let L, be the line transformed into L,, and so on. Similarly, let 47, be the line transformed into L when (x, y) goes into (x, d, y), and so on. In the
sector LOL,, A is used first, followed by B, as shown below.

y

My

L

BA AB A*B

LY Lo

0

X

igure 4

Hence, for (x, y) in this sector we obtain

(2)

f(x,y) = a,x + ay + pof (C2 %, y)

= A,X + a,y + po(di Cex + bey) + Go baf (Cox, C2)

= (a, + po bile) X + (a2 + po be) ¥ + bs Galaf(%,y)

6 The boundary line, whose equation obtained as above, is [2,(1 — q,) + 4, (p,e, — 1) ]x = [6, (1 — P,) + a, (4,4, —1) ly
71

STOCHASTIC MULTI-STAGE DECISION PROCESS

This yields

(3)

f(x, 9) =

(a, + pe dy Co) x + (@, + fo be) y 1 — D2 92 Ce

for (x, y) in LOL,. Similarly, we obtain a linear expression for fin LOM,. Having obtained the representationsin these sectors, it 1s clear that we obtain linear expressions in L, OL,, and so on.

§ 12. The problem for a finite number of stages Let us first establish THEOREM 5. Consider the recurrence relations

(1)

fi (x, y) = Max {0111 Ax,:fo p2ylYr}yx + (Qn) #91)

JuN+ +1 (%, , 9) =Max 5 beltey + fn (x, (1 — 72) y)] ,

N =1,2,....

For each N, there are two decision regions.

PRooF. For each N > 2, the points determined by the condition that AB plus an optimal continuation for the remaining (N-2) moves1s equivalent to BA plus an optimal continuation for the remaining (N-2) moveslie on the same line L we have determined above, namely

Pit x _ Pots. V

”)

m7 1—p, 1—pf,”

Figure 4a

For the N-stage process, any policy, and consequently, any optimal policy has the form

(3)

Sn: A% Bo, ... Atn Bon,

where the a; and 0; are positive integers or zero, restricted by the condition, XY (a; + b:) = N.

12

STOCHASTIC MULTI-STAGE DECISION PROCESS
Let us now consider a point P = P (x, y) lying above L. If A is used at P, there are two possibilities: either A is used & times in succession, and then followed by B,

or Sn = A%. Let us consider the first case. If A is used (k — 1) times in succession, we reach a point P’ further above L. At P’, AB cannot be the first two moves in an optimal (N — k + 1)-stage policy, since BA plus an optimal continuation is superior.
Consequently above L, either B is usedfirst, or the optimal policy is AN, Let us now showthat if A” is optimal at P, then it is optimal in the region between OP andthe x-axis.
To demonstrate this we begin with the observation that it is permis-
sible to assume that x + y=1,0< x,y <1, because of the homogeneity of fw (x, y) asa function of x and y. Considering the N-stage process, we see that there are 2” possible policies, say P,, P2, ..., Pen. Each of these policies used at a point (x, y) yields a N-stage return which is a linear function of x and y, say Li (x, y). For x + y = 1, we may plot these functions obtaininga set of 2% straight lines,

“ASea
aN

O

x

Figure 5

If N were 2, so that the four policies AA, AB,BA,BB yielded four lines as above, the maximumreturn as a function of x would have the form

f (x,y)

O

X

Figure 6

| 13

STOCHASTIC MULTI-STAGE DECISION PROCESS
It is clear that A is an optimal policy for y = 0, x = 1. It follows thatif Ais optimal at (x, y), 0 < y <1, the line corresponding to A will dominate all other lines for x << x <1.
Combining the aboveresults we see that for any N,, the boundary between the A-region and the B-region will either be AB = BA orAY =M,, where M, 1s a policy of complicated form, or BY = M, 1s also a complicated policy.
Wecan now establish a sharperresult:
THEOREM 6. The decision regions for fy converge towards those of f as N —~ co in a monotone fashion. There is always an integer No with the property that for N > N,the regions for fn are identical with those off.
ProoF: Consider the situation for N = 3. Let L, be the boundary line for the two-stage process, and assume that therelative positions of L, and L are as shownbelow.

y

Lo

Le (am!)

L: AB=BA

Figure 7
Let L, (A-) denote the line transformed into L, whenA is used at a point on L,(A-), which is to say when(x, y) is transformed into (cx, y). Let Q be a point in the sector between L, and L, (A-1). If A is used at Q as the first move in a three-stage policy, B is used next, since the transformed point is in the B-region for a two stage process. However, if Q 1s aboveL, we know that AB cannot be the first two moves of an optimalpolicy. Hence B is used at Q. This shows that the B-region for the three-stage processis at least that containing the region above L, (A-1). This process may be continuedfor larger and larger N until L; (A-"), for somefinite &, lies below L. At this point, the boundary line becomes AB = BA, and remainsso for all larger N.
§ 13. A three-choice problem Let us now assume thatin addition to the two A and B choicesalready discussed, we have a third choice which is a compromise between the A
T4

STOCHASTIC MULTI-STAGE DECISION PROCESS

and B choices. The equation we obtain in this case takes the form

(1)

f(x,y) = Max]

A:
B:

py [rn «
polrey

+ f((l— 1) x, y)]
+ f(x, (1 —1) y)]

|C: ps[rsx + ray + f((1 —72) x, (1 — 1) y)]

where 0< 73,7, << landO< #; <1, and the quantities ,, 3, 71,73 satisfy the previous inequalities.
On the basis of what we know concerningthesolution of the equation where the C-term is missing, it might be suspected that the solution of this equation would be determined in the following way: There are three
decision regions, as in the figure below, with A, B and C each optimal first choices in these regions

Figure 8
Unfortunately, a counter-example has been constructed showing that this is not true generally. It shows, by meansof a fairly complicated but straightforward calculation, that the solution can, for suitable values of the parameter, take the form shownin Figure 9 below.
Thesolution of (1) above seemsto be quite a difficult problem, and very little is known concerning the characterof the solution.
y

Figure 9 15

STOCHASTIC MULTI-STAGE DECISION PROCESS

It is not even known whetheror not the numberof decision regionsis always finite and whether the numberis uniformly boundediffinite. To obtain some information about this problem in a part of the parameter space, we shall consider a continuousversion in Chapter8, where with the aid of variational techniques the decision regions may be determined.
For the continuous version they do assume the simple form shown in the first figure above, Figure 8.

§ 14. A stability theorem

Let us now derive a stability theorem for the solution ? of the equation

(1)

I (%, y) = Max fAF:

bpail[reny*++ff((x(,l(—lL n—)72x),

0M
¥)]

THEOREM 7. Let g (x, y) be the solution of

(2)

g (x9) — Max |AB::

£,[71,%
pal[rey

+g
+g

((1
(x,

—1) x,
(1 —72)

y)]
¥)]

|

+ 4(e9).

Then, in any rectangle R:0<x< X,0< ys Y

(3)

I (%, ¥) — 8 (x, y) |< Max | h(x, y) |/9,

where q = Min ((1 — 4,), (1 — ,)).

Proor. The proof proceeds by successive approximations, as in the correspondingsection in Chapter 1. Consequently, we shall merely sketch the details. Set

(4)

fi (*, y) = Max [p,7, x, bo 12]

gi (*, ¥) = Max [p71 %, Petey] + A(x, y).

and, generally,

A: py [11% + fa ((l — 14) *, My
tn +1 (Xx, y) = Max * he [%. V + fn (x, (1 —_—- 12) ¥)]
(5)

gn +1(x, y)

=

Max

és
B:

Di [11% + gn ((1 —7,) x, y)
pe [%2¥ + gn (x, (1 — 72) mI

Ahy (x,) ¥).

It 1s clear that

(6)

fi (% 9) —&i(% y) |< Max | A(x, y) |.

7 By the term “‘solution’’, here and in the following pages, we shall mean the unique solution in the appropriate function class. 76

STOCHASTIC MULTI-STAGE DECISION PROCESS Applying the techniques used repeatedly above, we see that (7) Max | fn +1 (%, Y) — gn til(%y) |< Max Ps | fn (%, y) — gn (x, ¥)|
+ Max |/ |
where p; = Max(,, ,). Iteration of this inequality yields
(8) Max | fu (*, ¥) — 8m (*, y) | Max |b | (1 + bs +--+ bs"),
forn = 2, ...,. Letting » — oo we obtain the stated result.
Exercises and Research Problems for Chapter IT 1. With reference to the process described in § 2, consider the case where the purpose of the process is to maximize the expected value of gy (R), where R is the total return, and @ (z) is a given function of z. Define the function f (x, y, a) = expected value of y(R) obtained employing an optimal
policy with initial quantities x and y in the respective mines and a quantity a already mined.
Show that f(x, y, a) satisfies the following functional equation I (x, y, a) = Max A: pte ea Sn) ehel] x,y >0
B: pef (x, 72'¥,@ +729) + pe’ V(@l>
Ft (9, O, a) = y (4).
Here p,' = 1— ,, d,. = 1— fa, 7) = 1—1n, 7.) =1—”7, 2. Establish an existence and uniqueness theorem for this equation. 3. Consider the case where 9 (z) is defined as follows: o (z) = 0, O0<z<4u,9(z)=1,2 >4u4, whereu<x+/y.
4, Let g (x, y) = MaPx Exp(e’"), b > 0, where Exp stands for expected value and we maximizeoverall policies P. Show that g (x, y) satisfies the equation
A: py e"%7 g(r,’ x,y) + a
§ (x, y) = Max Ie be eor.y g (x, ro y) a: be’
TT

