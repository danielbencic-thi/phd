IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathMenu.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Conferences > 2021 IEEE International Confe...
Vision-Based Mobile Robotics Obstacle Avoidance With Deep Reinforcement Learning
Publisher: IEEE
Cite This
PDF
  << Results   
Patrick Wenzel ; Torsten Schön ; Laura Leal-Taixé ; Daniel Cremers
All Authors
View Document
1
Paper
Citation
159
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Background
    III.
    Deep Reinforcement Learning for Control
    IV.
    Experimental Results
    V.
    Conclusion

Authors
Figures
References
Citations
Keywords
Metrics
More Like This
Footnotes

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: Obstacle avoidance is a fundamental and challenging problem for autonomous navigation of mobile robots. In this paper, we consider the problem of obstacle avoidance in si... View more
Metadata
Abstract:
Obstacle avoidance is a fundamental and challenging problem for autonomous navigation of mobile robots. In this paper, we consider the problem of obstacle avoidance in simple 3D environments where the robot has to solely rely on a single monocular camera. In particular, we are interested in solving this problem without relying on localization, mapping, or planning techniques. Most of the existing work consider obstacle avoidance as two separate problems, namely obstacle detection, and control. Inspired by the recent advantages of deep reinforcement learning in Atari games and understanding highly complex situations in Go, we tackle the obstacle avoidance problem as a data-driven end-to-end deep learning approach. Our approach takes raw images as input and generates control commands as output. We show that discrete action spaces are outperforming continuous control commands in terms of expected average reward in maze-like environments. Furthermore, we show how to accelerate the learning and increase the robustness of the policy by incorporating predicted depth maps by a generative adversarial network.
Published in: 2021 IEEE International Conference on Robotics and Automation (ICRA)
Date of Conference: 30 May-5 June 2021
Date Added to IEEE Xplore : 18 October 2021
ISBN Information:
ISSN Information:
INSPEC Accession Number: 21257529
DOI: 10.1109/ICRA48506.2021.9560787
Publisher: IEEE
Conference Location: Xi'an, China
Contents
SECTION I.
Introduction

Safe and effective exploration in an unknown environment is a fundamental and challenging problem for mobile robots. To develop an approach for addressing these challenges a robotic system is faced with enormous challenges in perception, control, and localization. Obstacle avoidance is indeed an important and crucial part of being able to successfully navigate. Typically, in robotic navigation problems, simultaneous localization and mapping (SLAM) [1] , [2] , [3] algorithms are a fundamental part of such a system. These algorithms tackle this problem by constantly updating a map of the environment while simultaneously keeping track of the robot’s location. However, this is a challenging problem because it is hard to design features or mapping techniques that work under a wide variety of environments the robot may encounter. Therefore, we are interested in utilizing learning techniques for mapping sensor inputs to control commands. In particular, we are interested in solving this issue end-to-end in a data-driven way without relying on an obstacle map. However, despite all that outstanding success in a lot of computer vision problems, many recent deep learning approaches still have evident drawbacks for robotics [4] .

In the last few years, deep reinforcement learning (DRL) has shown great potential to solve difficult problems that previously seemed beyond our grasp. While the improvement on tasks like Atari games [5] and the game of Go [6] have been dramatic, the progress has been primarily in 2D environments. Therefore, in this paper, we are interested in the problem of obstacle avoidance in simple 3D environments for mobile robots without any hand-crafted features or prior knowledge of the environment (see Figure 1 ). We use DRL to learn visual control strategies that allow robots to explore an unknown environment by using raw sensor data only. Furthermore, we are particularly interested in the comparison between discrete and continuous action spaces for learning control strategies.
Fig. 1 - A simulated mobile robot in a virtual environment learning how to navigate autonomously without crashing. The robot was trained based on grayscale images (example shown in the upper right part of the figure) captured by a monocular camera. The range finder readings are shown as blue lines emerging from the center of the agent.
Fig. 1

A simulated mobile robot in a virtual environment learning how to navigate autonomously without crashing. The robot was trained based on grayscale images (example shown in the upper right part of the figure) captured by a monocular camera. The range finder readings are shown as blue lines emerging from the center of the agent.

Show All

Deep learning approaches have shown huge success in image-based pattern recognition problems due to their ability to learn complex and hierarchically abstract feature representations. In this paper, we train visualmotor policies that perform both perception and control together for robotic navigation tasks. The policies are represented by deep convolutional neural networks (CNNs) which are being learned from raw visual input data, captured by a single monocular camera mounted on a mobile robot platform. For a more comprehensive overview of deep learning approaches in mobile robotics, we refer the reader to the survey by Tai and Liu [7] .

Since transferring knowledge learned in simulation to real-world settings is an important step in developing a complete robotics system, we are also interested in bridging the gap between those two. This is a rather challenging problem for perception-based approaches since the visual fidelity of images generated in simulation environments is inferior compared to images from the real world. This motivates us to tackle the problem by using depth images instead of images from a monocular camera. However, in practical scenarios perception is often limited to monocular vision systems without any access to depth images. Hence, we propose to use conditional generative adversarial networks (GANs) [8] to overcome this issue by predicting depth images from monocular images.

Particularly, this paper presents the following contributions:

    We show that discrete action spaces outperform continuous action spaces on the task of vision-based robot navigation in maze-like environments.

    We investigate the influence of on-policy and off-policy DRL algorithms as well as the impact of different sensor modalities for visual navigation tasks in synthetic environments. The experimental findings can be used as a guideline for similar tasks.

    We show that the robustness of the policy and the learning performance can be increased by additionally fusing predicted depth images generated by a generative model.

The remainder of this paper is organized as follows. In Section II , the necessary background on DRL for robot navigation is reviewed. Section III describes the problem statement and the approach. Training and evaluation details are presented in Section IV . Finally, Section V concludes the paper.
SECTION II.
Background

In this work, we consider a Markov decision process (MDP), where an agent interacts with the environment through a sequence of observations, actions, and reward signals. At each time step t , the agent executes an action a t ∈ A from its current state s t ∈ S , according to its policy π : S → A . The received reward at time t which is obtained after interaction with the environment is denoted by r t : S → A → ℝ and transits to the next state s t +1 according to the transition probabilities of the environment. The aim of the agent is to find a policy π ( a|s ; θ ) (where θ are the parameters of the policy) that maximizes the sum of discounted rewards, R t = ∑ T τ = 0 γ τ r t + τ , in expectation, i.e., E [ R t ] , where γ is the discount factor and T is the time horizon. The discount factor trades off the balance between the immediate and the future reward. Since the transition probabilities are unknown, reinforcement learning (RL) techniques can be used to learn a policy for navigating a mobile robot in an unknown environment without crashing.
A. Deep Reinforcement Learning

In a model-free RL setting, the agent is trained without access to the underlying model of the environment. However, DRL methods exploit the idea of using deep neural networks (DNNs) to approximate the value function, the policy, or even the model. In general, today’s leading DRL algorithms can be divided into two main classes: off-policy methods and on-policy methods. In the first method, the policy used for the control, called the behavior policy, may have no correlation to the policy that is being updated, called the estimation policy. In the second method, the policy used for the control of the MDP is the same which is being updated. In the following, we briefly introduce two recent model-free RL methods.
1) Deep Q-Network

Deep Q-Network (DQN) proposed by Mnih et al. [5] is an off-policy method and the actionvalue function Q ( s,a ; θ ) is learned by minimizing the temporal difference error instead of directly parameterizing a policy. The Q-value estimator is optimized by repeated gradient descent steps on the objective: E [ ( y i − Q ( s i , a i ; θ ) ) 2 ] , where y i is the estimated Q-value given by y i = r i + γ max a Q ( s i +1 , a ; θ ). The policy selects the action maximizing value: a ∗ = argmax a Q ( s,a ; θ ). We make use of several enhancements to DQN as suggested by [9] , like dueling networks [10] , double Q-learning [11] and prioritized experience replay [12] to improve performance. We use the proposed hyperparameters by OpenAI baselines [13] except a reduced experience replay capacity of 2.5 × 10 5 samples.
2) Proximal Policy Optimization

Proximal Policy Optimization (PPO) [14] is a recently released on-policy method that improves sample complexity compared to standard policy gradient methods. In policy gradient methods, the policy is directly parameterized in the form π ( a|s ; θ ), where π is a probability distribution over actions a when observing state s , as parameterized by θ , a deep neural network. The objective to optimize is the following:
E ^ t [ min ( r t ( θ ) A ^ t , clip ( r t ( θ ) , 1 − ∈ , 1 + ∈ ) A ^ t ) ] ,
View Source Right-click on figure for MathML and additional features. \begin{equation*}{\hat E_t}\left[ {\min \left( {{r_t}(\theta ){{\hat A}_t},\operatorname{clip} \left( {{r_t}(\theta ),1 - \in ,1 + \in } \right){{\hat A}_t}} \right)} \right],\end{equation*} where A ^ t is the estimated advantage, E ^ t is the empirical expectation over timesteps, and ϵ a hyperparameter.

B. DRL for Navigation

Perception and control for robotics with DRL have been studied recently in the context of obstacle avoidance and planning for mobile robots. A mapless motion planner, using laser range findings, previous velocities and relative target positions as sensory input trained with Asynchronous Deep Deterministic Policy Gradient (ADDPG) was successfully developed by [15] . The experiments show that the proposed method can navigate the mobile robot in virtual and real environments to the desired targets without colliding with any obstacles. Mirowski et al. [16] proposed to jointly learn the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. The results show that the combination with supervised auxiliary tasks significantly enhances the overall learning performance. Also, [17] demonstrated the ability to build a cognitive exploration strategy using an RGB-D sensor and DQN. Furthermore, [17] proved the ability to transfer the models trained solely in virtual environments to the real world. The navigation policies generalized well to the previously unseen real environments with dynamic obstacles. Compared to previous work, we show how to accelerate the learning and how to increase the robustness of the policy by means of fusing information from two different sensor modalities. Moreover, in this work we suggest guidelines on which algorithms and sensor modalities are adequate for visual navigation tasks and how discrete or continuous action spaces influence the performance.

Another line of research is to use evolutionary methods for visual reinforcement learning tasks. In one example, Koutník et al. [18] proposed the idea of using evolutionary computation to train vision-based control policies for the TORCS driving game.
SECTION III.
Deep Reinforcement Learning for Control

In this paper, we focus on incorporating predicted depth information into off-policy and on-policy methods for visual control of a mobile robot based on raw sensory data. Our vision-based control problem can be considered as a decision-making task, where the agent is interacting in an unknown environment with its sensors.
A. Reward Function

Reward shaping is arguably one of the biggest challenges for every RL algorithm. One big drawback of RL is the fact that reward functions are mostly hand-engineered and domain-specific. There is quite some work on imitation learning (IL) [19] , which tries to directly recover the expert policy and inverse reinforcement learning (IRL) [20] , which provides a way to automatically acquire a cost function from expert demonstrations. However, most of these methods suffer from heavy computation costs and the optimization of finding a reward function that best represents the expert trajectory is essentially ill-posed [21] .

There are two approaches on how to design a reward function. Dense reward functions, giving reward in all states during exploration and in contrast, sparse rewards, which only provide a reward at the terminal state, and no reward anywhere else. For the obstacle avoidance problem, a dense reward is much easier to learn, since the reward function will provide feedback about every step even when the policy hasn’t figured out a full solution to the problem yet. Whereas a sparse reward function won’t allow for safe navigation of the environment and also crashes will be more likely, e.g., since we don’t punish the robot for navigating close to the walls. We define the dense reward at timestep t as follows:
r t = { r collision  , ( 1 c d + 1 ) 2 + 1 2 v cos ( ω ) ,  if collides,  otherwise,
View Source Right-click on figure for MathML and additional features. \begin{equation*}{r_t} = {\begin{cases} {{r_{{\text{collision }}}},}&{{\text{ if collides,}}} \\ {{{\left( {\frac{1}{{{c_d} + 1}}} \right)}^2} + \frac{1}{2}v\cos (\omega ),}&{{\text{ otherwise,}}} \end{cases}} \end{equation*} where v and ω are local linear and angular velocity published to the mobile robot, respectively. The reward function is composed of two parts: the first part enforces the robot to keep as far as possible away from the obstacles. The second part encourages the robot to move as fast as possible whilst avoiding rotation. The center deviation defines the closeness of the robot to the obstacles and is close to zero if the robot is equally far away from the walls and is increased as soon as the robot moves towards an object. The center deviation is determined by evaluating the range finder readings. The range finder is a sensor with 100 line segments covering a 270° field of view (FOV) surrounding the agent; its response is the distance to an object intersecting with the ray. The center deviation c d is calculated as the absolute difference between the sum of right-view and left-view distance measurements of the robot.

Fig. 2 - Raw sensor information is fed as an input to the network and the output activation is directly used to send control commands to the robot.
Fig. 2

Raw sensor information is fed as an input to the network and the output activation is directly used to send control commands to the robot.

Show All

If the robot crashes against the wall, the episode ends immediately with a reward of r collision = −1. An episode is considered as solved if the mobile robot navigates without crashing for a total of T = 2000 timesteps. The total reward is the sum of the rewards of all steps within an episode. The reward is directly used by the algorithm without clipping or normalization.
B. Policy Learning

For the policy learning, we use the well-known network architecture proposed by Mnih et al. [5] . The architecture is depicted in Figure 2 . The network is composed of three convolutional layers having 32, 64, and 64 output channels, respectively with filters of sizes 8 × 8, 4 × 4, and 3 × 3, and strides of 4, 2, and 1. The fully-connected layer has 512 neurons and is followed by an output layer which differs between the discrete and continuous action space. All hidden layers are followed by rectified linear units (ReLU) as activation functions.
C. Action and Observation Space

The action space definition varies between the discrete and continuous setting. Each action a t corresponds to a moving command which is then executed by the mobile robot.

Discrete space. The space is defined as five predefined actions corresponding to the following commands: move forward , move hard right , move soft right , move soft left , and move hard left. The linear velocity for the forward direction is set to be 0.3ms −1 , while the angular velocity is 0rads −1 . The linear velocity for the turning actions is set to be 0.05ms −1 , while the angular velocity is − π 6 , − π 12 , π 12 , or  π 6  rad  s − 1 for right and left turns, respectively.

Continuous space. The actions will be sampled initially from a Gaussian distribution with mean µ = 0 and standard deviation of σ = 1. For the mapping, the distribution is clipped at [ µ −3 σ,µ +3 σ ], since 99.7% of the data lie within that band. The linear velocity can take values between the interval [0.05,0.3] and the angular velocity can take values between [ − π 6 , π 6 ] . The policy values from the Gaussian distribution will be linearly mapped to the corresponding action domains.
Fig. 3 - A sample image pair used for training the conditional GAN.
Fig. 3

A sample image pair used for training the conditional GAN.

Show All

Observation space. The observation o t ∈ ℝ W×H×C (where W , H , and C are the width, height, and channels of the image) is is an image representation of the scene obtained from an RGB camera mounted on the robot platform. The dimension of the grayscale observation is W = 84, H = 84, and C = 1. The state s t is composed of four consecutive observations to model temporal dependencies. We assume that the agent interacts with the simulation environment over discrete time steps. At each time step t , the agent receives an observation o t and a scalar reward denoted by r t from the environment and then transits to the next state s t +1 .
D. Training the GAN Architecture

For the training of the paired image-to-image translation, we used the GAN architecture proposed by Isola et. al [8] . The training was done on 1313 pairs of pixel-wise corresponding RGB-depth images. An example pair is shown in Figure 3 . The image resolution of the images from each domain is resized to 84 × 84. The model is trained for a total of 200 epochs and the latest model is used for inference during training the DRL policies. This approach is useful for domain adaptation, where we condition on an input image (RGB image) and generate a corresponding output image (depth image). An advantage of using a GAN over a simple CNN is the fact that this is a generative model and hence not limited to the available training data and can generalize across environments which we validate in our experiments. For more details on the loss and training objective, please see [8] .
SECTION IV.
Experimental Results
A. Environment Setup

We used the gym-gazebo toolkit [22] to evaluate recent DRL algorithms in different simulation environments. This toolkit is an extension of OpenAI gym [23] for robotics using the Robot Operating System (ROS) [24] and Gazebo [25] that allows for an easy benchmark of different algorithms with the same virtual conditions. For the conducted experiments in this work, two different modified simulation environments from the original project are used; namely Circuit2-v0 and Maze-v0 ( Figure 4 ). As a simulated robotics platform, a TurtleBot2 with a Kobuki base is used. This model comes with a simulated camera with 80° FOV, 480×480 resolution, and clipping at 0.05 and 8.0 distances. The laser range sensor information is provided by a simple sensor model, which is mounted on top of the robot base. The sensor has a 270° FOV and outputs sparse range readings (100 points). In all our simulation results, each plot shows a 95% confidence interval of the mean across 3 seeds.
Fig. 4 - The virtual training environments were simulated by Gazebo. We used two different indoor environments with walls around them. The environments are named Circuit2-v0, and Maze-v0, respectively. A TurtleBot2 was used as the robotics platform.
Fig. 4

The virtual training environments were simulated by Gazebo. We used two different indoor environments with walls around them. The environments are named Circuit2-v0 , and Maze-v0 , respectively. A TurtleBot2 was used as the robotics platform.

Show All
B. Training

For training the models, we use an Adam [26] optimizer and train for a total of 2 million frames for each environment on an NVIDIA GeForce GTX 1080 GPU. All models are trained with TensorFlow. It is important to know that the laser range finder is only used during training time for determining the rewards and is not required for inference.

In each episode, the mobile robot is randomly spawned in the environment with a random orientation. The angle of the orientation is randomly sampled from an uniform distribution ϕ ∼ U (0,2 π ). To ensure a fair comparison at test time, the agent is spawned at the same location with the same orientation and the reward is averaged over 12 test episodes in each environment. The average episodic reward is considered as the performance measure of the trained networks. In the following, we present the experimental results.
C. Comparison Between Discrete and Continuous Action Spaces

In this experiment, we are particularly interested in finding out the differences of discrete actions compared to continuous actions for visual navigation. Ideally, we want to deal with continuous action spaces for robotic control, since defining discrete action spaces is a cumbersome process and the actions may be limited when performing tricky maneuvers. In Figure 5 , the learning curves of DQN (discrete), PPO (discrete), and PPO (continuous) for the Circuit2-v0 and Maze-v0 environment are shown. The experiments show that PPO discrete is able to outperform both PPO continuous and DQN with discrete action spaces by a wide margin. A viable explanation for the performance differences between PPO discrete and PPO continuous comes from the fact that a relationship between the linear and angular velocity for the continuous domain needs to be learned, whereas prior knowledge about this relation is already available in the discrete domain. Furthermore, in general, the linear and angular velocities are inversely proportional to each other. For example, during turns, the angular velocity is generally high and the linear velocity is low. It is notable to mention that the continuous variant of PPO is able to outperform the discrete DQN method, despite dealing with a much more complicated action space and no prior knowledge about the relationship between linear and angular velocity. Table I shows the inference results for the different evaluated methods. We see that among the tested methods, PPO with discrete actions is able to achieve the highest scores.
Fig. 5 - The learning curves of DQN (red), PPO discrete (green), and PPO continuous (blue) on the TurtleBot2 robot when trained end-to-end to navigate in an unknown environment. The results depict that all algorithms perform well in the environment and learn a good policy, however, PPO discrete is able to outperform the other methods by a significant margin.
Fig. 5

The learning curves of DQN (red), PPO discrete (green), and PPO continuous (blue) on the TurtleBot2 robot when trained end-to-end to navigate in an unknown environment. The results depict that all algorithms perform well in the environment and learn a good policy, however, PPO discrete is able to outperform the other methods by a significant margin.

Show All
D. Comparison Between Varying Discrete Action Spaces

In this experiment, the impact of the number of actions on the learning performance is analyzed. The linear and the angular velocity for the move forward action is the same as before and only the angular velocities of the turning actions are varied. Table II shows the corresponding angular velocities for the different action spaces. We are comparing 3 different numbers of actions. In Figure 6 , the learning curves of the different variants of DQN are shown. The experiments show that a larger number of actions results in a higher average reward in both evaluated environments. Moreover, the experiments show that more fine-grained action spaces are particularly helpful in environment Maze-v0 , where the agent has to perform more complicated maneuvers as compared to the ones in environment Circuit2-v0 .
TABLE I Inference results for the evaluated DRL methods for grayscale inputs. We evaluated the performance for 12 test rollouts, reporting mean and standard deviation.
Table I- Inference results for the evaluated DRL methods for grayscale inputs. We evaluated the performance for 12 test rollouts, reporting mean and standard deviation.
TABLE II Angular velocities for different action spaces. Velocities sorted from rightmost to leftmost angular direction.
Table II- Angular velocities for different action spaces. Velocities sorted from rightmost to leftmost angular direction.
E. Comparison Between Sensor Modalities

In the previous section, all methods were trained on a single visual input modality. In this experiment, we compare agents equipped with different sensor modalities to investigate the impact of fused sensory inputs. The different sensor modalities are depicted in Figure 7 .

We propose to fuse multiple sources to obtain improved learning performance. The idea is to combine the grayscale images with depth images which are predicted by a conditional GAN. This is done by concatenating a grayscale image with the predicted depth image and using it as an observation. This sensor combination is denoted as Fused. In Figure 8 , the learning curves with Grayscale, Depth, Depth prediction, and Fused sensor inputs for the Circuit2-v0 and Maze-v0 environment are shown. All sensor configurations are trained with the PPO continuous method. From the red learning curve, we can see that the advantage of using the Fused approach increases the average reward compared to using the grayscale input only. From the figure, it can be seen that the depth image is also advantageous in terms of learning performance compared to the raw grayscale input. This result is consistent with earlier research results of Peasley [27] which shows that depth information is vital for tasks like exploration and obstacle avoidance.

Table III shows the inference results for the different evaluated sensor setups. We see that among the tested sensor modalities, the depth image achieves the highest average reward. The difference in the average reward between using depth images or grayscale images as input is significant. However, using our proposed Fused sensor setup, we can achieve almost similar results as with using ground truth depth images.
Fig. 6 - The learning curves of DQN (3 actions), DQN (5 actions), and DQN (21 actions).
Fig. 6

The learning curves of DQN (3 actions), DQN (5 actions), and DQN (21 actions).

Show All
Fig. 7 - Examples of the different input modalities. From left to right: monocular grayscale image, ground-truth depth image, and predicted depth image by the conditional GAN approach.
Fig. 7

Examples of the different input modalities. From left to right: monocular grayscale image, ground-truth depth image, and predicted depth image by the conditional GAN approach.

Show All
SECTION V.
Conclusion

In this paper, we have presented a DRL approach for vision-based obstacle avoidance with a mobile robot from raw sensory data. Our method solely relies on a monocular camera for perception, which enables low-cost, lightweight, and low-power-consuming hardware solutions. We validated our approach on several simulated experiments, moreover, we analyzed the applicability of two different kinds of algorithms, namely PPO and DQN. We showed that control policies for discrete and continuous action spaces can be learned in an end-to-end manner. Our experiments show that these algorithms can learn to navigate in simple maze-like environments without prior knowledge of the environment. Furthermore, we showed how the average reward can be increased by additionally fusing predicted depth images by means of a GAN. Moreover, the impact of the number of actions on the learning behavior is analyzed. This result indicates a promising direction to continue research on DRL for mapless navigation. In future work, we intend to work on transferring the policies trained in simulation to realistic environments. Furthermore, another interesting direction would be to use imitation learning techniques to solve the sample-inefficiency problem and potentially make the training process faster and safer.
Fig. 8 - The learning curves of Fused (red), Grayscale (green), Depth (blue), and Depth prediction (yellow) on the TurtleBot2 robot when trained end-to-end to navigate in an unknown environment. The results depict that all sensor modalities perform well in the environment and learn a good policy, however, the depth sensor is able to outperform the monocular camera by a significant margin. By fusing predicted depth images with grayscale images, we can increase the learning performance.
Fig. 8

The learning curves of Fused (red), Grayscale (green), Depth (blue), and Depth prediction (yellow) on the TurtleBot2 robot when trained end-to-end to navigate in an unknown environment. The results depict that all sensor modalities perform well in the environment and learn a good policy, however, the depth sensor is able to outperform the monocular camera by a significant margin. By fusing predicted depth images with grayscale images, we can increase the learning performance.

Show All
TABLE III Inference results for the evaluated DRL methods for different sensor modalities. We evaluated the performance for 12 test rollouts, reporting mean and standard deviation.
Table III- Inference results for the evaluated DRL methods for different sensor modalities. We evaluated the performance for 12 test rollouts, reporting mean and standard deviation.

Authors
Figures
References
Citations
Keywords
Metrics
Footnotes
   Back to Results   
More Like This
Biologically inspired reinforcement learning for mobile robot collision avoidance

2017 International Joint Conference on Neural Networks (IJCNN)

Published: 2017
Autonomous Navigation for Exploration of Unknown Environments and Collision Avoidance in Mobile Robots Using Reinforcement Learning

2019 SoutheastCon

Published: 2019
Show More
References
1. J. Engel, T. Schöps and D. Cremers, "LSD-SLAM: Large-scale direct monocular SLAM", Proceedings of the European Conference on Computer Vision (ECCV) , 2014.
Show in Context Google Scholar
2. J. Engel, V. Koltun and D. Cremers, "Direct sparse odometry", IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) , vol. 40, no. 3, pp. 611-625, 2017.
Show in Context View Article Full Text: PDF (2650) Google Scholar
3. R. Mur-Artal, J. M. M. Montiel and J. D. Tardos, "ORB-SLAM: a versatile and accurate monocular SLAM system", IEEE Transactions on Robotics (T-RO) , vol. 31, no. 5, pp. 1147-1163, 2015.
Show in Context Google Scholar
4. J. M. Wong, "Towards lifelong self-supervision: A deep learning direction for robotics", 2016.
Show in Context Google Scholar
5. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, et al., "Human-level control through deep reinforcement learning", Nature , vol. 518, no. 7540, pp. 529-533, 2015.
Show in Context CrossRef Google Scholar
6. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, et al., "Mastering the game of Go with deep neural networks and tree search", Nature , vol. 529, no. 7587, pp. 484-489, 2016.
Show in Context CrossRef Google Scholar
7. L. Tai and M. Liu, "Deep-learning in mobile robotics - from perception to control systems: A survey on why and why not", 2017.
Show in Context Google Scholar
8. P. Isola, J.-Y. Zhu, T. Zhou and A. A. Efros, "Image-to-image translation with conditional adversarial networks", Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017.
Show in Context View Article Full Text: PDF (2033) Google Scholar
9. M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, et al., "Rainbow: Combining improvements in deep reinforcement learning", Proceedings of the National Conference on Artificial Intelligence (AAAI) , 2018.
Show in Context Google Scholar
10. Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot and N. de Freitas, "Dueling network architectures for deep reinforcement learning", Proceedings of the International Conference on Machine Learning (ICML) , 2016.
Show in Context Google Scholar
11. H. van Hasselt, A. Guez and D. Silver, "Deep reinforcement learning with double Q-learning", Proceedings of the National Conference on Artificial Intelligence (AAAI) , 2016.
Show in Context Google Scholar
12. T. Schaul, J. Quan, I. Antonoglou and D. Silver, "Prioritized experience replay", 2015.
Show in Context Google Scholar
13. P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, et al., "OpenAI baselines", 2017, [online] Available: https://github.com/openai/baselines.
Show in Context Google Scholar
14. J. Schulman, F. Wolski, P. Dhariwal, A. Radford and O. Klimov, "Proximal policy optimization algorithms", 2017.
Show in Context Google Scholar
15. L. Tai, G. Paolo and M. Liu, "Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation", Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2017.
Show in Context View Article Full Text: PDF (3844) Google Scholar
16. P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard, A. Banino, et al., "Learning to navigate in complex environments", Proceedings of the International Conference on Learning Representations (ICLR) , 2017.
Show in Context Google Scholar
17. L. Tai and M. Liu, "Towards cognitive exploration through deep reinforcement learning for mobile robots", 2016.
Show in Context Google Scholar
18. J. Koutník, G. Cuccu, J. Schmidhuber and F. Gomez, "Evolving large-scale neural networks for vision-based reinforcement learning", Genetic and Evolutionary Computation Conference (GECCO) , 2013.
Show in Context CrossRef Google Scholar
19. J. Ho and S. Ermon, "Generative adversarial imitation learning", 2016.
Show in Context Google Scholar
20. J. Fu, K. Luo and S. Levine, "Learning robust rewards with adversarial inverse reinforcement learning", 2017.
Show in Context Google Scholar
21. S. Arora and P. Doshi, "A survey of inverse reinforcement learning: Challenges methods and progress", 2018.
Show in Context Google Scholar
22. I. Zamora, N. G. Lopez, V. M. Vilches and A. H. Cordero, "Extending the OpenAI gym for robotics: a toolkit for reinforcement learning using ROS and gazebo", 2016.
Show in Context Google Scholar
23. G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, et al., "OpenAI gym", 2016.
Show in Context Google Scholar
24. M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, et al., "ROS: an open-source robot operating system", ICRA workshop on open source software , 2009.
Show in Context Google Scholar
25. N. Koenig and A. Howard, "Design and use paradigms for gazebo an open-source multi-robot simulator", Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2004.
Show in Context View Article Full Text: PDF (512) Google Scholar
26. D. P. Kingma and J. L. Ba, "Adam: A method for stochastic optimization", Proceedings of the International Conference on Learning Representations (ICLR) , 2015.
Show in Context Google Scholar
27. B. Peasley and S. Birchfield, "Real-time obstacle detection and avoidance in the presence of specular surfaces using an active 3d sensor", 2013 IEEE Workshop on Robot Vision (WORV) , 2013.
Show in Context View Article Full Text: PDF (2054) Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
