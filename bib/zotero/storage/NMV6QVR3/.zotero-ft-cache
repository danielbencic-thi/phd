Enhanced Pix2pix Dehazing Network
Yanyun Qu1∗ Yizi Chen1∗ Jingying Huang1 Yuan Xie2,3† 1Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Fujian, China
2School of Computer Science and Software Engineering, East China Normal University, Shanghai, China
3Institute of Advanced Artiﬁcial Intelligence in Nanjing, Horizon Robotic, Nanjing, China
yyqu@xmu.edu.cn, atavism@msn.cn, jyhuang33@outlook.com, yxie@sei.ecnu.edu.cn

Abstract
In this paper, we reduce the image dehazing problem to an image-to-image translation problem, and propose Enhanced Pix2pix Dehazing Network (EPDN), which generates a haze-free image without relying on the physical scattering model. EPDN is embedded by a generative adversarial network, which is followed by a well-designed enhancer. Inspired by visual perception global-ﬁrst [5] theory, the discriminator guides the generator to create a pseudo realistic image on a coarse scale, while the enhancer following the generator is required to produce a realistic dehazing image on the ﬁne scale. The enhancer contains two enhancing blocks based on the receptive ﬁeld model, which reinforces the dehazing effect in both color and details. The embedded GAN is jointly trained with the enhancer. Extensive experiment results on synthetic datasets and real-world datasets show that the proposed EPDN is superior to the state-ofthe-art methods in terms of PSNR, SSIM, PI, and subjective visual effect.

1. Introduction
Haze is a typical atmospheric phenomenon, and it causes color distortion, blurring and low contrast for the photographed image, which results in the difﬁculties of subsequent tasks, such as object recognition and image understanding. Thus, more and more attentions are attracted to image dehazing.
Most of the successful approaches depend on the physical scattering model [12] [14], which is formulated as

I(z) = J(z)t(z) + A(z)(1 − t(z)),

(1)

∗Equal contribution †Corresponding author

(a) Haze

(b) GFN

(c) DCPDN

(d) Ours

Figure 1. A single image haze removal example. Our method pro-

duces a haze-free image with faithful color and rich details com-

pared with GFN [16] and DCPDN [21].

where I is the observed hazy image, J is the scene radiance, t is the transmission map, A is the atmospheric light and z is the pixel location. The solution of the haze-free image depends on the estimation of the atmospheric light and the transmission map. Early dehazing methods are mostly prior-based methods, e.g. DCP [7], which estimates the transmission map by investigating the dark channel prior. These prior-based methods can achieve good dehazing effect to a certain extent. However, the prior may be easily violated in practice, which leads to an inaccurate estimation of transmission map so that the quality of the dehazing image is not desirable. With the rising up of deep learning, the estimations of the transmission map or the atmospheric light are estimated by the convolutional neural network rather than relying on priors. Some methods utilize the deep convolutional neural network to estimate the transmission map [3] [15], some employ the deep convolutional neural network to jointly estimate the atmospheric light and

8160

the transmission map [20] [23]. Either early dehazing methods or exsiting deep learning based ones almost depend on the physical scattering model, and the estimation accuracies of the atmospheric light and the transmission map greatly inﬂuence the quality of the dehazing image.
In order to disentangle image dehazing from the physical scattering model, we try to transform a hazy image to a haze-free image pixel by pixel directly. Motivated by the success of generative adversarial networks (GANs) in image-to-image translation [9] [19] [24], we marry GAN to image dehazing. However, GANs for image-to-image translation can not be directly applied to image dehazing because image haze is the depth-dependent noise and nonuniform. Directly application will produce undesirable results which are overcolored and lack of details. It is known as the visual perception global-ﬁrst theory [5], an object or scene is discriminated only in a global way, not necessarily depending on the details of realistic images, while the creation of a realistic image must be dependent on details as more as possible.
In this paper, we propose Enhanced Pix2pix Dehazing Network (EPDN). EPDN includes three parts: the discriminator, the generator, and the enhancer. The GAN module with generator and discriminator is embedded in EPDN, in which the discriminator just supervises the intermediate result of EPDN. The enhancer following the generator will reinforce the output of the GAN, which is designed according to the receptive ﬁeld model. To the best of our knowledge, EPDN is the ﬁrst work to embed GAN for image dehazing according to the visual perception theory. Moreover, regarding the proposed architecture, we develop a joint training scheme which alternatively optimizes the embedding GAN (generator and discriminators) and the generator along with the enhancer.
The proposed method can generate a more realistic image in terms of color and details. Fig. 1 shows the visual effect of our method. Compared with GFN [16] and DCPDN [21], our method achieves more realistic dehazing effect with faithful color and structures. Moreover, we introduce the Perceptual Index (PI) to evaluate the performance of image dehazing including Peak Signal to Noise Ratio (PSNR) and Structural Similarity (SSIM). As we know, visual effect is a subject estimation, which is not convenient for computation. PI is computable by simulating the visual perception.
The contributions of our work are as follow:
1. EPDN is proposed for image dehazing, which does not rely on the physical scattering model, while adopts the way of image-to-image translation alternatively.
2. Inspired by the global-ﬁrst property of visual perception, the embedded GAN and enhancer are designed to produce a perceptually pleasing images with more details.

3. A joint training scheme is developed for updating the embedded GAN and enhancer through reasonably combining four kinds of loss functions.
4. The Perceptual Index (PI) is introduced for quantitative evalution from the perceptual perspective. In addition, extensive experiments on both synthetic datasets and real-world dataset indicate that EPDN performs favourably against the state-of-the-art methods. Especially, our results are outstanding in visual perception.
2. Related Work
Our work is related to two topics: single image dehazing and generative adversarial networks which are brieﬂy discussed in this section.
Single image dehazing. Most of the existing dehazing methods depend on the physical scattering model [12] [14] Eq. (1), which are divided into two classes: the prior based methods and the learning-based methods. The physical model contains two important factors: the transmission map and the atmospheric light. Efforts are made to estimate the two factors for a solution of haze removal.
Prior-based dehazing. Tan et al. [17] made a model to maximize the contrast of an image for image dehazing because it is observed that haze-free images have higher contrast than hazy images. He et al. [7] [8] proposed a dark channel prior for the estimation of the transmission map. Zhu et al. [25] recovered depth information via color attenuation prior. Tang et al. [18] systematically investigated a variety of haze-relevant priors in a regression framework to learn the best prior combination for image dehazing. A haze line prior is proposed by Dana Berman et al. [1], which assumes that colors of a haze-free image are well approximated by a few hundred distinct colors. Though the priorbased dehazing methods have achieved promising results, the prior is not robust to the unconstraint environment in the wild, thus, the dehazing performance is not always desirable.
Learning-based dehazing. Different from the priorbased methods, the learning-based methods directly estimate the transmission map or atmospheric light rather than relying on the priors. Cai et al. [3] proposed an end-toend dehazing model based on convolutional neural network (CNN) named DehazeNet, which estimates the transmission map. Ren et al. [15] proposed a multi-scale deep model to estimate the transmission map. Li et al. [10] reformulated the physical scattering model, and design AODNet to learn a mapping function based on CNN. Ren et al. [16] used an encoder-decoder network and adopted a novel fusion-based strategy to directly restore a clear image from a hazy image.
Generative Adversarial Networks (GANs). Recently, great progress is made in GAN [6]. GAN includes two

8161

parts: the discriminator and the generator. They are trained simultaneously in an adversarial way so that the generator could produce a realistic image which confuses the discriminator. GAN is widely used in many computer vision applications. Especially, GAN has achieved promising results in image synthesis [9] [19] [24]. Inspired by the success of GAN, we utilize it for image dehazing. DCPDN [21] implements GAN on image dehazing which learns transmission map and atmospheric light simultaneously in the generators by optimizing the ﬁnal dehazing performance for haze-free images. Yang et al. [20] proposed the disentangled dehazing network, which uses unpaired supervision. The GAN proposed by Yang et al. [20] contains three generators: the generator for the haze-free image, the generator for the atmospheric light, and the generator for transmission map. DehazeGAN [23] draws lessons from the differential programming to use GAN for simultaneous estimations of the atmospheric light and the transmission map. The marriage of GAN and image dehazing is still in the beginning. The current dehazing methods via GAN all depend on the physical scattering model. Until now, little is discussed how to deal with image dehazing independent of the physical scattering model. As discussed in Introduction, it is meaningful to investigate a model-free dehazing method via GAN.
3. Proposed Method
3.1. The Architecture of EPDN
In this paper, we cast the single image dehazing problem as a task of image-to-image translation. Hazy images and haze-free images are regarded as two different image styles. The framework of EPDN is shown in Fig. 2, which consists of a multi-resolution generator module, the enhancer module, and the multi-scale discriminator module. The GAN architecture similar to pix2pixHD [19] is embedded in EPDN, followed by the enhancer. The enhancer contains two welldesigned enhancing blocks, each of which is built depending on the receptive ﬁeld model. And a shot-cut is employed to maintain the color information of original images. In the following, we detail the architecture of EPDN.
Multi-resolution generator. The multi-resolution generator of GAN module consists of global sub-generator G1 and a local sub-generator G2, as shown in Fig. 2. Both G1 and G2 include a convolutional front-end, three residual blocks, and a transposed convolutional back-end. The input of G1 is 2× downsampled from the original hazy images. G1 is embedded in G2, and the element-wise sum of the output of G1 and the feature maps obtained by the convolutional front-end of G2 is fed into the residual block of G2. The multi-resolution structure has been proven successful in image-to-image translation. The global sub-generator creates an image on a coarse scale, while the local subgenerator creates an image on a ﬁne scale. And the com-

bination of the two sub-generators produce an image from coarse-to-ﬁne.
Multi-scale discriminator. The embedded GAN module contains a multi-scale discriminator module which contains two-scale discriminators named D1 and D2. D1 and D2 have the same architecture, and the input of D2 is 2× downsampled from the input of D1. The output of the generator is fed into D1. The multi-scale discriminators could guide the generator from coarse-to-ﬁne. On the one hand, D2 guides the generator to produce a global pseudo realistic image on a coarse scale. On the other hand, D1 guides the generator on a ﬁne scale.
Enhancing block Even though pix2pixHD utilizes the coarse-to-ﬁne feature, the results obtained from only pix2pixHD still lack details and are overcolored. One possible reason is that the existing discriminator is limited in guiding the generator to create realistic details. In other words, the discriminator should merely direct the generator to restore structure imfromation rather than details.
To efﬁciently solve this problem, a pyramid pooling block [21] [22] is implemented to make sure the details of features from different scales are embedded in the ﬁnal result. We name it enhancing block. Drawing lesson from global context information in object recognition, details of features in various scales are needed. Thus, the enhancing block is designed according to the recepive ﬁeld model which can extract information on different scales. The enhancing block is shown in Fig. 3. In detail, there are two 3 × 3 front-end convolution layers in the enhancing block. The output of the front-end convolution layer is downsampled by factors of 4×, 8×, 16×, 32× to build a four-scale pyramid. Feature maps on different scales provide different receptive ﬁelds, which helps to reconstruct an image on various scales. And then, 1 × 1 convolution is implemented for dimension reduction. Actually, 1 × 1 convolution implies the attention mechanism which weights the channel adaptively. After that, we upsample the feature maps to the original size and concatenate them together with the output of the front-end convolution layer. Finally, the 3 × 3 convolution is implemented on the concatenation of the feature maps.
In EPDN, the enhancer includes two enhancing blocks. Moreover, the ﬁrst enhancing block is fed by the concatenation of the original image and the feature maps of the generator which are also fed to the second enhancing block.
3.2. Overall Loss Function
In order to optimize EPDN, we utilize four loss functions as Eq. (2), the adversarial loss LA, the feature matching loss LF M , the perceptual loss LV GG, and the ﬁdelity loss LF . The adversarial loss together with the feature matching loss is used to make the GAN module learn the global information and recover the original image structure by using multi-

8162

Figure 2. The architecture of EPDN. EPDN includes three parts: the multi-resolution generator, i.e. G1 and G2, the muti-scale discriminator, i.e. D1 and D2, and the enhancer.

Figure 3. The structure of enhancing block

scale features. Perceptual loss and ﬁdelity loss are used to reinforce the ﬁne features and preserve original color information. To simplify the model, thes coefﬁcients of the feature matching loss and perceptual loss are set to be the same.

LEP = LA + λLF M + λLV GG + LF .

(2)

Adversarial loss. We adopt the adversarial loss of GAN. The generator is initialized to translate a hazy image to the haze-free image, while the discriminator aims to distinguish whether an image is real or fake. Considering there are the two-scale discriminators D1, D2, the adversarial loss is formulated as a multi-task learning loss

∼

LA = min[ max

ℓA(G, Dk)],

(3)

∼
G

D1,D2 k=1,2

∼
where ℓA(G, Dk) is the single adversarial loss of the k-th

discriminator Dk, it is formulated as,

ℓA

∼
(G,

Dk

)

=

E(X

)[log

Dk

(X

)]

+

E(X)[log(1

−

Dk

(G∼ (Xˆ

)))],

(4)

and X and Xˆ denote real haze-free images and hazy im-

ages. G∼(Xˆ ) represents the output produced by the generator

of the GAN module, but not the ﬁnally result of EPDN.

Feature matching loss. In order to make a realistic

image, the adversarial loss is improved by incorporating a

feature matching loss based on the discriminator. We use

this loss to make the generator produce natural multi-scale

statistical information. The intermediate feature maps are

learned to match between the real and the synthesized im-

age. The feature matching loss function is formulated as

∼

LF M = min[ ℓF M (G, Dk)].

(5)

∼

G k=1,2

∼
ℓF M (G, Dk) is the feature matching loss with the k-th discriminator Dk.

∼
ℓF M (G, Dk) = E(X)

T i=1

1[ Ni

Dk(i)(X)−Dk(i)(G∼(Xˆ ))

1],

(6)

where T is a total number of layers used for feature extraction, Ni is a number of elements in each layer, Dk(i) is the operator of the feature extraction of the i-th layer in Dk.
Perceptual loss. In order to keep the perceptual and se-
mantic ﬁdelity, we use perceptual loss function to measure
high-level difference between the hazy image and its coun-
terpart dehazing image. Based on a pre-trained VGGNet for image classiﬁcation, we extract the activations of the ith layers of VGGNet, denoted by φi(), which are treated

8163

as the perceptual feature. We use the pixel-wise distance to measure the difference between the perceptual features of the hazy and dehazing image. The perceptual loss function is as follow

LφV,Gi G(Yˆ , X)

=

1 CiHiWi

φi(Yˆ ) − φi(X) 1,

(7)

where Yˆ is the ﬁnal result of EPDN. Hi and Wi are the height and width of the i-th feature map, and Ci indicates the channel.
Fidelity loss. The Euclidean distance between the hazefree image X and ﬁnal output Yˆ is regarded as the ﬁdelity loss, which is deﬁned as

LF = X − Yˆ 2.

(8)

3.3. Training

Algorithm 1: EPDN training algorithm

Input:

nb ← the batch size; λ ← the hyper-parameter;

1 for num = 1; num ≤ trainingiterations do 2 Sample hazy examples Xˆ = {xˆ(1), ..., xˆ(nb)};

3 Sample clean examples X = {x(1), ..., x(nb)};

4 M ← G∼(Xˆ ), the output of the muti-resolution

generator;

5 Y ← EP (Xˆ ), the output of EPDN;

6 Mk ← 2k−1 time downsample(M ); 7 Xk ← 2k−1 time downsample(X); 8 for k = 1, 2 do

9

Update the discriminators Dk by ascending

the gradient of Eq. (3);

∼
10 Update the multi-resolution generator(G) by descending the gradient of the sum of Eq. (3) and Eq. (5);
∼
11 Update G and enhancer by descending the gradient of the sum of Eq. (7) and Eq. (8);

Because GAN is only a part of the whole architecture of EPDN, we cannot implement the training scheme of GAN directly on EPDN. We develop a new training scheme. We adopt the alternative iteration algorithm. Firstly, the GAN architecture is optimized with the adversarial loss function Eq. (3) and the feature matching loss function Eq. (5). In detail, we ﬁrst update the multi-scale discriminator by ascending its gradient and then update the multi-resolution generator by descending its gradient. Secondly, the enhancer and the multi-resolution generator is optimized by descending the gradient of the sum of perceptual loss Eq. (7) and the

Table 1. Ablation study settings.

Method Enhancing block short-cut

GAN+E

1

-

GAN+E+S

1

GAN+EE

2

-

GAN+

2

GAN

0

-

Ours

2

Embedded
-

Table 2. Comparison of variants with different components on the outdoor dataset of SOTS.

Method
GAN+E GAN+E+S GAN+EE
GAN+ GAN Ours

PSNR
20.56 18.66 21.47 21.73 20.78 22.57

SSIM
0.7553 0.7636 0.7992 0.8716 0.7455 0.8630

PI
3.2394 2.2374 3.1153 2.556 2.7397 2.3858

ﬁdelity loss Eq. (8). We summarize the algorithm as Algorithm 1. Different from the original GAN training scheme, our generator is updated twice respectively with the discriminator and the enchancer, which satisﬁes the global-ﬁrst theory.
4. Experiments
In this section, we implement the proposed method on both the synthesis dataset and the real-world dataset to demonstrate the effectiveness of the proposed method. We compare our proposed method with ﬁve state-of-theart methods: DCP [7] (He CVPR’09), DehazeNet [3] (Cai TIP’16), AOD-Net [10](Li ICCV’17), GFN [16] (Ren CVPR’18), and DCPDN [21] ( Zhang CVPR’18). For the fairness of comparison, the source codes of the compared methods are presented publicly by the authors. In addition, we do an ablation study to demonstrate the effectiveness of our embedding GAN and the enhancing block.
4.1. Experiment Settings
Dataset. RESIDE [11] is a new large-scale hazy image dataset and it consists of ﬁve subsets: Indoor Training Set (ITS), Outdoor Training Set (OTS), Synthetic Objective Testing Set (SOTS), Real World task-driven Testing Tet (RTTS), and Hybrid Subjective Testing Set (HSTS). Among the ﬁve subsets, ITS, OTS, SOTS are synthetic datasets, RTTS is the real-world dataset, both synthetic data and realword hazy data are involved in HSTS. On the one hand, ITS and SOTS which contain both indoor and outdoor hazy images are respectively employed for our training and testing. On the other hand, we collect the real-world images used by

8164

Table 3. Comparison results of the state-of-the-art dehazing methods on SOTS.

Method

DCP [7] DehazeNet [3] AOD-NET [10] DCPDN [21] GFN [16]

indoor

PSNR SSIM
PI

16.62 0.8179 3.9535

21.14 0.8472 4.0458

19.06 0.8504 3.6961

15.85 0.8175 4.7485

22.30 0.8800 4.1146

PSNR outdoor SSIM
PI

19.13 0.8148 2.5061

22.46 0.8514 2.4346

20.29 0.8765 2.4280

19.93 0.8449 2.7269

21.55 0.8444 2.1608

Ours
25.06 0.9232 4.0620
22.57 0.8630 2.3858

Input

GAN+E GAN+E+S GAN+EE GAN+

GAN

Ours

GT

Figure 4. Comparison results of variants with different components in visual effect on outdoor images.

previous methods, and compare our method with the stateof-the-art methods on this dataset.
Training Details. During training, ITS is used as the training dataset which is also used as the training dataset for the compared methods. We adopt Adam optimizer with a batch size of 1, and set a learning rate as 0.0002, the exponential decay rates as (β1, β2) = (0.6, 0.999). The hyperparameter of loss function is set as λ = 10. We implement our model with the PyTorch framework and a TITAN GPU.
Quality Measures. To evaluate the performance of our method, we adopt three metrics: the Peak Signal to Noise Ratio (PSNR), the Structural Similarity index (SSIM) and Perceptual Index (PI). As we know, image qualiﬁcation evaluation is very important for image restoration. It includes the objective measurement and subjective measurement. For the former, PSNR and SSIM are usually used in image dehazing. For the latter, visual effect is used to evaluate the image dehazing performance. However, it is not convenient to use for image qualiﬁcation evaluation. PI is a new criterion which bridges the visual effect with computable index. And it has been recognized to be effective in image super-resolution [2]. In the experiment, we use PI to evaluate the performance of image dehazing. The lower the image quality is, the higher PI is. PI is formulated as

PI

=

1 2

((10

−

M a)

+

N IQE),

where M a and N IQE are two image qualiﬁcation indexes which are detailed in [4] and [13].
4.2. Ablation Study
To better demonstrate the effectiveness of the architecture of our method, we conduct an ablation study by considering the combination of four factors: GAN, one enhancing block, two enhancing blocks, and the short-cut skip. We construct the following variants with different component combinations: 1) GAN: only pix2pixHD [19] is used; 2) GAN+E: only one enhancing block follows the embedding pix2pixHD; 3) GAN+E+S: the variant GAN+E combines a short-cut skip which connects the original image to the enhancer; 4) GAN+EE: two enhancing blocks follow the embedding pix2pixHD; 5) GAN+: the whole architecture is a GAN, in which the the whole generator is the combination of the generator of pix2pixHD and two enhancing blocks and the short-cup skip connects the original image to the ﬁrst enhancing block. The ablation conﬁgurations are given in Table. 1.
We compare EPDN with ﬁve variants with different components on the outdoor dataset of SOTS. The results are shown in Table. 2 and Fig. 4. It demonstrates that the proposed EPDN achieves the best performance of image dehazing in PSNR and SSIM and the best visual effects. Compared with pix2pixHD [19], the improvement gains in

8165

Input

DCP [7] DehazeNet [3] AOD-Net [10] DCPDN [21] GFN [16]

Ours

GT

Figure 5. Comparison of the state-of-the-art dehazing methods on SOTS. The upper three rows show the dehazing results on outdoor

images and the bottom three rows show the dehazing results on indoor images.

PI: 2.2518

PI: 1.9735

PI: 2.002

PI: 2.4211

PI: 1.7300

PI: 1.8513

PI: 2.4675

PI: 2.2955

PI: 2.7286

PI: 2.9415

PI: 1.8820

PI: 2.2123

PI: 3.9853

PI: 3.0106

PI: 3.1469

PI: 2.9118

PI: 3.0304

PI: 2.9904

Input

DCP [7] DehazeNet [3] AOD-Net [10] DCPDN [21] GFN [16]

Ours

Figure 6. Comparison of the state-of-the-art dehazing methods on the real dataset. The images in the ﬁrst row and the third row are the

close-up of the red boxes in the second row. PIs are shown at the top of each image.

PSNR and SSIM are 2.02 dB and 0.11 respectively, which shows that EPDN is better than pix2pixHD. GAN+E+S is the worst of the ablation study variant, because the hazy image through the short-cut skip add more noises on the output of generator. GAN+ is superior to GAN+E+S, be-

cause two enhancing blocks dehaze more effectively than one. We also compare the variant with and without the short-cut skip and observe that the variant with the shortcut skip is better than those without the short-cut skip in PI, because the re-enter of the original image keep the faith-

8166

ful color and details. EPDN and GAN+ are in about the same performance, but EPDN is better than GAN+ in two of three criteria, especially it is better in PI, thus, we adapt the architecture in this paper. From Fig. 4, we observe that the proposed EPDN achieve the closest result to the ground truth. Though GAN+ achieves the similar performance to ours in PSNR and SSIM, but it’s visual effect is inferior to ours. GAN+ is overcolored obviously. Moreover, without the short-cut-skip the dehazing results looks a little darker in the ﬁrst row of results, which demonstrate the effectiveness of the short-cut skip.
These ablation study demonstrates that the enhancing blocks, the short-cut skip, and the embedded structure are effective for image dehazing.
4.3. Comparisons with State-of-the-art Methods
Results on synthesis dataset. The comparison results are shown in Table. 3 in which the digital valuse are the averages of the results on SOTS in terms of PSNR, SSIM, and PI. It demonstrates that EPDN achieves the best performance of image dehazing in terms of both PSNR and SSIM on the indoor dataset of SOTS, and it achieves the breakthrough gain with 2.76 dB in PSNR and 0.0432 in SSIM compared with the second place method GFN [16].
On the outdoor dataset of SOTS, EPDN achieves the best performance in PSNR, ranks the second among the compared methods in SSIM and PI. GFN [16] rank the second in PSNR and the ﬁrst in PI. The distance between the best and the second best is 0.11 dB and 0.01 in PSNR and SSIM, which is smaller than the counterpart distance on indoor data.
Fig. 5 gives the comparison of visual effect in which the comparison results on outdoor data are shown in the upper three row and those on indoor data are shown in the bottom three row. It is observed that there remains some haze in the dehazing images. DCP [7] suffers from color distortion where the results are usually darker than the ground truth images. DCPDN [21] also suffers from color distortion and it fails in details restoration. Most of the color information has been lost in the GFN [16], at the same time, it generates some artifacts. EPDN makes the dehazing image look more like the ground truth image. Furthermore, it is obvious that our model exactly outperforms the above-mentioned methods in details recovery, and it improves the dehazing results qualitatively and quantitatively.
Results on a real-world dataset. Fig. 6 shows the comparison results of visual effects on real hazy images. It is observed that: 1) Though the proposed EPDN is trained on synthesis data, it still achieves desirable dehazing results on the real-world dataset, which show the robustness of EPDN. 2) DCP [7] results in color distortion in the sky area and suffers from blur. DehazeNet [3] and AOD-Net [10] cannot remove haze effectively. DCPDN [21] and GFN [16] can

(a) Input

(b) Ours

(c) GT

Figure 7. A dehazing example for heavily hazy scene. Our method

is not robust enough when the haze is extremely thick in the origi-

nal input.

not remove haze effectively in heavily hazy scene. 3) Our method is comparable in terms of PI among those state-ofthe-art methods, and achieves best visual effect. Comparing to the results of ﬁve state-of-the-art methods, it can be seen that our results (EPDN) are superior in both visual effect and quantitative criteria.

4.4. Limitation
Our method is not very robust for heavily hazy scene. As shown in Fig. 7, the edges of objects in heavily haze can not be recovered naturally. The limitation might be solved by applying more enhancing blocks in our network.

5. Conclusion
In this paper, we propose Enhanced Pix2pix Dehazing Network (EPDN) which does not rely on the estimations of the transmission map and atmospheric light. We transform the problem of image dehazing to the problem of imageto-image translation. Draw lessons from the global-ﬁrst [5] theory of visual perception, we embed a GAN in our architecture, which is followed by two well-designed enhancing blocks, and the discriminator only guides the output of the multi-resolution generator. Experimental results on both the synthesis dataset and the real-world dataset demonstrate that the proposed method achieves the best performance of image dehazing in both the quantitative and qualitative evaluations. Especially, it keeps the faithful color and structures.

6. Acknowledgements
This work was supported by the National Natural Science Foundation of China under Grant 61876161, Grant 61772524, and Grant U1065252, and partly by the Beijing Municipal Natural Science Foundation under Grant 4182067.

8167

References
[1] Dana Berman, Shai Avidan, et al. Non-local image dehazing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1674–1682, 2016.
[2] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. 2018 PIRM challenge on perceptual image super-resolution. arXiv preprint arXiv:1809.07517, 2018.
[3] Kui Jia Chunmei Qing Dacheng Tao Bolun Cai, Xiangmin Xu. Dehazenet: an end-to-end system for single image haze removal. IEEE Transactions on Image Processing, 25(11):5187–5198, 2016.
[4] Xiaokang Yang Ming Hsuan Yang Chao Ma, ChihYuan Yang. Learning a no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 2017.
[5] Lin Chen. The topological approach to perceptual organization. Visual Cognition, 12(4):553–637, 2005.
[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680, 2014.
[7] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal using dark channel prior. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1956–1963, 2009.
[8] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal using dark channel prior. IEEE Trans Pattern Anal Mach Intell, 33(12):2341–2353, 2011.
[9] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125–1134, 2017.
[10] Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, and Dan Feng. Aod-net: All-in-one dehazing network. In IEEE International Conference on Computer Vision, pages 4780– 4788, 2017.
[11] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking singleimage dehazing and beyond. IEEE Transactions on Image Processing, 28(1):492–505, 2019.
[12] Earl J McCartney. Optics of the atmosphere: scattering by molecules and particles. New York, John Wiley and Sons, Inc., 1976. 421 p., 1976.
[13] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a “completely blind” image quality analyzer. IEEE Signal Processing Letters, 20(3):209–212, 2013.
[14] Srinivasa G Narasimhan and Shree K Nayar. Vision and the atmosphere. International journal of computer vision, 48(3):233–254, 2002.
[15] Wenqi Ren, Si Liu, Hua Zhang, Jinshan Pan, Xiaochun Cao, and Ming-Hsuan Yang. Single image dehazing via multiscale convolutional neural networks. In European conference on computer vision, pages 154–169. Springer, 2016.

[16] Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, and Ming-Hsuan Yang. Gated fusion network for single image dehazing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
[17] Robby T. Tan. Visibility in bad weather from a single image. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–8, 2008.
[18] Ketan Tang, Jianchao Yang, and Jue Wang. Investigating haze-relevant features in a learning framework for image dehazing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2995–3000, 2014.
[19] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
[20] Xitong Yang, Zheng Xu, and Jiebo Luo. Towards perceptual image dehazing by physics-based disentanglement and adversarial training. In AAAI, 2018.
[21] He Zhang and Vishal M Patel. Densely connected pyramid dehazing network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
[22] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6230–6239, 2017.
[23] Hongyuan Zhu, Xi Peng, Vijay Chandrasekhar, Liyuan Li, and Joo-Hwee Lim. Dehazegan: when image dehazing meets differential programming. In IJCAI, pages 1234–1240, 2018.
[24] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. arXiv preprint, 2017.
[25] Qingsong Zhu, Jiaming Mai, and Ling Shao. A fast single image haze removal algorithm using color attenuation prior. IEEE Transactions on Image Processing, 24(11):3522–3533, 2015.

8168

