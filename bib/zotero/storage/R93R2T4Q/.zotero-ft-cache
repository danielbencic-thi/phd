A Reinforcement Learning based Path Planning Approach in 3D Environment
Geesara Kulathunga, Innopolis University

arXiv:2105.10342v2 [cs.RO] 12 Feb 2022

Abstract—Optimal motion planning involves obstacles avoidance where path planning is the key to success in optimal motion planning. Due to the computational demands, most of the path planning algorithms can not be employed for real-time based applications. Model-based reinforcement learning approaches for path planning have received certain success in the recent past. Yet, most of such approaches do not have deterministic output due to the randomness. We analyzed several types of reinforcement learning-based approaches for path planning. One of them is a deterministic tree-based approach and other two approaches are based on Q-learning and approximate policy gradient, respectively. We tested preceding approaches on two different simulators, each of which consists of a set of random obstacles that can be changed or moved dynamically. After analysing the result and computation time, we concluded that the deterministic tree search approach provides highly stable result. However, the computational time is considerably higher than the other two approaches. Finally, the comparative results are provided in terms of accuracy and computational time as evidence.

algorithms is to ﬁnd the best action at each state by expanding the tree or graph to minimize the cost or maximize the reward. However, tree or graph expansion becomes computationally expensive when state-space (states and actions combinations) complexity grows due to the enamours look ahead information. For example, state-space complexity of Reversi game is 1028, for Tic-tac-toe it is 103, and for Go it is 10171 [1].
Our main contribution of the work is to explore the potentials of both RL-based path planning and deterministicbased path planning and how can incorporate the best of both to develop a fast and robust path planning approach, especially for quadrotors. For that, initially, we formulated the path planning problem for a deterministic system (actions are discrete and known) in which the optimal action in each state was selected by using the Optimal Planning of Deterministic Systems (OPD) algorithm [2].

I. INTRODUCTION
Over the past decade, deep learning-enabled approaches have been obtained satisfactory results in various domains such as optimal motion planning, hyperspectral image classiﬁcation, and path planning. In comparison to typical path planning algorithms, reinforcement learning (RL) based approaches were received signiﬁcant attention in the recent past due to the success of the deep learning and computer vision. However, RL-based approaches without incorporating deep learning and computer vision were there for decades without proper success.
Most RL-based approaches consist of V (value function) or Q (function), whose policies are deﬁned in deterministic ways. However, the recent development of deep learning can help to incorporate sophisticated learning to formulate V or Q functions appropriately. RL-based path planning can be classiﬁed in two ways: model-base, model free. Model-based path panning approaches have several advantages over modelfree approaches, for example, learning dynamics of quadrotor (model-free) is rather complicated because using a modelbased approach does guarantee the dynamic feasibility since such approaches depend on the actual motion model.
Typical graph-based or tree-based search algorithms, e.g., A*, Dijkstra and Min-Max, can be applied for planning, scheduling, discrete optimization, and games. In such search algorithms, in general, the algorithm describes by a set of states and actions that correspond to each state, which can be described explicitly in most situations. The objective of these

Fig. 1. The improved tree based RL path planner. xs and xg denote start and goal positions, respectively. In blue colored trajectory depict the planned paths whose the start position kept the same position while changing the goal position
II. RELATED WORK
Motion planning can be carried out several different ways [3]: search-based, sampling-based, and optimizationbased. In search-based approaches, search space (or state space) is discretized into partitions, namely lattices and a path between the initial position and the end position are connected through a sequences of straight lines yields the shortest path between initial and end poses. Dijkstra’s algorithm [4], A* [5], and D* [6] are a few widely used search-based algorithms. In most of the situations, real-time constraints are not met with search-based approaches due to high computational demands and memory footprints for state space representation.

Sampling-based approaches do address state representation by

incrementally drawing the state space. RRT* [7] and PRM [8]

are the mostly used ones. Optimization-based motion planning

approaches can alleviate most of the problems preceding two

approaches have. There are various techniques are proposed

for this kind. Minimum-snap [9] one of the ﬁrst proposed

techniques for optimal motion planing for Quadrotors.

In motion planning, agent (robot) is described by its state

(st ∈ S) for a given time index where S is the state space. Based on the reasoning approach, agent can take several

different actions, e.g., at ∈ A which lead agent to move to st+1 ∈ S. In other word, let P ∈ M (S)S×A be the system
dynamics, st+1 is drawn based on a conditional probability,
i.e., P (st+1 | st, at), where M (S) denotes the a set of states within S. Moreover, control policy π ∈ M (A)S,

which agent’s actions are drawn from, can be deﬁned as π(at | st). Hence, ﬁnding an optimal control policy π∗

is the main objective of agent at each time index t. There are various approaches can be applied for ﬁnding an π∗.

Yet this is open research problem due to several reasons,

including high computational demands, execution time, and

the nature of the problem: linear or non-linear. In Rein-

forcement Learning (RL), ﬁnding an optimal control policy

can be designed as a leaning-based decision making pro-

cess, namely Marko Decision Process (MDP): (S, A, P, R, γ).

Agent gets rewards R(st, at), where R ∈ [0, 1]S×A when

agent moving from st to st+1. The ultimate objective is to

maximize the rewards at each t, which eventually gives π∗.

Thus, total reward is estimated by Rπ =

N t=0

γtR(st

,

at

),

where N number of steps agent moves along a trajectory

Π = ((s0, a0), (s1, a1), (s2, a3), ..., (sN , aN )). There could be

exist multiple trajectories that lead to optimal solution. Hence,

performance of a trajectory (policy π) is estimated through
a value function, namely V π(s) = E[Rπ | s0 = s]. Thus, objective is to ﬁnd V ∗(s) = maxπ V π(s), ∀s ∈ S.

Model-based and model-free are the main families in RL. Either of such families can be used for ﬁnding π∗. Yet, some

of the work well over others due to problem characteristics.

Model-based learning is preferred when the model dynamics

are simple ( [10]–[12]), and but π estimation is complex.

On the contrary, model-free learning is preferred for model

with complex dynamics( [13], [14]) yet π estimation relatively

simple. Dynamic Programming (DP) approaches (e.g., value

iteration [15], policy iteration [16]) ensure the computational

complexity around O(|S| |A| logγ( (1 − γ))) for discrete systems whose action-space is deﬁned by S ×A. On the contrary,

when S becomes continuous, Approximate Dynamic Program-

ming (ADP) gives feasible solution over exact DP, where

value function or policy is approximated within a provided

hypothesis. Other than ADP, sampling-based optimization(or

black-box optimizations) (e.g., Cross Entropy Method (CEM)

[17], Covariance Matrix Adaptation Evolution Strateg (CEM-

ES) [18]) are used when S is large or continuous. Such

approaches does not depends on the complete knowledge

of MDP, but rather depends on the generator model, i.e., simulator, which can produce next state s‘ ∼ P (s‘ | s, a)

and corresponding reword R(s, a).

Monte Carlo Tree Search (MCTS) [19] is preferred when

the action space is discrete. MCTS has look-ahead three

that starts from the current state and look for a optimal

action to be taken as each step progressively expanding by

sampling the trajectories using the generative model. The basic

MCTS algorithm consists of four steps: selection, expansion,

simulation and backpropagation. Selection function is applied

to end of the current tree and one tree node is created

in the expansion step. Take possible actions from such a

expanded tree node and simulate till look-ahead node reach

to its terminal state. In the backpropagation step, sum the

total reward from the expansion node to terminal node is

calculated by vi + C ·

lnN ni

,

where

C

is

a

exploration

factor,

vi mean value of node ni, i.e., success ratio, ni is the number

of times ni was visited and the total simulations count is

given by N . The optimal or near-optimal action is identiﬁed

using Probability Approximation Correct (PAC) [20]. How-

ever, applicability for real-time applications is limited due

to the high computational demands. In [2] proposed one of

the ﬁrst polynomial regret bound algorithms which is quite

faster, but applicability limited for deterministic dynamic and

rewards. Later was extended [21] for supporting stochastic

rewards and dynamics in a open-loop setting, i.e, sequence

of actions. The preceding algorithm was further improved

in [22]. The modiﬁed version constraints on upper-conﬁdence

which helps to improve the performance considerably. For

upper-bounds identiﬁcation was determined with the help of

Kullback-Leibler algorithm [23]. On the contrary, graph-based

optimistic algorithms can capture across the states, which

does not supports in the preceding algorithms (tree based

algorithms). Such a algorithm was initially proposed by Silver

et al. [12] where states values are derived from a shared

Neural Network. Yet, authors of [24] proposed state-space

partitioning into smaller sets and aggregating similar states.

The proximal policy optimization (PPO) [25] is a super-

vised learning technique, where policy is deﬁned by a deep

neural network, e.g., multi layer perceptron, dueling network,

convolutional network, ego attention network, and variational

autoencoders [26], depending on the situation.

III. METHODOLOGY

Fig. 2. Other than true interaction between agent and environment, planning node simulates possible trajectories and pick the optimal action

The deterministic control problem is deﬁned as follows: (X, A, f, r, where action space and state space are denotes by A, X, respectively. Transition dynamics is given by f : X × A ←− X, and r denotes the reward function, which lies in the interval [0,1]. state space can be large yet action space should be deterministic, i.e., K. For any policy π : X ←− A, value function and Q-value function are deﬁned by:

V π(x) = γtr(xt, π(xt)) 0 ≤ γ < 1

t≥0

(1)

Qπ(x, a) = r(x, a) + γV π(f (x, a))

Thus, corresponding optimal V and Q functions can be deter-

mined as:

V ∗(x) = max r(x, a) + γV ∗(f (x, a))

a∈A
Q∗(x, a) = r(x, a) + γ max Q∗(f (x, a), b)

(2)

b∈A

Let A(n) be the action selection at each state, then regret yields selecting action from A(n) over selecting optimal action, which can be formulated as:

RA(n) = max Q∗(x, a) − Q∗(x, A(n))

(3)

a∈A

As listed in the Related Work section, a variety of tree search algorithms for calculating optimal controlling polices have been suggested. However, due to high computational demands, most of them are not a feasible solution for realtime applications such as trajectory planning for quadrotors. For real-time planning, control policy must be recalculated at least at 5Hz to obtain a smooth ﬂight experience. Hren and Munos [2] proposed an algorithm, namely, OPD, that focuses on the the accuracy and computational demands at the same time for deterministic systems. The OPD uses the best possible resources (CPU time, memory) to solve the sequential decision making process. If execution is limited by time, the algorithm returns sub-optimal policy, otherwise, it returns higher accuracy policy. The OPD constructs a look-ahead tree at time instance t. In general, in order to explore a look-ahead tree, it is needed to get through all possible reachable states from the current state xt. However, OPD proposed an optimal strategy for exploring the look-ahead tree from the current state xt; thus, OPD works faster, which makes it more applicable for real-time applications. Three approaches of OPD application are Uniform look-ahead tree searching, Deep Q-Network, and Proximal Policy Optimization. They are detailed in further sections.

A. Uniform look-ahead tree searching
The foremost objective of Uniform look-ahead tree searching is to retrieve an optimal action given a state x. In each state x, all possible reachable states are considered applying available actions. Such expanding new states are the children c(i), i = 0, ..., K of the current state, when initial state is denoted as the root. The same process is applied recursively on each child to construct the next level of the tree. The order of the tree, d, is the number of the levels the tree β grows. Hence, the cost of reaching any child vi of the tree is the

supersum vi = maxj∈C(i)vj and the optimal value of the root v∗ = maxi∈βvi = max{v1, ..., vK }. At any iteration, βn denotes the nodes that already expanded and βs = β − βn denotes they have not expanded yet. Such unexpanded nodes are the possible nodes that can be expanded in the next iteration.
Fig. 3. The regions βn and βs belong to expanded and to be expanded nodes. The number of children K is 2 at each node and tree depth d is 3
B. Deep Q-Network Deep Q-Network (DQN) is the ﬁrst deep reinforcement
learning method proposed by DeepMind [27]. DQN can learn the control policies from the sensor data directly. Control policy is approximated from a deep neural network, e.g., convolutional neural network, whose input is constituted with the raw image and whose output is estimated using a value function, i.e., the reward function; the value function calculates the future action and uses the said action to estimate the control policy. Afterwards, the obtained control policy is learned by training a variant of Q-learning. The input for the network depends on several factors: robot environment, robot behaviour, and sensing capabilities of the robot. For example, in this research, we experimentally evaluated three different types of deep neural networks: multilayer perceptron, dueling network, and ego attention network for learning the control policy. For all the listed types, input for the network was obtained by the observations and distances to close-in obstacle positions after the normalization. Finally, the network output in DQN provides the probable action to be applied to the system. C. Proximal Policy Optimization
The proximal policy optimization (PPO) [25] is a supervised learning technique, which ﬁll-in sampling data through interaction with the environment, that uses policy gradient methods for updating gradients, i.e, stochastic gradient ascent. Due the novel objective, i.e., Lclip(θ) = Eˆt[min(rt(θ)Aˆt), clip(rt(θ), 1 − , 1 + )Aˆt], where θ is the policy parameter, Eˆt and Aˆt are the estimated expectation over timesteps and advantage, respectively, is a hyperparameter, and rt is the ratio of probability of new and old policies, that enable minibatch updates on multiple epochs. Moreover, PPO is able to update the trust region using trust region policy optimization (TRPO). Sampling complexity quite low due the preceding novel objective function.

D. The problem formulation

Despite the recent achievements in reinforcement learning, e.g., [12], most of the proposed solutions are not yet applied to industrial applications due to the undesirable behaviour. However, model-based learning approaches can alleviate such undesirable behaviours often better than their model-free counterparts. Elouard [28] introduced an arbitrary reward function R which can be maximized to shortages that come in critical setting where mistakes that are made from the model to be minimized. More precisely, consider the following linear system

x˙ (t) = A(θ)x(t) + Bu(t) + D (t), (t) ≥ 0, (4)
y(t) = x˙ (t) + Cz(t)

where θ in the state matrix A(θ) ∈ Rnx×nx ⊂ Ψ ∈ Rd, control matrix B ∈ Rnx×nu , disturbance matrix D ∈ Rnx×nu . x ∈ Rnx, u ∈ Rnu and ∈ Rnr are state, control and disturbances, respectively. The system observation is given by y(t), where z(t) ∈ Rnn and C ∈ Rnx×nn . B, C, D are known matrices
with respect to the considered system. Afterwards, they deﬁned
a robust control objective aims to maximize the worse-case
next state output with respect to a conﬁdent region Cn,δ

max V r(u)

(5)

u∈RN

, where V r(u)

=

inf
θ∈CN,δ

[Σ∞ n=N

+1

γ

n

R(xn

(u,

ω))],

γ

∈

(0, 1). Since system is linear A is known, A(θ) is estimated

as follows:

A(θ) = A + Σdi=1θiπi

(6)

More information regarding the θ estimation [28]. Simulator
we developed for avoiding the obstacles in 3D space from start
position to desired goal position. The system is described by x = [px py pz vx vy vz]T , where pµ, vµ, µ = x, y, z are position and velocity along the µ direction. Action space u = (ux, uy, uz) ∈ [−1, 1]3 is divided into 8 directions: forward, backward, left, right, top, and down, as follows:

A = {(0, 0, 0), (1, 0, 0), (−1, 0, 0), (0, 1, 0), (0, −1, 0),

(0, 0, 1), (0, 0, −1)]}

The reward encodes cost towards to desired goal state xg, while avoiding collisions with obstacles:

R(x) = δ(x)/(1 + x − xg 2),

δ(x) =

0 δ(x)

δ(x) < obsd otherwise

,

(7)

where obsd denotes the minimum acceptance distance, i.e., 1m. The system dynamics consist with friction parameters (θx, θy, θz):

p˙x 0 0 0 1 0 0  px  0 

p˙y 0 0 0 0 1

p˙z

 

v˙x

 

=

0 0

0 0

0 0

0 −θx

0 0

0  py  0 

1 0

   

pz vx

   

+

β

 

0

ux

   

,

v˙y

 

0 0 0

0

−θy

0

 

vy

 

uy

 

v˙z

0 0 0 0 0 −θz vz

uz

Fig. 4. Back and red spheres denote the goal positions and obstacles positions, respectively. The quadrotor starts at the origin (0,0,0) moving towards to goal position while avoiding the obstacles
where 0 < β ≤ 1 is a scaling parameter that control the direction of the action.
IV. EXPERIMENTAL RESULTS
We have modiﬁed the approach that proposed in [28], supporting for 3D environment. In the improved version we incorporated OctoMap based map representation where obstaclefree distance can be calculated from a desired position. Such a distance estimation was helped to faster convergence of look-ahead tree search, while keeping the almost the same accuracy. We have ran theirs with the proposed over 100 episodes and calculated mean execution time over as given in Table I. Furthermore, we have tried with Proximal Policy Optimization with three different control policy generation, i.e., Convolutional Neural Network, Variational Autoencoder, and Dueling Neural Network. However, we have not achieved high accurate trajectory when we used Proximal Policy Optimization for predicting of next optimal action. Success fraction is ratio of the number of successful trajectories to total number of tries, i.e., 100 times. Mean execution time is calculated total time is deﬁned as ratio of time utilized for the total trajectory calculation to number of steps. With reference to Table. I, it is clear that the proposed approach has the optimal in comparison to the other approach that have compared.
V. CONCLUSION AND DISCUSSION
Accuracy, computation time, and memory consumption make path planning a challenging task for real time application. Current advancement in the ﬁeld do not allow mobile robots, e.g., drones and autonomous ground vehicles, to handle dynamic obstacles robustly. Two main classes of approaches that have been used to address the said constrains are RL-based and deterministic-based approaches. In this work, we have compared three RL-based approaches and one deterministic approach for the same path planning task, i.e., ﬁnding an obstacle-free path between start and goal position. Then, we proposed a hybrid approach that combines Monticalo tree search and RL-based approach to solve the same task. In terms of reaching the goal position, the accuracy of the proposed hybrid approach is considerably higher than the

TABLE I COMPARISON THE PROPOSED APPROACH WITH A FEW OTHER VARIANTS

Approach [28]
Improved version of [28]
Proximal Policy Optimization
Proximal Policy Optimization Proximal Policy Optimization Mean Execution Time (s)
2.234 1.230 0.345 0.334 0.315

Control Policy Estimation Deterministic Tree Search Uniform look-ahead Tree Search Convolutional Neural Network Variational Autoencoder Dueling Neural Network Success Fraction
0.967 0.912 0.235 0.342 0.453

other three approaches: Proximal policy optimization, deep
Q-network, and uniform tree search; this ensures the safety
and dynamic feasibility. However, computational demands of
the proposed hybrid approach are sightly higher compared
to the aforementioned three approaches, which constitutes a
limitation of the proposed hybrid approach because such de-
mands yield ineffability to apply in real-world motion planning
applications. Thus, the next stage of this research project
focuses on incorporating Proximal Policy Optimization-based
planning to further reduction of computational time while
keeping the high accuracy. Further research could also focus
on memory footage of the algorithm.
REFERENCES
[1] K. Rocki and R. Suda, “Large-scale parallel monte carlo tree search on gpu,” in 2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum. IEEE, 2011, pp. 2034–2037.
[2] J.-F. Hren and R. Munos, “Optimistic planning of deterministic systems,” in European Workshop on Reinforcement Learning. Springer, 2008, pp. 151–164.
[3] B. Paden, M. Cˇ a´p, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey of motion planning and control techniques for self-driving urban vehicles,” IEEE Transactions on intelligent vehicles, vol. 1, no. 1, pp. 33–55, 2016.
[4] M. Barbehenn, “A note on the complexity of dijkstra’s algorithm for graphs with weighted vertices,” IEEE transactions on computers, vol. 47, no. 2, p. 263, 1998.
[5] P. E. Hart, N. J. Nilsson, and B. Raphael, “A formal basis for the heuristic determination of minimum cost paths,” IEEE transactions on Systems Science and Cybernetics, vol. 4, no. 2, pp. 100–107, 1968.
[6] D. Ferguson and A. Stentz, “The delayed d* algorithm for efﬁcient path replanning,” in Proceedings of the 2005 IEEE international conference on robotics and automation. IEEE, 2005, pp. 2045–2050.
[7] L. Jaillet, J. Corte´s, and T. Sime´on, “Transition-based rrt for path planning in continuous cost spaces,” in 2008 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2008, pp. 2145– 2150.
[8] D. Hsu, J.-C. Latombe, and H. Kurniawati, “On the probabilistic foundations of probabilistic roadmap planning,” The International Journal of Robotics Research, vol. 25, no. 7, pp. 627–643, 2006.
[9] D. Mellinger and V. Kumar, “Minimum snap trajectory generation and control for quadrotors,” in 2011 IEEE international conference on robotics and automation. IEEE, 2011, pp. 2520–2525.
[10] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., “Mastering the game of go with deep neural networks and tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.
[11] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering the game of go without human knowledge,” nature, vol. 550, no. 7676, pp. 354–359, 2017.

[12] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., “A general reinforcement learning algorithm that masters chess, shogi, and go through self-play,” Science, vol. 362, no. 6419, pp. 1140–1144, 2018.
[13] T. Degris, P. M. Pilarski, and R. S. Sutton, “Model-free reinforcement learning with continuous action in practice,” in 2012 American Control Conference (ACC). IEEE, 2012, pp. 2177–2182.
[14] J. Gla¨scher, N. Daw, P. Dayan, and J. P. O’Doherty, “States versus rewards: dissociable neural prediction error signals underlying modelbased and model-free reinforcement learning,” Neuron, vol. 66, no. 4, pp. 585–595, 2010.
[15] B. Bhowmik, “Dynamic programming. its principles, applications, strengths, and limitations,” Criterion, vol. 4, p. 7, 2010.
[16] R. A. Howard, “Dynamic programming and markov processes.” 1960. [17] B. Amos and D. Yarats, “The differentiable cross-entropy method,” in
International Conference on Machine Learning. PMLR, 2020, pp. 291– 302. [18] M. W. Iruthayarajan and S. Baskar, “Covariance matrix adaptation evolution strategy based design of centralized pid controller,” Expert systems with Applications, vol. 37, no. 8, pp. 5775–5781, 2010. [19] G. M. J.-B. C. Chaslot, Monte-carlo tree search. Maastricht University, 2010. [20] M. Ekdahl and T. Koski, “Bounds for the loss in probability of correct classiﬁcation under model based approximation.” Journal of Machine Learning Research, vol. 7, no. 11, 2006. [21] S. Bubeck and R. Munos, “Open loop optimistic planning.” in COLT, 2010, pp. 477–489. [22] E. Leurent and O.-A. Maillard, “Practical open-loop optimistic planning,” in Machine Learning and Knowledge Discovery in Databases, U. Brefeld, E. Fromont, A. Hotho, A. Knobbe, M. Maathuis, and C. Robardet, Eds. Cham: Springer International Publishing, 2020, pp. 69–85. [23] O. Cappe´, A. Garivier, O.-A. Maillard, R. Munos, G. Stoltz et al., “Kullback–leibler upper conﬁdence bounds for optimal sequential allocation,” Annals of Statistics, vol. 41, no. 3, pp. 1516–1541, 2013. [24] J. Hostetler, A. Fern, and T. Dietterich, “State aggregation in monte carlo tree search,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 28, no. 1, 2014. [25] Y. Wang, H. He, and X. Tan, “Truly proximal policy optimization,” in Uncertainty in Artiﬁcial Intelligence. PMLR, 2020, pp. 113–122. [26] A. R. Rivera, A. Khan, I. E. I. Bekkouch, and T. S. Sheikh, “Anomaly detection based on zero-shot outlier synthesis and hierarchical feature distillation,” IEEE Transactions on Neural Networks and Learning Systems, 2020. [27] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement learning,” arXiv preprint arXiv:1312.5602, 2013. [28] E. Leurent, D. Eﬁmov, and O.-A. Maillard, “Robust-adaptive control of linear systems: beyond quadratic costs,” in NeurIPS 2020-34th Conference on Neural Information Processing Systems, 2020.

