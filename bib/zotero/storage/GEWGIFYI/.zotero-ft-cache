SPECIAL SECTION ON MISSION CRITICAL SENSORS AND SENSOR NETWORKS (MC-SSN)
Received November 15, 2019, accepted November 24, 2019, date of publication November 29, 2019, date of current version December 23, 2019. Digital Object Identifier 10.1109/ACCESS.2019.2956747

Image Enhancement via Indented Frame Over Fusion
BAOJU ZHANG 1,2, WENRUI YAN 1,2, GANG LI 3,4, JINGQI FEI 1,2, CUIPING ZHANG 1,2, AND CHUYI CHEN 1,2
1Tianjin Key Laboratory of Wireless Mobile Communications and Power Transmission, Tianjin Normal University, Tianjin 300387, China 2School of Electronics and Communication Engineering, Tianjin Normal University, Tianjin 300387, China 3State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, Tianjin 300072, China 4Tianjin Key Laboratory of Biomedical Detecting Techniques and Instruments, Tianjin University, Tianjin 300072, China
Corresponding author: Baoju Zhang (wdxyzbj@163.com)
This work was supported in part by the National Natural Science Foundation of China under Grant 61971310, in part by the Natural Youth Science Foundation of China under Grant 61401301, in part by the Natural Science Foundation of Tianjin City under Grant 18JCYBJC86400, and in part by the Provincial and Ministerial Level Key Program under Grant 53H18028.

ABSTRACT In this era of artiﬁcial intelligence (AI), the Internet of things (IoT) with the capability of connecting a great number of heterogeneous terminals and the popularity of mobile devices, which makes more devices available as another pair of eyes for people, such as video surveillance and smart navigation. As image enhancement is assisting people to better work together and helping people live a smarter life, it is therefore becoming increasingly important. Nevertheless, many vision systems are sensitive to factors like the scattering of light or motion which may cause blur. This paper promotes an image enhancement method and is dedicated to reduce the adverse effects of blurred images on vision systems. We ﬁrst researched the theory of indented frame over fusion (IFOF) and presented an automatic screening function. Then subtly combined color reversal, registration and transformation. Lastly, we tested two examples with this method and gained good results. Image processed by this method, facing strong scattering of light environment, has improved its quality and visual effects. With the development of this kind of technology, there will be more practical and intelligent applications in our lives, such as license plate numbers recognition in bad weather and important items search on ﬁre scenes.

INDEX TERMS Indented frame over fusion, automatic screening function, image enhancement.

I. INTRODUCTION Nowadays, as the life of humans becomes ever more digital, humans are beginning to enter the era of intelligence. Meanwhile, with the improvement of algorithms and the emergence of artiﬁcial intelligence (AI), internet of things (IoT) and wireless communication (e.g., 5th generation wireless systems [1]), people get down to develop the architecture of smart cities [2] and new types electronic products are becoming popular [3], such as smart watches and smart glasses. That means, only the smart devices (e.g., smart phone) and its relevant applications enable people to control every object and gain diversiﬁed information through the internet in the daily life [4]. For example, it can assist in supervising or predicting trafﬁc conditions before commuting with the help of smart phones, and the house owner can adjust the light intensity
The associate editor coordinating the review of this manuscript and
approving it for publication was Qilian Liang .

and monitor indoor temperatures and humidity by connecting the smart phone to internet. Besides, it is an inevitable trend for enterprises and factories to bring in intelligent facilities as a competitive edge over the shortage of labor force which improves production efﬁciency and ensures their sustainable yield [5]. Applications of IoT and other technologies are also employed in other domains, such as in agriculture [6], [7], smart hospitals [8], [9], geological detection and topography and environmental change [10]. It cannot be denied that these new sorts of technologies such as IoT, AI have already become part of our lives and various work ﬁelds, which indeed bring about tremendous convenience to human.
With the advancement in technologies like IoT and service applications, there will be an increasing demand for image enhancement to keep up with the pace of advancement. Firstly, people need to gain information via certain equipment such as sensors and webcam to process and analyze data [11]. In some situations such as overexposure or low light

181092

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ VOLUME 7, 2019

B. Zhang et al.: Image Enhancement via IFOF

[12]–[15], depending on the hardware used, image enhancement becomes necessary. In the medical sector, image enhancement plays an important role in detecting diseases which are difﬁcult for human eyes to directly diagnose, due to the limitations of the imaging mechanism of the equipment [16], [17] from unprocessed images [18], [19]. With the help of image enhancement, the valuable content will be highlighted [20] and the noise will be reduced [21], [22] which certainly enhance the accuracy and reliability of the diagnosis results. Image processing is also applied in forest management [23], facial recognition technology [24], [25], magnetic resonance imaging (MRI) [26], surveillance immune inﬂuence of variations in weather conditions [27] and target tracking [28]–[31]. Image processing technology can help extract features which are required in classiﬁcation [32] and discrimination. Furthermore, the demand of customers for the sharpness of images in smart phones is growing, and some Applications downloaded onto smart phones like ﬁlters and beauty cameras [33], are supported by image processing technology. Hence, image processing technology including image enhancement has real signiﬁcance in promoting the development of intelligent life and construction of smart cities.
This paper proposes a method to enhance the image quality. Firstly, we presented a theoretical study of image enhancement in which the indented frame over fusion (IFOF) and automatic screening function are expounded. The calculation method of the signal-to-noise ratio (SNR) multiples, which, in theory, can be improved by IFOF, was given in this section. Secondly, this article creatively combined color reversal, image matching, image transformation, IFOF with automatic screening function organically. Finally, the image enhancement method proposed by this paper veriﬁed its feasibility through two types of speciﬁc data collected by our team.
Section II studied and discussed theories used in this image enhancement method. In Section III, the method was validated through two speciﬁc cases. Lastly, Section IV discusses the conclusions reached in this paper.
II. RELATED WORK A. IMAGE ENHANCEMENT The origin of image enhancement dates back to the early 1920s when the ﬁrst picture was transmitted from London to New York by Bartlane method of simulating the centre hue to recover the picture [34]. In the initial period of image enhancement techniques often involved setting hardware parameters, such as the choice of printing process and the distribution of brightness levels. In the early 1960s, the ﬁrst mainframe computer with the function of digital image processing was manufactured, which symbolized the advent of digital image processing era. The research of Jet Propulsion Laboratory (JPL) adopted geometric correction, gray transform, suppression of noise, Fourier transform and two-dimensional linear ﬁltering to deal with the thousands of moon pictures from the space probe Prowler 7 in 1964,
VOLUME 7, 2019

and repainted the image of the lunar surface [35]. In the end of 1960s and the early 1970s, the researchers started to combine image processing with remote sensing techniques, medical imaging and astronomy, and in 1895 X-ray which is used widely in medical image nowadays, was discovered by Wilhelm Roentgen [36]. With the development of electronic devices toward excellent performance, small size and high reliability [37], in 1990s, the technology of image enhancement had embedded in every aspect of human life and social development. At the present, with the booming of cloud computing, big data, deep learning [38] and neural network algorithms [39], [40], image enhancement becomes increasingly generalized. There are brightness enhancement, gray enhancement [41] and sharpness enhancement [42] in the ﬁeld of image enhancement, applied in speciﬁc circumstances such as foggy weather [43].
B. IMAGE FUSION Image fusion which can integrate advanced information is a combination of multiple images in the same scene. Its processing level can be divided into three categories: 1) pixel level 2) feature level 3) decision level [44]. At the outset, frame accumulated can be deemed to a special image fusion. The SNR can be improved n times when n source images are almost equal [45]. Burt and Adelson [46] proposed Laplacian pyramid fusion scheme which applied the concept of pyramidal decomposition to the image fusion. The gradient pyramid adopted gradient operator of Gaussian pyramid’s every level. Wavelet fusion [47] was ﬁrst put forward in 1990 as the better method. Rockinger and Fechner [48] used the undecimated discrete wavelet transform (UDWT) to solve artifacts. Compared with UDWT, this strategy which introduces the dual-tree complex wavelet transformation has less redundancy. The above methods are all pixel level image fusion techniques. As for the feature level, Golipour et al. [49] devised another method, under the Bayesian framework, which integrated hierarchical segmentation results with Markov random ﬁeld prior, classiﬁed hyperspectral image in spectral space. Fauvel et al. [50] proposed a multiple classiﬁers fusion scheme which offered complementary services for information. Decision fusion is a cognitive-based approach. For example, the pansharpening method proposed by Luo et al. [51] and the object recognition method proposed by Mahmoudi et al. [52] belongs to the application of decision fusion level.
III. EXPERIMENTAL BASIS A. INDENTED FRAME OVER FUSION In some common strong scattering environments, such as fog or haze, it is hard for the naked eye or camera to have a clear view. At this point, the importance of improving the quality of the captured image is revealed. It can help people see things in such environment and get more information they need. We set out to come up with a method which would be an efﬁcient
181093

B. Zhang et al.: Image Enhancement via IFOF

way to address this kind of problems: IFOF, which enhances the image SNR and contrast.
In this scenario, the video will be divided into multiple frames. After that, we select the ﬁrst frame and the next frame to match the feature points, and transform the next frame image according to the matching point, so that the correspondence of the pixel points between the two frames is more accurate, and the two images are respectively fused by the ratio of u and v to obtain the new image. The new image continues to match, transform, and blend with the next frame until the last frame merges. Because the time interval between the adjacent frames is too short to make great changes and easy for subsequent discussion, the image signal of each frame, approximate to equal, s1 = s2 = . . . = sn = s; the random noise of each frame, z1, z2, . . ., zn, can be considered as independent and identically distributed. Suppose random noise zi satisﬁes distribution G, then we can use σ 2 to represent the variance of zi:

D (zi) = σ 2

(1)

where i=1, 2, 3, . . . , n. The SNR of the ﬁrst frame could be expressed as

s2

SNR1 = σ 2

(2)

Concentrate the ﬁrst and second maps by assigning weights u and v respectively, and then a new image is obtained. Its SNR is as follows:

SNR2 =

(u ∗ s1 + v ∗ s2)2 D(u ∗ z1 + v ∗ z2)

(u + v)2 s2

= u2 + v2 ∗ σ 2

(3)

It is worth mentioning that this method is intended to try to increase the contrast of the image by over fusion which is reﬂected in following inequalities: 0< u <1, 0< v <1, 1< u + v.
Continue to fusion the newly obtained image with the subsequent frame to get the updated image, whose SNR is

SNR3 =

[u ∗ (u ∗ s1 + v ∗ s2) + v ∗ s3]2 D (u ∗ (u ∗ z1 + v ∗ z2) + v ∗ z3)

u2 + u ∗ v + v 2 s2

= u4 + u2 ∗ v2 + v2 ∗ σ 2

(4)

...
The nth updated image SNR is

SNRn

(un−1 + un−2 ∗ v + . . . + u ∗ v + v)2

s2

= (un−1)2 + (un−2 ∗ v)2 + . . . + (u ∗ v)2 + v2 ∗ σ 2

(1 − u2) ∗ (1 − v) ∗ un−1 + (1 + u) ∗ v ∗ (1 − un) 2 = (1 − u2)2 ∗ (1 − v2) ∗ u2n−2 + (1 − u2) ∗ v2 ∗ (1 − u2n)

181094

FIGURE 1. The multiple of the nth image compared to the SNR of the first image.

s2 ∗σ2

(1 − u2) ∗ (1 − v) ∗ un−1 + (1 + u) ∗ v ∗ (1 − un) 2 = (1 − u2)2 ∗ (1 − v2) ∗ u2n−2 + (1 − u2) ∗ v2 ∗ (1 − u2n)

SNR1

(5)

where n =2, 3, 4. . . Deﬁne m is the ratio of SNRn to SNR1, which means the
times of SNRn is SNR1, then we get
m = SNRn SNR1 (1 − u2) ∗ (1 − v) ∗ un−1 +(1 + u) ∗ v ∗ (1 − un) 2
= (1 − u2)2 ∗ (1−v2) ∗ u2n−2 +(1−u2) ∗ v2 ∗(1−u2n) (6)

When n is greater than 1, obviously, m is constant greater than 1 and SNR is improved. For a more vivid description, we plot Figure 1 with n from 2 to 100 where u is 0.9 and v is 0.3:
The m is from approximately 1.3 to close to 4.4. By calculation we get that the value of m when n tends to inﬁnity is approximately 4.4. When the value of u, v is replaced under the above constraints, a similar scatter plot will be obtained, the value of m will also be changed.

B. AUTOMATIC SCREENING FUNCTION
In practice, the pixel depth for ordinary image is 8 bits, which means that the pixel value will overﬂow after being added to 255. Additionally, the actual transformed image sometimes does not accurately compensate for the difference between frames, which results in an unsatisfactory image obtained by the fusion. In order to get better results, how to screen out high-quality images is especially important. To this end, this paper proposes a screening function to help select better quality images.
First and foremost, we plan to ﬁlter the image from two perspectives: clarity and structural stability of the image. Hence the clearer the image is, the more popular it becomes. The ﬁrst aspect is well understood. The second aspect is to avoid image distortion caused by various reasons, which

VOLUME 7, 2019

B. Zhang et al.: Image Enhancement via IFOF

will be described in details later. GS and SS represent the formulas that measure the clarity and structural stability of the image respectively. We assign GS and SS the corresponding weights, so we get the following formula,

FS = λ∗GS + (1 − λ)∗SS

(7)

where λ is from 0 to 1. As we all know, gradient can help us extract edge infor-
mation, and clearer images have larger gradients due to their more distinct edge. In order to reduce the running time, the calculation of the gradient is approximated by the difference method, and only the lateral case is considered. Find the sum of squares of horizontal gradient of all points as the criterion for the image clarity. The evaluation formula is as follows,

X −1 Y −1

GS =

(I mg(x + 1, y) − Img(x, y))2 (8)

x=0 y=0

where Img’s size is X×Y, Img (x, y) means the gray value of Img at point (x, y).
In addition, the inaccurate image transformation and the limitations of the gray value 255 of the general image contribute to the loss and distortion of image. Therefore, we ﬁlter out the image with less deformation by comparing the image structure with the original ones. Here is the ﬁrst frame of the video as the original image, and the formula is as follows,

SS

cov(Imgo, Img) + K = σI∗mgo σImg + K

(9)

as cov(Imgo, Img) represents the covariance between I mgo and I mg, σimgo and σimgo represent their respective variances, K is a constant that prevents the denominator from being 0,

its deﬁnition is as follows,

K = (C∗R)2

(10)

where C is a constant much smaller than 1 and R is the dynamic range of the pixel value, such as 255.
Last but not the least. This paper normalizes GS and SS to eliminate the impact of the dimensions in the ﬁnal result. Combining the arc tangent and GS , we have

GS

=

arctan(η∗GS

)∗

2 π

(11)

as η is a constant which has an effect on adjusting the degree

of data difference.

Similarly, in order to balance the degree of difference in

data, this article also changed SS whose original value range

is 0 to 1.

SS = (SS )θ

(12)

where θ is a constant that has a similar effect to η. Finally, we get the automatic screening function:

FS

=

λ∗

(arctan(η

∗(

X−1Y

−1
(I

mg(x

+1,

y)

−

Img(x

,

y))2)∗

2 π

)

x=0y=0

VOLUME 7, 2019

+

(1

−

λ)

∗

(

cov(Imgo, Img) σ Imgo ∗ σ Img

+ +

(C (C

∗ ∗

R)2 R)2

)θ

(13)

It could be used to ﬁlter out the higher quality image.

C. SUPPLEMENTARY
In this section, we will brieﬂy introduce and explain some of the processes in the whole method. These theories are generally known, and their speciﬁc combination is still important, which can help get a clearer image.
In order to better help build smart cities, promote the use of more popular low-cost devices to capture images. However, the image data collected by these devices, if not specially processed, generally has a pixel depth of 8, and each pixel of each channel ranges from 0 to 255. In the face of images obtained under strong light or strong scattering environment, we utilize color inversion to accommodate the fusion of more images, thus providing the possibility to utilize more image information. The color inversion formula is:

Img (x, y,t) = 255 − Img(x, y, t)

(14)

as Img(x, y,t) represents the pixel before color inversion at

(x, y) position of channel t, and Img (x, y,t) represents the

pixel after color inversion.

Furthermore, for more people to participate in the con-

struction of intelligent life and the concept of easy opera-

tion, we allow the photographer to capture images without

additional ﬁxtures, which may cause differences between

frames, such as differences in target position and size. Faced

with differences between frames, we creatively performed

matching feature points employing speed-up robust features

(SURF) [53] and the rigid transformation (RT) before each

fusion. This operation compensates for differences between

frames, ensures the stability of IFOF, and also provides more

information for image enhancement by taking advantages of

differences between frames. The RT formula is

 x’   a11 a12 a13   x 

 y’  =  a21 a22 a23   y 

(15)

1

001 1

with a11 to a23 can be obtained through the relationship between feature points matched by SURF.

IV. PROCESS AND RESULTS A. EXPERIMENT PROCESS This paper adopted two different examples to illustrate the method. One is a video of a construction site gate shot in real foggy weather, and the other is simulated mammary gland tissue video. The architecture of this method is shown in Figure 2.
Based on the above architecture, we further explain the complete experimental process:
1) Capture video and frame it. 2) Reverse the color of each frame. 3) Selecting two frames of images, obtaining matching
feature points among the two frames by SURF and performing the rigid transformation according to the

181095

B. Zhang et al.: Image Enhancement via IFOF

FIGURE 2. The architecture of the method based on Indented Frame Over Fusion.

points. The two frames of images are: the image of the last fusion and the image to be merged in the next frame. Wherein at the initial stage, without the merged image, the ﬁrst frame and the second frame image are chosen. 4) Frame over fusion. 5) Evaluate the new image through the screening function and update the record when there is a better image to get. 6) Repeat step 3) through 5), until the last frame of complete fusion. 7) At the end of the iteration, the image with the highest score, the resulting image, is granted.

TABLE 1. Each Indicator Score for Each Image in Case One.

B. CASE ONE The method proposed in this paper can be applied to weathersensitive outdoor vision systems such as video surveillance, target tracking, and intelligent navigation to enhance the system’s adaptability in severe weather conditions. It can also be implemented in mobile applications, developing a camera function with better stability in scenes that are prone to blur. It is worth noting that the normal mobile phone camera enjoys a kind of function named live, which provides a sequence of images for a few seconds. This method can obtain a natural and clear image according to the image sequence provided.
In real foggy weather, we collected a video of a construction site gate (frame rate of 30f/s) through a normal Vivo Z3 mobile phone. The acquisition did not require a ﬁxed device such as a tripod, so the captured video might appear to shake to a certain extent. In order to better illustrate only a few seconds of image sequence, which could be provided by the ordinary mobile phone’s live function, were enough for this method, only the very short ﬁrst 2 seconds of the captured video (60 frames in total) was taken as input, and after processing this method, a clearer, more visually natural dehazed image was obtained. We took the ﬁrst frame of captured video as the original image and applied some common image enhancement methods to process the original image as comparison. Result images are presented in Figure 3.
From Figure 3, we can easily ﬁnd that although the original image has been brighter by the way of brightness enhanced, it also lost much signiﬁcant information, such as the building gate. Similarly, (g) also has the problem of losing details.
181096

For example, the top left corner of (g) missed some of the branches that existed in the original image. (f) which averaged 60 frames’ information became blurred due to the failure to solve the small movement problem between pictures. Other methods have improved image clarity from various angles, in which our method has not only a better performance in terms of quality enhancement, but also more natural in image color temperature.
Moreover, this paper evaluates all of these images from different aspects through various evaluation standards. Measured scores of each image are shown in Table 1.
By comparison, we can ﬁnd that the improvement of (b) and (c) on each indicator is not obvious, and there is even a fallback phenomenon. For instance, BD, IE and SMD scores of (b) are lower than the corresponding values of (a). Almost all the evaluation values of (f) and (g) are less than (a). (i) and (j) perform well on all indicators except IE. Though (d), (e), and (h) have a good performance among all of the six
VOLUME 7, 2019

B. Zhang et al.: Image Enhancement via IFOF

FIGURE 3. Original image and images are obtained by various methods in case one: (a) Original image. (b) Color enhancement image. (c) Brightness enhancement image. (d) Contrast enhancement image. (e) Sharpness enhancement image. (f) Frame accumulated average image. (g) Robust retinex model enhancement image. (h) Gamma corrected enhancement image. (i) Low-light image enhancement (LIME) image. (j) Multi-scale retinex (MSR) enhancement image. (k) Image handled by our method.

FIGURE 4. Original image and images are obtained by various methods in case two: (a) Original image. (b) Color enhancement image. (c) Brightness enhancement image. (d) Contrast enhancement image. (e) Sharpness enhancement image. (f) Frame accumulated average image. (g) Robust retinex model enhancement image. (h) Gamma corrected enhancement image. (i) LIME image. (j) MSR enhancement image. (k) Image handled by our method.

standards, (k) is the best one among the entire image sets, since each evaluation score is improved greatly.

TABLE 2. Each indicator score for each image in case two.

C. CASE TWO The following is an example of this method for applications in the ﬁeld of biomedical applications to assist heterogeneity detection in biological tissues. The biological tissue has optical characteristics of strong scattering and low absorption, which damage the image clarity detected by the ordinary device, and this method can assist in improving both the SNR and the visual effect.
Firstly, we simulated the heterogeneous scene of the breast tissue: a self-made device which contains milk, potato and meat. Among them, the potato and meat were used to simulate the heterogeneous body in the breast tissue. After that, we took a video clip in the same way as case one, and took the ﬁrst 2 seconds of video as the input of our method. The ﬁrst frame of the captured video was selected as the original image and the image was processed via using various existing image enhancement methods respectively. Lastly, the present method processed images and the images obtained by our method were put together for comparison and analysis. Getting images as shown in Figure 4.
Depending on Figure 4, it can be viewed that (c) and (g) lost some details while brightening. The performance of (f) was mixed. On the one hand, it restored some of the information
VOLUME 7, 2019

(such as the partial color of the potato). On the other hand, it lost some details, just like the edge of the meat was blurred. (b) and (e) enhanced the clarity with respect to (a), but the improvement was not obvious. (d), (h) and (k) achieved good visual effects, while (k) recovered more details.
Further comparison was presented in Table 2.
181097

B. Zhang et al.: Image Enhancement via IFOF

Table 2 reported resulting image score under each indicator in the columns. (f) and (g) did not perform well due to some information missed in (a). Except for very few values, such as IE score of (i), the six indicator score of the remaining methods were basically improved compared to (a). As a whole, the performance of (e) was better than the images enhanced by other existing methods, but (k) increased more than (e) in general.
V. CONCLUSION This paper studies theory of IFOF and proposes an image enhanced method based on IFOF. According to the above two cases, it can be seen that our method has achieved a good result. This is because our method handles the difference and redundant information between frames well. The method of frame accumulated average results in poor robustness to the data due to the inability to ﬂexibly utilize the interframe relationships. In the face of data sources with unstable inter-frame relationships, the image quality after this method is reduced. Moreover, our method avoids the loss of detail caused by pixel value overﬂowing through our automatic screening function. Such missing detail problems appear in some of the methods above, like Brightness enhancement method and robust retinex model based enhancement method.
There will be also more improvements base on this paper. Optimization of running time and reduction of deformation of the image are all worthy of studying. Considerable amount of work, expectantly, will be done in this promising area.
REFERENCES
[1] N. Abbas, Y. Zhang, A. Taherkordi, and T. Skeie, ‘‘Mobile edge computing: A survey,’’ IEEE Internet Things J., vol. 5, no. 1, pp. 450–465, Feb. 2018.
[2] A. Zanella, N. Bui, A. Castellani, L. Vangelista, and M. Zorzi, ‘‘Internet of Things for smart cities,’’ IEEE Internet Things J., vol. 1, no. 1, pp. 22–32, Feb. 2014.
[3] J. Pan and J. McElhannon, ‘‘Future edge cloud and edge computing for Internet of Things applications,’’ IEEE Internet Things J., vol. 5, no. 1, pp. 439–449, Feb. 2018.
[4] S. Chen, H. Xu, D. Liu, B. Hu, and H. Wang, ‘‘A vision of IoT: Applications, challenges, and opportunities with china perspective,’’ IEEE Internet Things J., vol. 1, no. 4, pp. 349–359, Aug. 2014.
[5] V. Muthusamy, A. Slominski, and V. Ishakian, ‘‘Towards enterprise-ready ai deployments minimizing the risk of consuming AI models in business applications,’’ in Proc. 1st Int. Conf. Artif. Intell. Ind. (AI4I), Laguna Hills, CA, USA, Sep. 2018, pp. 108–109.
[6] O. Elijah, T. A. Rahman, I. Orikumhi, C. Y. Leow, and M. N. Hindia, ‘‘An overview of Internet of Things (IoT) and data analytics in agriculture: Beneﬁts and challenges,’’ IEEE Internet Things J., vol. 5, no. 5, pp. 3758–3773, Oct. 2018.
[7] N. Ahmed, D. De, and I. Hussain, ‘‘Internet of Things (IoT) for smart precision agriculture and farming in rural areas,’’ IEEE Internet Things J., vol. 5, no. 6, pp. 4890–4899, Dec. 2018.
[8] H. Zhang, J. Li, B. Wen, Y. Xun, and J. Liu, ‘‘Connecting intelligent things in smart hospitals using NB-IoT,’’ IEEE Internet Things J., vol. 5, no. 3, pp. 1550–1560, 2018.
[9] L. Catarinucci, D. De Donno, L. Mainetti, L. Palano, L. Patrono, M. L. Stefanizzi, and L. Tarricone, ‘‘An IoT-aware architecture for smart healthcare systems,’’ IEEE Internet Things J., vol. 2, no. 6, pp. 515–526, Dec. 2015.
[10] Y. Yang, L. Wu, G. Yin, L. Li, and H. Zhao, ‘‘A survey on security and privacy issues in Internet-of-Things,’’ IEEE Internet Things J., vol. 4, no. 5, pp. 1250–1258, Oct. 2017.
181098

[11] B. Cheng, G. Solmaz, F. Cirillo, E. Kovacs, K. Terasawa, and A. Kitazawa, ‘‘FogFlow: Easy programming of IoT services over cloud and edges for smart cities,’’ IEEE Internet Things J., vol. 5, no. 2, pp. 696–707, Apr. 2018.
[12] J. Cai, S. Gu, and L. Zhang, ‘‘Learning a deep single image contrast enhancer from multi-exposure images,’’ IEEE Trans. Image Process., vol. 27, no. 4, pp. 2049–2062, Apr. 2018.
[13] B. Masia, A. Serrano, and D. Gutierrez, ‘‘Dynamic range expansion based on image statistics,’’ Multimedia Tools Appl., vol. 76, no. 1, pp. 631–648, 2017.
[14] X. Guo, Y. Li, and H. Ling, ‘‘LIME: Low-light image enhancement via illumination map estimation,’’ IEEE Trans. Image Process., vol. 26, no. 2, pp. 982–993, Feb. 2017.
[15] M. Li, J. Liu, W. Yang, X. Sun, and Z. Guo, ‘‘Structure-revealing lowlight image enhancement via robust Retinex model,’’ IEEE Trans. Image Process., vol. 27, no. 6, pp. 2828–2841, Jun. 2018.
[16] Y. Konno, ‘‘Image processing device, radiation imaging device, and image processing method,’’ U.S. Patent 10 010 303 B2, Nov./Jul. 3, 2018.
[17] V. Papyan and M. Elad, ‘‘Multi-scale patch-based image restoration,’’ IEEE Trans. Image Process., vol. 25, no. 1, pp. 249–261, Jan. 2016.
[18] T. Sakamoto, ‘‘Medical image processing apparatus, medical image processing method, and medical image processing system,’’ U.S. Patent 10 002 423 B2, Nov./Jun. 19, 2018.
[19] T. Sasaki, K. Nishihara, and M. Azegami, ‘‘Medical image diagnosis apparatus and medical image processing apparatus,’’ U.S. Patent 010 157 639 B2, Nov./Dec. 18, 2018.
[20] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, ‘‘Salient object detection: A benchmark,’’ IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706–5722, Dec. 2015.
[21] M. Koziarski and B. Cyganek, ‘‘Image recognition with deep neural networks in presence of noise—Dealing with and taking advantage of distortions,’’ Integr. Comput.-Aided Eng., vol. 24, no. 4, pp. 337–349, Sep. 2017.
[22] M. Ichikawa and H. Maruyama, ‘‘Image processing apparatus, image processing method, and computer-readable recording medium,’’ U.S. Patent 10 158 811 B2, Nov./Dec. 18, 2018.
[23] M. Matiu, L. Bothmann, R. Steinbrecher, and A. Menzel, ‘‘Monitoring succession after a non-cleared windthrow in a Norway spruce mountain forest using webcam, satellite vegetation indices and turbulent CO2 exchange,’’ Agricult. Forest Meteorol., vol. 244, pp. 72–81, Oct. 2017.
[24] T.-H. Chan, K. Jia, S. Gao, J. Lu, and Z. Zeng, Y. Ma, ‘‘PCANet: A simple deep learning baseline for image classiﬁcation?’’ IEEE Trans. Image Process, vol. 24, no. 12, pp. 5017–5032, Dec. 2015.
[25] K. Zhang, Y. Huang, Y. Du, and L. Wang, ‘‘Facial expression recognition based on deep evolutional spatial-temporal networks,’’ IEEE Trans. Image Process., vol. 26, no. 9, pp. 4193–4203, Sep. 2017.
[26] S. A. Morris and T. C. Slesnick, ‘‘Magnetic resonance imaging,’’ in Visual Guide to Neonatal Cardiology. Hoboken, NJ, USA: Wiley, 2018, pp. 104–108.
[27] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, ‘‘DehazeNet: An end-toend system for single image haze removal,’’ IEEE Trans. Image Process., vol. 25, no. 11, pp. 5187–5198, Nov. 2016.
[28] W. G. Campbell, M. Miften, and B. L. Jones, ‘‘Automated target tracking in kilovoltage images using dynamic templates of ﬁducial marker clusters,’’ Med. Phys., vol. 44, no. 2, pp. 364–374, Dec. 2016.
[29] A. J. Lipton, H. Fujiyoshi, and R. S. Patil, ‘‘Moving target classiﬁcation and tracking from real-time video,’’ in Proc. 4th IEEE Workshop Appl. Comput. Vis., Princeton, NJ, USA, Oct. 1998, pp. 8–14.
[30] Y. Nakabo, M. Ishikawa, H. Toyoda, and S. Mizuno, ‘‘1 ms column parallel vision system and its application of high speed target tracking,’’ in Proc. IEEE Int. Conf. Robot. Automat. (ICRA), San Francisco, CA, USA, vol. 1, Apr. 2000, pp. 650–655.
[31] T. Yamazaki, ‘‘A 1 ms high-speed vision chip with 3D-stacked 140 GOPS column-parallel PEs for spatio-temporal image processing,’’ in IEEE Int. Solid-State Circuits Conf. (ISSCC) Dig. Tech. Papers, San Francisco, CA, USA, Feb. 2017, pp. 82–83.
[32] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ‘‘Image denoising by sparse 3-D transform-domain collaborative ﬁltering,’’ IEEE Trans. Image Process., vol. 16, no. 8, pp. 2080–2095, Aug. 2007.
[33] Y. Fu and S. WANG, ‘‘System for beauty, cosmetic, and fashion analysis,’’ U.S. Patent 10 339 685 B2, Jul. 2, 2019.
[34] M. D. McFarlane, ‘‘Digital pictures ﬁfty years ago,’’ Proc. IEEE, vol. 60, no. 7, pp. 768–770, Jul. 1972.
VOLUME 7, 2019

B. Zhang et al.: Image Enhancement via IFOF
[35] O. Ummin, H. Tian, H. Zhu, and F. Liu, ‘‘Application of the digital image technology in the visual monitoring and prediction of shuttering construction safety,’’ IOP Conf. Ser., Earth Environ. Sci., vol. 128, Mar. 2018, Art. no. 012059.
[36] B. Pasveer, ‘‘Knowledge of shadows: The introduction of X-ray images in medicine,’’ Sociol. Health Illness, vol. 11, no. 4, pp. 360–381, Dec. 1989.
[37] F. Udrea, G. Deboy, and T. Fujihira, ‘‘Superjunction power devices, history, development, and future prospects,’’ IEEE Trans. Electron Devices, vol. 64, no. 3, pp. 713–727, Mar. 2017.
[38] M. Xu, T. Li, Z. Wang, X. Deng, R. Yang, and Z. Guan, ‘‘Reducing complexity of HEVC: A deep learning approach,’’ IEEE Trans. Image Process., vol. 27, no. 10, pp. 5044–5059, Oct. 2018.
[39] Z. Wang, P. Yi, K. Jiang, J. Jiang, Z. Han, T. Lu, and J. Ma, ‘‘Multimemory convolutional neural network for video super-resolution,’’ IEEE Trans. Image Process., vol. 28, no. 5, pp. 2530–2544, May 2019.
[40] Y. Li, D. Liu, H. Li, L. Li, Z. Li, and F. Wu, ‘‘Learning a convolutional neural network for image compact-resolution,’’ IEEE Trans. Image Process., vol. 28, no. 3, pp. 1092–1107, 2019.
[41] C. Huo, F. Akhtar, and P. Li, ‘‘A novel grading method of cataract based on AWM,’’ in Proc. IEEE 43rd Annu. Comput. Softw. Appl. Conf. (COMPSAC), Milwaukee, WI, USA, Jul. 2019, pp. 368–373, doi: 10.1109/COMPSAC.2019.10234.
[42] S. Ahn, W. Kim, J. Kim, J. Kim, and S. Lee, ‘‘Visual preference prediction for enhanced images on ultra-high-deﬁnition display,’’ in Proc. 25th IEEE Int. Conf. Image Process. (ICIP), Athens, Greece, Oct. 2018, pp. 3548–3552.
[43] T. H. Yu, X. Meng, M. Zhu, and M. Han, ‘‘An improved multi-scale retinex fog and haze image enhancement method,’’ in Proc. Int. Conf. Inf. Syst. Artif. Intell. (ISAI), Hong Kong, Jun. 2016, pp. 557–560.
[44] C. Pohl, ‘‘Multisensor image fusion guidelines in remote sensing,’’ Rev. Article, Int. J. Remote Sens., vol. 19, no. 5, pp. 823–854, 1998.
[45] G. Li, H. Tang, D. Kim, J. Gao, and L. Lin, ‘‘Employment of frame accumulation and shaped function for upgrading low-light-level image detection sensitivity,’’ Opt Lett., vol. 37, no. 8, pp. 1361–1363, Apr. 2012.
[46] P. J. Burt and E. H. Adelson, ‘‘Merging images through pattern decomposition,’’ Proc. SPIE, vol. 575, pp. 173–181, Dec. 1985.
[47] H. Li, B. S. Manjunath, and S. K. Mitra, ‘‘Multisensor image fusion using the wavelet transform,’’ Graph. Models Image Process., vol. 57, no. 3, pp. 236–245, 1995.
[48] O. Rockinger and T. Fechner, ‘‘Pixel-level image fusion: The case of image sequences,’’ Proc. SPIE, vol. 3374, pp. 378–388, Jul. 1998.
[49] M. Golipour, H. Ghassemian, and F. Mirzapour, ‘‘Integrating hierarchical segmentation maps with MRF prior for classiﬁcation of hyperspectral images in a Bayesian framework,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 54, no. 2, pp. 805–816, Feb. 2016.
[50] M. Fauvel, J. Chanussot, and J. A. Benediktsson, ‘‘Decision fusion for the classiﬁcation of urban remote sensing images,’’ IEEE Trans. Geosci. Remote Sens., vol. 44, no. 10, pp. 2828–2838, Oct. 2006.
[51] B. Luo, M. M. Khan, T. Bienvenu, J. Chanussot, and L. Zhang, ‘‘Decisionbased fusion for pansharpening of remote sensing images,’’ IEEE Geosci. Remote Sens. Lett., vol. 10, no. 1, pp. 19–23, Jan. 2013.
[52] F. T. Mahmoudi, F. Samadzadegan, and P. Reinartz, ‘‘Object recognition based on the context aware decision-level fusion in multiviews imagery,’’ IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 8, no. 1, pp. 12–22, Jan. 2015.
[53] H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool, ‘‘Speeded-up robust features (SURF),’’ Comput. Vis. Image Understand., vol. 110, no. 3, pp. 346–359, 2008.

VOLUME 7, 2019

BAOJU ZHANG received the B.S. and M.S. degrees from Tianjin Normal University, in 1990 and 1993, respectively, and the PhD. degree from Tianjin University, in 2002. She is currently a Professor with the College of Electronic and Communication Engineering, Tianjin Normal University. Her main research interests include compressive sensing, signal processing, audio and video processing, data stream clustering, image processing, image recognition, and object detection.

WENRUI YAN was born in Tai’an, China, in 1995. She received the B.S. degree from Northeast Petroleum University, in 2018. She is currently pursuing the M.S. degree in intelligent science and technology with Tianjin Normal University. Her research interests include computer vision, artiﬁcial intelligence, image processing, and laser vibration.
GANG LI received the B.Sc. degree from the Hefei University of Technology, in 1982, and the M.Sc. and Ph.D. degrees from Tianjin University, in 1987 and 1998, respectively. He is currently a Professor and a Supervisor for Ph.D. candidates with the School of Precision Instrument and Opto-Electronics Engineering, Tianjin University. His main research interests include signal detection and analysis, precision medical instrument, biological image processing, and tissue image detection.
JINGQI FEI received the B.S. degree from Tianjin Normal University, Tianjin, China, where she is currently pursuing the master’s degree. Her research interests include image processing, image registration, multispectral image detection, and iris recognition.
CUIPING ZHANG received the M.S. degree from Tianjin Normal University, Tianjin, China, in 2018. She is with the College of Electronic and Communication Engineering, Tianjin Normal University. Her main research interests include wireless sensor networks and image processing.
CHUYI CHEN is currently pursuing the bachelor’s degree with Tianjin Normal University, Tianjin, China. Her research interests include image processing, object detection, deep learning, and pattern recognition.
181099

