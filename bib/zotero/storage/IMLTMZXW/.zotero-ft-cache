Optimal State Estimation

Optimal State Estimation
Kalman, H,, and Nonlinear Approaches
Dan Simon
Cleveland State University
A JOHN WILEY & SONS, INC., PUBLICATION

Copyright 6 2006 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey. Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 646-8600, or on the web at www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-601 1, fax (201) 748-6008 or online at http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing this book, they make no representations or warranties with respect to the accuracy or completeness of the contents of this book and specifically disclaim any implied warranties of merchantability or fitness for a particular purpose. No warranty may be created or extended by sales representatives or written sales materials. The advice and strategies contained herein may not be suitable for your situation. You should consult with a professional where appropriate. Neither the publisher nor author shall be liable for any loss of profit or any other commercial damages, including but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our Customer Care Department within the U S . at (800) 762-2974, outside the U S . at (317) 5723993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic format. For information about Wiley products, visit our web site at www.wiley.com.
Library of Congress Cataloging-in-Publicationis available.
ISBN-13 978-0-471-70858-2 ISBN-10 0-47 1-70858-5
Printed in the United States of America.
10 9 8 7 6 5 4 3 2 1

CONTENTS

Acknowledgments

xiii

Acronyms

xv

List of algorithms

xvii

Introduction

xxi

PART I INTRODUCTORY MATERIAL

1 Linear systems theory

3

1.1 Matrix algebra and matrix calculus

4

1.1.1 Matrix algebra

6

1.1.2 The matrix inversion lemma

11

1.1.3 Matrix calculus

14

1.1.4 The history of matrices

17

1.2 Linear systems

18

1.3 Nonlinear systems

22

1.4 Discretization

26

1.5 Simulation

27

1.5.1 Rectangular integration

29

1.5.2 Trapezoidal integration

29

1.5.3 Rung-Kutta integration

31

1.6 Stability

33

V

vi CONTENTS

1.6.1 Continuous-time systems

33

1.6.2 Discret6time systems

37

1.7 Controllability and observability

38

1.7.1 Controllability

38

1.7.2 Observability

40

1.7.3 Stabilizability and detectability

43

1.8 Summary

45

Problems

45

2 Probability theory

49

2.1 Probability

50

2.2 Random variables

53

2.3 Transformations of random variables

59

2.4 Multiple random variables

61

2.4.1 Statistical independence

62

2.4.2 Multivariate statistics

65

2.5 Stochastic Processes

68

2.6 White noise and colored noise

71

2.7 Simulating correlated noise

73

2.8 Summary

74

Problems

75

3 Least squares estimation

79

3.1 Estimation of a constant

80

3.2 Weighted least squares estimation

82

3.3 Recursive least squares estimation

84

3.3.1 Alternate estimator forms

86

3.3.2 Curve fitting

92

3.4 Wiener filtering

94

3.4.1 Parametric filter optimization

96

3.4.2 General filter optimization

97

3.4.3 Noncausal filter optimization

98

3.4.4 Causal filter optimization

100

3.4.5 Comparison

101

3.5 Summary

102

Problems

102

4 Propagation of states and covariances

107

4.1 Discretetime systems

107

4.2 Sampled-data systems

111

4.3 Continuous-time systems

114

CONTENTS vii

4.4 Summary

117

Problems

117

PART II THE KALMAN FILTER

5 The discretetime Kalman filter

123

5.1 Derivation of the discretetime Kalman filter

124

5.2 Kalman filter properties

129

5.3 One-step Kalman filter equations

131

5.4 Alternate propagation of covariance

135

5.4.1 Multiple state systems

135

5.4.2 Scalar systems

137

5.5 Divergence issues

139

5.6 Summary

144

Problems

145

6 Alternate Kalman filter formulations

149

6.1 Sequential Kalman filtering

150

6.2 Information filtering

156

6.3 Square root filtering

158

6.3.1 Condition number

159

6.3.2 The square root time-update equation

162

6.3.3 Potter’s square root measurement-update equation

165

6.3.4 Square root measurement update via triangularization

169

6.3.5 Algorithms for orthogonal transformations

171

6.4 U-D filtering

174

6.4.1 U-D filtering: The measurement-update equation

174

6.4.2 U-D filtering: The timeupdate equation

176

6.5 Summary

178

Problems

179

7 Kalman filter generalizations

183

7.1 Correlated process and measurement noise

184

7.2 Colored process and measurement noise

188

7.2.1 Colored process noise

188

7.2.2 Colored measurement noise: State augmentation

189

7.2.3 Colored measurement noise: Measurement differencing

190

7.3 Steady-state filtering

193

7.3.1 a-/I filtering

199

7.3.2 a-p-y filtering

202

7.3.3 A Hamiltonian approach to steady-state filtering

203

7.4 Kalman filtering with fading memory

208

viii CONTENTS

7.5 Constrained Kalman filtering

212

7.5.1 Model reduction

212

7.5.2 Perfect measurements

213

7.5.3 Projection approaches

214

7.5.4 A pdf truncation approach

218

7.6 Summary

223

Problems

225

8 The continuous-time Kalrnan filter

229

8.1 Discretetime and continuous-time white noise

230

8.1.1 Process noise

230

8.1.2 Measurement noise

232

8.1.3 Discretized simulation of noisy continuous-time systems

232

8.2 Derivation of the continuous-time Kalman filter

233

8.3 Alternate solutions to the Riccati equation

238

8.3.1 The transition matrix approach

238

8.3.2 The Chandrasekhar algorithm

242

8.3.3 The square root filter

246

8.4 Generalizations of the continuous-time filter

247

8.4.1 Correlated process and measurement noise

248

8.4.2 Colored measurement noise

249

8.5 The steady-state continuous-time Kalman filter

252

8.5.1 The algebraic Riccati equation

253

8.5.2 The Wiener filter is a Kalman filter

257

8.5.3 Duality

258

8.6 Summary

259

Problems

260

9 Optimal smoothing

263

9.1 An alternate form for the Kalman filter

265

9.2 Fixed-point smoothing

267

9.2.1 Estimation improvement due to smoothing

270

9.2.2 Smoothing constant states

274

9.3 Fixed-lag smoothing

274

9.4 Fixed-interval smoothing

279

9.4.1 Forward-backward smoothing

280

9.4.2 RTS smoothing

286

9.5 Summary

294

Problems

294

10 Additional topics in Kalman filtering
10.1 Verifying Kalman filter performance 10.2 Multiplemodel estimation 10.3 Reduced-order Kalman filtering
10.3.1 Anderson’s approach to reduced-order filtering 10.3.2 The reduced-order Schmidt-Kalman filter 10.4 Robust Kalman filtering 10.5 Delayed measurements and synchronization errors 10.5.1 A statistical derivation of the Kalman filter 10.5.2 Kalman filtering with delayed measurements 10.6 Summary Problems
PART 111 T H E H, FILTER
11 The H, filter
11.1 Introduction 11.1.1 An alternate form for the Kalman filter 11.1.2 Kalman filter limitations
11.2 Constrained optimization 11.2.1 Static constrained optimization 11.2.2 Inequality constraints 11.2.3 Dynamic constrained optimization
11.3 A game theory approach to H, filtering 11.3.1 Stationarity with respect to 20 and Wk 11.3.2 Stationarity with respect to 2 and y 11.3.3 A comparison of the Kalman and H, filters 11.3.4 Steady-state H, filtering 11.3.5 The transfer function bound of the H, filter
11.4 The continuous-time H, filter 11.5 Transfer function approaches 11.6 Summary
Problems
12 Additional topics in H, filtering
12.1 Mixed Kalman/H, filtering 12.2 Robust Kalman/H, filtering 12.3 Constrained H, filtering 12.4 Summary
Problems

CONTENTS

iX

297
298 301 305 306 309 312 317 318 320 325 326

333
334 334 336 337 337 339 341 343 345 347 354 354 357 361 365 367 369
373
374 377 381 388 389

X

CONTENTS

PART IV NONLINEAR FILTERS

13 Nonlinear Kalman filtering

395

13.1 The linearized Kalman filter

397

13.2 The extended Kalman filter

400

13.2.1 The continuous-time extended Kalman filter

400

13.2.2 The hybrid extended Kalman filter

403

13.2.3 The discretetime extended Kalman filter

407

13.3 Higher-order approaches

410

13.3.1 The iterated extended Kalman filter

410

13.3.2 The second-order extended Kalman filter

413

13.3.3 Other approaches

420

13.4 Parameter estimation

422

13.5 Summary

425

Problems

426

14 The unscented Kalman filter

433

14.1 Means and covariances of nonlinear transformations

434

14.1.1 The mean of a nonlinear transformation

434

14.1.2 The covariance of a nonlinear transformation

437

14.2 Unscented transformations

441

14.2.1 Mean approximation

44 1

14.2.2 Covariance approximation

444

14.3 Unscented Kalman filtering

447

14.4 Other unscented transformations

452

14.4.1 General unscented transformations

452

14.4.2 The simplex unscented transformation

454

14.4.3 The spherical unscented transformation

455

14.5 Summary

457

Problems

458

15 The particle filter

461

15.1 Bayesian state estimation

462

15.2 Particle filtering

466

15.3 Implementation issues

469

15.3.1 Sample impoverishment

469

15.3.2 Particle filtering combined with other filters

477

15.4 Summary

480

Problems

481

Appendix A: Historical perspectives Appendix B: Other books on Kalman filtering Appendix C: State estimation and the meaning of life References Index

CONTENTS xi
485 489 493 501 521

ACKNOWLEDGMENTS
The financial support of Sanjay Garg and Donald Simon (no relation to the author) at the NASA Glenn Research Center was instrumental in allowing me to pursue research in the area of optimal state estimation, and indirectly led to the idea for this book. I am thankful to Eugenio Villaseca, the Chair of the Department of Electrical and Computer Engineering at Cleveland State University, for his encouragement and support of my research and writing efforts. Dennis Feucht and Jonathan Litt reviewed the first draft of the book and offered constructive criticism that made the book better than it otherwise would have been. I am also indebted to the two anonymous reviewers of the proposal for this book, who made suggestions that strengthened the material presented herein. I acknowledge the work of Sandy Buettner, Joe Connolly, Classica Jain, Aaron Radke, Bryan Welch, and Qing Zheng, who were students in my Optimal State Estimation class in Fall 2005. They contributed some of the problems a t the end of the chapters and made many suggestions for improvement that helped clarify the subject matter. Finally I acknowledge the love and support of my wife, Annette, whose encouragement of my endeavors has always been above and beyond the call of duty.
D. J. S.
xiii

ACRONYMS

ACR ARE CARE DARE EKF erf FPGA GPS HOT iff INS LHP LTI LTV MCMC MIMO
N ( a ,b) Pdf

Acronym Algebraic Riccati equation Continuous ARE Discrete ARE Extended Kalman filter Error function Field programmable gate array Global Positioning System Higher-order terms If and only if Inertial navigation system Left half plane Linear time-invariant Linear time-varying Markov chain Monte Carlo Multiple input, multiple output Normal pdf with a mean of a and a variance of b Probability density function

xv

xvi ha of acronyms

PDF
QED
RHP RMS
RPF
RTS
RV
SIR SISO
sss
SVD
TF
U ( a ,b) UKF
wss

Probability distribution function Quod erat demonstrandum (i.e., “that which was to be demonstrated” ) Right half plane Root mean square Regularized particle filter Rauch-Tung-Striebel Random variable Sampling importance resampling Single input, single output Strict-sense stationary Singular value decomposition Transfer function Uniform pdf that is nonzero on the domain [u,b] Unscented Kalman filter Wide-sense stationary

LIST OF ALGORITHMS

Chapter 1: Linear systems theory

Rectangular integration

29

Trapezoidal integration

31

Fourth-order Runge-Kutta integration

32

Chapter 2: Probability theory

Correlated noise simulation

74

Chapter 3: Least squares estimation

Recursive least squares estimation

86

General recursive least squares estimation

88

Chapter 5: The discrete-time Kalman filter

The discrete-time Kalman filter

128

Chapter 6: Alternate Kalman filter formulations

The sequential Kalman filter

151

The information filter

156

The Cholesky matrix square root algorithm

160

Potter’s square root measurement-update algorithm

166

The Householder algorithm

171

The Gram-Schmidt algorithm

172

The U-D measurement update

175

The U-D time update

177

xvii

XViii

List of algwithms

Chapter 7: Kalman filter generalizations

The general discretetime Kalman filter

186

The discrete-time Kalman filter with colored measurement noise

191

The Hamiltonian approach to steady-state Kalman filtering

207

The fading-inemory filter

210

Chapter 8: The continuous-time Kalman filter

The continuous-time Kalman filter

235

The Chandrasekhar algorithm

244

The continuous-time square root Kalman filter

247

The continuous-time Kalman filter with correlated noise

249

The continuous-time Kalman filter with colored measurement noise

251

Chapter 9: Optimal smoothing

The fixed-point smoother

269

The fixed-lag smoother

278

The RTS smoother

293

Chapter 10: Additional topics in Kalman filtering

The multiplemodel estimator

302

The reduced-order Schmidt-Kalman filter

312

The delayed-measurement Kalman filter

324

Chapter 11: The H, filter

The discretetime H, filter

353

Chapter 12: Additional topics in H, filtering

The mixed Kalman/H, filter

374

The robust mixed Kalman/H, filter

378

The constrained H, filter

385

Chapter 13: Nonlinear Kalman filtering

The continuous-time linearized Kalman filter

399

The continuous-time extended Kalman filter

401

The hybrid extended Kalman filter

405

The discretetime extended Kalman filter

409

The iterated extended Kalman filter

41 1

The second-order hybrid extended Kalman filter

416

The second-order discretetime extended Kalman filter

419

The Gaussian sum filter

421

Chapter 14: The unscented Kalman filter

The unscented transformation

446

The unscented Kalman filter

448

The simplex sigma-point algorithm

454

The spherical sigma-point algorithm

455

Chapter 15: The particle filter The recursive Bayesian state estimator The particle filter Regularized particle filter resampling The extended Kalman particle filter

L ~ s tof algorithms

XiX

465 468 473 478

INTRODUCTION
This book discusses mathematical approaches to the best possible way of estimating the state of a general system. Although the book is firmly grounded in mathematical theory, it should not be considered a mathematics text. It is more of an engineering text, or perhaps an applied mathematics text. The approaches that we present for state estimation are all given with the goal of eventual implementation in s0ftware.l The goal of this text is to present state estimation theory in the most clear yet rigorous way possible, while providing enough advanced material and references so that the reader is prepared to contribute new material to the state of the art. Engineers are usually concerned with eventual implementation, and so the material presented is geared toward discretetime systems. However, continuoustime systems are also discussed for the sake of completeness, and because there is still room for implementations of continuous-time filters.
Before we discuss optimal state estimation, we need to define what we mean by the term state. The states of a system are those variables that provide a complete representation of the internal condition or status of the system at a given instant of time.2 This is far from a rigorous definition, but it suffices for the purposes of
lI use the practice that is common in academia of referring t o a generic third person by the word we. Sometimes, I use the word we t o refer t o the reader and myself. Other times, I use the word we to indicate that I am speaking on behalf of the control and estimation community. The distinction should be clear from the context. However, I encourage the reader not to read too much into my use of the word we; it is more a matter of personal preference and style rather than a claim to authority. 21n this book, we use the terms state and state vanable interchangably. Also, the word state could refer to the entire collection of state variables, or it could refer t o a single state variable. The specific meaning needs to be inferred from the context.
xxi

xxii

INTRODUCTION

this introduction. For example, the states of a motor might include the currents through the windings, and the position and speed of the motor shaft. The states of an orbiting satellite might include its position, velocity, and angular orientation. The states of an economic system might include per-capita income, tax rates, unemployment, and economic growth. The states of a biological system might include blood sugar levels, heart and respiration rates, and body temperature.
State estimation is applicable to virtually all areas of engineering and science. Any discipline that is concerned with the mathematical modeling of its systems is a likely (perhaps inevitable) candidate for state estimation. This includes electrical engineering, mechanical engineering, chemical engineering, aerospace engineering, robotics, economics, ecology, biology, and many others. The possible applications of state estimation theory are limited only by the engineer’simagination, which is why state estimation has become such a widely researched and applied discipline in the past few decades. State-space theory and state estimation was initially developed in the 1950s and 1960s, and since then there have been a huge number of applications. A few applications are documented in [Sor85]. Thousands of other applications can be discovered by doing an Internet search on the terms “state estimation” and “application,” or “Kalman filter” and ”application.”
State estimation is interesting to engineers for at least two reasons:
0 Often, an engineer needs to estimate the system states in order to implement a state-feedback controller. For example, the electrical engineer needs to estimate the winding currents of a motor in order to control its position. The aerospace engineer needs to estimate the attitude of a satellite in order to control its velocity. The economist needs to estimate economic growth in order to try to control unemployment. The medical doctor needs to estimate blood sugar levels in order to control heart and respiration rates.
0 Often an engineer needs to estimate the system states because those states are interesting in their own right. For example, if an engineer wants to measure the health of an engineeringsystem, it may be necessary to estimate the internal condition of the system using a state estimation algorithm. An engineer might want to estimate satellite position in order to more intelligently schedule future satellite activities. An economist might want to estimate economic growth in order to make a political point, A medical doctor might want to estimate blood sugar levels in order to evaluate the health of a patient.
There are many other fine books on state estimation that are available (see Appendix B). This begs the question: Why yet another textbook on the topic of state estimation? The reason that this present book has been written is to offer a pedagogical approach and perspective that is not available in other state estimation books. In particular, the hope is that this book will offer the following:
0 A straightforward, bottom-up approach that assists the reader in obtaining a clear (but theoretically rigorous) understanding of state estimation. This is reminiscent of Gelb’s approach [Ge174],which has proven effective for many state estimation students of the past few decades. However, many aspects of Gelb’s book have become outdated. In addition, many of the more recent books on state estimation read more like research monographs and are not entirely accessible to the average engineering student. Hence the need for the present book.

INTRODUCTION xxiii
0 Simple examples that provide the reader with an intuitive understanding of the theory. Many books present state estimation theory and then follow with examples or problems that require a computer for implementation. However, it is possible to present simple examples and problems that require only paper and pencil to solve. These simple problems allow the student to more directly see how the theory works itself out in practice. Again, this is reminiscent of Gelb’s approach [Ge174].
0 MATLABbased source code3 for the examples in the book is available at the author’s Web sitea4A number of other texts supply source code, but it is often on disk or CD, which makes the code subject to obsolescence. The author’s e-mail address is also available on the Web site, and I enthusiastically welcome feedback, comments, suggestions for improvements, and corrections. Of course, Web addresses are also subject to obsolescence, but the book also contains algorithmic, high-level pseudocode listings that will last longer than any specific software listings.
0 Careful treatment of advanced topics in optimal state estimation. These topics include unscented filtering, high-order nonlinear filtering, particle filtering, constrained state estimation, reduced-order filtering, robust Kalman filtering, and mixed Kalman/H, filtering. Some of these topics are mature, having been introduced in the 1960s, but others of these topics are recent additions to the state of the art. This coverage is not matched in any other books on the topic of state estimation.
Some of the other books on state estimation offer some of the above features, but no other books offer all of these features.
Prerequisites
The prerequisites for understanding the material in this book are a good foundation in linear systems theory and probability and stochastic processes. Ideally, the reader will already have taken a graduate course in both of these topics. However, it should be said that a background in linear systems theory is more important than probability. The first two chapters of the book review the elements of linear systems and probability that are essential for the rest of the book, and also serve to establish the notation that is used during the remainder of the book.
Other material could also be considered prerequisite to understanding this book, such as undergraduate advanced calculus, control theory, and signal processing. However, it would be more accurate to say that the reader will require a moderately high level of mathematical and engineering maturity, rather than trying to identify a list of required prerequisite courses.
3MATLAB is a registered trademark of The Mathworks, Inc. 4http://academic.csuohio.edu/simond/estimatio-nif the Web site address changes, it should be easy to find with an internet search.

xxiv

INTRODUCTION

Problems
The problems at the end of each chapter have been written to give a high degree of flexibility to the instructor and student. The problems include both written exercises and computer exercises. The written exercises are intended to strengthen the student’s grasp of the theory, and deepen the student’s intuitive understanding of the concepts. The computer exercises are intended to help the student learn how to apply the theory to problems of the type that might be encountered in industrial or government projects. Both types of problems are important for the student to become proficient at the material. The distinction between written exercises and computer exercises is more of a fuzzy division rather than a strict division. That is, some of the written exercises include parts for which some computer work might be useful (even required), and some of the computer exercises include parts for which some written analysis might be useful (even required).
A solution manual to all of the problems in the text (both written exercises and computer exercises) is available from the publisher to instructors who have adopted this book. Course instructors are encouraged to contact the publisher for further information about out how to obtain the solution manual.

Outline of the book
This book is divided into four parts. The first part of the book covers introductory material. Chapter 1 is a review of the relevant areas of linear systems. This material is often covered in a first-semester graduate course taken by engineering students. It is advisable, although not strictly required, that readers of this book have already taken a graduate linear systems course. Chapter 2 reviews probability theory and stochastic processes. Again, this is often covered in a first-semester graduate course. In this book we rely less on probability theory than linear systems theory, so a previous course in probability and stochastic processes is not required for the material in this book (although it would be helpful). Chapter 3 covers least squares estimation of constants and Wiener filtering of stochastic processes. The section on Wiener filtering is not required for the remainder of the book, although it is interesting both in its own right and for historical perspective. Chapter 4 is a brief discussion of how the statistical measures of a state (mean and covariance) propagate in time. Chapter 4 provides a bridge from the first.three chapters to the second part of the book.
The second part of the book covers Kalman filtering, which is the workhorse of state estimation. In Chapter 5 , we derive the discrete-time Kalman filter, including several different (but mathematically equivalent) formulations. In Chapter 6, we present some alternative Kalman filter formulations, including sequential filtering, information filtering, square root filtering, and U-D filtering. In Chapter 7, we d i s cuss some generalizations of the Kalman filter that make the filter applicable t o a wider class of problems. These generalizations include correlated process and measurement noise, colored process and measurement noise, steady-state filtering for computational savings, fading-memory filtering, and constrained Kalman filtering. In Chapter 8, we present the continuous-time Kalman filter. This chapter could be skipped if time is short since the continuous-time filter is rarely implemented in practice. In Chapter 9, we discuss optimal smoothing, which is a way to estimate

INTRODUCTION

XXV

the state of a system at time r based on measurements that extend beyond time r. As part of the derivation of the smoothing equations, the first section of Chapter 9 presents another alternative form for the Kalman filter. Chapter 10 presents some additional, more advanced topics in Kalman filtering. These topics include verification of filter performance, estimation in the case of unknown system models, reduced-order filtering, increasing the robustness of the Kalman filter, and filtering in the presence of measurement synchronization errors. This chapter should provide fertile ground for students or engineers who are looking for research topics or projects.
The third part of the book covers H, filtering. This area is not as mature as Kalman filtering and so there is less material than in the Kalman filtering part of the book. Chapter 11 introduces yet another alternate Kalman filter form as part of the H, filter derivation. This chapter discusses both time domain and frequency domain approaches to H, filtering. Chapter 12 discusses advanced topics in H, filtering, including mixed Kalman/H, filtering and constrained H, filtering. There is a lot of room for further development in H, filtering, and this part of the book could provide a springboard for researchers to make contributions in this area.
The fourth part of the book covers filtering for nonlinear systems. Chapter 13 discusses nonlinear filtering based on the Kalman filter, which includes the widely used extended Kalman filter. Chapter 14 covers the unscented Kalman filter, which is a relatively recent development that provides improved performance over the extended Kalman filter. Chapter 15 discusses the particle filter, another recent developmentthat provides a very general solution to the nonlinear filtering problem. It is hoped that this part of the book, especially Chapters 14 and 15, will inspire researchers to make further contributions to these new areas of study.
The book concludes with three brief appendices. Appendix A gives some historical perspectives on the development of the Kalman filter, starting with the least squares work of Roger Cotes in the early 1700s, and concluding with the space program applications of Kalman filtering in the 1960s. Appendix B discusses the many other books that have been written on Kalman filtering, including their distinctive contributions. Finally, Appendix C presents some speculations on the connections between optimal state estimation and the meaning of life.
Figure 1.1 gives a graphical representation of the structure of the book from a prerequisite point of view. For example, Chapter 3 builds on Chapters 1 and 2. Chapter 4 builds on Chapter 3, and Chapter 5 builds on Chapter 4. Chapters 6-11 each depend on material from Chapter 5, but are independent from each other. Chapter 12 builds on Chapter 11. Chapter 13 depends on Chapter 8, and C h a p ter 14 depends on Chapter 13. Finally, Chapter 15 builds on Chapter 3. This structure can be used to customize a course based on this book.

A note on notation
Three dots between delimiters (parenthesis, brackets, or braces) means that the quantity between the delimiters is the same as the quantity between the previous set of identical delimiters in the same equation. For example,
+ + ( A BCD) (. *)T = ( A+ BCD)+ ( A+ BCD)T
A + [ B ( C + D ) ] - l E [ . . . ] = A + [ B ( C + D ) ] - ' E [ B ( C + D ) ] (1.1)

XXVi

INTRODUCTION

Chapter 14: The unscented Kalman filter Chapter 13: Nonlinear Kalman filtering

Chapter 12: Additio topics in H_ filtering

Chapter 6: Alternate Kalman filter formulations Chapter 7: Kalman filter
eanaralimtinnr
Chapter 8: The continuous-time Kalman
filter
Chapter 9: Optimal smoothing I
Chapter 10: Additional topics in Kalman filtering Chapter 15: The particle filter

Chapter 5 : The discrete-time Kalman filter

1

I Chapter 4: Propagation of stat= and covanances

I

I

I

Chapter 3: Least squares esumation

Chapter 1: Linear systems theory

Chapter 2: Probability theory

Figure I.1 Prequisite structure of the chapters in this book.

PART I
INTRODUCTORY MATERIAL
Optzmal State Estamataon, Fzrst Edztzon. By Dan J. Simon ISBN 0471708585 0 2 0 0 6 John Wiley li Sons. Inc.

CHAPTER 1

Linear systems theory

Finally, we make some remarks on why linear systems are so important. The answer is simple: because we can solve them!
--Richard Feynman [Fey63, p. 25-41

This chapter reviews some essentials of linear systems theory. This material is typically covered in a linear systems course, which is a first-semester graduate level course in electrical engineering. The theory of optimal state estimation heavily relies on matrix theory, including matrix calculus, so matrix theory is reviewed in Section 1.1. Optimal state estimation can be applied t o both linear and nonlinear systems, although state estimation is much more straightforward for linear systems. Linear systems are briefly reviewed in Section 1.2 and nonlinear systems are discussed in Section 1.3. State-space systems can be represented in the continuoustime domain or the discrete-time domain. Physical systems are typically described in continuous time, but control and state estimation algorithms are typically implemented on digital computers. Section 1.4 discusses some standard methods for obtaining a discrete-time representation of a continuous-time system. Section 1.5 discusses how to simulate continuous-time systems on a digital computer. Sections 1.6 and 1.7 discuss the standard concepts of stability, controllability, and observability of linear systems. These concepts are necessary to understand some of the optimal state estimation material later in the book. Students with a strong

Optimal State Estimation, First Edition. By Dan J. Simon

3

ISBN 0471708585 0 2 0 0 6 John Wiley & Sons, Inc.

4

LINEAR SYSTEMS THEORY

background in linear systems theory can skip the material in this chapter. However, it would still help t o at least review this chapter to solidify the foundational concepts of state estimation before moving on to the later chapters of this book.

1.1 MATRIX ALGEBRA A N D M A T R I X CALCULUS

In this section, we review matrices, matrix algebra, and matrix calculus. This

is necessary in order to understand the rest of the book because optimal state

estimation algorithms are usually formulated with matrices.

A scalar is a single quantity. For example, the number 2 is a scalar. The number
+1 3 j is a scalar (we use j in this book to denote the square root of -1). The

number T is a scalar.

A vector consists of scalars that are arranged in a row or column. For example,

the vector

P 3 TI

(1.1)

is a %element vector. This vector is a called a 1 x 3 vector because it has 1 row and 3 columns. This vector is also called a row vector because it is arranged as a single row. The vector

is a 4-element vector. This vector is a called a 4 x 1 vector because it has 4 rows

and 1 column. This vector is also called a column vector because it is arranged as

a single column. Note that a scalar can be viewed as a 1-element vector; a scalar

is a degenerate vector. (This is just like a plane can be viewed as a 3-dimensional

shape; a plane is a degenerate 3-dimensional shape.)

A matrix consists of scalars that are arranged in a rectangle. For example, the

matrix

r-2 3 1

is a 3 x 2 matrix because it has 3 rows and 2 columns. The number of rows and columns in a matrix can be collectively referred to as the dimension of the matrix. For example, the dimension of the matrix in the preceding equation is 3 x 2. Note that a vector can be viewed as a degenerate matrix. For example, Equation (1.1)is a 1 x 3 matrix. A scalar can also be viewed as a degenerate matrix. For example, the scalar 6 is a 1 x 1 matrix.
The rank of a matrix is defined as the number of linearly independent rows. This is also equal to the number of linearly independent columns. The rank of a matrix A is often indicated with the notation p ( A ) . The rank of a matrix is always less than or equal to the number of rows, and it is also less than or equal t o the number of columns. For example, the matrix

MATRIX ALGEBRA AND MATRIX CALCULUS

5

has a rank of one because it has only one linearly independent row; the two rows are multiples of each other. It also has only one linearly independent column; the two columns are multiples of each other. On the other hand, the matrix
:] A = [ :
has a rank of two because it has two linearly independent rows. That is, there are no nonzero scalars c1 and cz such that

C l [ l 3]+c2[2 4 ] = [ 0 01

(1.6)

so the two rows are linearly independent. It also has two linearly independent columns. That is, there are no nonzero scalars c1 and c2 such that

so the two columns are linearly independent. A matrix whose elements are comprised entirely of zeros has a rank of zero. An n x m matrix whose rank is equal to min(n,m) is called full rank. The nullity of an n x m matrix A is equal t o [m - P ( 4 1 .
The transpose of a matrix (or vector) can be taken by changing all the rows to columns, and all the columns to rows. The transpose of a matrix is indicated with a T superscript, as in AT.l For example, if A is the r x n matrix
A=
then AT is the n x r matrix

Note that we use the notation A,, to indicate the scalar in the ith row and j t h column of the matrix A. A symmetric matrix is one for which A = AT.
The hermitian transpose of a matrix (or vector) is the complex conjugate of the transpose, and is indicated with an H superscript, as in AH. For example, if

[A = 4j 5 + j 1 - 3 j

(1.10)

then

(1.11)

A hermitian matrix is one for which A = AH.
lMany papers or books indicate transpose with a prime, as in A’, or with a lower case t , as in A t .

6

LINEAR SYSTEMSTHEORY

1.1.1 Matrix algebra

Matrix addition and subtraction is simply defined as element-by-element addition and subtraction. For example,

" 1 1 2 3
[ 3 2 I ] + [ ' 1 -1 -2I ] = [ ' 4 1 -1

(1.12)

+ The sum ( A B ) and the difference ( A- B ) is defined only if the dimension of A
is equal to the dimension of B. Suppose that A is an n x T matrix and B is an T x p matrix. Then the product of
A and B is written as C = AB. Each element in the matrix product C is computed

as

r
ctj = AikBkj

i = 1,. ..,n

j = 1, . . . , p

(1.13)

k=l

The matrix product AB is defined only if the number of columns in A is equal t o
the number of rows in B. It is important to note that matrix multiplication does
not commute. In general, AB # BA.
Suppose we have an n x 1 vector x. We can compute the 1 x 1product xTx, and
the n x n product xxT as follows:
[ " 13 xTx = [ 21 * . * xn

Xn

[ i'] = z:+...+x;

xxT =

3 [ 2 1 * * ' xn

Xn

(1.14)

Suppose that we have a p x n matrix H and an n x n matrix P. Then HT is a n x p matrix, and we can compute the p x p matrix product HPHT.

(1.15) This matrix of sums can be written as the following sum of matrices:

MATRIX ALGEBRA AND MATRIX CALCULUS

7

(1.16)

where we have used the notation that Hk is the kth column of H .

Matrix division is not defined; we cannot divide a matrix by another matrix

(unless, of course, the denominator matrix is a scalar).

An identity matrix I is defined as a square matrix with ones on the diagonal and

zeros everywhere else. For example, the 3 x 3 identity matrix is equal to

[ a :] I= 0 1 0

(1.17)

The identity matrix has the property that A I = A for any matrix A, and I A = A (as long the dimensions of the identity matrices are compatible with those of A). The 1 x 1 identity matrix is equal to the scalar 1.
The determinant of a matrix is defined inductively for square matrices. The determinant of a scalar (i.e., a 1 x 1 matrix) is equal to the scalar. Now consider an n x n matrix A . Use the notation A(iJ) to denote the matrix that is formed by deleting the ith row and j t h column of A. The determinant of A is defined as

for any value of i E [I,n]. This is called the Laplace expansion of A along its ith row. We see that the determinant of the n x n matrix A is defined in terms of the determinants of ( n - 1) x (n- 1) matrices. Similarly, the determinants of (n- 1) x (n- 1) matrices are defined in terms of the determinants of ( n-2) x ( n-2) matrices. This continues until the determinants of 2 x 2 matrices are defined in terms of the determinants of 1 x 1 matrices, which are scalars. The determinant of A can also be defined as
(1.19) 2=1
for any value of j E [l,n ] . This is called the Laplace expansion of A along its j t h column. Interestingly, Equation (1.18) (for any value of i) and Equation (1.19) (for any value of j ) both give identical results. From the definition of the determinant

8

LINEAR SYSTEMS THEORY

we see that

(1.20)

Some interesting properties of determinants are

IABI = IAlIBI

(1.21)

assuming that A and B are square and have the same dimensions. Also,

n
1-41 =
a=1

(1.22)

where A, (the eigenvalues of A) are defined below. The inverse of a matrix A is defined as the matrix A-l such that AA-l =
A-lA = I. A matrix cannot have an inverse unless it is square. Some square
matrices do not have an inverse. A square matrix that does not have an inverse is called singular or invertible. In the scalar case, the only number that does not have an inverse is the number 0. But in the matrix case, there are many matrices that are singular. A matrix that does have an inverse is called nonsingular or invertible. For example, notice that

[ [ [ 1 0

10

10

2 3 1 -2/3 1/31 = 0 1 1

(1.23)

Therefore, the two matrices on the left side of the equation are inverses of each other. The nonsingularity of an n x n matrix A can be stated in many equivalent ways, some of which are the following [Hor85]:
0 A is nonsingular. 0 A-l exists. 0 The rank of A is equal to n. 0 The rows of A are linearly independent. 0 The columns of A are linearly independent.
IAl # 0.
0 A z = b has a unique solution z for all b. 0 0 is not an eigenvalue of A .

MATRIX ALGEBRA AND MATRIX CALCULUS

9

The trace of a square matrix is defined as the sum of its diagonal elements:
(1.24)
a
The trace of a matrix is defined only if the matrix is square. The trace of a 1 x 1 matrix is equal to the trace of a scalar, which is equal to the value of the scalar. One interesting property of the trace of a square matrix is

a
That is, the trace of a square matrix is equal to the sum of its eigenvalues. Some interesting and useful characteristics of matrix products are the following:

(1.26)
This assumes that the inverses exist for the inverse equation, and that the matrix dimensions are compatible so that matrix multiplication is defined. The transpose of a matrix product is equal to the product of the transposes in the opposite order. The inverse of a matrix product is equal to the product of the inverses in the opposite order. The trace of a matrix product is independent of the order in which the matrices are multiplied.
The two-norm of a column vector of real numbers, also called the Euclidean norm, is defined as follows:
])x))2= d z
(1.27)
From (1.14) we see that

Taking the trace of this matrix is

(1.28)

(1.29)

An n x n matrix A has n eigenvalues and n eigenvectors. The scalar X is an

eigenvalue of A, and the n x 1 vector x is an eigenvector of A, if the following

equation holds:

AX = AX

(1.30)

The eigenvalues and eigenvectors of a matrix are collectively referred to as the eigendata of the matrix.2 An n x n matrix has exactly n eigenvalues, although

2Eigendatahave also been referred to by many other terms over the years, including characteristic roots, latent roots and vectors, and proper numbers and vectors [Fad59].

10

LINEAR SYSTEMS THEORY

some may be repeated. This is like saying that an nth order polynomial equation has exactly n roots, although some may be repeated. From the above definitions of eigenvalues and eigenvectors we can see that

AX = XX A2x = AXX
= X(Ax) = X(Xz)
= X2x

(1.31)

So if A has eigendata (X,z), then A2 has eigendata (X2,z). It can be shown that A-l exists if and only if none of the eigenvalues of A are equal to 0. If A is symmetric then all of its eigenvalues are real numbers.
A symmetric n x n matrix A can be characterized as either positive definite, positive semidefinite, negative definite, negative semidefinite, or indefinite. Matrix A is:
0 Positive definite if xTAx > 0 for all nonzero n x 1vectors z.This is equivalent
to saying that all of the eigenvalues of A are positive real numbers. If A is positive definite, then A-' is also positive definite.
0 Positive semidefinite if z T A z 2 0 for all n x 1vectors z.This is equivalent to saying that all of the eigenvalues of A are nonnegative real numbers. Positive
semidefinite matrices are sometimes called nonnegative definite.
0 Negative definite if z T A z < 0 for all nonzero n x 1vectors z.This is equivalent
to saying that all of the eigenvalues of A are negative real numbers. If A is negative definite, then A-' is also negative definite.
0 Negative semidefinite if z T A z 5 0 for all n x 1vectors 2 . This is equivalent to saying that all of the eigenvalues of A are nonpositive real numbers. Negative
semidefinite matrices are sometimes called nonpositive definite.

0 Indefinite if it does not fit into any of the above four categories. This is equivalent to saying that some of its eigenvalues are positive and some are negative.

Some books generalize the idea of positive definiteness and negative definiteness to include nonsymmetric matrices.
The weighted two-norm of an n x 1 vector x is defined as

;1.1 = mz

(1.32)

where Q is required t o be an n x n positive definite matrix. The above norm is also called the Q-weighted two-norm of 2 . A quantity of the form xTQz is called a quadratic in analogy to a quadratic term in a scalar equation.
The singular values g of a matrix A are defined as

02(A) = X(ATA) = X(AA~)

(1.33)

MATRIX ALGEBRA AND MATRIX CALCULUS

11

If A is an n x m matrix, then it has min(n,m) singular values. AAT will have
n eigenvalues, and ATA will have m eigenvalues. If n > m then AAT will have
the same eigenvalues as ATA plus an additional ( n - m) zeros. These additional zeros are not considered to be singular values of A , because A always has min(n, m)
singular values. This knowledge can help reduce effort during the computation of singular values. For example, if A is a 13 x 3 matrix, then it is much easier to compute the eigenvalues of the 3 x 3 matrix A T A rather than the 13 x 13 matrix AAT. Either computation will result in the same three singular values.

1.1.2 The matrix inversion lemma

In this section, we will derive the matrix inversion lemma, which is a tool that we

will use many times in this book. It is also a tool that is frequently useful in other

[ :E ] areas of control, estimation theory, and signal processing.

Suppose we have the partitioned matrix

where A and D are invertible

square matrices, and the B and C matrices may or may not be square. We define E and F matrices as follows:

E = D-CA-lB F = A-BD-lC

(1.34)

Assume that E is invertible. Then we can show that

=[: ;]

(1.35)

Now assume that F is invertible. Then we can show that

[ :E ] [ I F-1

-A-1BE-1

-D-lCF-l

E-1

AF-l- BD-lCF-l

-BE-1 + BE-l

CF-l- CF-l -CA-IBE-l+ DE-l

= [[ 1 = ( A- BD-lC)F-'

0

1 0

(D- C A - ~ B ) E - ~

=[: ;]

(1.36)

[ : 1 . Equations (1.35) and (1.36) are two expressions for the inverse of

Since

these two expressions are inverses of the same matrix, they must be equal. We

therefore conclude that the upper-left partitions of the matrices are equal, which

gives

~ - l =A-1 + A - ~ B E - ~ c A - ~

(1.37)

12

LINEAR SYSTEMS THEORY

Now we can use the definition of F to obtain
+ ( A- B D - l C ) - l = A-' A-lB(D - CA-'B)-lCA-l

(1.38)

This is called the matrix inversion lemma. It is also referred to by other terms, such as the Sherman-Morrison formula, Woodbury's identity, and the modified matrices formula. One of its earliest presentations ww in 1944 by William Duncan [Dun44], and similar identities were developed by Alston Householder [Hou53]. An account of its origins and variations (e.g., singular A ) is given in [Hen81]. The matrix inversion lemma is often stated in slightly different but equivalent ways. For example,

+ + ( A B D - l C ) - ' = A-' - A-'B(D CA-lB)-lCA-l

(1.39)

The matrix inversion lemma can sometimes be used to reduce the computational
effort of matrix inversion. For instance, suppose that A is n x n, B is n x p , C is p x n, D is p x p , and p < n. Suppose further that we already know A - l , and we want
to add some quantity to A and then compute the new inverse. A straightforward computation of the new inverse would be a n n x n inversion. But if the new matrix
to invert can be written in the form of the left side of Equation (1.39), then we can
use the right side of Equation (1.39) t o compute the new inverse, and the right side
of Equation (1.39) requires a p x p inversion instead of an n x n inversion (since we already know the inverse of the old A matrix).

EX AMP LEI.^

At your investment firm, you notice that in January the New York Stock Ex-

change index decreased by 2%, the American Stock Exchange index increased

by 1%,and the NASDAQ stock exchange index increased by 2%. As a result,

investors increased their deposits by 1%. The next month, the stock exchange

indices changed by -4%, 3%, and 2%, respectively, and investor deposits in-

creased by 2%. The following month, the stock exchange indices changed by

+ + -5%, 1%, and 5%, respectively, and investor deposits increased by 2%. You
suspect that investment changes y can be modeled as y = g 1 q ~ 2 x 2 ~ 3 x 3 ,

where the 2%variables are the stock exchange index changes, and the gi are

unknown constants. In order to determine the gi constants you need to invert

the matrix

-2 1 2

A = [ -4 3 2 1

(1.40)

-5 1 5

The result is

[ 13 -3 -4

1 A-' =

10 0 - 4 1

11 -3 -2

= ;[-11

(1.41)

MATRIX ALGEBRA AND MATRIX CALCULUS

13

This allows you to use stock exchange index changes to predict investment

changes in the following month, which allows you t o better schedule person-

nel and computer resources. However, soon afterward you find out that the

NASDAQ change in the third month was actually 6% rather than 5%. This

means that in order to find the gi constants you need to invert the matrix

[ -2 1 2
A'= -4 3 2 1

(1.42)

-5 1 6

You are tired of inverting matrices and so you wonder if you can somehow use the inverse of A (which you have already calculated) to find the inverse of A'. Remembering the matrix inversion lemma, you realize that A' = A + B D - l C , where

B = [ 0 0 1IT
c = [ o 0 11
D=1

(1.43)

You therefore use the matrix inversion lemma to compute

+ (A')-l = ( A BD-lC)-l + = A-l - A-lB(D C A - l B ) - l C A - l

(1.44)

+ The ( D C A - l B ) term that needs to be inverted in the above equation is a
scalar, so its inversion is simple. This gives

(A')-' g

= =

4.00 1.00 3.50 -0.50
[ 2.75 -0.75
[ i ] (A')-'

I-1.00
-1.00 -0.50

[ ] - i.5

(1.45)

0.25

In this example, the use of the matrix inversion lemma is not really necessary because A' (the new matrix to invert) is only 3 x 3. However, with larger matrices, such as 1000 x 1000 matrices, the computational savings that is realized by using the matrix inversion lemma could be significant.
vvv
Now suppose that A , B , C, and D are matrices, with A and D being square. Then it can be seen that

[ ] 4 AI - l I0 ] [ CA DB ] [ 0I - AI- I B ] = [ A0 D - CA-lB

(1.46)

(1.47)

14

LINEAR SYSTEMS THEORY

Similarly, it can be shown that
(1.48)
These formulas are called product rules for determinants. They were first given by the Russian-born mathematician Issai Schur in a German paper [Schl7] that was reprinted in English in [Sch86].

1.1.3 Matrix calculus
In our first calculus course, we learned the mathematics of derivatives and integrals and how to apply those concepts to scalars. We can also apply the mathematics of calculus to vectors and matrices. Some aspects of matrix calculus are identical t o scalar calculus, but some scalar calculus concepts need to be extended in order to derive formulas for matrix calculus.
As intuition would lead us to believe, the time derivative of a matrix is simply equal to the matrix of the time derivatives of the individual matrix elements. Also, the integral of a matrix is equal to the matrix of the integrals of the individual matrix elements. In other words, assuming that A is an m x n matrix, we have

A(t) =

1A(t) dt =

J Aii(t) dt J Anl(t) dt

* * * * * *

1J Ain(t) dt
J Ann(t) dt

(1.49)

Next we will compute the time derivative of the inverse of a matrix. Suppose that matrix A(t), which we will denote as A, has elements that are functions of time. We know that AA-l = I; that is, AA-l 6s a constant matrix and therefore has a time derivative of zero. But the time derivative of AA-l can be computed as

d

d

-d(tAA-l) = A A - l + A-d((tA-')

(1.50)

Since this is zero, we can solve for d(A-')/dt as

-d( ~ - 1 ) = - A - ~ A A - ~ dt

(1.51)

Note that for the special case of a scalar A, this reduces to the familiar equation

-dd(tI/A)

= - a(1- /A) dA
dA dt = -A/A2

(1.52)

Now suppose that x is an n x 1vector and f(x) is a scalar function of the elements

of 2. Then

- ad Xf = [ af/axl ... af/axn ]

(1.53)

MATRIX ALGEBRA AND MATRIX CALCULUS

15

Even though x is a column vector, d f / d x is a row vector. The converse is also true - if x is a row vector, then d f / d x is a column vector. Note that some authors define this the other way around. That is, they say that if x is a column vector then df / d z is also a column vector. There is no accepted convention for the definition of the partial derivative of a scalar with respect to a vector. It does not really matter which definition we use as long as we are consistent. In this book, we will use the convention described by Equation (1.53).
Now suppose that A is an m x n matrix and f ( A )is a scalar. Then the partial derivative of a scalar with respect to a matrix can be computed as follows:

(1.54)

With these definitions we can compute the partial derivative of the dot product of two vectors. Suppose x and y are n-element column vectors. Then
+ xTy = x l y l + . . . znyn
--

(1.55)

Likewise, we can obtain

(1.56)

Now we will compute the partial derivative of a quadratic with respect to a vector.

First write the quadratic as follows:

[ X ~ A X= [ 2 1 ' * * xn ]

:I

Ann An1 * * a

[ Xx nl ]

Now take the partial derivative of the quadratic as follows:

(1.57)

16

LINEAR SYSTEMS THEORY

If A is symmetric, as it often is in quadratic expressions, then A = AT and the above expression simplifies to

d

(

xTA ax

x

)

=

2xTA

ifA=AT

(1.59)

[ ''r)] [ : ' I . Next we define the partial derivative of a vector with respect to another vector.

Suppose g(z)=

andx=

Then

gm (x)

Xn

(1.60)

If either g(x)or x is transposed, then the partial derivative is also transposed.

(1.61)

With these definitions, the following important equalities can be derived. Suppose A is an m x n matrix and x is an n x 1 vector. Then

-a(aAx-x) - A

d(xTA)
ax

=

A

(1.62)

Now we suppose that A is an m x n matrix, B is an n x n matrix, and we want to compute the partial derivative of Tr(ABAT)with respect to A. First compute A B A ~as follows:

(1.64)

MATRIXALGEBRA AND MATRIXCALCULUS

17

(1.65)

If B is symmetric, as it often is in partial derivatives of the form above, then this can be simplified to

~

T

~( dA

A

B=A2 A~B)

ifB=BT

(1.66)

A number of additional interesting results related to matrix calculus can be found in jSke98, Appendix B].

1.1.4 The history of matrices
This section is a brief diversion to present some of the history of matrix theory. Much of the information in this section is taken from [OCo96].
The use of matrices can be found as far back as the fourth century BC. We see in ancient clay tablets that the Babylonians studied problems that led to simultaneous linear equations. For example, a tablet dating from about 300 BC contains the following problem: “There are two fields whose total area is 1800 units. One produces grain at the rate of 2/3 of a bushel per unit while the other produces grain at the rate of 1 / 2 a bushel per unit. If the total yield is 1100 bushels, what is the size of each field?”
Later, the Chinese came even closer to the use of matrices. In [She991 (originally published between 200 BC and 100 AD) we see the following problem: “There are three types of corn, of which three bundles of the first, two of the second, and one of the third make 39 measures. Two of the first, three of the second, and one of the third make 34 measures. And one of the first, two of the second and three of the third make 26 measures. How many measures of corn are contained in one bundle of each type?” At that point, the ancient Chinese essentially use Gaussian elimination (which was not well known until the 19th century) to solve the problem.
In spite of this very early beginning, it was not until the end of the 17th century that serious investigation of matrix algebra began. In 1683, the Japanese

18

LINEAR SYSTEMS THEORY

mathematician Takakazu Seki Kowa wrote a book called “Method of Solving the Dissimulated Problems.” This book gives general methods for calculating determinants and presents examples for matrices as large as 5 x 5. Coincidentally, in the same year (1683) Gottfried Leibniz in Europe also first used determinants to solve systems of linear equations. Leibniz also discovered that a determinant could be expanded using any of the matrix columns.
In the middle of the 1700s,Colin Maclaurin and Gabriel Cramer published some major contributions to matrix theory. After that point, work on matrices became rather regular, with significant contributions by Etienne Bezout, Alexandre Vandermonde, Pierre Laplace, Joseph Lagrange, and Carl Gauss. The term “determinant” was first used in the modern sense by Augustin Cauchy in 1812 (although the word was used earlier by Gauss in a different sense). Cauchy also discovered matrix eigenvalues and diagonalization, and introduced the idea of similar matrices. He was the first to prove that every real symmetric matrix is diagonalizable.
James Sylvester (in 1850) was the first to use the term “matrix.” Sylvester moved to England in 1851 to became a lawyer and met Arthur Cayley, a fellow lawyer who was also interested in mathematics. Cayley saw the importance of the idea of matrices and in 1853 he invented matrix inversion. Cayley also proved that 2 x 2 and 3 x 3 matrices satisfy their own characteristic equations. The fact that a matrix satisfies its own characteristic equation is now called the Cayley-Hamilton theorem (see Problem 1.5). The theorem has William Hamilton’s name associated with it because he proved the theorem for 4 x 4 matrices during the course of his work on quaternions.
Camille Jordan invented the Jordan canonical form of a matrix in 1870. Georg Frobenius proved in 1878 that all matrices satisfy their own characteristic equation (the Cayley Hamilton theorem). He also introduced the definition of the rank of a matrix. The nullity of a square matrix was defined by Sylvester in 1884. Karl Weierstrass’s and Leopold Kronecker’s publications in 1903 were instrumental in establishing matrix theory as an important branch of mathematics. Leon Mirsky’s book in 1955 [MirSO] helped solidify matrix theory as a fundamentally important topic in university mathematics.

1.2 LINEAR SYSTEMS
Many processes in our world can be described by statespace systems. These include processes in engineering, economics, physics, chemistry, biology, and many other areas. If we can derive a mathematical model for a process, then we can use the tools of mathematics to control the process and obtain information about the process. This is why statespace systems are so important to engineers. If we know the state of a system at the present time, and we know all of the present and future inputs, then we can deduce the values of all future outputs of the system.
Statespace models can be generally divided into linear models and nonlinear models. Although most real processes are nonlinear, the mathematical tools that are available for estimation and control are much more accessible and well understood for linear systems. That is why nonlinear systems are often approximated as linear systems. That way we can use the tools that have been developed for linear systems to derive estimation or control algorithms.

LINEAR SYSTEMS

19

A continuous-time, deterministic linear system can be described by the equations

j. = A z + B u y = cx

(1.67)

where x is the state vector, u is the control vector, and y is the output vector. Matrices A, B , and C are appropriately dimensioned matrices. The A matrix is often called the system matrix, B is often called the input matrix, and C is often called the output matrix. In general, A , B , and C can be time-varying matrices and the system will still be linear. If A , B , and C are constant then the solution
to Equation (1.67) is given by

lo + t
x ( t ) = eA(t-tO)x(to) eA(t-')Bu(r)d r

Y(t) = C 4 t )

(1.68)

where t o is the initial time of the system and is often taken to be 0. This is easy
to verify when all of the quantities in Equation (1.67) are scalar, but it happens to be true in the vector case also. Note that in the zero input case, x ( t )is given as

x ( t )= eA(t-to)x(to), zero input case

(1.69)

For this reason, eAt is called the state-transition matrix of the ~ y s t e m .It~ is the matrix that describes how the state changes from its initial condition in the absence of external inputs. We can evaluate the above equation at t = t o t o see that

eAO = I

(1.70)

in analogy with the scalar exponential of zero. As stated above, even if x is an n-element vector, then Equation (1.68) still
describes the solution of Equation (1.67). However, a fundamental question arises in this case: How can we take the exponential of the matrix A in Equation (1.68)? What does it mean to raise the scalar e to the power of a matrix? There are many different ways to compute this quantity [Mo103]. Three of the most useful are the following:

Cj! * (At)j
eAt =
j=O
= LC-'[(s-1 A)-']

- QefttQ-l

(1.71)

The first expression above is the definition of eAt, and is analogous t o the definition of the exponential of a scalar. This definition shows that A must be square in order for eAt to exist. From Equation (1.67), we see that a system matrix is always square. The definition of eAt can also be used t o derive the following properties.

(1.72)
3The MATLAB function EXPM computes the matrix exponential. Note that the MATLAB function EXP computes the element-by-element exponential of a matrix, which is generally not the same as the matrix exponential.

20

LINEAR SYSTEMS THEORY

In general, matrices do not commute under multiplication but, interestingly, a matrix always commutes with its exponential.
The first expression in Equation (1.71) is not usually practical for computational purposes since it is an infinite sum (although the latter terms in the sum often
decrease rapidly in magnitude, and may even become zero). The second expression in Equation (1.71) uses the inverse Laplace transform to compute eAt. In the third expression of Equation (1.71), Q is a matrix whose columns comprise the
eigenvectors of A, and A is the Jordan form4 of A. Note that Q and A are well
defined for any square matrix A , so the matrix exponential eAt exists for all square
matrices A and all finite t. The matrix A is often diagonal, in which case eat is
easy to compute:

(1.73)
This can be computed from the definition of eAt in Equation (1.71). Even if the Jordan form matrix A is not diagonal, eAt is easy to compute [Bay99,Che99, Kai801. We can also note from the third expression in Equation (1.71) that
(1.74)
(Recall that A and - A have the same eigenvectors, and their eigenvalues are negatives of each other. See Problem 1.10.) We see from this that eAt is always invertible. This is analogous to the scalar situation in which the exponential of a scalar is always nonzero.
Another interesting fact about the matrix exponential is that all of the individual elements of the matrix exponential eA are nonnegative if and only if all of the individual elements of A are nonnegative [Be160, Be1801.
EXAMPLE1.2
As an example of a linear system, suppose that we are controlling the angular acceleration of a motor (for example, with some applied voltage across the motor windings). The derivative of the position is the velocity. A simplified motor model can then be written as
41n fact, Equation (1.71)can be used to define the Jordan form of a matrix. That is, if eAt can be written as shown in Equation (1.71),where Q is a matrix whose columns comprise the
eigenvectors of A, then A is the Jordan form of A. More discussion about Jordan forms and their
computation can be found in most linear systems books [Kai80, Bay99, Che991.

LINEAR SYSTEMS

21

o=w
w = u+w1

(1.75)

The scalar w1 is the acceleration noise and could consist of such factors as uncertainty in the applied acceleration, motor shaft eccentricity, and load disturbances. If our measurement consists of the angular position of the motor
I:[ then a state space description of this system can be written as = [ o0 1o ] [ : ] + [ ! ] ~ + [ : ' ]

y = [ l O]z+w

(1.76)

The scalar w consists of measurement noise. Comparing with Equation (1.67), we see that the state vector z is a 2 x 1 vector containing the scalars 13 and w .
vvv

EXAMPLE 1.3

In this example, we will use the three expressions in Equation (1.71) to compute the state-transition matrix of the system described in Example 1.2. From the first expression in Equation (1.71) we obtain

+ + + - + * - = (At)'

(At)'

y(Ajjt-)2

(At)3 3!

= I+At

(1.77)

where the last equality comes from the fact that Ak = 0 when k > 1 for the
A matrix given in Example 1.2. We therefore obtain

;] 1 0
= [ o 1]+[:

= [:E ]

(1.78)

From the second expression in Equation (1.71) we obtain

eAt = ,C-'[(sI-A)-']

[ ] 11s 1 1 2
= c-l 0 11s

= [;I ]

(1.79)

22

LINEAR SYSTEMS THEORY

In order to use the third expression in Equation (1.71) we first need to obtain the eigendata (i.e., the eigenvalues and eigenvectors) of the A matrix. These are found as

(1.80)

This shows that

= [:k ]

= [ k !]

(1.81)

Note that in this simple example A is already in Jordan form, so A = A and Q = I . The third expression in Equation (1.71) therefore gives

= [ k El

(1.82)

vvv

1.3 NONLINEAR SYSTEMS
The discussion of linear systems in the preceding section is a bit optimistic, because in reality linear systems do not exist. Real systems always have some nonlinearities. Even a simple resistor is ultimately nonlinear if we apply a large enough voltage across it. However, we often model a resistor with the simple linear equation V = I R because this equation accurately describes the operation of the resistor over a wide operating range. So even though linear systems do not exist in the real world, linear systems theory is still a valuable tool for dealing with nonlinear systems.
The general form of a continuous-time nonlinear system can be written as

(1.83)
where f(.) and h ( . ) are arbitrary vector-valued functions. We use w to indicate process noise, and w to indicate measurement noise. If f(.) and h(.) are explicit functions of t then the system is time-varying. Otherwise, the system is time-
+ + + invariant. If f(2,u,w)= A s Bu w, and h(s,v) = Ha: w,then the system is
linear [compare with Equation (1.67)]. Otherwise, the system is nonlinear. In order to apply tools from linear systems theory to nonlinear systems, we need
to linearize the nonlinear system. In other words, we need to find a linear system

NONLINEAR SYSTEMS

23

that is approximately equal to the nonlinear system. To see how this is done, let us start with a nonlinear vector function f ( a ) of a scalar x . We expand f (x) in a Taylor series around some nominal operating point (also called a linearization point) x = 2 , defining 5 = x - 2:

Now suppose that zis a 2 x 1vector. This implies that f (x) is a nonlinear function of
two independent variables x1 and 2 2 . The Taylor series expansion of f(x) becomes

(1.85)

This can be written more compactly as

(1.86)
Extending this to the general case in which x is an n x 1 vector, we see that any continuous vector-valued function f (x) can be expanded in a Taylor series as

(1.87)

Now we define the operation 0;f as

z Using this definition we write the Taylor series expansion of f (x) as

(1.88)

+ + + + f ( ~=)f(Z) Dzf -10:f -1Dg f * * *

2!

3!

(1.89)

If the nonlinear function f ( x ) is “sufficiently smooth,” then high-order derivatives
of f(x) should be “somewhat small.” Also, if f (x) is expanded around a point such

24

LINEAR SYSTEMS THEORY

that x is “close” to 2 , then 4 will be “small” and the higher powers of 4 in Equ& tion (1.89)will be “small.” Finally, the higher-order derivatives in the Taylor series expansion of Equation (1.89) are divided by increasingly large factorials, which further diminishes the magnitude of the higher-order terms in Equation (1.89). This justifies the approximation

M f(z)+A2

(1.90)

where A is the matrix defined by the above equation.
Returning to our nonlinear system equations in Equation (1.83), we can expand the nonlinear system equation f ( x ,u,w ) around the nominal operating point
(2,0a,).We then obtain a linear system approximation as follows.

*=

= k+AAb+Bii+LtC

(1.91)

where the 0 subscript means that the function is evaluated at the nominal point
( 2 ,ii,a),and A, B , and L are defined by the above equations. Subtracting h from

both sides of Equation (1.91) gives

i = AZ + ~ i+iLG

(1.92)

Since w is noise, we will set t i j = 0 so that tC = w and we obtain
i = AZ + ~ i+iLW

(1.93)

We see that we have a linear equation for i in terms of 2, 12,and w. We have a
linear equation for the deviations of the state and control from their nominal values. As long as the deviations remain small, the linearization will be accurate and the linear equation will accurately describe deviations of x from its nominal value 2.
In a similar manner we can expand the nonlinear measurement equation given by Equation (1.83) around a nominal operating point x = 2 and v = V = 0. This results in the linearized measurement equation

= Cf+Dv

(1.94)

where C and D are defined by the above equation. Equations (1.93) and (1.94)
comprise a linear system that describes the deviations of the state and output from their nominal values. Recall that the tilde quantities in Equations (1.93) and (1.94) are defined as

(1.95)

NONLINEAR SYSTEMS

25

EXAMPLE1.4

Consider the following model for a two-phase permanent magnet synchronous motor:

la = --iR,+-sineW+X-

Ua

L

L

L

ib =

-R WX - iLb - - C O S e L+ -

ub
L

-3x

3x

& = - 2J i, s i n e + -2iJb C O S 6

e=w

-

- FJW-

-Ti
J

(1.96)

where i, and zb are the currents through the two windings, R and L are the resistance and inductance of the windings, 8 and w are the angular position and velocity of the rotor, A is the flux constant of the motor, U a and U b are the voltages applied across the two windings, J is the moment of inertia of
the rotor and its load, F is the viscous friction of the rotor, and Z is the
load torque. The time variable does not explicitly appear on the right side of the above equation, so this is a time-invariant system. However, the system is highly nonlinear and we therefore cannot directly use any linear systems tools for control or estimation. However, if we linearize the system around a nominal (possibly time-varying) operating point then we can use linear system tools for control and estimation. We start by defining a state vector
3.' as x = [ i, zb w 8 With this definition we write

(1.97)
We linearize the system equation by taking the partial derivative of f ( x ,u ) with respect to x and u to obtain

(1.98)

26

LINEAR SYSTEMSTHEORY

where s4 = sin 2 4 and c4 = cos 2 4 . The linear system
i=~z++ii

(1.99)

approximately describes the deviation of 2 from its nominal value 5. The nonlinear system was simulated with the nominal control values fia(t)= sin2.rrt and fib(t) = cos2.rrt. This resulted in a nominal state trajectory Z ( t ) . The linear and nonlinear systems were then simulated with nonnominal control values. Figure 1.1shows the results of the linear and nonlinear simulations when the control magnitude deviation from nominal is a small positive number. It can be seen that the simulations result in similar state-space trajectories, although they do not match exactly. If the deviation is zero, then the linear and nonlinear simulations will match exactly. As the deviation from nominal increases, the difference between the linear and nonlinear simulations will increase.

s

C

-0.5

-0.5

0

-1

-1

0

0.5

1

0

0.5

1

Seconds

Seconds

Figure 1.1 Example 1.4 comparison of nonlinear and linearized motor simulations.

vvv

1.4 DISCR E TI2ATI0N

Most systems in the real world are described with continuous-time dynamics of the

type shown in Equations (1.67) or (1.83). However, state estimation and control

algorithms are almost always implemented in digital electronics. This often requires

a transformation of continuous-time dynamics to discrete-time dynamics. This

section discusses how a continuous-time linear system can be transformed into a

discretetime linear system.

Recall from Equation (1.68) that the solution of a continuous-time linear system

is given by

+ ~ ( t=)eA(t-to)~(tO) e A ( t - ' ) B ~ (d~ )

(1.100)

SIMULATION 27
Let t = t k (some discrete time point) and let the initial time t o = t k - 1 (the previous discrete time point). Assume that A(T),B ( T )a, nd U ( T ) are approximately constant
in the interval of integration. We then obtain
(1.101)
Now define At = t k - t k - 1 , define Q = T - t k - 1 , and substitute for T in the above
equation to obtain

+ JdAt
- eAAtz(tk-l) eAAt e-Aa daBu(tk-1)
+ X k = Fk-12k-1 G k - 1 U k - 1

(1.102)

where X k , Fk, Gk, and U k are defined by the above equation. This is a linear discretetime approximation to the continuous-time dynamics given in Equation (1.67). Note that this discretetime system defines X k only at the discrete time points { t k } ; it does not say anything about what happens to the continuous-time
signal z ( t )in between the discrete time points.
The difficulty with the above discretetime system is the computation of the integral of the matrix exponential, which is necessary in order to compute the G matrix. This computation can be simplified if A is invertible:

Jd + At [ I - AT A 2 ~ 2 / 2-! d r a]
+ [IT - Ar2/2! A2r3/3!- * * *] At

+ [IAt - A(At)2/2! A2(At)3/3!- * .] + [AAt - (AAt)2/2! (AAt)3/3! - * * .] A-l 1 - [ I- e-AAt A 1

(1.103)

The conversion from continuous-time system matrices A and B to discretetime system matrices F and G can be summarized as follows:
F = e AAt
At
G = F L e-ATdTB

= F [I- e-AAt]A-lB where At is the discretization step size.

(1.104)

1.5 SIMULATION
In this section, we discuss how to simulate continuous-time systems (either linear or nonlinear) on a digital computer. We consider the following form of the general

28

LINEAR SYSTEMSTHEORY

system equation from Equation (1.83):

i = f(z,u,t )

(1.105)

where u ( t )is a known control input. In order to simulate this system on a computer, we need to program a computer to solve for z(tp) at some user-specified value of t f . In other words, we want to compute

Often, the initial time is taken as t o = 0, in which case we have the slightly simpler
looking equation
(1.107)
We see that in order to find the solution z ( t f )to the differential equation i = f(z,u,t)w,e need to compute an integral. The problem of finding the solution z ( t f )is therefore commonly referred to as an integration problem.
Now suppose that we divide the time interval [0,t f ]into L equally spaced in-
tervals so that t k = kT for k = 0,. ..,L, and the time interval T = t f / L . From
this we note that t f = t L . With this division of the time interval, we can write the solution of Equation (1.107) as
(1.108)
More generally, for some n E [0,L - 11,we can write z(tn)and z(t,+l) as

which means that
If we can find a way to approximate the integral on the right side of the above equation, we can repeatedly propagate our z ( t ) approximation from time t, to time tn+l, thus obtaining an approximation for z ( t ) at any desired time t. The algorithm could look something like the following.

SIMULATION 29

Differential equation solution
Assume that z(0) is given
fort=O:T:tf-T
s," Find an approximation I ( t ) x + + z(t 2') = z ( t ) TI(t)
end

f [ z ( t )u,(t),t]dt

In the following sections, we present three different ways to approximate this integral. The approximations, in order of increasing computational effort and increasing accuracy, are rectangular integration, trapezoidal integration, and fourth-order Runge-Kutta integration.

1.5.1 Rectangular integration
Ifthe time interval (tn+l-tn) is small, then f [ z ( t )u,( t ) ,t]is approximately constant
in this interval. Equation (1.110) can therefore be approximated as

Equation (1.109) can therefore be approximated as

(1.111)

+ c = 4 9

n
f [ X ( t k ) , U ( t k ) , tk1T

k=O

(1.112)

This is called Euler integration, or rectangular integration, and is illustrated in Figure 1.2. As long as T is sufficiently small, this gives a good approximation for
4tn).
This gives the following algorithm for integrating continuous-time dynamics using rectangular integration. The time loop in the algorithm is executed for
t = 0, T,2T, . . .,t f - T.
Rectangular integration Assume that z(0) is given fort=O:T: tf-T
+ + Compute f[z(t)4, %tl
z ( t T ) = z ( t ) f [ z ( t )u,( t ) ,t]T end

1.5.2 Trapezoidal integration
An inspection of Figure 1.2 suggests an idea for improving the approximation for z ( t ) . Instead of approximating each area as a rectangle, what if we approximate each area as a trapezoid? Figure 1.3 shows how an improved integration algorithm can be implemented. This is called modified Euler integration, or trapezoidal integration. A comparison of Figures 1.2 and 1.3 shows that trapezoidal integration

30 LINEAR SYSTEMSTHEORY
time
Figure 1.2 An illustration of rectangular integration. We have 2 = f(z),so z ( t )is the
+ area under the f(z)curve. This area can be approximated as the sum of the rectangular
areas A,. That is, ~ ( 0 . 5 )M A1, z(1) M A1 Az, + .-.
time
Figure 1.3 An illustration of trapezoidal integration. We have 2 = f(z),so z ( t )is the
+ + area under the f(s)curve. This area can be approximated BS the s u m of trapezoidal areas
A,. That is, z(1) rn A I , 4 2 ) M A1 Az, and 4 3 ) M A1 Az + A s . appears to give a better approximation than rectangular integration, even though the time axis is only divided into half as many intervals in trapezoidal integration.
With rectangular integration we approximated f [ z ( t )u, ( t ) ,t] as a constant in the interval t E [tn,t,+l]. With trapezoidal integration, we instead approximate f[z(t),u(t),t]as a linear function in the interval t E [tn,t,+l]. That is,

for t E [tn,tn+l]

SIMULATION 31 (1.113)

This equation to approximate z(tn+l),however, has z(t,+l) on the right side of the equation. How can we plug z(tn+l) into the right side of the equation if we do not yet know z(t,+l)? The answer is that we can use the rectangular integrs tion approximation from the previous section for z(tn+l) on the right side of the equation. The above equation can therefore be written as

Ax1 = f [ z ( t n )u,(tn),tnIT
Ax2 = f[z(tn+l), u(tn+l)r tn+l]T
+ f [ z ( t n ) Az1, u(tn+l), tn+lIT z(tn+l) = z(tn)+ 51 (AZl + Az2)

(1.115)

This gives the following algorithm for integrating continuous-time dynamics using trapezoidal integration. The time loop in the algorithm is executed for
t = 0, T, 2T,. . .,t f - T.
Trapezoidal integration Assume that z(0) is given fort=O:T: tf-T
+ + Azi = f[z(t),~ ( tt1)T,
+ + + A22 = f [ z ( t ) Az1, u(t T ) ,t TIT + z(t T) = z ( t ) (Ax1 A22)/2
end

1.5.3 Runge-Kutta integration
From the previous sections, we see that rectangular integration involves the calculation of one function value at each time step, and trapezoidal integration involves the calculation of two function values at each time step. In order to further improve the integral approximation, we can perform additional function calculations at each time step. nth-order Runge-Kutta integration is the approximation of an integral

32 LINEAR SYSTEMS THEORY

by performing n function calculations at each time step. Rectangular integration is therefore equivalent to first-order Runge-Kutta integration, and trapezoidal integration is equivalent to second-order Runge-Kutta integration.
The most commonly used integration scheme of this type is fourth-order RungeKutta integration. We present the fourth-order Runge-Kutta integration algorithm (without derivation) as follows:

Ax1 = f[Z(tk), 'ZL(tk),tk]T
+ Az2 = f [ x ( t k ) Az1/2, U(tk+1/2),tk+i/z]T

Ax3 = f [ x ( t k )f A22/2, u(tk+l/2),tk+l/2]T

AX4 = f [ z ( t k ) A23, U ( t k + i ) , tk+i]T
+ + + + x(tk+i) M z ( t k ) ( h i 2Ax2 2AX3 A24) /6

(1.116)

+ where t k + 1 / 2 = t k T/2. Fourth-order Runge-Kutta integration is more computa-

tionally demanding than rectangular or trapezoidal integration, but it also provides

far greater accuracy. This gives the following algorithm for integrating continuous-

time dynamics using fourth-order Runge-Kutta integration. The time loop in the
algorithm is executed for t = 0, T,2T, . ,t f - T.

Fourth-order Runge-Kutta integration

Assume that z(0) is given

fort=O: T : tf-T

ti = t +T/2
w , Ax1 = f[z(t), t]T
+ AXZ= . f [ X ( t ) A x I / ~U,( t i ) , ti]T + Ax3 = f[.(t) Ax2/2, U ( t l ) , tl]T
+ + + A24 = f[z(t) Az3, u(t T),t TIT
+ + + + + z(t T )= z ( t ) (A21 2Ax2 2Ax3

A24) /6

end

Runge-Kutta integration was invented by Carl Runge, a German mathematician and physicist, in 1895. It was independently invented and generalized by Wilhelm Kutta, a German mathematician and aerodynamicist, in 1901. More accurate integration algorithms have also been derived and are sometimes used, but fourth-order Runge-Kutta integration is generally considered a good trade-off between accuracy and computational effort. Further information and derivations of numerical integration algorithms can be found in many numerical analysis texts, including [Atk89].

EXAMPLE1.5

Suppose we want to numerically compute z ( t )at t = 1based on the differential

equation

x =cost

(1.117)

with the initial condition z(0) = 0. We can analytically integrate the equation to find out that z(1)= sin1 M 0.8415. If we use a numerical integration scheme, we have to choose the step size T. Table 1.1 shows the error of the rectangular, trapezoidal, and fourth-order Runge-Kutta integration methods for this example for various values of T. As expected, Runge-Kutta is more accurate than trapezoidal, and trapezoidal is more accurate than rectangular.

STABILITY

33

Also as expected, the error for given method decreases as T decreases. However, perhaps the most noteworthy feature of Table 1.1 is how the integration error decreases with T . We can see that with rectangular integration, when T is halved, the integration error is also halved. With trapezoidal integration, when T is halved, the integration error decreases by a factor of four. With Runge-Kutta integration, when T is halved, the integration error decreases by a factor of 16. We conclude that (in general) the error of rectangular integration is proportional to T , the error of trapezoidal integration is proportional to T 2 ,and the error of Runge-Kutta integration is proportional to T4.

Table 1.1 Example 1.5 results. Percent errors when numerically integrating
x = cos t kom t = 0 to t = 1, for various integration algorithms, and for various time
step sizes T.

Rectangular Trapezoidal Fourth-order Runge-Kutta

T = 0.1
2.6 0.083 3.5 x

T = 0.05
1.3 0.021 2.2 x

T = 0.025
0.68 0.0052 1.4 x

vvv

1.6 STABILITY
In this section, we review the concept of stability for linear time-invariant systems. We first deal with continuous-time systems in Section 1.6.1, and then discrete-time systems in Section 1.6.2. We state the important results here without proof. The interested reader can refer to standard books on linear systems for more details and additional results [Kai80, Bay99, Che991.

1.6.1 Continuous-time systems

Consider the zero-input, linear, continuous-time system

X = AX y = ex

(1.118)

The definitions of marginal stability and asymptotic stability are as follows.

Definition 1 A linear continuous-time, time-invariantsystem is marginally stable if the state x ( t )is bounded for all t and for all bounded initial states x(0).

Marginal stability is also called Lyapunov stability.

Definition 2 A linear continuous-time, time-invariant system is asymptotically stable if, for all bounded initial states x(O),

lim x ( t )= 0
t+w

(1* 119)

34

LINEAR SYSTEMS THEORY

The above two definitions show that a system is marginally stable if it is asymptotically stable. That is, asymptotic stability is a subset of marginal stability. Marginal stability and asymptotic stability are types of internal stability. This is because they deal with only the state of the system (i.e., the internal condition of the system) and do not consider the output of the system. More specific categories of internal stability (e.g., uniform stability and exponential stability) are given in some books on linear systems.
Since the solution of Equation (1.118) is given as

z ( t )= exp(At)z(O)

(1.120)

we can state the following theorem.

Theorem 1 A linear continuous-time, time-invariant system is marginally stable

if and only if

lim exp(At) 5 M < 00
t+w

(1.121)

for some constant matrix M. This is just a way of saying that the matrix exponential

does not increase without bound.

The “less than or equal to” relation in the above theorem raises some questions,
because the quantities on either side of this mathematical symbol are matrices.
What does it mean for a matrix to be less than another matrix? It can be interpreted
several ways. For example, to say that A < B is usually interpreted to mean that
( B - A) is positive definite.5 In the above theorem we can use any reasonable definition for the matrix inequality and the theorem still holds.
A similartheorem can be stated by combiningDefinition (2) with Equation (1.120).

Theorem 2 A linear continuous-time, time-invariant system is asymptotically sta-

ble i f and only i f

lim exp(At) = 0
t +bo

(1.122)

Now recall that exp(At) = Qexp(At)Q-’, where Q is a constant matrix containing the eigenvectors of A, and A is the Jordan form of A. The exponential eXp(At) therefore contains terms like exp(Ait), texp(Ait), t 2 e x p ( A i t ) ,and so on, where A, is an eigenvalue of A. The boundedness of exp(At) is therefore related to the eigenvalues of A as stated by the followingtheorems.

Theorem 3 A linear continuous-time, time-invariant system is marginally stable i f and only i f one of the following conditions holds.
1. All of the eigenvalues of A have negative real parts.

2. All of the eigenvalues of A have negative or zero real parts, and those with real parts equal to zero have a geometric multiplicity equal to their algebraic multiplicity. That is, the Jordan blocks that are associated with the eigenvalues that have real parts equal to zero are first order.

Theorem 4 A linear continuous-time, time-invariant system is asymptotically stable i f and only i f all of the eigenvalues of A have negative real parts.

5Sometime5 the statement A < B means that every element of A is less than the corresponding element of B.However, we will not use that definition in this book.

STABILITY

35

EXAMPLE1.6

Consider the system

[I : :J x= 0 0 0 x

(1.123)

Since the A matrix is upper triangular, we know that its eigenvalues are on the diagonal; that is, the eigenvalues of A are equal to 0, 0, and -1. We see that the system is asymptotically unstable since some of the eigenvalues are nonnegative. We also note that the A matrix is already in Jordan form, and we see that the Jordan block corresponding to the 0 eigenvalue is second order. Therefore, the system is also marginally unstable. The solution of this system is

[ : 4 : ] z(t) = ap(At)a:(O)

=

z(0)

0 0 e-t

(1.124)

The element in the first row and second column of exp(At) increases without bound as t increases, so there are some initial states x ( 0 ) that will result in unbounded x ( t ) . However, there are also some initial kates z(0) that will
result in bounded z ( t ) .For example, if z(0) = [ 1 0 1 ]T , then

x(t) =

= [.at]

(1.125)

and z ( t )will be bounded for all t. However, this does not say anything about
the stability of the system; it only says that there exists some z(0) that results
in a bounded z ( t ) .If we instead choose x(0)= [ 0 1 0 ]T , then

(1.126)
and z ( t )increases without bound. This proves that the system is asymptotically unstable and marginally unstable.
vvv

36

LINEAR SYSTEMSTHEORY

EXAMPLE 1.7

Consider the system

[::x= 0 0 0 x :1]

(1.127)

The eigenvalues of A are equal to 0, 0, and -1. We see that the system is

asymptotically unstable since some of the eigenvalues are nonnegative. In

order to see if the system is marginally stable, we need to compute the geo-

metric multiplicity of the 0 eigenvalue. (This can be done by noticing that A

is already in Jordan form, but we will go through the exercise more completely

for the sake of illustration.) Solving the equation
[ i ] (XI - A). =

(1.128)

(where X = 0) for nonzero vectors 21, we see that there are two linearly inde-

. = [ % ] , [ % I pendent solutions given as

(1.129)

This shows that the geometric multiplicity of the 0 eigenvalue is equal to 2, which means that the system is marginally stable. The solution of this system is

~ ( t )= exp(At)x(O)

[ : :];

x(0)

= 0 0 e-t

(1,130)

Regardless of x(O), we see that x(t) will always be bounded, which means that
the system is marginally stable. Note that x(t) may approach 0 as t increases,
depending on the value of x(0). For example, if x(0) = [ 0 0 -1 ] T, then

[ ] [ [ 1 0 0
x(t>= 0o o1 e0-t

!l] = - ! - t ]

(1.131)

and x(t) approaches 0 as t increases. However, this does not say anything about the asymptotic stability of the system; it only says that there exists some x(0) that results in state z ( t )that asymptotically approaches 0. If we
instead choose x(0) = [ 0 1 0 ] T, then

10 0
. O = [ o0 01 e-ot ] [ ; ] = [ ; ]

(1.132)

and x(t) does not approach 0. This proves that the system is asymptotically unstable.
vvv

STABILITY

37

1.6.2 Discrete-time systems Consider the zero-input, linear, discretetime, timeinvariant system

(1.133)

The definitions of marginal stability (also called Lyapunov stability) and asymptotic stability are analogous to the definitions for continuous-time systems that were given in Section 1.6.1.

Definition 3 A linear discrete-time, time-invariant system is marginally stable if the state Xk is bounded for all k and for all bounded initial states XO.

Definition 4 A linear discrete-time, time-invariant system is asymptotically stable

if

lim X k = 0
k+co

(1.134)

for all bounded initial states XO.

Marginal stability and asymptotic stability are types of internal stability. This is because they deal with only the state of the system (i.e., the internal condition of the system) and do not consider the output of the system. More specific categories of internal stability (e.g., uniform stability and exponential stability) are given in some books on linear systems.
Since the solution of Equation (1.133) is given as

xk = AkXO

(1.135)

we can state the following theorems.

Theorem 5 A linear discrete-time, time-invariant system is marginally stable if

and only if

lim Ak 5 M < 00
k-co

(1.136)

for some constant matrix M. This is just a way of saying that the powers of A do not increase without bound.

Theorem 6 A linear discrete-time, time-invariant system is asymptotically stable

af and only if

lim Ak = 0
k+w

(1.137)

Now recall that Ak = QAkQ-l, where Q is a constant mFtrix containing the eigenvectors of A , and A is the Jordan form of A . The matrix Ak therefore contains
terms like A,! kA;, k2$, and so on, where A, is an eigenvalueof A. The boundedness of Ak is therefore related to the eigenvaluesof A as stated by the followingtheorems.

Theorem 7 A linear discrete-time, time-invariant system is marginally stable if and only if one of the following conditions holds.
1. All of the eigenvalues of A have magnitude less than one.

38

LINEAR SYSTEMS THEORY

2. All of the eigenvalues of A have magnitude less than or equal to one, and those with magnitude equal to one have a geometric multiplicity equal to their algebraic multiplicity. That is, the Jordan blocks that are associated with the eigenvalues that have magnitude equal to one are first order.
Theorem 8 A linear discrete-time, time-invariant system is asymptotically stable
if and only if all of the eigenvalues of A have magnitude less than one.

1.7 CONTROLLABILlTY A N D 0BSERVABILlTY
The concepts of controllability and observability are fundamental to modern control theory. These concepts define how well we can control a system (i.e., drive the state to a desired value) and how well we can observe a system (i.e., determine the initial conditions after measuring the outputs). These concepts are also important to some of the theoretical results related to optimal state estimation that we will encounter later in this book.

1.7.1 Controllability

The following definitions and theorems give rigorous definitions for controllability for linear systems in the both the continuous-time and discretetime cases.

Definition 5 A continuous-time system is controllable if for any initial state x(0)
and any final time t > 0 there exists a control that transfers the state to any desired
value at time t .

Definition 6 A discrete-time system is controllable if for any initial state xo and some final time k there exists a control that transfers the state to any desired value at time k .

Note the controllability definition in the continuous-time case is much more demanding than the definition in the discretetime case. In the continuous-time case, the existence of a control is required for any final time. In the discretetime case, the existence of a control is required for some final time. In both cases,

controllability is independent of the output equation. There are several tests for controllability. The followingequivalent theorems can
be used to test for the controllability of continuous linear timeinvariant systems.

+ Theorem 9 The n-state6 continuous linear time-invariant system x = Ax Bu
has the controllability matrix P defined by

P = [ B AB

A"-lB 3

(1.138)

The system i s controllable if and only if p ( P )= n.

+ Theorem 10 The n-state continuous linear time-invariant system x = Ax Bu

is controllable if and only if the controllability grammian defined by
iteArBBTeATdr7

(1.139)

6The notation n - s t a t e s y s t e m indicates a system that has n elements in its state variable z.

CONTROLLABILITY AND OBSERVABILITY 39

is positive definite for some t E ( 0 , ~ ) .
+ Theorem 11 The n-state continuous linear time-invariant system x = Ax Bu
is controllable if and only if the differential Lyapunov equation

W(0) = 0
W = WA~+AW+BB~

(1.140)

has a positive definite solution W ( t )for some t E ( 0 , ~ ) . This i s also called a
Sylvester equation.

Similar to the continuous-time case, the following equivalent theorems can be used to test for the controllability of discrete linear timeinvariant systems.
+ Theorem 12 The n-state discrete linear time-invariant system X k = Fxk-1
Guk-1 has the controllability matrix P defined by

P = [ G FG * a * F"-lG ]

(1.141)

The system is controllable af and only i f p(P) = n.
+ Theorem 13 The n-state discrete h e a r time-invariant system X k = FXk-1
Guk-1 is controllable if and only i f the controllability grammian defined by

k
Ak-ZBBT(AT)k-i
2=0

(1.142)

is positive definite for some k E ( 0 , ~ ) .
+ Theorem 14 The n-state discrete h e a r tame-invariant system X k = Fxk-1
GUk-1 is controllable if and only i f the difference Lyapunov equation

wo = 0
w % +=~ F W , F ~ + G G ~

(1.143)

has a positive definite solution wk for some k E (0, w). This is also called a Stein equation.

Note that Theorems 9 and 12 give identical tests for controllability for both continuous-time and discretetime systems. In general, these are the simplest controllability tests. Controllability tests for timevarying linear systems can be obtained by generalizing the above theorems. Controllability for nonlinear systems is much more difficult to formalize.

EXAMPLE 1.8
The RLC circuit of Figure 1.4 has the system description

(1.144)

40

LINEAR SYSTEMS THEORY

where v c is the voltage across the capacitor, i~ is the current through the inductor, and u is the applied voltage. We will use Theorem 9 to determine the conditions under which this system is controllable. The controllability matrix is computed as

P = [ B AB]

[ 1 1/RC 1ILC - 2/R2C2

= 1/L

-1IRLC

(1.145)

From this we can compute the determinant of P as

[PI= 1/R2LC2 - 1/L%

(1.146)

f l . The determinant of P is 0 only if R =

So the system is controllable

m. unless R =

It would be very difficult to obtain this result from

Theorems 10 and 11.

Figure 1.4 RLC circuit for Example 1.8.
vvv
1.7.2 0bservability
The following definitions and theorems give rigorous definitions for observability for linear systems in both the continuous-time and discrete-time cases.
Definition 7 A continuous-time system is observable if for any initial state x ( 0 )
and any final time t > 0 the initial state x ( 0 ) can be uniquely determined b y knowl-
edge of the input U ( T ) and output y ( ~f)or all T E [0,t ] .
Definition 0 A discrete-time system is observable if for any initial state xo and some final time k the initial state XIJ can be uniquely determined by knowledge of the input uz and output yd for all i E [0,k ] .
Note the observability definition in the continuous-time case is much more demanding than the definition in the discrete-time case. In the continuous-time case, the initial state must be able to be determined at any final time. In the discretetime case, the initial state must be able to be determined at some final time. If a system is observable then the initial state can be determined, and if the initial state can be determined then all states between the initial and final times can be determined.

CONTROLLABILITY AND OBSERVABILITY

41

There are several tests for controllability. The followingequivalent theorems can be used to test for the controllability of continuous linear time-invariant systems.

Theorem 15 The n-state continuous linear time-invariant system

k = Ax+Bu
" 1y = c x
has the observability matrix Q defined by
Q=[ C

(1.147)

(1.148)

CAn-l

The system is observable if and only if p(Q) = n.

Theorem 16 The n-state continuous linear time-invariant system

k = Ax+Bu y = cx

(1.149)

is observable if and only if the observability grammian defined by
I" e A T 7 C T C e AdrT

(1.150)

is positive definite for some t E (0,m).

Theorem 17 The n-state continuous linear time-invariant system

k = Ax+Bu y = cx

(1.151)

is observable if and only if the differential Lyapunov equation

W(t) = 0
-I&' = W A + A T W + C T C

(1.152)

has a positive definite solution W ( T )for some T E ( 0 , t ) . This is also called a Sylvester equation.

Similar to the continuous-time case, the following equivalent theorems can be used to test for the observability of discrete linear time-invariant systems.

Theorem 18 The n-state discrete linear time-invariant system

(1.153)

" 1 42

LINEAR SYSTEMSTHEORY

has the observability matrix Q defined by
Q=[ H

(1.154)

HF~-~

The system is observable i f and only i f p(Q) = n.

Theorem 19 The n-state discrete linear time-invariant system

xk = FXk-l+GUk-i Yk = Hxk is observable if and only if the observability grammian defined by

(1.155)

k
2=0
is positive definite f o r some k E (0,m).

(1.156)

Theorem 20 The n-state discrete linear time-invariant system
+ x k = FXk-1 GUk-1
?4k = Hxk

( 1.157)

is observable if and only i f the difference Lyapunov equation

wk = 0
w, = F ~ W , + ~ F + H ~ H

(1.158)

has a positive definite solution Wo f o r some k E (0,m). This is also called a Stein equation.

Note that Theorems 15 and 18 give identical tests for observability for both continuoustime and discretetime systems. In general, these are the simplest observability tests. Observabilitytests for timevarying linear systems can be obtained by generalizing the above theorems. Observability for nonlinear systems is much more difficult to formalize.

EXAMPLE 1.9

The RLC circuit of Example 1.8 has the system description

[ ? ] [= -1/L 0

I: [ y = [ - 1 0 1

( 1.159)

where vc is the voltage acrom the capacitor, i L is the current through the inductor, and u is the applied voltage. We will use Theorem 15 to determine

CONTROLLABILITY AND OBSERVABILITY

43

the conditions under which this system is observable. The observability matrix is computed as

(1.160)

The determinant of the observability matrix can be computed as
IQI = 1/C

(1.161)

The determinant of Q is nonzero, so the system is observable. On the other

hand, suppose that R = L = C = 1 and the output equation is

::1[ y" -1 13

(1.162)

Then the observability matrix can be computed as
Q = [ -: -:]

IQI = 0

(1.163)

So the system is unobservable. It would be very difficult to obtain this result from Theorems 16 and 17.
vvv

1.7.3 Stabilizability and detectability

The concepts of stabilizability and detectability are closely related to controllability and observability, respectively. These concepts are also related to the modes of a system. The modes of a system are all of the decoupled states after the system is transformed into Jordan form. A system can be transformed into Jordan form as follows. Consider the system

5 = Ax+Bu y = CX+DU

(1.164)

First find the eigendata of the system matrix A. Suppose the eigenvectors are

denoted as v1,. . .,v,. Create an n x n matrix M by augmenting the eigenvectors

as follows.

M = [ 211 . . . v, ]

(1.165)

Define a new system as

5 = M-~AM%+M-~B = A%+Bu
y = CMZ+Du = C'Z+Du

(1.166)

The new system is called the Jordan form representation of the original system. Note that the matrix M will always be invertible because the eigenvectors of a matrix can always be chosen to be linearly independent. The two systems of Equations (1.164) and (1.166)are called algebraically equivalent systems. This is because they have the same input and the same output (and therefore they have the same transfer function) but they have different states.

44

LINEAR SYSTEMS THEORY

EXAMPLE 1.10
Consider the system
i = Ax+Bu
= [ o1 1 32 ] ] . + [ ; ] .
0 0 -2 y = CX+DU
= [ l 0 0]+2u
This system has the same transfer function as
" ] . + [ a ] . 6 = A5+Bu
[;; = 0 0 -2

(1.167)

y = C'Z+Du
= [ 1 0 1]Z+2u

(1.168)

The eigenvector matrix of A is

[ ; : : ] [ ] M = 'u1 'u2 'un 0 0 -3

(1.169)

Note the equivalences

A = M-~AM B = M-~B
C' = C M

(1.170)

The Jordan form system has two decoupled modes. The first mode is

$1 = [ o1 l1 ] x- l + [ ; ] u

Y1 = [ 1 0 1 5 1
The second mode is

(1.171)

52 = -252+0u 92 = 5 2

(1.172)

vvv

Definition 9 If a system is controllable or stable, then it is also stabilizable. If a system is uncontrollable OT unstable, then it is stabilizable if i t s uncontrollable
modes are stable.

SUMMARY 45
In Example 1.10, the first mode is unstable (both eigenvalues at +1) but controllable. The second mode is stable (eigenvalueat -2) but uncontrollable. Therefore, the system is stabilizable.
Definition 10 If a system is observable or stable, then it i s also detectable. If a system is unobservable or unstable, then it i s detectable if its unobservable modes are stable.
In Example 1.10, the first mode is unstable but observable. The second mode is both stable and observable. Therefore, the system is detectable.
Controllability and observability were introduced by Rudolph Kalman at a conference in 1959 whose proceedings were published in an obscure Mexican journal in 1960 [KalGOb]. The material was also presented at an IFAC conference in
1960 [KalGOc], and finallypublished in a more widely accessible format in 1963[Ka163].
1.8 SUMMARY
In this chapter we have reviewed some of the basic concepts of linear systems theory that are fundamental to many approaches to optimal state estimation. We began with a review of matrix algebra and matrix calculus, which proves to be indispensable in much of the theory of state estimation techniques. For additional information on matrix theory, the reader can refer to several excellent texts [Hor85, Go189, Ber051. We continued in this chapter with a review of linear and nonlinear systems, in both continuous time and discrete time. We regard time as continuous for physical systems, but our simulations and estimation algorithms operate in discrete time because of the popularity of digital computing. We discussed the discretization of continuous-time systems, which is a way of obtaining a discretetime mathematical representation of a continuoustime system. The concept of stability can be used to tell us if a system’s states will always remain bounded. Controllability tells us if it is possible to find a control input to force system states to our desired values, and observability tells us if it is possible to determine the initial state of a system on the basis of output measurements. State-space theory in general, and linear systems theory in particular, is a wideranging discipline that is typically covered in a one-semester graduate course, but there is easily enough material to fill up a two-semester course. Many excellent textbooks have been written on the subject, including [Bay99, Che99, KaiOO] and others. A solid understanding of linear systems will provide a firm foundation for further studies in areas such as control theory, estimation theory, and signal processing.
PROBLEMS
Written exercises
1.1 Find the rank of the matrix
1.2 Find two 2 x 2 matrices A and B such that A # B , neither A nor B are diagonal, A # cB for any scalar c, and A B = BA. Find the eigenvectors of A and

46

LINEAR SYSTEMSTHEORY

B. Note that they share an eigenvector. Interestingly, every pair of commuting matrices shares at least one eigenvector [Hor85, p. 511.

1.3 Prove the three identities of Equation (1.26).

1.4 Find the partial derivative of the trace of AB with respect to A.

1.5 Consider the matrix

!A A = [ ;

L

J

Recall that the eigenvalues of A are found by find the roots of the polynomial

P(A) = 1x1 - A [ . Show that P ( A ) = 0. (This is an illustration of the Cayley-

Hamilton theorem [Bay99,Che99, KaiOO].)

1.6 Suppose that A is invertible and

Find B and C in terms of A [Lie67].

1.7 Show that AB may not be symmetric even though both A and B are symmetric.

1.8 Consider the matrix

.=I; !A

L

J

where a, b, and c are real, and a and c are nonnegative.

a) Compute the solutions of the characteristic polynomial of A to prove that the eigenvalues of A are real.

b) For what values of b is A positive semidefinite?

1.g Derive the properties of the state transition matrix given in Equation (1.72).

1.10 Suppose that the matrix A has eigenvalues A, and eigenvectors wi (i =
-1,. .,n). What are the eigenvalues and eigenvectors of -A?

1.11 Show that leAtl = elAlt for any square matrix A.

1.12 Show that if A = BA, then

-d l A=J Tr(B)IAI
dt

1.13 The linear position p of an object under constant acceleration is
+ 1
p = po +pt -2jt2

where po is the initial position of the object.
a) Define a state vector as z = [ p p p ]T and write the state space e q u s
tion h = Az for this system.
b) Use all three expressions in Equation (1.71) to find the state transition matrix eAt for the system.
c) Prove for the state transition matrix found above that eAo = I .

1.14 Consider the following system matrix.
" 1 A = [ ' 0 -1

PROBLEMS 47

satisfies the relation S ( t ) = AS(t), but S ( t ) is not the state transition matrix of the system.
1.15 Give an example of a discrete-time system that is marginally stable but not asymptotically stable.
1.16 Show (H,F ) is an observable matrix pair if and only if ( H ,F-') is observable (assuming that F is nonsingular).
Computer exercises
1.17 The dynamics of a DC motor can be described as
~e + ~e = T
where t9 is the angular position of the motor, J is the moment of inertia, F is the coefficient of viscous friction, and T is the torque applied to the motor.
a) Generate a two-state linear system equation for this motor in the form
+ X = AX BU
b) Simulate the system for 5 s and plot the angular position and velocity.
Use J = 10 kg m2, F = 100 kg m2/s, z(0) = [ 0 0 ] T , and T = 10 N
m. Use rectangular integration with a step size of 0.05 s. Do the output plots look correct? What happens when you change the step size A t to 0.2 s? What happens when you change the step size t o 0.5 s? What are the eigenvalues of the A matrix, and how can you relate their magnitudes to the step size that is required for a correct simulation?
1.18 The dynamic equations for a series RLC circuit can be written as
u = IR+LI+V,
I = cvc
where u is the applied voltage, I is the current through the circuit, and V, is the
voltage across the capacitor. a) Write a state-space equation in matrix form for this system with 2 1 as the capacitor voltage and 2 2 as the current.
b) Suppose that R = 3, L = 1, and C = 0.5. Find an analytical expression
for the capacitor voltage for t 2 0, assuming that the initial state is zero, and the input voltage is u(t)= e-2t.

48

LINEAR SYSTEMS THEORY

c ) Simulatethe system using rectangular, trapezoidal, and fourth-order RungeKutta integration to obtain a numerical solution for the capacitor voltage. Simulate from t = 0 to t = 5 using step sizes of 0.1 and 0.2. Tabulate the RMS value of the error between the numerical and analytical solutions for the capacitor voltage for each of your six simulations.
1.19 The vertical dimension of a hovering rocket can be modeled as

x1 = 2 2

+ x2 = KU- g22 - GM

23

( R .1)2

x3 = -U

where 2 1 is the vertical position of the rocket, 2 2 is the vertical velocity, 23 is the mass of the rocket, u is the control input (the flow rate of rocket propulsion), K = 1000 is the thrust constant of proportionality, g = 50 is the drag constant, G = 6.6733 - 11 m3/kg/s2 is the universal gravitational constant, M = 5.98324 kg is the mass of the earth, and R = 6.3736 m is the radius of the earth radius.
a) Find u(t)= u ~ ( tsu)ch that the system is in equilibrium at q ( t )= 0 and x2(t) = 0.
b) Find x3(t) when z l ( t )= 0 and x2(t) = 0. c ) Linearize the system around the state trajectory found above.
+ d) Simulate the nonlinear system for five seconds and the linearized system
for five seconds with u ( t ) = u g ( t ) Aucos(t). Plot the altitude of the rocket for the nonlinear simulation and the linear simulation (on the same plot) when Au = 10. Repeat for Au = 100 and Au = 300. Hand in your source code and your three plots. What do you conclude about the accuracy of your linearization?

CHAPTER 2

Probability theory

The most we can know is in terms of probabilities. --Richard Feynman [Fey63, p. 6-11]

While writing my book [StochasticProcesses, first published in 19531 I had an argument with Feller. He asserted that everyone said “random variable” and I asserted that everyone said “chancevariable.” We obviously had to use the same name in our books, so we decided the issue by a stochastic procedure. That is, we tossed for it and he won.
-Joseph Doob [Sne97, p. 3071

Probabilities do not exist.

-Bruno de Finetti [deF74]

In our &tempt to filter a signal, we will be trying t o extract meaningful information from a noisy signal. In order to accomplish this, we need to know something about what noise is, some of its characteristics, and how it works. This chapter reviews probability theory. We begin by discussing the basic concept of probability in Section 2.1, and then move on to random variables (RVs) in Section 2.2. The chapter then continues with the following topics:

0 An RV is a general case of the normal scalars that we are familiar with, and so just as we can apply a functional mapping t o a number, we can also apply

Optimal State Estimation, First Edition. By Dan J. Simon

49

ISBN 0471708585 0 2 0 0 6 John Wiley & Sons, Inc.

50

PROBABILITY THEORY

a functional mapping to an RV. We discuss functions (transformations) of random variables in Section 2.3.
a Just as we can have vectors of numbers, we can also have vectors of RVs, and so we discuss groups of random variables and random vectors in Section 2.4.
a Just as we can have scalar functions of time, we can also have RVs that are functions of time, and so we discuss RVs that change with time (stochastic processes) in Section 2.5.
a Stochastic processes can be divided into two categories: white noise and colored noise, and we discuss these concepts in Section 2.6.
We conclude in Section 2.7 with a high-level discussion of how to write a computer simulation of a noise process.
This chapter is only a brief introduction and review of probability and stochastic processes, and more detail can be found in many other books on the subject, such as [Pap02, PeeOl].

2.1 PROBABILITY

How shall we define the concept of probability? Suppose we run an experiment a certain number of times. Sometimesevent A occurs and sometimes it does not. For instance, our experiment may be rolling a six-sided die. Event A may be defined as the number 4 showing up on the top surface of the die after we roll the die. Common sense tells us that the probability of event A occuring is 1/6. Likewise, we would expect that if we run our experiment many times, then we would see the number 1 appearing about 1/6 of the time. This intuitive explanation forms the basis for our formal description of the concept of probability. We define the probability of event A as

P

(

A

)

=

Number of times A occurs Total number of outcomes

(2.1)

This commonsense understanding of probability is called the relative frequency
definition. A more formal and mathematically rigorous definition of probability
can be obtained using set theory [Bi195, Nel871, which was pioneered by Andrey Kolomogorov in the 1930s. But for our purposes, the relative frequency definition is adequate.
In general, we know that there are n-choose-k different ways of selecting Ic objects from a total of n objects (assuming that the order of the objects does not matter), where n-choose-k is denoted and defined as

( ) n! = (n - k)!k!

For instance, suppose we have a penny (P),nickel (N), dime (D), and quarter (Q). How many distinct subsets of three coins can we pick from that set? We can pick PND, PNQ, PDQ, or NDQ, for a total of four possible subsets. This is equal to 4choose-3.

PROBABILITY

51

EXAMPLE2.1
What is the probability of being dealt four of a kind' in poker? The total number of possible poker hands can be computed as the total number of subsets of size five that can be picked from a deck of 52 cards. The total number of possible hands is 52-choose5 = 2,598,960. Out of all those hands, there are 48 possible hands containing four aces, 48 possible hands containing four kings, and so on. So there are a total of 13 x 48 hands containing four of a kind. Therefore the probability of being dealt four of a kind is

= 1/4165 M 0.024%
vvv
The conditional probability of event A given event B can be defined if the probability of B is nonzero. The conditional probability of A given B is defined as

P(A1B) is the conditional probability of A given B , that is, the probability that A occurs given the fact that B occurred. P ( A , B )is the joint probability of A and B , that is, the probability that events A and B both occur. The probability of a single event [for instance, P ( A )or P ( B ) ]is called an a priori probability because it applies to the probability of an event apart from any previously known information. A conditional probability [forinstance, P(AIB)]is called an a posteriori probability because it applies to a probability given the fact that some information about a possibly related event is already known.
For example, suppose that A is the appearance of a 4 on a die, and B is the appearance of an even number on a die. P ( A )= 1/6. But if we know that the die has an even number on it, then P(A) = 1/3 (since the even number could be either a 2, 4, or 6). This example is intuitive, but we can also obtain the answer using Equation (2.4). P ( A ,B ) is the probability that both A occurs (we roll a 4) and B occurs (we roll an even number), so P ( A ,B ) = 1/6. So Equation (2.4) gives

= 1/3

(2.5)

The a priori probability of A is 1/6. But the a posteriori probability of A given B is 1/3.

EXAMPLE2.2
Cnsider the eight shapes in Figure 2.1. We have three circles and five squares, so P(circ1e) = 3/8. Only one of the shapes is a gray circle, so P(gray, circle)
'Once I was dealt four sevens while playing poker with some friends (unfortunately, I was not playing for money at the time). I don't expect to see it again in my lifetime.

52

PROBABILITY THEORY

= 1/8. Of the three circles, only one is gray, so P(gray I circle) = 1/3. This
last probability can be computed using Equation (2.4) as
P(gray, circle) P(graylcirc1e) = P(circle)

Figure 2.1 Some shapes for illustrating probability and Bayes’ Rule.

vvv
Note that we can use Equation (2.4) to write P(BIA) = P ( A , B ) / P ( A ) .We can solve both this equation and Equation (2.4) for P ( A , B ) and equate the two
expressions for P ( A ,B ) to obtain Bayes’ Rule.

P(AIB)P(B)= P(BIA)P(A)

(2.7)

Bayes’ Rule is often written by rearranging the above equation to obtain

As an example, consider Figure 2.1. The probability of picking a gray shape given the fact that the shape is a circle can be computed from Bayes’ Rule as

P(circle1gray)P (gray)

P(graylcirc1e) =

P(circ1e)

We say that two events are independent if the occurrence of one event has no effect on the probability of the occurrence of the other event. For example, if A is the appearance of a 4 after rolling a die, and B is the appearance of a 3 after rolling another die, then A and B are independent. Mathematically, independence of A and B can be expressed several different ways. For example, we can write

P(A,B) = P(A)P(B) P(AIB) = P(A)
P(B1A) = P(B)

(2.10)

RANDOM VARIABLES

53

if A and B are independent. As an example, recall from Equation (2.5) that if A
is the appearance of a 4 on a die, and B is the appearance of an even number on a die, then P(A) = 1/6 and P(A1B)= 1/3. Since P(A1B)# P(A)we see that A and B are dependent events.

2.2 RANDOM VARIABLES

We define a random variable (RV) as a functional mapping from a set of experi-
mental outcomes (the domain) to a set of real numbers (the range). For example, the roll of a die can be viewed as a RV if we map the appearance of one dot on the
die to the output one, the appearance of two dots on the die to the output two, and so on.
Of course, after we throw the die, the value of the die is no longer a random variable - it becomes certain. The outcome of a particular experiment is not an RV. If we define X as an RV that represents the roll of a die, then the probability that X will be a four is equal to 1/6. If we then roll a four, the four is a realization of the RV X. If we then roll the die again and get a three, the three is another
realization of the RV X. However, the RV X exists independently of any of its
realizations. This distinction between an RV and its realizations is important for understanding the concept of probability. Realizations of an RV are not equal to the RV itself. When we say that the probability of X = 4 is equal to 1/6, that means that there is a 1 out of 6 chance that each realization of X will be equal to 4. However, the RV X will always be random and will never be equal to a specific value.
An RV can be either continuous or discrete. The throw of a die is a discrete random variable because its realizations belong to a discrete set of values. The high temperature tomorrow is a continuous random variable because its realizations belong to a continuous set of values.
The most fundamental property of an RV X is its probability distribution function (PDF)F x ( x ) ,defined as

F x ( x ) = P ( X 5 z)

(2.11)

In the above equation, F x ( x ) is the PDF of the RV X, and z is a nonrandom independent variable or constant. Some properties of the PDF that can be obtained
from its definition are

The probability density function (pdf) f x ( x ) is defined as the derivative of the PDF.
(2.13)

54

PROBABILITY THEORY

Some properties of the pdf that can be obtained from this definition are

FXb) = s_a.(.)dz
fX(2) 2 0
00
J_mfx(z)dz = 1

P ( a < z I b ) = JIbfX(s)d5

(2.14)

The Q-function of an RV is defined as one minus the PDF. This is equal to the probability that the RV is greater than the argument of the function:

(2.15)
Just as we spoke about conditional probabilities in Equation (2.4), we can also speak about the conditional PDF and the conditional pdf. The conditional distribution and density of the RV X given the fact that event A occurred are defined as
Fx(2lA) = P ( X 5 z l A )

(2.16) Bayes’ Rule, discussed in Section 2.1, can be generalized to conditional densities. Suppose we have random variables XI and X 2 . The conditional pdf of the RV X 1 given the fact that RV X 2 is equal to the realization 2 2 is defined as
(2.17)
Although this is not entirely intuitive, it can be derived without too much difficulty [PapOS, PeeOl]. Now consider the following product of two conditional pdf’s:

- f(51,22,23,24)
f(54)
= f[(Xl, 52,53)1541

(2.18)

Note that in the above equation we have dropped the subscripts on the f(.) func-
tions for ease of notation. This is commonly done if the random variable associated with the pdf is clear from the context. This is called the Chapman-Kolmogorov equation [Pap02]. It can be extended to any number of RVs and is fundamental to the Bayesian approach to state estimation (Chapter 15).
The expected value of an RV X is defined as its averagevalue over a large number
of experiments. This can also be called the expectation, the mean, or the average of

RANDOM VARIABLES

55

the RV. Suppose we run the experiment N times and observe a total of m different
outcomes. We observe that outcome A1 occurs n1 times, A2 occurs n2 times, . . .,
and A,,, occurs n, times. Then the expected value of X is computed as

.m
E(X)= I A i n ,
N 2=1

(2.19)

E(X)is also often written as E(x),X ,or 2 . At this point, we will begin to use lowercase x instead of uppercase X when the
meaning is clear. We have been using uppercase X to refer to an RV, and lowercase x to refer to a realization of the RV, which is a constant or independent variable.
However, it should be clear that, for example, E ( x )is the expected value of the RV
X ,and so we will interchange x and X in order to simplify notation.
As an example of the expected value of an RV, suppose that we roll a die an infinite number of times. We would expect to see each possible number (one through six) 1/6 of the time each. We can compute the expected value of the roll of the die as

+...+ E(X) =

1

lim
N+-m

- N [(1)(N/6)

(6)(N/6)]

= 3.5

(2.20)

Note that the expected value of an RV is not necessarily what we would expect to see when we run a particular experiment. For example, even though the above
expected value of X is 3.5, we will never see a 3.5 when we roll a die.
We can also talk about a function of an RV, just as we can talk about a function of any scalar. (We will discuss this in more detail in Section 2.3.) If a function,
say g(X),acts upon an RV, then the output of the function is also an RV. For example, if X is the roll of a die, then P(X = 4) = 1/6. If g(X)= X 2 ,then P[g(X=)161 = 1/6. We can compute the expected value of any function g(X)as

J-00

(2.21)

where fx(x)is the pdf of X. If g(X)= X ,then we can compute the expected value of X as
(2.22)
J --m
The variance of an RV is a measure of how much we expect the RV to vary from its mean. The variance is a measure of how much variability there is in an RV. In
the extreme case, if the RV X always is equal to one value (for example, the die is loaded and we always get a 4 when we roll the die), then the variance of X is equal to 0. On the other extreme, if X can take on any value between ztco with equal probability, then the variance of X is equal to 00. The variance of an RV is
formally defined as

(2.23)

56

PROBABILITY THEORY

The standard deviation of an RV is 0, which is the square root of the variance. Sometimes we denote the standard deviation as ox if we need to be explicit about the RV whose standard deviation we are discussing. Note that the variance can be written as

g2 = E[X2--2X1+22]
+ = E ( X 2 ) - 212 1 2
= E(X2)-Z2

(2.24)

We use the notation

x N (z,a2)

(2.25)

to indicate that X is an RV with a mean of 5 and a variance of g2.

The skew of an RV is a measure of the asymmetry of the pdf around its mean. Skew is defined as

skew = E [ ( X- Z ) 3 ]

(2.26)

The skewness, also called the coefficient of skewness, is the skew normalized by the cube of the standard deviation:

skewness = skew/g3

(2.27)

In general, the ith moment of a random variable X is the expected value of the
ith power of X . The ith central moment of a random variable X is the expected
value of the ith power of X minus its mean:

ith moment of X = E ( X ' ) ith central moment of X = E [ ( X - Z)']

(2.28)

For example, the first moment of a random variable is equal to its mean. The first central moment of a random variable is always equal to 0. The second central moment of a random variable is equal to its variance.
An RV is called uniform if its pdf is a constant value between two limits. This indicates that the RV has an equally likely probability of obtaining any value between its limits, but a zero probability of obtaining a value outside of its limits:

x E [a,b]
0 otherwise

(2.29)

Figure 2.2 shows the pdf of an RV that is uniformly distributed between fl. Note that the area of this curve is one (as is the area of all pdf's).

EXAMPLE 2.3
In this example we will find the mean and variance of an RV that is uniformly distributed between 1 and 3. The pdf of the RV is given as

112 x E [1,3] 0 otherwise

(2.30)

t I 0.4

RANDOM VARIABLES

57

X
Figure 2.2 Probability density function of an RV uniformly distributed between f l .

The mean is computed as follows:

6

= [ixdz =2 The variance is computed as follows:

(2.31)

l 3 = i(z- 2)2dx

(2.32)

vvv
An RV is called Gaussian or normal if its pdf is given by

[ I 1

-(x - Z)2

f X ( 2 ) = - a f i e x p

2a2

(2.33)

This is called the Laplace distribution in France, but it had many other discoverers, including Robert Adrain. Note that Z and 0 in the above pdf are the mean and standard deviation of the Gaussian RV. We use the notation

x N N(5,02)

(2.34)

to indicate that X is a Gaussian RV with a mean of Z and a variance of 02. Figure 2.3 shows the pdf of a Gaussian RV with a mean of zero and a variance

58

PROBABILITY THEORY

of one. If the mean changes, the pdf will shift to the left or right. If the variance increases, the pdf will spread out. If the variance decreases, the pdf will be squeezed in. The PDF of a Gaussian RV is given by

(2.35)

This integral does not have a closed-form solution, and so it must be evaluated numerically. However, its evaluation can be simplified by considering the normalized Gaussian PDF of an RV with zero mean and unity variance:

(2.36)

It can be shown that any Gaussian PDF can be expressed in terms of this normalized PDF as
(2.37)
In addition, a Gaussian PDF can be approximated as the following closed-form expression [Bor79]:

a = 0.339 b = 5.510

(2.38)

0.45

0.4 -

0.35 -

0.3 -

-p 0.25.
v
'0
0.2-

0.15 -
0O.0' 5l I

L I

nI
2-3-2-1

0 12 3 4

X

Figure 2.3 Probability density function of a Gaussian RV with a mean of zero and a variance of one.
Suppose we have a random variable X with a mean of zero and a symmetric pdf [i.e., f x ( x ) = fx(-x)]. This is the case, for example, for the pdf's shown in

TRANSFORMATIONS OF RANDOM VARIABLES

59

Figures 2.2 and 2.3. In this case, the ith moment of X can be written as

If i is odd then xz = - ( - z ) ~ . Combined with the fact that fx(x) = fx(-z), we
see that

(2.40)
So for odd i, the ith moment in Equation (2.39) is zero. We see that all of the odd moments of a zero-mean random variable with a symmetric pdf are equal to 0.

2.3 TRANSFORMATIONS OF R A N D O M VARIABLES

In this section, we will look at what happens to the pdf of an RV when we pass the RV through some function. Suppose that we have two RVs, X and Y , related to one another by the monotonic2 functions g ( . ) and h(.):

y = dX)
x = g-l(Y)=h(Y)

(2.41)

If we know the pdf of X [fx(x)],then we can compute the pdf of Y [fy(y)]as
follows:
+ + P(X E [x,x dz]) = P(Y E [Y,Y dy]) (dx > 0)
if dy > 0 if dy < 0

(2.42)
where we have used the assumption of small dx and dy in the above calculation.
'A monotonic function is a function whose slope is either always nonnegativeor always nonpositive. If the slope is always nonnegative, then the function is monotonically nondecreasing. If the slope is always positive, then the function is monotonically increasing. If the slope is always nonpositive, then the function is monotonically nonincreasing. If the slope is always negative, then the function is monotonically decreasing.

60

PROBABILITY THEORY

EXAMPLE2.4

+ In this example, we will find the pdf of a linear function of a Gaussian RV.
Suppose that X N N ( 3 ,u;) and Y = g ( X ) = a X b, where a # 0 and b are
any real constants. Then

x = h(Y)

= (Y - b ) / a

h‘(y) = l / a

fY(Y) = Ih’(Y)IfX[h(Y)l

= lAl-exp{1 a uxfi

- -

1

auxa

-[(y - b)/a - 212
I 2u:
{ -[y - (aZ+b)]2
I 2 a 2 4

(2.43)

In other words, the RV Y is Gaussian with a mean and variance given by

g = aZ+b
u: = a2a:

(2.44)

This important example shows that a linear transformation of a Gaussian RV results in a new Gaussian RV.
vvv

EXAMPLE2.5
Suppose that we pass a Gaussian RV X N N ( 0 , u ; ) through the nonlinear function Y = g(X) = X3:
x = h(Y)
- yv3 y-2/3
h’(Y) = 3
fY(Y) = Ih’(Y)lfX[h(Y)l

(2.45)

We see that the nonlinear transformation Y = X 3 converts a Gaussian RV to a non-Gaussian RV.It can be seen that f ~ ( y )approaches 00 as y + 0.
Nevertheless, the area under the fy(y) curve is equal t o 1 since it is a pdf.

vvv

In the more general case of RVs related by the function Y = g(X), where g(.) is

a nonmonotonic function, the pdf of Y (evaluated at y) can be computed from the

pdf of X as

C ~ Y ( Y =) fx(za)/Ig’(zt)I

(2.46)

a

MULTIPLE RANDOM VARIABLES

61

where the 2%values are the solutions of the equation y = g(z).

2.4 MULTIPLE R A N D O M VARIABLES
We have already defined the probability distribution function of an RV. For exam-
ple, if X and Y are RVs, then their distribution functions are defined as

(2.47)

Now we define the probability that both X Iz and Y I y as the joint probability
distribution function of X and Y:

FXY(X, y) = P(X 5 2,y 5 9)

(2.48)

If the meaning is clear from the context, we often use the shorthand notation F ( z ,y) to represent the distribution function Fxy(x,y). Some properties of the joint distribution function are

F(z,d E [OJI
F(z,-oo)=F(-oo,y) = 0
F(m,oo)= 1
F ( a , c ) 5 F ( b , d ) if a I b and c I d
+ P ( u < 2 I b, c < y I d) = F ( b ,d ) F ( u ,C) - F ( u ,d ) - F(b,C)
F(z,oo) = F(x)

F(W,Y) = F(Y)

(2.49)

Note from the last two properties that the distribution function of one RV can be obtained from the joint distribution function. When the distribution function for a single RV is obtained this way it is called the marginal distribution function.
The joint probability density function is defined as the following derivative of the joint PDF:

(2.50)

As before, we often use the shorthand notation f ( z , y ) to represent the density function f x y ( z , y). Some properties of the joint pdf that can be obtained from this definition are

P(u < x 5 b, c < y 5 d) f (Y)

(2.51)

62

PROBABILITY THEORY

Note from the last two properties that the density function of one RV can be obtained from the joint density function. When the density function for a single RV is obtained this way it is called the marginal density function. Computing the expected value of a function g(., .) of two RVs is similar to computing the expected value of a function of a single RV:

(2.52)

2.4.1 Statistical independence

Recall from Section 2.1 that two events are independent if the occurrence of one

event has no effect on the probability of the occurrence of the other event. We

extend this to say that RVs X and Y are independent if they satisfy the following

relation:

P(X 5 2,Y 5 y) = P(X5 z)P(Y5 y) for all 2,y

(2.53)

From our definition of joint distribution and density functions, we see that this implies

F X Y (2,Y) = F X ( . ) F Y (Y) f X Y (2,Y) = f X ( Z ) f Y (Y)

(2.54)

The central limit theorem says that the sum of independent RVs tends toward a Gaussian RV, regardless of the pdf of the individual RVs that contribute to the sum. This is why so many RVs in nature seem to have a Gaussian distribution. Many RVs in nature are actually the sum of many individual and independent RVs. For example, the high temperature on any given day in any given location tends to follow a Gaussian distribution. This is because the high temperature is affected by clouds, precipitation, wind, air pressure, humidity, and other factors. Each of these factors is in turn determined by other random factors. The combination of many independent random variables determines the high temperature, which has a Gaussian pdf.
We define the covariance of two scalar RVs X and Y as

c x y = E[(X-X)(Y- Y ]
= E(XY)-XP

(2.55)

We define the correlation coefficient of two scalar RVs X and Y as

(2.56)

The correlation coefficient is a normalized measurement of the independence be-
tween two RVs X and Y.If X and Y are independent, then p = 0 (although the converse is not necessarily true). If Y is a linear function of X then p = f l (see
Problem 2.9).
We define the correlation of two scalar RVs X and Y as

Rxy = E(XY)

(2.57)

Two RVs are said to be uncorrelated if R x y = E(X)E(Y).

MULTIPLERANDOM VARIABLES 63

From the definition of independence, we see that if two RVs are independent
then they are also uncorrelated. Independence implies uncorrelatedness, but uncorrelatedness does not necessarily imply independence. However, in the special
case in which two RVs are both Gaussian and uncorrelated, then it follows that
they are also independent.
Two RVs are said to be orthogonal if R x y = 0. If two RVs are uncorrelated, then they are orthogonal only if at least one of them is zero-mean. If two RVs are
orthogonal, then they may or may not be uncorrelated.

EXAMPLE2.6

Two rolls of the dice are represented by the RVs X and Y . The two RVs are
independent because one roll of the die does not have any effect on a second roll of the die. Each roll of the die has an equally likely probability (1/6) of being a 1,2, 3, 4, 5, or 6. Therefore,

E ( X )= E ( Y )

=

1+2+3+4+5+6 6

= 3.5

(2.58)

There are 36 possible combinations of the two rolls of the die. We could get the combination ( l , l ) ,(1,2), and so on. Each of these 36 combinations have an equally likely probability (1/36). Therefore, the correlation between X and Y is

z y r i j 1 6 6
Rxy=E(XY) =

z=1 j=l
= 12.25

= E(X)E(Y)

(2.59)

Since E ( X Y ) = E ( X ) E ( Y ) w, e see that X and Y are uncorrelated. However,
R x y # 0, so X and Y are not orthogonal.
vvv

H EXAMPLE2.7
A slot machine is rigged so you get 1 or -1 with equal probability the first spin X , and the opposite number the second spin Y . We have equal probabilities of
obtaining ( X ,Y )outcomes of (1,-1) and ( - 1 , l ) . The two RVs are dependent because the realization of Y depends on the realization of X . We also see that
E(X) = 0 E(Y) = 0

= -1

(2.60)

We see that X and Y are correlated because E ( X Y ) # E ( X ) E ( Y ) .We also see that X and Y are not orthogonal because E ( X Y ) # 0.

vvv

64

PROBABILITY THEORY

EXAMPLE2.8

A slot machine is rigged so you get -1, 0, or +1 with equal probability the
first spin X . On the second spin Y you get 1 if X = 0, and 0 if X # 0. The
two RVs are dependent because the realization of Y depends on the realization
of X . We also see that

E(X)

=

-1+0+1 3

=o

E(Y) = 0+1+0
3

=o

(2.61)

We see that X and Y are uncorrelated because E ( X Y ) = E ( X ) E ( Y ) .We also see that X and Y are orthogonal because E ( X Y ) = 0. This example illustrates the fact that uncorrelatedness does not necessarily imply independence.
vvv

EXAMPLE2.9
+ Suppose that x and y are independent RVs, and the RV z is computed as
z = g(x) h(y). In this example, we will calculate the mean of z:

E ( z ) = E [ g ( 4+h(Y)l

(2.62)

As a special case of this example, we see that the mean of the sum of two independent RVs is equal to the sum of their means. That is,

+ + E ( z y) = E ( z ) E(y) if x and y are independent

(2.63)

vvv

EXAMPLE 2.10
Suppose we roll a die twice. What is the expected value of the sum of the two outcomes? We use X and Y to refer to the two rolls of the die, and we use

MULTIPLE RANDOM VARIABLES

65

+ 2 to refer to the sum of the two outcomes. Therefore, 2 = X Y.Since X
and Y are independent, we have

E(2) = E(X)+E(Y) = 3.5+3.5 =7

(2.64)

VVQ

EXAMPLE 2.11

Consider the circuit of Figure 2.4. The input voltage V is uniformly distributed on [-1,1]. Voltage V has units of volts, and the two currents have units of amps.

{ I1 =

0 ifV>O V ifVIO

{ I2 =

V ifV2O 0 ifV<O

(2.65)

We see that I1 is uniformly distributed on [-1,0] and I2 is uniformly distributed on [0,1]. The RVs V, 11,and I2 have expected values

E(V) = 0 E(I1) = -1/2 E(12) = 1/2

(2.66)

The RVs I1 and I2 are not independent because they are related to each other;
if I2 # 0 then 11 = 0, and if I1 # 0 then I2 = 0. Since either 11 or I2 is equal
to 0 at every time instant, I 1 4 = 0 and E(IlI2) = 0. Therefore 11and 12 are
orthogonal. Since E ( I l ) E ( I 2 )= -1/4, we see that E(I1I2) # E ( I l ) E ( I 2 ) ,
and I1 and I2 are correlated.

vvv

Figure 2.4 Circuit for Example 2.11.

2.4.2 Multivariate statistics
The discussion in the previous subsection can be generalized for RVs that are vectors. In this case, the quantities defined earlier become vectors and matrices. Given

66

PROBABILITY THEORY

an n-element RV X and an rn-element RV Y (assuming that both X and Y are
column vectors), their correlation is defined as

-

(2.67)

Their covariance is defined as

c x y = E[(X- X ) ( Y - F)T] = E(XYT)-XPT

(2.68)

The autocorrelation of the n-element RV X is defined as

Rx = E[XXT] E[X?] * * a EIXIXn]
EIXnX1] * * * E[X3

(2.69)

Note that E(X,X,)= E(X,X,)so R x = R$. An autocorrelation matrix is always
symmetric. Also note that for any n-element column vector z we have

(2.70)

So an autocorrelation matrix is always positive semidefinite.
The autocovariance of the n-element RV X is defined as

c x = E[(X-X)(X-X)T]

[ ““1 1 E[(X1-Q2]

* * * E[(X1- X1)(Xn- Xn)]

= [ E[(Xn- Xn)(X1-XI)] *

E[(Xn- %I2]

-

*..

(2.71)

Unl * . a

u;

Note that u,j = uj, so Cx = CT.An autocovariance matrix is always symmetric.
Also note that for any n-element column vector z we have

zTCxz = zTE[(-X X)(X- X)T]z = E[zT(X- X)(X- X)TZ]
= E[(zT(X- X))2]
LO

(2.72)

MULTIPLE RANDOM VARIABLES

67

So an autocovariance matrix is always positive semidefinite. An n-element RV X is Gaussian ( n ~ r m a li)f ~

(2.73)

Now consider a Gaussian RV X that undergoes a linear transformation:

(2.74)
where A is a constant n x n matrix, and b is a constant n-element vector. If A is invertible, then

From Equation (2.42) we obtain

(2.75)

v N N(AZ+b,ACxAT)

(2.76)

This shows that normality is preserved in linear transformations of random vectors (just as it is preserved in linear transformations of random scalars, as seen in Example 2.4).
3FkancisEdgeworth (1845-1926), an Irish economist and mathematician, first provided a general description and study of the multivariate Gaussian probability distribution in 1892 [Sor80].

68

PROBABILITY THEORY

2.5 STOCHASTIC PROCESSES

A stochastic process, also called a random process, is a very simple generalization of the concept of an RV. A stochastic process X ( t ) is an RV X that changes with time.4 A stochastic process can be one of four types.

0 If the RV at each time is continuous and time is continuous, then X ( t ) is a continuousrandom process. For example, the temperature at each moment of the day is a continuous random process because both temperature and time are continuous.

0 If the RV at each time is discrete and time is continuous, then X ( t )is a discrete random process. For example, the number of people in a given building at each moment of the day is a discrete random process because the number of people is a discrete variable and time is continuous.

0 If the RV at each time is continuous and time is discrete, then X ( t ) is a continuous random sequence. For example, the high temperature each day is a continuous random sequence because temperature is continuous but time is discrete (day one, day two, etc.).

0 If the RV at each time is discrete and time is discrete, then X ( t ) is a discrete random sequence. For example, the highest number of people in a given building each day is a discrete random sequence because the number of people is a discrete variable and time is also discrete.

Since a stochastic process is an RV that changes with time, it has a distribution and density function that are functions of time. The PDF of X ( t ) is

Fx(x,t)= P(X(t)5 x )

(2.77)

If X ( t ) is a random vector, then the inequality above is an element-by-element inequality. For example, if X ( t ) has n elements, then

(2.78)

The pdf of X ( t ) is

(2.79)

If X ( t ) is a random vector, then the derivative above is taken once with respect to each element of 2. For example, if X ( t ) has n elements, then

(2.80)

The mean and covariance of X ( t ) are also functions of time:
00
z(t) = S__xf(z,t)dx

4Actually, the independent variable does not have t o be time; for example, it could be spatial location or something else. But typically the independent variable is time, and in this book it will always be time.

STOCHASTIC PROCESSES

69

(2.81)
Note that X(t) at two different times (tl and t2) comprise two different random variables [X(tl) and X(t2)l. Therefore, we can talk about the joint distribution and joint density functions of X(t1) and X(t2). These are called the second-order distribution function and the second-order density function:

(2.82)

As discussed earlier, if X(t) is an n-element random vector, then the inequality that defines F ( Q , z2, t l , t2) actually consists of 2 n inequalities, and the derivative that defines f(sl,z2,tl, t2) actually consists of 2 n derivatives.
The correlation between the two RVs X(t1) and X(t2) is called the autocorrelation of the stochastic process X(t):

R X ( t l , t 2 ) = E [X(tl)XT(t2)]

(2.83)

The autocovariance of a stochastic process is defined as

(2.84)

For some stochastic processes, the pdf does not change with time. For example, if we flip a coin ten times then we can view that process as a stochastic process with the statistics of the process being the same at each of the ten time instances. In this case, the stochastic process is called strict-sense stationary (SSS), or just stationary for short. In this case, the mean of the stochastic process is constant with respect to time, and the autocorrelation is a function of the time difference t2 - tl (not a function of the absolute times):

(2.85)

For some stochastic processes, these two conditions are true even though the pdf does change with time. Stochastic processes for which these two conditions are true are called widesense stationary (WSS). A stationary process is widesense stationary, but a widesense stationary process may or may not be stationary. From the definition of autocorrelation, it can be shown that for a widesense stationary process the following properties hold:

Rx(O) = EIX(t)XT(t)l
Rx(-.) = Rx(.)

(2.86)

For scalar stochastic processes, it can be shown that

70

PROBABILITY THEORY

EXAMPLE 2.12

1. The high temperature each day can be considered a stochastic process. However, this process is not stationary. The high temperature on a day in July might be an RV with a mean of 100 degrees Fahrenheit, but the high temperature on a day in December might have a mean of 30 degrees. This is a stochastic process whose statistics change with time, so the process is not stationary.
2. Electrical noise in a voltmeter might have a mean of zero and a variance of one millivolt. If we come back the next day and measure the noise again, the mean and variance may be the same as before. If the statistics of the noise are the same every day, then the electrical noise is a stationary process. Note that in reality the noise statistics will eventually change. For example, after a few decades the instrument will begin degrading and the electrical noise mean and variance will change. In this sense, there is no such thing as a stationary random process. Eventually, the universe will freeze and all signals will change. But for practical purposes, if the statistics of a random process do not change over the time interval of interest, then we consider the process to be stationary.
3. Tomorrow’s closing price of the Dow Jones Industrial Average might be an RV with a certain mean and variance. However, 100 years ago the closing price had a mean that was much lower. The closing price of the stock market is an RV whose mean generally increases with time. Therefore, the stock market price is a nonstationary stochastic process.

vvv
Suppose we have a stochastic process X ( t ) . Further suppose that the process has a realization z(t). The time average of X ( t ) is denoted as A [ X ( t ) ]a,nd the time autocorrelation of X ( t ) is denoted as R [ X ( t ) ] .These quantities are defined
for continuous-time random processes as

rT

A[X(t)] =

lim
T+m

I 2T -] T z ( t ) d t

R [ X ( t )‘,r] = A [ X ( t ) X T ( t4-T ) ]

(2.88)

The definitions for discretetime random processes are straightforward extensions of the continuoustime definitions.
An ergodic process is a stationary random process for which

(2.89)
In the real world, we are often limited to only a few realizations of a stochastic process. For example, if we measure the fluctuation of a voltmeter reading, we are actually only measuring only one realization of a stochastic process. We can compute the time average, time autocorrelation, and other time-based statistics of the realization. If the random process is ergodic, then we can use those time averages to estimate the statistics of the stochastic process.

WHITE NOISE AND COLORED NOISE

71

EXAMPLE 2.13
1. Suppose each unit of an electrical instrument is manufactured with a small random bias. Is the noise of the instrumentation ergodic? If we measure the noise of one instrument then we measure its bias, which is equal t o its mean. However, if we measure the noise of another instrument it might have a different mean because it has a different bias. In other words, we cannot obtain the mean of the stochastic process by simply investigating one instrument (i.e., one realization of the stochastic process). Therefore, the stochastic process is not ergodic.
2. Suppose each unit of an electrical instrument is manufactured identically, each with zero-mean stationary Gaussian noise. Is the noise ergodic? In this case we could measure the mean of the process by measuring the noise of many separate instruments at one instant of time, or by measuring the noise of one instrument over an extended period of time. Either experiment would correctly inform us that the mean of the stochastic process is zero. We could find the statistics of the stochastic process using all the instruments a t a single time, or using a single instrument a t many different times. Therefore, the stochastic process is ergodic.

vvv
The definitions of correlation and covariance can be extended to two stochastic processes X ( t ) and Y ( t ) .The cross correlation of X ( t ) and Y ( t )is defined as

RXY(t1,t 2 ) = E[X(tl)YT(t2)1

(2.90)

Two random processes X ( t ) and Y ( t )are said to be uncorrelated if Rxy(t1,t 2 ) =

E [ X ( t 1 ) ] E [ Y T ( t 2f)o]r all tl and t z . The cross covariance of X ( t ) and Y ( t )is

defined as

C x y ( t 1 ,t2) = E { [ X ( t i )- x ( t i > I [ Y (-t ~F()tz)lT}

(2.91)

2.6 WHITE NOISE AND COLORED NOISE

If the RV X ( t 1 )is independent from the RV X ( t z )for all tl # t z then X ( t ) is called
white noise. Otherwise, X ( t ) is called colored noise. The whiteness or color content of a stochastic process can be characterized by its
power spectrum. The power spectrum Sx(w) of a widesense stationary stochastic process X ( t ) is defined as the Fourier transform of the autocorrelation. The autocorrelation is the inverse Fourier transform of the power spectrum.

SX(W) = Rx(r) =

00
[ 0R0 x(r)e-'"'dT
'/ W Sx(w)@""dw 2n -00

(2.92)

These equations are called the Wiener-Khintchine relations after Norbert Wiener and Aleksandr Khinchin. Note that some authors put the term 1/2n on the right side of the S x ( w ) definition, in which case the 1127~term on the right side of the

72

PROBABILITY THEORY

RX(T)definition disappears. The power spectrum is sometimes referred to as the power density spectrum, the power spectral density, or the power density. The power of a wide-sense stationary stochastic process is defined as

(2.93)

The cross power spectrum of two wide-sense stationary stochastic processes X ( t ) and Y ( t )is the Fourier transform of the cross correlation:

S_,M

~xy(w)=

Rxy(r)e-jwrdr

1, l W
RXY(7) = 2;; SXU(w)e'"'

(2.94)

Similar definitions hold for discrete-time random processes. The power spectrum of a discretetime random process is defined as

w E [-n,7r]

(2.95)

{ A discretetime stochastic process X ( t ) is called white noise if Rx(k) = u2 i f k = O 0 ifk#O

= U26k

(2.96)

where 6k is the Kronecker delta function, defined as

{ 1 ifk=O
6k = 0 ifk#O

(2.97)

The definition of discrete-time white noise shows that it does not have any correlation with itself except at the present time. If X(k) is a discretetime white noise
process, then the RV X ( n ) is uncorrelated with X(m)unless n = m. This shows
that the power of a discrete-time white noise process is equal at all frequencies:

Sx(w)= Rx(0) for all w E [-n,n]

(2.98)

For a continuous-time random process, white noise is defined similarly. White noise has equal power at all frequencies (like white light):

SX(W)= Rx(0) for all w

(2.99)

Substituting this expression for Sx(w) into Equation (2.92),we see that for continuous-

time white noise

Rx(7) = Rx(0)6(7)

(2.100)

where 6(r)is the continuous-time impulse function. That is, 6 ( ~ )is a function that is zero everywhere except at r = 0; it has a width of 0, a height of CCJ, and an area of 1. Continuous-time white noise is not something that occurs in the real world because it has infinite power, as seen by comparing Equations (2.93) and (2.99). Nevertheless, many continuous-time processes approximate white noise and are useful in mathematical analyses of signals and systems.

SIMULATING CORRELATED NOISE

73

EXAMPLE 2.14

Suppose that a zero-mean stationary stochastic process has the autocorrela-

tion function

R ~ ( T=)u2e-PI71

(2.101)

where ,B is a positive real number. The power spectrum is computed from Equation (2.92) as

oil
02e-PITI e - j W 7 d 7

J-w

Jo

+- -

U2

IS2

p-jw P+jw

+ -- - 2 2 p W2 p2

The variance of the stochastic process is computed as

J E [ X 2 ( t ) ] = -1 oil - 2u2p dw 27r - o i l w 2 + p 2

(2.102)

vvv

(2.103)

2.7 SIMULATING CORRELATED NOISE

In optimal filtering research and experiments, we often have to simulate correlated

white noise. That is, we need to create random vectors whose elements are cor-

related with each other according to some predefined covariance matrix. In this

section, we will present one way of accomplishing this.

Suppose we want to generate an n-element random vector w that has zero mean

and covariance Q:

(y! . . . U l n

Q=[ !

i2]

(2.104)

Uln

un

Since Q is a covariance matrix, we know that all of its eigenvalues are real and nonnegative. We can therefore denote its eigenvalues as p::

X(Q) = p i (k = 1 , . . .,n)

(2.105)

Suppose the eigenvectors of Q are found to be d l , ., dn. Augment the d, vectors
together to obtain an n x n matrix D. Since Q is symmetric, we can always choose

74

PROBABILITY THEORY

the eigenvectors such that D is orthogonal, that is, D-l = D T . We therefore obtain the Jordan form decomposition of Q as

Q = DQDT

(2.106)

where Q is the diagonal matrix of the eigenvalues of Q. That is,

Q = diag(p.:, a * 7 P:)

(2.107)

Now we define the random vector v as v = D - l w , so that w = Dv. Therefore,

qVV=T)E ( D ~ W U I ~ D )
= D~QD
=Q
= diag(p:, . - ,p:)

(2.108)

This shows how we can generate an n-element random vector w with a covariance matrix of Q. The algorithm is given as follows.

Correlated noise simulation
- - 1. Find the eigenvalues of Q , and denote them as p:, ., p,2

- 2. Find the eigenvectors of Q, and denote them as d l , ., d,, such that

[ ] D = d , d l * * a
D-l = DT

(2.109)

3. For i = 1,. - ,n compute the random variable vi = pir2,where each rZ is an
independent random number with a variance of 1 (unity variance).
4. Set w = Dv.

2.8 SUMMARY
In this chapter, we have reviewed the basic concepts of probability, random variables, and stochastic processes. The probability of some event occurring is simply and intuitively defined as the number of times the event occurs divided by the number of chances the event has to occur. A random variable (RV) is a variable whose value is not certain, but is governed by the laws of probability. For example, your score on the test for this chapter is not deterministic, but is a random variable. Your actual score, after you take the test, will be a specific, deterministic number. But before you take the test, you do not know what you will get on the test. You may suppose that you will probably get between 80% and 90% if you have a decent understanding of the material, but your actual score will be determined by random events such as your health, how well you sleep the night before, what topics the instructor decides to cover on the test versus what topics you study, what the traffic was like on the way to school, the mood of the instructor when she grades the test, and so on. A stochastic process is a random variable that changes with time,

PROBLEMS

75

such as your performance on all of the quizzes and homework assignments for this course. The expected value of your test grades may be constant throughout the duration of the course if you are a consistent person, or it may increase if you tend to study harder as the course progresses, or it may decrease if you tend to study less as the course progresses. Probability, random variables, stochastic processes, and related topics form a huge area of study that we have only touched on in this chapter. Additional information on these topics can be found in many textbooks, including [Pap02,PeeOl]. A study of these topics will allow a student to delve into many practical engineering subjects, including control and estimation theory, signal processing, and communications theory.

PROBLEMS

Written exercises
2.1 What is the 0th moment of an RV? What is the 0th central moment of an RV? 2.2 Suppose a deck of 52 cards is randomly divided into four piles of 13 cards each. Find the probability that each pile contains exactly one ace [GreOl]. 2.3 Determine the value of a in the function
ax(1- x) x E [O, 11 otherwise
so that fx (x)is a valid probability density function [Lie67].
2.4 Determine the value of a in the function

so that fx (x)is a valid probability density function. What is the probability that
1x15 l?
2.5 The probability density function of an exponentially distributed random variable is defined as follows.

where a 2 0.
a) Find the probability distribution function of an exponentially distributed random variable.
b) Find the mean of an exponentially distributed random variable. c) Find the second moment of an exponentially distributed random variable. d) Find the variance of an exponentially distributed random variable. e ) What is the probability that an exponentially distributed random variable
takes on a value within one standard deviation of its mean?

76

PROBABILITY THEORY

2.6 Derive an expression for the skew of a random variable as a function of its first, second, and third moments.
2.7 Consider the following probability density function:
+ fx(2)= - b2 abx 2 ’ b > O
a) Determine the value of a in the so that fx(2)is a valid probability density function. (The correct value of a makes fx(2)a Cauchy pdf.)
b) Find the mean of a Cauchy random variable.
2.8 Consider two zero-mean uncorrelated random variables W and V with stan-
+ dard deviations gw and uv, respectively. What is the standard deviation of the
random variable X = W V ?
2.9 Consider two scalar RVs X and Y . a) Prove that if X and Y are independent, then their correlation coefficient
p = 0. b) Find an example of two RVs that are not independent but that have a
correlation coefficient of zero. c ) Prove that if Y is a linear function of X then p = f l .
2.10 Consider the following function [Lie67].
ae-2xe-3Y x > 0, y > o
otherwise
a) Find the value of a so that fxy(2,y) is a valid joint probability density function.
b) Calculate and g.
c ) Calculate E ( X 2 ) ,E ( Y 2 )a, nd E ( X Y ) .
09, d) Calculate the autocorrelation matrix of the random vector [ X Y ]T .
e ) Calculate the variance the variance ci,and the covariance C x y .
f ) Calculate the autocovariance matrix of the random vector [ X Y ]T.
g ) Calculate the correlation coefficient between X and Y.
2.11 A stochastic process has the autocorrelation R x ( T )= Ae-klT1,where A and k are positive constants.
a) What is the power spectrum of the stochastic process? b) What is the total power of the stochastic process?
c ) What value of k results in half of the total power residing in frequencies
less than 1Hz?
2.12 Suppose X is a random variable, and Y ( t )= X cost is a stochastic process. a) Find the expected value of Y ( t ) . b) Find A [ Y ( t ) ]th,e time average of Y ( t ) . c ) Under what condition is y ( t ) = A[Y(t)]?
+ 2.13 Consider the equation 2 = X V . The pdf’s of X and B are given in
Figure 2.5. a) Plot the pdf of ( Z l X ) as a function of X for 2 = 0.5.

PROBLEMS 77
b) Given Z = 0.5, what is conditional expectation of X? What is the most probable value of X? What is the median value of X?

-1.5

-1

-05

0

0.5

1

1.5

X

0.5

-1.5

-1

-05

0

0.5

1

1.5

Figure 2.5 pdf’s for Problem 2.13 [Sch73].
2.14 The temperature at noon in London is a stochastic process. Is it ergodic?
Computer exercises
2.15 Generate N = 50 independent random numbers, each uniformly distributed between 0 and 1. Plot a histogram of the random numbers using 10 bins. What is the sample mean and standard deviation of the numbers that you generated? What would you expect to see for the mean and standard deviation (i.e., what are the theoretical mean and standard deviation)? Repeat for N = 500 and N = 5,000 random numbers. What changes in the histogram do you see as N increases? 2.16 Generate 10,000 samples of (z1+ 2 2 ) / 2 , where each 2%is a random number
+ + uniformly distributed on [-1/2, +1/2]. Plot the 50-bin histogram. Repeat for
( X I + xz 23 24)/4. Describe the difference between the two histograms.

