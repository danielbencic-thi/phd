1342

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 31, NO. 4, APRIL 2021

DesnowGAN: An Efﬁcient Single Image Snow Removal Framework Using Cross-Resolution
Lateral Connection and GANs
Da-Wei Jaw , Shih-Chia Huang , Senior Member, IEEE, and Sy-Yen Kuo, Fellow, IEEE

Abstract— In this paper, we present a simple, efﬁcient, and highly modularized network architecture for single-image snow-removal. To address the challenging snow-removal problem in terms of network interpretability and computational complexity, we employ a pyramidal hierarchical design with lateral connections across different resolutions. This design enables us to incorporate high-level semantic features with other feature maps at different scales to enrich location information and reduce computational time. In addition, a reﬁnement stage based on recently introduced generative adversarial networks (GANs) is proposed to further improve the visual quality of the resulting snow-removed images and make a reﬁned image and a clean image indistinguishable by a computer vision algorithm to avoid the potential perturbations of machine interpretation. Finally, atrous spatial pyramid pooling (ASPP) is adopted to probe features at multiple scales and further boost the performance. The proposed DesnowGAN (DS-GAN) performs signiﬁcantly better than state-of-the-art methods quantitatively and qualitatively on the Snow100K dataset.
Index Terms— Snow Removal, deep learning, generative adversarial networks, image enhancement, image restoration.

Fig. 1. A real-world winter photograph (top) and the corresponding snow-removed result (bottom) obtained by the proposed method. Labels and conﬁdences are supported by YOLO v3 [1].

I. INTRODUCTION
A TMOSPHERIC phenomena removal has attracted much attention nowadays, but the interpretation of computer vision systems can be adversely affected by the unpredictable impairment of images under bad weather conditions. As shown in Figure 1, the falling snow considerably affects state-of-the-art object detection techniques [1].
Numerous studies have been performed to address the challenging snow-removal problem. Speciﬁcally, learning-based methods have utilized synthetic data to simulate images under different weather conditions and train neural networks to eliminate atmospheric particles that appear in images.
Manuscript received February 3, 2020; revised April 21, 2020 and May 19, 2020; accepted June 2, 2020. Date of publication June 17, 2020; date of current version April 5, 2021. This work was supported by the Ministry of Science and Technology, Taiwan, under Grant MOST 108-2918-I-027-001, Grant MOST 107-2218-E-009-062, Grant MOST 106-2221-E-027-126-MY2, Grant MOST 108-2221-E-002-072-MY3, Grant MOST 109-2622-E-027-006-CC3, Grant MOST 108-2221-E-027-047MY3, Grant MOST 106-2221-E-027-017-MY3, Grant MOST 108-2638E-002-002-MY2, Grant MOST 108-2218-E-009-056, and Grant MOST 105-2923-E-027-001-MY3. This article was recommended by Associate Editor Q. Huang. (Corresponding author: Shih-Chia Huang.)
Da-Wei Jaw and Sy-Yen Kuo are with the Department of Electrical Engineering, National Taiwan University, Taipei 10617, Taiwan (e-mail: sykuo@ntu.edu.tw).
Shih-Chia Huang is with the Department of Electronic Engineering, National Taipei University of Technology, Taipei 10608, Taiwan (e-mail: schuang@ntut.edu.tw)
Color versions of one or more of the ﬁgures in this article are available online at https://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TCSVT.2020.3003025

In particular, convolutional neural networks (CNNs) that are used to remove haze [2] and rain [3], [4] have considerably improved performance in terms of the resultant image quality and generalization ability as compared with handcrafted methods.
In [5], a fundamental snow-removal framework consisting of a synthesized snowy image dataset (Snow100K ) and a multistage network architecture is proposed. The framework outperforms state-of-the-art haze-removal and rain-removal CNNs quantitatively and qualitatively on the Snow100K dataset. In [6], Liu et al. proposed a dual residual connection mechanism and various network architectures to correspond with different image regression tasks, obtaining ﬁne results in these tasks.
Despite their good results on synthetic data, learning-based methods still suffer from artifacts or failed detection of particles when it comes to real-world applications. Two major factors can explain this ﬁnding. The ﬁrst factor concerns the use of synthetic datasets in CNN-based frameworks, where the dataset is manually made according to general human perceptions or handcrafted rules. The second factor concerns the use of pixel-level loss function to train the neural networks, which neither reproduces the human visual system nor detects unnatural texture in structures.
To manage this problem, we consider four major challenges in designing CNNs for the snow removal task: (1) computational complexity, (2) network interpretability, (3) image quality, and (4) generalization ability.

1051-8215 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 14,2022 at 09:31:45 UTC from IEEE Xplore. Restrictions apply.

JAW et al.: DesnowGAN: AN EFFICIENT SINGLE IMAGE SNOW REMOVAL FRAMEWORK

1343

The major constraint of challenges (1) and (2) is that the resolution of the input snowy image and the resulting output snow-free image must be identical. Therefore, the computational complexity and interpretability of the snow variance are critical for the network design.
In Liu et al.’s work [5], Inception-v4 [7] and Dilati on Pyr ami d are employed as the backbone feature extractors while removing all pooling and striding operations to maintain the same resolutions. However, despite the good results, this type of full-resolution network are time-consuming, and the model size of Inception-v4 is too large for lightweight applications.
Therefore, in this study, we develop a bottom-up pyramidal hierarchical pathway to extract high-level semantic features and a top-down pathway with lateral connections to comprehensively aggregate the feature from different resolutions and to further beneﬁt the computational complexity and interpretability of the system. In addition, a pyramidal context feature extractor and a lightweight spli t-tr ans f or m-merge topology are employed to further enhance the context awareness and feature diversity of the proposed network.
Regarding challenges (3) and (4), to further improve the resultant image quality and generalization ability, we develop a two-stage network architecture. The ﬁrst snow removal stage involves removing all falling snow that appears in an image, and the second reﬁnement stage generates a reﬁnement complement to enrich the details and recover unnatural areas. To constrain the network to focus on structural details and generate a natural image that corresponds to human perceptions, we use the structural similarity (SSIM) index loss and a Discriminator module to further boost the performance and push the solution space to the ground-truth manifold, respectively. Speciﬁcally, a generative adversarial network (GAN) framework is used to reﬁne the resultant image quality through the approximation of manifolds and to enhance the generalization ability through its unsupervised discriminability.
The contributions of this study are described as follows: • A novel loss function is proposed to facilitate the neural
network on jointly learning the awareness of sharpness, structure, and realisticness. In addition, the generalization capacity to identify snow in the real-world image has signiﬁcantly improved through the usage of the proposed unsupervised learning procedures. • A novel deep network architecture with a top-down pathway and lateral cross-resolution connections is proposed to exploit high-level semantic features and low-level spatial features to increase overall efﬁciency and computing speed. The use of the split-transformmerge topology greatly reduces the model size and computational costs, and the use of atrous spatial pyramid pooling (ASPP) to explore the multiscale and global receptive ﬁeld further improves efﬁciency. The proposed method outperforms state-of-the-art methods qualitatively and quantitatively as shown in the experimental results. Speciﬁcally, the proposed method achieved efﬁcient computational speeds and light model sizes up to

1736% faster and 28.96% smaller than the state-of-the-art single-image snow removal approach [5]. • We performed detailed qualitative and quantitative analyses to evaluate the respective abilities, especially the visual and quantitative generalization capability on real-world snowy images. The remainder of this paper is organized as follows. Section II discusses existing learning-based atmosphericparticle-removal techniques and other related studies. Section III provides details on the proposed DS-GAN framework. Section IV describes qualitative and quantitative analyses of the DS-GAN framework. Finally, Section V presents our conclusion.
II. RELATED WORKS
In this section, we brieﬂy review studies on atmospheric-particle-removal approaches and GANs.

A. Atmospheric Particle Removal
As discussed in Section I, removing atmospheric degradation from images is challenging owing to the lack of ground-truth information and temporal information. Thus, numerous priors have been used to automatically detect and remove the particles.
For rain removal, sparsity prior [8]–[10], Gaussian Maxture Model [11], patch-rank prior [12] and HOG prior [13] have been utilized as the mainstream handcrafted features. By contrast, haze-removal techniques embrace dark channel priors [14]–[16] and an atmospheric scattering model [17] to estimate medium transmission and remove haze particles.
Most recently, to overcome the limited generalization ability of handcrafted priors, approaches based on the same-resolution CNNs and synthetic datasets have been proposed to learn the mapping between input degraded images and the corresponding clean ground truth. DehazeNet [2] and DerainNet [3] combine handcrafted priors and CNNs to accomplish the particle removal process. Conversely, JORDER [4] directly learns the mapping between the residual complement and input degraded image without using handcrafted priors with its well-designed network architecture. In [5], a two-stage cascaded full-resolution network architecture to estimate the different attributes of snows was proposed. The work achieved state-of-the-art quantitative and qualitative results on the Snow100K dataset, but the process is time-consuming and has bad generalization ability. In [6], a modular block named Dual Residual Block was introduced, along with ﬁve types of network architectures to deal with ﬁve different image regression tasks.
B. Generative Adversarial Networks
The GAN framework was ﬁrst proposed by Goodfellow et al. [18]. The purpose of GANs is to train a generative model G to produce samples from a given distribution Pz into a target distribution Pt . To this end, a min-max optimization framework with a generator model G and discriminator D is developed, which is described as follows:

min max E [log(D(x))] + E [log(1 − D(x˜ ))], (1)

G D x∼Pt

x˜ ∼PG

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 14,2022 at 09:31:45 UTC from IEEE Xplore. Restrictions apply.

1344

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 31, NO. 4, APRIL 2021

Fig. 2. Overview of the proposed DS-GAN.

Fig. 3. The proposed Snow Removal module of our DS-GAN.

where Pt is the target distribution, PG is the model distribution and x˜ = G(z).
However, the optimization framework suffers from the vanishing gradient problem on the generator G when D is trained and becomes an optimal discriminator that attempts to minimize the Jensen-Shannon divergence of Pdata and PG, which will be a constant value when the two distributions do not overlap. To deal with this problem, the Wasserstein GAN (WGAN) of Arjovsky et al. [19] utilized the Wasserstein distance between the real and generated data, where this distance is continuous and differentiable when D is under the 1-Li pschi tz constraint.
An improved version named WGAN-GP [20] was proposed to accelerate the convergence through a gradient penalty term. The loss function of the discriminator in the WGAN-GP framework is deﬁned as

L = E [D(x˜)] − E [D(x)]

x˜ ∼PG

x ∼Pt

+λ E [(
xˆ ∼Pxˆ

∇xˆ D(xˆ)

2 − 1)2],

(2)

where the ﬁrst two terms estimate Wasserstein’s distance between two sample spaces and the last term is the gradient penalty term that enforces the 1-Li pschi tz constraint. In addition, xˆ is the uniform sample along a straight line of x and x˜, and λ is the weighting of the gradient penalty term.
III. PROPOSED METHOD
Our DS-GAN framework, as shown in Fig. 2, consists of three modules: (1) the Snow Removal (SR) module as depicted in Fig. 3, which removes the appearance of falling snow in a given snowy image x; (2) the Reﬁnement module which generates a reﬁnement complement r f ∈ [−1, 1] to further improve the visual quality and enrich the details of the estimated snow-free result y ; and (3) the Discriminator module, which is trained to distinguish the generated clean image yˆ and the ground truth image y.
In the SR and reﬁnement modules, two modularized descriptors Dm and Dr are designed to extract multiscale features fm and fr , two interpreters S R and RG interprets the snow-free image y and reﬁnement complement r f to yield the generated clean image yˆ, respectively. The derivation of y and r f is described in the following subsection, and Fig. 4 visualizes the variables from Figs. 2 and 3.

Fig. 4. Visualization of variables in DS-GAN. (a) Natural snowy image x. (b) Heatmap of the estimated snow mask zˆ. (c) Snow-removed image y . (d) Residual complement rm (normalized). (e) Reﬁned image yˆ. (f) Reﬁnement complement r f (normalized).
A. Descriptor In this study, the design of our descriptor focuses on two
major purposes: network interpretability and computational complexity. The detailed architecture of the proposed descriptor, as shown in Fig. 5, consists of three major components: (1) pyramidal lateral connections, (2) ResNeXt blocks, and (3) atrous spatial pyramid pooling (ASPP). Notably, the network architectures of the two descriptors, Dm and Dr are identical.
1) Pyramidal Lateral Connection: Top-down networks with fused cross-resolution features have been proven efﬁcient in various computer vision tasks [21]–[24]. This type of architecture not only reduces computational complexity through its bottom-up feature hierarchical pathway but also combines

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 14,2022 at 09:31:45 UTC from IEEE Xplore. Restrictions apply.

JAW et al.: DesnowGAN: AN EFFICIENT SINGLE IMAGE SNOW REMOVAL FRAMEWORK
TABLE I HYPERPARAMETERS OF PROPOSED DESCRIPTOR

1345

Fig. 5. The proposed descriptor of our DS-GAN.
high-level semantic features with low-level ones to enrich localization information.
Fig. 5 shows the building blocks of the proposed descriptor, where the orange blocks denote the downsampling operation either by strided convolution or pooling, the purple blocks refer to the upsampling operation with transposed convolution (known as “deconvolution” in [25]), and the blue blocks indicate operations that have identical resolutions.
2) ResNeXt Block: In [5], Inception-v4 [7] is employed as the backbone of the descriptor. However, despite its good feature interpretability, it has too many hyperparameters to adjust when it comes to optimizing the network architecture. Therefore, we choose the ResNeXt block [26] as our fundamental feature extractor. Its spli t-tr ans f or m-merge strategy corresponds to the complex variation of snow, and its car di nali t y dimension design can further reduce the number of parameters and computational complexity while increasing the feature diversity, which are all consistent with our demands.
3) Atrous Spatial Pyramid Pooling: The Dilati on Pyr ami d described in [5] is a concatenation of multiple dilated convolution layers with various dilation factors. However, inspired by PSPNet [27], the latest ASPP module [28] not only concatenates features from multiple dilated convolution layers but also concatenates image-level features and a convolutional 1 × 1 layer, which represents an improved version of the Dilati on Pyr ami d.
Therefore, the latest ASPP module is utilized in a high-level feature space, as shown in Fig. 5, to explore context and global information. The output features are further upsampled and

concatenated with those from lower levels. Then, a convolutional 3 × 3 layer is employed prior to output.

B. Snow Removal Module

As shown in Fig. 3, after the feature extraction process of the descriptor is completed, the SR module uses the extracted feature fm as the input to infer the snow mask zˆ and residual complement rm. The estimated residual complement rm then conducts an element-wise addition with input image x to remove the falling snow from the image. Notably, instead of using an additional cascaded full-resolution network to generate a residual complement rm from the estimated snow mask zˆ as in [5], we simultaneously estimate the snow mask zˆ and residual complement rm. This multitask learning strategy reduces computational complexity and aids in optimization, as proven in [29].
After the snow removal process, the generated snow-free image y , residual complement rm and the snow mask zˆ are concatenated into a semantic prior fc as the input of the reﬁnement module.

C. Reﬁnement Module and GANs

The purpose of Reﬁnement module is to generate a reﬁnement complement r f from the estimated priors fc to yield a visually realistic snow-removed image yˆ. To this end, we utilize the discriminator module that was used in WGAN-GP [20] to differentiate the input image derived from either the ground-truth manifold or the generated manifolds. By optimizing the di scri mi nator and gener ator , the solution space of yˆ is favored to the natural image manifold.

D. Loss Functions

In this study, we used the pyramidal Euclidean loss function:
τ

L(m, mˆ ) =

P2i (m) − P2i (mˆ ) 22,

(3)

i =0

where m and mˆ denotes two matrices with identity size, τ ∈ R denotes the level of pyramidal loss, Pn denotes the nonoverlapping pooling operation with kernel size and stride n × n.
A multiscaled pixel-level loss is then conducted between (z, zˆ), (y, y ), and (y, yˆ) deﬁned as follows:

LEuclidean = L(z, zˆ) + L(y, y ) + L(y, yˆ),

(4)

where the L(·) is deﬁned in Eq. (3). SSIM loss as deﬁned in [30] is utilized to further enhance
the structure-awareness ability of the reﬁnement module:

LSS I M = 1 − SSIM(y, yˆ).

(5)

Therefore, the overall Gener ator loss in our GAN framework is deﬁned as

LG AN = −D(yˆ), LGenerator = LG AN + λp (LS S I M + LEuclidean ), (6)

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 14,2022 at 09:31:45 UTC from IEEE Xplore. Restrictions apply.

1346

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 31, NO. 4, APRIL 2021
TABLE II ABLATION STUDY OF THE PROPOSED DS-GAN

where the D denotes the Discriminator module, LG AN is the Generator loss term deﬁned in the WGAN-GP framework [20], and λp ∈ R denotes the weighting of pixel-level loss.
On the other hand, the Di scri mi nator loss is then followed by the WGAN-GP framework:

TABLE III COMPARISON OF DIFFERENT DESIGN IN THE PROPOSED DS-GAN

Lcritic = Eyˆ∼PG [D(yˆ)] − Ey∼Pr [D(y)],

Lgp = Ey˜∼Py˜ [( ∇y˜ D(y˜ ) 2 − 1)2],

LDiscriminator = Lcritic + λgpLgp,

(7)

where the PG denotes the Generator distribution, Pr denotes the natural data distribution, Py˜ denotes the data uniformly sampled along straight lines between pairs of points sampled from PG and Pr .
IV. EXPERIMENTS
A. Implementation Details
1) ResNeXt Block: The implementation of the ResNeXt block was based on [32], and the settings of the hyperparameters are listed in Table I. The car di nali t y was set to four for all ResNeXt blocks as in the original paper [26].
2) ASPP: The dilation factor was set at [21, 22, 23], the number of kernels in each branch was set to 64, and all convolution operations in ASPP were followed by a batch normalization [33].
3) Descriptor: The convolution layer after fin was Conv7 with a kernel number of 64, where the subscript n in Convn denotes the size of the kernel n × n. The convolution layer between Convfin and the Concat layer is a Conv1 with a kernel number of 64. All the Convt layers have a kernel number of 64, the kernel size and corresponding stride depending on the upsampling rate. The convolution layer before fout was Conv3 with a kernel number of 64.
4) Others: The implementation of the discriminator network was followed by the WGAN-GP [20] framework containing ﬁve convolution layers and one dense layer. λp in Eq. (6) was set to 3 and λgp in Eq. (7) was set to 10.
5) Data Augmentation: In our experiment, we used the following random augmentation techniques: (1) horizontal ﬂip, (2) brightness adjustment, (3) hue adjustment, (4) contrast adjustment, (5) random scale from 0.3 to 1.75, and (6) random crop with a size of 128 × 128 as the training patch.
B. Dataset
In our experiments, the Snow100K dataset [5] is utilized to train/test our framework. The dataset consists of 50k training and testing synthesized snowy/snow-free image pairs, and its

corresponding snow mask. The test set of the Snow100K dataset is further separated into three subsets: Snow100K-S, Snow100K-M and Snow100K-L, which refer to the number of snow masks overlapping on the snow-free image. Speciﬁcally, images in the Snow100K-S subset have one snow mask, images in the Snow100K-M subset have two snow masks, and images in the Snow100K-L subset have three snow masks.
C. Ablation Study
We conducted a comprehensive ablation study, as described in Table II, to illustrate the effects of different network architectures in our DS-GAN. The peak signal-to-noise ratio (PSNR) and SSIM are used as the evaluation metrics, and all performances are evaluated using the Snow100K test set. Notably, the baseline network is an identical-resolution network that directly outputted y and zˆ from the given snowy image x without performing downsampling, lateral connections, reﬁnement stage, ASPP, SSIM loss and GAN loss.
1) Descriptor: As previously mentioned, all pooling, strided convolution layers, transposed convolution layers, and ASPP are removed to evaluate the performance of the DS-GAN-baseline model. The resultant PSNR and SSIM are 27.1738 and 0.9008, respectively, as shown in Table II.
The frames per second (FPS) and number of parameters for each architecture are listed in Table III. We also list the FPS and number of parameters from [5]. The table clearly reveals that the proposed DS-GAN signiﬁcantly improves the model size and computational complexity. The FPS was tested on a 480 × 480 image using a GTX 1080TI.
2) Loss Functions: We compare the quantitative and qualitative performances with and without LSS I M and LG AN . The quantitative evaluation results are listed in Table II, and the qualitative comparison is shown in Fig. 6. The top row of Fig. 6 shows that DS-GAN-SS I M has a better image quality than DS-GAN- AS P P, whereas DS-GAN-G AN

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 14,2022 at 09:31:45 UTC from IEEE Xplore. Restrictions apply.

JAW et al.: DesnowGAN: AN EFFICIENT SINGLE IMAGE SNOW REMOVAL FRAMEWORK

1347

Fig. 6. Comparison of DS-GAN using different loss functions: (a) natural snowy image, (b) DS-GAN- AS P P, (c) DS-GAN-S S I M, and (d) DS-GAN-G AN .
TABLE IV PERFORMANCES OF VARIOUS METHODS ON SNOW100K’S TEST SET. THE RESULTS IN Synthesized Data ROW DENOTE THE SIMILARITIES
BETWEEN THE SYNTHESIZED SNOWY IMAGE x AND THE SNOW-FREE GROUND TRUTH y; THE Overall COLUMN PRESENTS THE AVERAGES OVER THE ENTIRE TEST SET

not only produced better image quality but also removed most snow.
In the bottom row, although DS-GAN- AS P P and DS-GAN-SS I M removed the appearance of falling snow, the image details in the background were blurry and unnatural, whereas G AN preserved high-frequency components.
D. Comparison
The quantitative comparison results with state-of-the-art methods are listed in Table IV. Speciﬁcally, we improve the performance by 1.0027 and 0.0177 on the PSNR and SSIM metrics with the Snow100K test set, achieving a 1736% faster computational speed and 71.04% reduced model size, respectively, as shown in Table III. In addition, the Snow100K-S, -M, -L subsets were introduced in Section IV-B. The proposed method not only outperforms state-of-the-art snow removal algorithm [5] in every subset on PSNR and SSIM metric, it also signiﬁcantly improves 1.0027 PSNR and 0.228 SSIM value on the most difﬁcult Snow100K-L subset.
The qualitative comparison results shown in Fig. 7 clearly show that the proposed DS-GAN has a signiﬁcantly better generalization ability on real-world snow removal task in terms

of the resultant image quality and the detection of real-world snow particles. The reason behind such visually appealing results is because of the proposed unsupervised training policy, where Discriminator networks enforces the Reﬁnement networks to convert snowy images from the snow-remain manifold (y ) into the snow-free manifold (y).
In addition, we conduct a user study to objectively evaluate the image quality of the snow-removed images. We additionally collect 100 realistic snowy images from Internet and categorize them in two categories and two attributes: (1) the Simple-BG class, which refers to images with relatively simple background, and (2) the Complex-BG class, which refers to images with relatively complex background. The two attributes are: (1) light snow, which are images with normal size of snowﬂakes, and (2) heavy snow, which refers to images that contain large size of snowﬂakes. Notably, each image is categorized into one class and with one attribute, examples for each class and attributes can be found in Fig. 8.
The top four algorithms, JORDER [4], DuRN-S-P [6], DesnowNet [5] and the proposed DesnowGAN, which are evaluated in Table IV, are chosen for the comparison. The detailed settings of the experiment included 10 natural snowy

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 14,2022 at 09:31:45 UTC from IEEE Xplore. Restrictions apply.

1348

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 31, NO. 4, APRIL 2021

Fig. 7. Comparison of DS-GAN and other methods. (a) Natural snowy image. (b) Zheng et al. [9]. (c) DehazeNet [2]. (d) JORDER [4]. (e) DuRN-S-P [6]. (f) DesnowNet [5]. (g) DS-GAN (purposed).
TABLE V SUBJECTIVE EVALUATION RESULTS. THE CLEARNESS AND QUALITY SCORES FROM 1 TO 5, WHILE HIGHER REFERS TO
BETTER SNOW REMOVING ABILITY AND IMAGE QUALITY, RESPECTIVELY

rank from 1 to 5 to the resultant image using two criteria, κc and κq , where κc evaluates the algorithm’s ability of detecting and removing snowﬂakes, and κq evaluates the overall resultant image quality and realisticness. In total, 15 people participated in our experiment and 100 natural snowy images were employed, whereas 63 and 37 images are in the Simple-BG and Complex-BG classes and 73 and 27 images have light and heavy attributes, respectively. The scores for the methods are listed in Table V. As the JORDER [4], DuRN-S-P [6] and DesnowNet [5] reached similar subject scores among the categories, the proposed method outperforms other methods in terms of its ability to detect and remove snow (κc) and the quality of the resultant snow-free image (κq).

Fig. 8. Examples of the classes and attributes of real-world snowy images (top row) and our corresponding snow-removed results (bottom row).
images with 40 estimated snow-free results shown to each participant in a random order. Each participant was asked to

V. CONCLUSION
In this paper, we propose a novel snow-removal framework that combines the functions of the pixel-, structural-, and perceptual-level losses to mimic the visual system of humans. A signiﬁcant breakthrough that reduced

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 14,2022 at 09:31:45 UTC from IEEE Xplore. Restrictions apply.

JAW et al.: DesnowGAN: AN EFFICIENT SINGLE IMAGE SNOW REMOVAL FRAMEWORK

1349

the computational complexity and improved the overall
snow-removing efﬁciency for synthesized and real-world data is the proposed descriptor with pyramidal lateral connections
across multiple resolutions. Compared with the state-of-the-art
single-image snow-removal approach [5], the proposed network architecture has been 1736% faster in processing, and
the model size was reduced by 71.04%. Ultimately, the appli-
cation of the GAN framework is a major change in the framework’s generalization ability, which provides visually
realistic snow-removed results as illustrated in the qualitative and subjective evaluations.
REFERENCES
[1] J. Redmon and A. Farhadi, “YOLOv3: An incremental improvement,” 2018, arXiv:1804.02767. [Online]. Available: http://arxiv.org/abs/1804.02767
[2] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, “DehazeNet: An end-to-end system for single image haze removal,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5187–5198, Nov. 2016.
[3] X. Fu, J. Huang, X. Ding, Y. Liao, and J. Paisley, “Clearing the skies: A deep network architecture for single-image rain removal,” IEEE Trans. Image Process., vol. 26, no. 6, pp. 2944–2956, Jun. 2017.
[4] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint rain detection and removal from a single image,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 1357–1366.
[5] Y.-F. Liu, D.-W. Jaw, S.-C. Huang, and J.-N. Hwang, “DesnowNet: Context-aware deep network for snow removal,” IEEE Trans. Image Process., vol. 27, no. 6, pp. 3064–3073, Jun. 2018.
[6] X. Liu, M. Suganuma, Z. Sun, and T. Okatani, “Dual residual networks leveraging the potential of paired operations for image restoration,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 7007–7016.
[7] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4, inception-resnet and the impact of residual connections on learning,” in Proc. AAAI, vol. 4, 2017, p. 12.
[8] L.-W. Kang, C.-W. Lin, and Y.-H. Fu, “Automatic single-image-based rain streaks removal via image decomposition,” IEEE Trans. Image Process., vol. 21, no. 4, pp. 1742–1755, Apr. 2012.
[9] X. Zheng, Y. Liao, W. Guo, X. Fu, and X. Ding, “Single-imagebased rain and snow removal using multi-guided ﬁlter,” in Proc. Int. Conf. Neural Inf. Process. Cham, Switzerland: Springer, 2013, pp. 258–265.
[10] Y. Luo, Y. Xu, and H. Ji, “Removing rain from a single image via discriminative sparse coding,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 3397–3405.
[11] Y. Li, R. T. Tan, X. Guo, J. Lu, and M. S. Brown, “Rain streak removal using layer priors,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 2736–2744.
[12] Y.-L. Chen and C.-T. Hsu, “A generalized low-rank appearance model for spatio-temporally correlated rain streaks,” in Proc. IEEE Int. Conf. Comput. Vis., Dec. 2013, pp. 1968–1975.
[13] D.-Y. Chen, C.-C. Chen, and L.-W. Kang, “Visual depth guided color image rain streaks removal using sparse coding,” IEEE Trans. Circuits Syst. Video Technol., vol. 24, no. 8, pp. 1430–1455, Aug. 2014.
[14] K. He, J. Sun, and X. Tang, “Single image haze removal using dark channel prior,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 12, pp. 2341–2353, Dec. 2011.
[15] S.-C. Huang, J.-H. Ye, and B.-H. Chen, “An advanced singleimage visibility restoration algorithm for real-world hazy scenes,” IEEE Trans. Ind. Electron., vol. 62, no. 5, pp. 2962–2972, May 2015.
[16] B.-H. Chen, S.-C. Huang, and F.-C. Cheng, “A high-efﬁciency and high-speed gain intervention reﬁnement ﬁlter for haze removal,” J. Display Technol., vol. 12, no. 7, pp. 753–759, Jul. 2016.
[17] E. J. McCartney, Optics of the Atmosphere: Scattering by Molecules and Particles. New York, NY, USA: Wiley, 1976, p. 421.

[18] I. Goodfellow et al., “Generative adversarial nets,” in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 2672–2680.
[19] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” 2017, arXiv:1701.07875. [Online]. Available: http://arxiv.org/ abs/1701.07875
[20] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, “Improved training of Wasserstein GANs,” in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 5769–5779.
[21] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 3431–3440.
[22] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 2117–2125.
[23] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent. Cham, Switzerland: Springer, 2015, pp. 234–241.
[24] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “DSSD : Deconvolutional single shot detector,” 2017, arXiv:1701.06659. [Online]. Available: http://arxiv.org/abs/1701.06659
[25] M. D. Zeiler, G. W. Taylor, and R. Fergus, “Adaptive deconvolutional networks for mid and high level feature learning,” in Proc. Int. Conf. Comput. Vis., Nov. 2011, pp. 2018–2025.
[26] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He, “Aggregated residual transformations for deep neural networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 5987–5995.
[27] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 2881–2890.
[28] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous convolution for semantic image segmentation,” 2017, arXiv:1706.05587. [Online]. Available: http://arxiv.org/abs/ 1706.05587
[29] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask RCNN,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 2980–2988.
[30] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image restoration with neural networks,” IEEE Trans. Comput. Imag., vol. 3, no. 1, pp. 47–57, Mar. 2017.
[31] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 4, pp. 834–848, Apr. 2018.
[32] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual networks,” in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2016, pp. 630–645.
[33] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” 2015, arXiv:1502.03167. [Online]. Available: http://arxiv.org/ abs/1502.03167
Da-Wei Jaw received the B.S. and M.Sc. degrees in electronic engineering from the National Taipei University of Technology, Taipei, Taiwan, in 2015 and 2017, respectively. He is currently pursuing the Ph.D. degree in electrical engineering with National Taiwan University. His research interests relating to digital image processing, machine learning, and neural networks.

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 14,2022 at 09:31:45 UTC from IEEE Xplore. Restrictions apply.

1350

IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 31, NO. 4, APRIL 2021

Shih-Chia Huang (Senior Member, IEEE) received the B.S. degree from National Taiwan Normal University, Taipei, Taiwan, the M.S. degree from National Chiao Tung University, Hsinchu, Taiwan, and the Ph.D. degree in electrical engineering from National Taiwan University, in 2009. He is currently a Full Professor with the Department of Electronic Engineering, National Taipei University of Technology, and an International Adjunct Professor with the Faculty of Business and Information Technology, University of Ontario Institute of Technology, Oshawa, ON, Canada. He has authored or coauthored more than 100 journal articles and conference papers and holds more than 60 patents in the U.S., Europe, Taiwan, and China. He was a recipient of the Kwoh-Ting Li Young Researcher Award in 2011 by the Taipei Chapter of the Association for Computing Machinery, the 5th National Industrial Innovation Award in 2017 by the Ministry of Economic Affairs, Taiwan, the Dr. Shechtman Young Researcher Award in 2012 by the National Taipei University of Technology, the Outstanding Research Award from the National Taipei University of Technology in 2014 and 2017, and the College of Electrical Engineering and Computer Science, National Taipei University of Technology, from 2014 to 2016. His research interests include intelligent multimedia systems, image processing and video coding, video surveillance systems, cloud computing and big data analytics, artiﬁcial intelligence, and mobile applications and systems. He is currently the Chapter Chair of the IEEE Taipei Section Broadcast Technology Society and an Associate Editor of the IEEE SENSORS JOURNAL and Electronic Commerce Research and Applications. In addition, he has been an Associate Editor of the Journal of Artiﬁcial Intelligence and a Guest Editor of the Engineering Applications of Artiﬁcial Intelligence, the Information Systems Frontiers, and the International Journal of Web Services Research. He was also the Services and Applications Track Chair of IEEE CloudCom 2016–2017 conference, the Applications Track Chair of the IEEE BigData Congress in 2015, a General Chair of the 2015–2016 IEEE BigData Taipei Satellite Session, and the Deep Learning, Ubiquitous and Toy Computing Minitrack Chair.

Sy-Yen Kuo (Fellow, IEEE) received the B.S. degree in electrical engineering from National Taiwan University in 1979, the M.S. degree in electrical and computer engineering from the University of California at Santa Barbara in 1982, and the Ph.D. degree in computer science from the University of Illinois at Urbana–Champaign (UIUC) in 1987. He was the Dean of the College of Electrical Engineering and Computer Science, NTU, from 2012 to 2015, and the Chairman of the Department of Electrical Engineering, NTU, from 2001 to 2004. He is currently a Distinguished Professor with the Department of Electrical Engineering, National Taiwan University (NTU), Taipei, Taiwan. He also took a leave from NTU and served as a Chair Professor and the Dean of the College of Electrical Engineering and Computer Science, National Taiwan University of Science and Technology, from 2006 to 2009. He spent his sabbatical years as a Visiting Professor with the Hong Kong Polytechnic University from 2011 to 2012 and the Chinese University of Hong Kong from 2004 to 2005, and a Visiting Researcher with the AT&T Labs-Research, New Jersey, from 1999 to 2000, respectively. He was a Faculty Member with the Department of Electrical and Computer Engineering, University of Arizona, from 1988 to 1991, and an Engineer with Fairchild Semiconductor and Silvar-Lisco, California, from 1982 to 1984. He has published 450 papers in journals and conferences and also holds 22 U.S. patents, 23 Taiwan patents, and 15 patents from other countries. His current research interests include dependable and secure systems, the internet of things, and quantum computing. He is a member of the IEEE Fellow Committee from 2018 to 2020. He is the Vice President of the IEEE Computer Society in 2020. He is also a Core Member and a Member of the Board of Governors of the IEEE Computer Society from 2017 to 2020. He received the Distinguished Academic Achievement Alumni Award from the UIUC Department of Computer Science in 2019, the Distinguished Research Award, and the Distinguished Research Fellow award several times from the National Science Council, Taiwan. He was also a recipient of the Best Paper Awards in the 1996 International Symposium on Software Reliability Engineering and the 1986 IEEE/ACM Design Automation Conference, and the National Science Foundation’s Research Initiation Award in 1989.

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on October 14,2022 at 09:31:45 UTC from IEEE Xplore. Restrictions apply.

