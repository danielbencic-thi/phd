The Sciences of the Artificial
Third edition
Herbert A. Simon

title author publisher isbn10 | asin print isbn13 ebook isbn13 language subject publication date lcc ddc subject

: The Sciences of the Artificial : Simon, Herbert Alexander. : MIT Press : 0262193744 : 9780262193740 : 9780585360102 : English
Science--Philosophy. : 1996 : Q175.S564 1996eb : 300.1/1 : Science--Philosophy.

Page iv
© 1996 Massachusetts Institute of Technology
All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.
This book was set in Sabon by Graphic Composition, Inc.
Printed and bound in the United States of America.
Library of Congress Cataloging-in-Publication Data

Page v
Simon, Herbert Alexander, 1916 The sciences of the artificial / Herbert A. Simon.3rd ed. p. cm. Includes bibliographical references and index. ISBN 0-262-19374-4 (alk. paper).ISBN 0-262-69191-4 (pbk.: alk. paper) 1. Science Philosophy. I. Title. Q175.S564 1996 300.1'1dc20 96-12633 CIP

Page vi

To

Allen

Newell

in memory of a friendship

Page vii

Contents

Preface to Third Edition

ix

Preface to Second Edition

xi

1

1

Understanding the Natural and Artificial Worlds

2

25

Economic Rationality: Adaptive Artifice

3

51

The Psychology of Thinking: Embedding Artifice in Nature

4

85

Remembering and Learning: Memory As Environment for

Thought

5

111

The Science of Design: Creating the Artificial

6

139

Social Planning: Designing the Evolving Artifact

7

169

Alternative Views of Complexity

8

183

The Architecture of Complexity: Hierarchic Systems

Name Index

217

Subject Index

221

Page ix
Preface to Third Edition
As the Earth has made more than 5,000 rotations since The Sciences of the Artificial was last revised, in 1981, it is time to ask what changes in our understanding of the world call for changes in the text.
Of particular relevance is the recent vigorous eruption of interest in complexity and complex systems. In the previous editions of this book I commented only briefly on the relation between general ideas about complexity and the particular hierarchic form of complexity with which the book is chiefly concerned. I now introduce a new chapter to remedy this deficit. It will appear that the devotees of complexity (among whom I count myself) are a rather motley crew, not at all unified in our views on reductionism. Various among us favor quite different tools for analyzing complexity and speak nowadays of "chaos," "adaptive systems," and "genetic algorithms." In the new chapter 7, "Alternative Views of Complexity'' ("The Architecture of Complexity" having become chapter 8), I sort out these themes and draw out the implications of artificiality and hierarchy for complexity.
Most of the remaining changes in this third edition aim at updating the text. In particular, I have taken account of important advances that have been made since 1981 in cognitive psychology (chapters 3 and 4) and the science of design (chapters 5 and 6). It is gratifying that continuing rapid progress in both of these domains has called for numerous new references that record the advances, while at the same time confirm and extend the book's basic theses about the artificial sciences. Changes in emphases in chapter 2 reflect progress in my thinking about the respective roles of organizations and markets in economic systems.

Page x

This edition, like its predecessors, is dedicated to my friend of half a lifetime, Allen Newell but now, alas, to his memory. His final book, Unified Theories of Cognition, provides a powerful agenda for advancing our understanding of intelligent systems.

I am grateful to my assistant, Janet Hilf, both for protecting the time I have needed to carry out this revision and for assisting in innumerable ways in getting the manuscript ready for publication. At the MIT Press, Deborah Cantor-Adams applied a discerning editorial pencil to the manuscript and made communication with the Press a pleasant part of the process. To her, also, I am very grateful.

In addition to those others whose help, counsel, and friendship I acknowledged in the preface to the earlier editions, I want to single out some colleagues whose ideas have been especially relevant to the new themes treated here. These include Anders Ericsson, with whom I explored the theory and practice of protocol analysis; Pat Langley, Gary Bradshaw, and Jan Zytkow, my co-investigators of the processes of scientific discovery; Yuichiro Anzai, Fernand Gobet, Yumi Iwasaki, Deepak Kulkarni, Jill Larkin, Jean-Louis Le Moigne, Anthony Leonardo, Yulin Qin, Howard Richman, Weimin Shen, Jim Staszewski, Hermina Tabachneck, Guojung Zhang, and Xinming Zhu. In truth, I don't know where to end the list or how to avoid serious gaps in it, so I will simply express my deep thanks to all of my friends and collaborators, both the mentioned and the unmentioned.

In the first chapter I propose that the goal of science is to make the wonderful and the complex understandable and simple but not less wonderful. I will be pleased if readers find that I have achieved a bit of that in this third edition of The Sciences of the Artificial.

HERBERT

A.

SIMON

PITTSBURGH, PENNSYLVANIA

JANUARY 1, 1996

Page xi
Preface to Second Edition
This work takes the shape of fugues, whose subject and counter subject were first uttered in lectures on the opposite sides of a continent and the two ends of a decade but are now woven together as the alternating chapters of the whole.
The invitation to deliver the Karl Taylor Compton lectures at the Massachusetts Institute of Technology in the spring of 1968 provided me with a welcome opportunity to make explicit and to develop at some length a thesis that has been central to much of my research, at first in organization theory, later in economics and management science, and most recently in psychology.
In 1980 another invitation, this one to deliver the H. Rowan Gaither lectures at the University of California, Berkeley, permitted me to amend and expand this thesis and to apply it to several additional fields.
The thesis is that certain phenomena are "artificial" in a very specific sense: they are as they are only because of a system's being moulded, by goals or purposes, to the environment in which it lives. If natural phenomena have an air of "necessity" about them in their subservience to natural law, artificial phenomena have an air of "contingency" in their malleability by environment.
The contingency of artificial phenomena has always created doubts as to whether they fall properly within the compass of science. Sometimes these doubts refer to the goal-directed character of artificial systems and the consequent difficulty of disentangling prescription from description. This seems to me not to be the real difficulty. The genuine problem is to show how empirical propositions can be made at all about systems that, given different circumstances, might be quite other than they are.

Page xii
Almost as soon as I began research on administrative organizations, some forty years ago, I encountered the problem of artificiality in almost its pure form:
. . . administration is not unlike play-acting. The task of the good actor is to know and play his role, although different roles may differ greatly in content. The effectiveness of the performance will depend on the effectiveness of the play and the effectiveness with which it is played. The effectiveness of the administrative process will vary with the effectiveness of the organization and the effectiveness with which its members play their parts. [Administrative Behavior, p. 252]
How then could one construct a theory of administration that would contain more than the normative rules of good acting? In particular, how could one construct an empirical theory? My writing on administration, particularly in Administrative Behavior and part IV of Models of Man, has sought to answer those questions by showing that the empirical content of the phenomena, the necessity that rises above the contingencies, stems from the inabilities of the behavioral system to adapt perfectly to its environment from the limits of rationality, as I have called them.
As research took me into other areas, it became evident that the problem of artificiality was not peculiar to administration and organizations but that it infected a far wider range of subjects. Economics, since it postulated rationality in economic man, made him the supremely skillful actor, whose behavior could reveal something of the requirements the environment placed on him but nothing about his own cognitive makeup. But the difficulty must then extend beyond economics into all those parts of psychology concerned with rational behavior thinking, problem solving, learning.
Finally, I thought I began to see in the problem of artificiality an explanation of the difficulty that has been experienced in filling engineering and other professions with empirical and theoretical substance distinct from the substance of their supporting sciences. Engineering, medicine, business, architecture, and painting are concerned not with the necessary but with the contingent not with how things are but with how they might be in short, with design. The possibility of creating a science or sciences of design is exactly as great as the possibility of creating any science of the artificial. The two possibilities stand or fall together.
These essays then attempt to explain how a science of the artificial is possible and to illustrate its nature. I have taken as my main examples the

Page xiii
fields of economics (chapter 2), the psychology of cognition (chapters 3 and 4), and planning and engineering design (chapters 5 and 6). Since Karl Compton was a distinguished engineering educator as well as a distinguished scientist, I thought it not inappropriate to apply my conclusions about design to the question of reconstructing the engineering curriculum (chapter 5). Similarly Rowan Gaither's strong interest in the uses of systems analysis in public policy formation is reflected especially in chapter 6.
The reader will discover in the course of the discussion that artificiality is interesting principally when it concerns complex systems that live in complex environments. The topics of artificiality and complexity are inextricably interwoven. For this reason I have included in this volume (chapter 8) an earlier essay, "The Architecture of Complexity," which develops at length some ideas about complexity that I could touch on only briefly in my lectures. The essay appeared originally in the December 1962 Proceedings of the American Philosophical Society.
I have tried to acknowledge some specific debts to others in footnotes at appropriate points in the text. I owe a much more general debt to Allen Newell, whose partner I have been in a very large part of my work for more than two decades and to whom I have dedicated this volume. If there are parts of my thesis with which he disagrees, they are probably wrong; but he cannot evade a major share of responsibility for the rest.
Many ideas, particularly in the third and fourth chapters had their origins in work that my late colleague, Lee W. Gregg, and I did together; and other colleagues, as well as numerous present and former graduate students, have left their fingerprints on various pages of the text. Among the latter I want to mention specifically L. Stephen Coles, Edward A. Feigenbaum, John Grason, Pat Langley, Robert K. Lindsay, David Neves, Ross Quillian, Laurent Siklóssy, Donald S. Williams, and Thomas G. Williams, whose work is particularly relevant to the topics discussed here.
Previous versions of chapter 8 incorporated valuable suggestions and data contributed by George W. Corner, Richard H. Meier, John R. Platt, Andrew Schoene, Warren Weaver, and William Wise.
A large part of the psychological research reported in this book was supported by the Public Health Service Research Grant MH-07722 from the National Institute of Mental Health, and some of the research on

Page

xiv

design reported in the fifth and sixth chapters, by the Advanced Research Projects Agency of the Office of the Secretary of Defense (SD-146). These grants, as well as support from the Carnegie Corporation, the Ford Foundation, and the Alfred P. Sloan Foundation, have enabled us at Carnegie-Mellon to pursue for over two decades a many-pronged exploration aimed at deepening our understanding of artificial phenomena.

Finally, I am grateful to the Massachusetts Institute of Technology and to the University of California, Berkeley, for the opportunity to prepare and present these lectures and for the occasion to become better acquainted with the research in the sciences of the artificial going forward on these two stimulating campuses.

I want to thank both institutions also for agreeing to the publication of these lectures in this unified form, The Compton lectures comprise chapters 1, 3, and 5, and the Gaither lectures, chapters 2, 4, and 6. Since the first edition of this book (The MIT Press, 1969) has been well received, I have limited the changes in chapters 1, 3, 5, and 8 to the correction of blatant errors, the updating of a few facts, and the addition of some transitional paragraphs.

Page 1
1 Understanding the Natural and the Artificial Worlds
About three centuries after Newton we are thoroughly familiar with the concept of natural science most unequivocally with physical and biological science. A natural science is a body of knowledge about some class of things objects or phenomena in the world: about the characteristics and properties that they have; about how they behave and interact with each other.
The central task of a natural science is to make the wonderful commonplace: to show that complexity, correctly viewed, is only a mask for simplicity; to find pattern hidden in apparent chaos. The early Dutch physicist Simon Stevin, showed by an elegant drawing (figure 1) that the law of the inclined plane follows in "self-evident fashion" from the impossibility of perpetual motion, for experience and reason tell us that the chain of balls in the figure would rotate neither to right nor to left but would remain at rest. (Since rotation changes nothing in the figure, if the chain moved at all, it would move perpetually.) Since the pendant part of the chain hangs symmetrically, we can snip it off without disturbing the equilibrium. But now the balls on the long side of the plane balance those on the shorter, steeper side, and their relative numbers are in inverse ratio to the sines of the angles at which the planes are inclined.
Stevin was so pleased with his construction that he incorporated it into a vignette, inscribing above it
Wonder, en is gheen wonder
that is to say: "Wonderful, but not incomprehensible."
This is the task of natural science: to show that the wonderful is not incomprehensible, to show how it can be comprehended but not to

Page 2
Figure 1 The vignette devised by Simon Stevin to
illustrate his derivation of the law of the inclined plane
destroy wonder. For when we have explained the wonderful, unmasked the hidden pattern, a new wonder arises at how complexity was woven out of simplicity. The aesthetics of natural science and mathematics is at one with the aesthetics of music and painting both inhere in the discovery of a partially concealed pattern. The world we live in today is much more a man-made,1 or artificial, world than it is a natural world. Almost every element in our environment shows evidence of human artifice. The temperature in which we spend most of our hours is kept artificially at 20 degrees Celsius; the humidity is added to or taken from the air we breathe; and the impurities we inhale are largely produced (and filtered) by man. Moreover for most of us the white-collared ones the significant part of the environment consists mostly of strings of artifacts called "symbols" that we receive through eyes and ears in the form of written and spoken language and that we pour out into the environment as I am now doing by mouth or hand. The laws that govern these strings of
1. I will occasionally use "man" as an androgynous noun, encompassing both sexes, and "he," "his," and "him" as androgynous pronouns including women and men equally in their scope.

Page 3
symbols, the laws that govern the occasions on which we emit and receive them, the determinants of their content are all consequences of our collective artifice.
One may object that I exaggerate the artificiality of our world. Man must obey the law of gravity as surely as does a stone, and as a living organism man must depend for food, and in many other ways, on the world of biological phenomena. I shall plead guilty to overstatement, while protesting that the exaggeration is slight. To say that an astronaut, or even an airplane pilot, is obeying the law of gravity, hence is a perfectly natural phenomenon, is true, but its truth calls for some sophistication in what we mean by "obeying" a natural law. Aristotle did not think it natural for heavy things to rise or light ones to fall (Physics, Book IV); but presumably we have a deeper understanding of "natural" than he did.
So too we must be careful about equating "biological" with "natural." A forest may be a phenomenon of nature; a farm certainly is not. The very species upon which we depend for our food our corn and our cattle are artifacts of our ingenuity. A plowed field is no more part of nature than an asphalted street and no less.
These examples set the terms of our problem, for those things we call artifacts are not apart from nature. They have no dispensation to ignore or violate natural law. At the same time they are adapted to human goals and purposes. They are what they are in order to satisfy our desire to fly or to eat well. As our aims change, so too do our artifacts and vice versa.
If science is to encompass these objects and phenomena in which human purpose as well as natural law are embodied, it must have means for relating these two disparate components. The character of these means and their implications for certain areas of knowledge economics, psychology, and design in particular are the central concern of this book.
The Artificial
Natural science is knowledge about natural objects and phenomena. We ask whether there cannot also be "artificial" science knowledge about artificial objects and phenomena. Unfortunately the term "artificial" has a pejorative air about it that we must dispel before we can proceed.

Page 4
My dictionary defines "artificial" as, "Produced by art rather than by nature; not genuine or natural; affected; not pertaining to the essence of the matter." It proposes, as synonyms: affected, factitious, manufactured, pretended, sham, simulated, spurious, trumped up, unnatural. As antonyms, it lists: actual, genuine, honest, natural, real, truthful, unaffected. Our language seems to reflect man's deep distrust of his own products. I shall not try to assess the validity of that evaluation or explore its possible psychological roots. But you will have to understand me as using "artificial" in as neutral a sense as possible, as meaning man-made as opposed to natural.2
In some contexts we make a distinction between "artificial" and "synthetic." For example, a gem made of glass colored to resemble sapphire would be called artificial, while a man-made gem chemically indistinguishable from sapphire would be called synthetic. A similar distinction is often made between "artificial" and "synthetic" rubber. Thus some artificial things are imitations of things in nature, and the imitation may use either the same basic materials as those in the natural object or quite different materials.
As soon as we introduce "synthesis" as well as "artifice," we enter the realm of engineering. For "synthetic" is often used in the broader sense of "designed" or "composed.'' We speak of engineering as concerned with "synthesis," while science is concerned with "analysis." Synthetic or artificial objects and more specifically prospective artificial objects having desired properties are the central objective of engineering activity and skill. The engineer, and more generally the designer, is concerned with how things ought to be how they ought to be in order to attain goals,
2. I shall disclaim responsibility for this particular choice of terms. The phrase "artificial intelligence" which led me to it, was coined, I think, right on the Charles River, at MIT. Our own research group at Rand and Carnegie Mellon University have preferred phrases like "complex information processing" and "simulation of cognitive processes." But then we run into new terminological difficulties, for the dictionary also says that "to simulate" means "to assume or have the mere appearance or form of, without the reality; imitate; counterfeit; pretend." At any rate, "artificial intelligence" seems to be here to stay, and it may prove easier to cleanse the phrase than to dispense with it. In time it will become sufficiently idiomatic that it will no longer be the target of cheap rhetoric.

Page 5
and to function. Hence a science of the artificial will be closely akin to a science of engineering but very different, as we shall see in my fifth chapter, from what goes currently by the name of "engineering science."
With goals and "oughts" we also introduce into the picture the dichotomy between normative and descriptive. Natural science has found a way to exclude the normative and to concern itself solely with how things are. Can or should we maintain this exclusion when we move from natural to artificial phenomena, from analysis to synthesis?3
We have now identified four indicia that distinguish the artificial from the natural; hence we can set the boundaries for sciences of the artificial:
1. Artificial things are synthesized (though not always or usually with full forethought) by human beings.
2. Artificial things may imitate appearances in natural things while lacking, in one or many respects, the reality of the latter.
3. Artificial things can be characterized in terms of functions, goals, adaptation.
4. Artificial things are often discussed, particularly when they are being designed, in terms of imperatives as well as descriptives.
The Environment As Mold
Let us look a little more closely at the functional or purposeful aspect of artificial things. Fulfillment of purpose or adaptation to a goal involves a relation among three terms: the purpose or goal, the character of the artifact, and the environment in which the artifact performs. When we think of a clock, for example, in terms of purpose we may use the child's definition: "a clock is to tell time." When we focus our attention on the clock itself, we may describe it in terms of arrangements of gears and the
3. This issue will also be discussed at length in my fifth chapter. In order not to keep readers in suspense, I may say that I hold to the pristine empiricist's position of the irreducibility of "ought" to "is," as in chapter 3 of my Administrative Behavior (New York: Macmillan, 1976). This position is entirely consistent with treating natural or artificial goal-seeking systems as phenomena, without commitment to their goals. Ibid., appendix. See also the well-known paper by A. Rosenbluth, N. Wiener, and J. Bigelow, ''Behavior, Purpose, and Teleology," Philosophy of Science, 10 (1943):18 24.

Page 6
application of the forces of springs or gravity operating on a weight or pendulum.
But we may also consider clocks in relation to the environment in which they are to be used. Sundials perform as clocks in sunny climates they are more useful in Phoenix than in Boston and of no use at all during the Arctic winter. Devising a clock that would tell time on a rolling and pitching ship, with sufficient accuracy to determine longitude, was one of the great adventures of eighteenth-century science and technology. To perform in this difficult environment, the clock had to be endowed with many delicate properties, some of them largely or totally irrelevant to the performance of a landlubber's clock.
Natural science impinges on an artifact through two of the three terms of the relation that characterizes it: the structure of the artifact itself and the environment in which it performs. Whether a clock will in fact tell time depends on its internal construction and where it is placed. Whether a knife will cut depends on the material of its blade and the hardness of the substance to which it is applied.
The Artifact As "Interface"
We can view the matter quite symmetrically. An artifact can be thought of as a meeting point an "interface" in today's terms between an "inner" environment, the substance and organization of the artifact itself, and an ''outer" environment, the surroundings in which it operates. If the inner environment is appropriate to the outer environment, or vice versa, the artifact will serve its intended purpose. Thus, if the clock is immune to buffeting, it will serve as a ship's chronometer. (And conversely, if it isn't, we may salvage it by mounting it on the mantel at home.)
Notice that this way of viewing artifacts applies equally well to many things that are not man-made to all things in fact that can be regarded as adapted to some situation; and in particular it applies to the living systems that have evolved through the forces of organic evolution. A theory of the airplane draws on natural science for an explanation of its inner environment (the power plant, for example), its outer environment (the character of the atmosphere at different altitudes), and the relation between its inner and outer environments (the movement of an air foil

Page 7
through a gas). But a theory of the bird can be divided up in exactly the same way.4
Given an airplane, or given a bird, we can analyze them by the methods of natural science without any particular attention to purpose or adaptation, without reference to the interface between what I have called the inner and outer environments. After all, their behavior is governed by natural law just as fully as the behavior of anything else (or at least we all believe this about the airplane, and most of us believe it about the bird).
Functional Explanation
On the other hand, if the division between inner and outer environment is not necessary to the analysis of an airplane or a bird, it turns out at least to be highly convenient. There are several reasons for this, which will become evident from examples.
Many animals in the Arctic have white fur. We usually explain this by saying that white is the best color for the Arctic environment, for white creatures escape detection more easily than do others. This is not of course a natural science explanation; it is an explanation by reference to purpose or function. It simply says that these are the kinds of creatures that will "work;" that is, survive, in this kind of environment. To turn the statement into an explanation, we must add to it a notion of natural selection, or some equivalent mechanism.
An important fact about this kind of explanation is that it demands an understanding mainly of the outer environment. Looking at our snowy surroundings, we can predict the predominant color of the creatures we are likely to encounter; we need know little about the biology of the creatures themselves, beyond the facts that they are often mutually hostile, use visual clues to guide their behavior, and are adaptive (through selection or some other mechanism).
4. A generalization of the argument made here for the separability of "outer" from "inner" environment shows that we should expect to find this separability, to a greater or lesser degree, in all large and complex systems, whether they are artificial or natural. In its generalized form it is an argument that all nature will be organized in "levels:" My essay "The Architecture of Complexity,'' included in this volume as chapter 8, develops the more general argument in some detail.

Page 8
Analogous to the role played by natural selection in evolutionary biology is the role played by rationality in the sciences of human behavior. If we know of a business organization only that it is a profit-maximizing system, we can often predict how its behavior will change if we change its environment how it will alter its prices if a sales tax is levied on its products. We can sometimes make this prediction and economists do make it repeatedly without detailed assumptions about the adaptive mechanism, the decision-making apparatus that constitutes the inner environment of the business firm.
Thus the first advantage of dividing outer from inner environment in studying an adaptive or artificial system is that we can often predict behavior from knowledge of the system's goals and its outer environment, with only minimal assumptions about the inner environment. An instant corollary is that we often find quite different inner environments accomplishing identical or similar goals in identical or similar outer environments airplanes and birds, dolphins and tuna fish, weight-driven clocks and battery-driven clocks, electrical relays and transistors.
There is often a corresponding advantage in the division from the standpoint of the inner environment. In very many cases whether a particular system will achieve a particular goal or adaptation depends on only a few characteristics of the outer environment and not at all on the detail of that environment. Biologists are familiar with this property of adaptive systems under the label of homeostasis. It is an important property of most good designs, whether biological or artifactual. In one way or an other the designer insulates the inner system from the environment, so that an invariant relation is maintained between inner system and goal, independent of variations over a wide range in most parameters that characterize the outer environment. The ship's chronometer reacts to the pitching of the ship only in the negative sense of maintaining an invariant relation of the hands on its dial to the real time, independently of the ship's motions.
Quasi independence from the outer environment may be maintained by various forms of passive insulation, by reactive negative feedback (the most frequently discussed form of insulation), by predictive adaptation, or by various combinations of these.

Page 9
Functional Description and Synthesis
In the best of all possible worlds at least for a designer we might even hope to combine the two sets of advantages we have described that derive from factoring an adaptive system into goals, outer environment, and inner environment. We might hope to be able to characterize the main properties of the system and its behavior without elaborating the detail of either the outer or inner environments. We might look toward a science of the artificial that would depend on the relative simplicity of the interface as its primary source of abstraction and generality.
Consider the design of a physical device to serve as a counter. If we want the device to be able to count up to one thousand, say, it must be capable of assuming any one of at least a thousand states, of maintaining itself in any given state, and of shifting from any state to the "next" state. There are dozens of different inner environments that might be used (and have been used) for such a device. A wheel notched at each twenty minutes of arc, and with a ratchet device to turn and hold it, would do the trick. So would a string of ten electrical switches properly connected to represent binary numbers. Today instead of switches we are likely to use transistors or other solid-state devices.5
Our counter would be activated by some kind of pulse, mechanical or electrical, as appropriate, from the outer environment. But by building an appropriate transducer between the two environments, the physical character of the interior pulse could again be made independent of the physical character of the exterior pulse the counter could be made to count anything.
Description of an artifice in terms of its organization and functioning its interface between inner and outer environments is a major objective of invention and design activity. Engineers will find familiar the language of the following claim quoted from a 1919 patent on an improved motor controller:
What I claim as new and desire to secure by Letters Patent is:
1 In a motor controller, in combination, reversing means, normally effective field-weakening means and means associated with said reversing means for
5. The theory of functional equivalence of computing machines has had considerable development in recent years. See Marvin L. Minsky, Computation: Finite and Infinite Machines (Englewood Cliffs, N.J.: Prentice-Hall, 1967), chapters 1 4.

Page 10
rendering said field-weakening means ineffective during motor starting and thereafter effective to different degrees determinable by the setting of said reversing means . . .6
Apart from the fact that we know the invention relates to control of an electric motor, there is almost no reference here to specific, concrete objects or phenomena. There is reference rather to "reversing means" and "field-weakening means," whose further purpose is made clear in a paragraph preceding the patent claims:
The advantages of the special type of motor illustrated and the control thereof will be readily understood by those skilled in the art. Among such advantages may be mentioned the provision of a high starting torque and the provision for quick reversals of the motor.7
Now let us suppose that the motor in question is incorporated in a planing machine (see figure 2). The inventor describes its behavior thus:
Referring now to [figure 2], the controller is illustrated in outline connection with a planer (100) operated by a motor M, the controller being adapted to govern the motor M and to be automatically operated by the reciprocating bed (101) of the planer. The master shaft of the controller is provided with a lever (102) connected by a link (103) to a lever (104) mounted upon the planer frame and projecting into the path of lugs (105) and (106) on the planer bed. As will be understood, the arrangement is such that reverse movements of the planer bed will, through the connections described, throw the master shaft of the controller back and forth between its extreme positions and in consequence effect selective operation of the reversing switches (1) and (2) and automatic operation of the other switches in the manner above set forth.8
In this manner the properties with which the inner environment has been endowed are placed at the service of the goals in the context of the outer environment. The motor will reverse periodically under the control of the position of the planer bed. The "shape" of its behavior the time path, say, of a variable associated with the motor will be a function of the "shape" of the external environment the distance, in this case, between the lugs on the planer bed.
The device we have just described illustrates in microcosm the nature of artifacts. Central to their description are the goals that link the inner
6. U.S. Patent 1,307,836, granted to Arthur Simon, June 24, 1919. 7. Ibid. 8. Ibid.

Page 11
Figure 2 Illustrations from a patent for a motor controller
to the outer system. The inner system is an organization of natural phenomena capable of attaining the goals in some range of environments, but ordinarily there will be many functionally equivalent natural systems capable of doing this. The outer environment determines the conditions for goal attainment. If the inner system is properly designed, it will be adapted to the outer environment, so that its behavior will be determined in large part by the

Page 12
behavior of the latter, exactly as in the case of "economic man." To predict how it will behave, we need only ask, "How would a rationally designed system behave under these circumstances?" The behavior takes on the shape of the task environment.9
Limits of Adaptation
But matters must be just a little more complicated than this account suggests. "If wishes were horses, all beggars would ride." And if we could always specify a protean inner system that would take on exactly the shape of the task environment, designing would be synonymous with wishing. "Means for scratching diamonds" defines a design objective, an objective that might be attained with the use of many different substances. But the design has not been achieved until we have discovered at least one realizable inner system obeying the ordinary natural laws one material, in this case, hard enough to scratch diamonds.
Often we shall have to be satisfied with meeting the design objectives only approximately. Then the properties of the inner system will "show through." That is, the behavior of the system will only partly respond to the task environment; partly, it will respond to the limiting properties of the inner system.
Thus the motor controls described earlier are aimed at providing for "quick" reversal of the motor. But the motor must obey electromagnetic and mechanical laws, and we could easily confront the system with a task where the environment called for quicker reversal than the motor was capable of. In a benign environment we would learn from the motor only what it had been called upon to do; in a taxing environment we would learn something about its internal structure specifically about those aspects of the internal structure that were chiefly instrumental in limiting performance.10
9. On the crucial role of adaptation or rationality and their limits for economics and organization theory, see the introduction to part IV, "Rationality and Administrative Decision Making," of my Models of Man (New York: Wiley, 1957); pp. 38 41, 80 81, and 240 244 of Administrative Behavior; and chapter 2 of this book. 10. Compare the corresponding proposition on the design of administrative organizations: "Rationality, then, does not determine behavior. Within the area of rationality behavior is perfectly flexible and adaptable to abilities, goals, and
(footnote continued on next page)

Page 13
A bridge, under its usual conditions of service, behaves simply as a relatively smooth level surface on which vehicles can move. Only when it has been overloaded do we learn the physical properties of the materials from which it is built.
Understanding by Simulating
Artificiality connotes perceptual similarity but essential difference, resemblance from without rather than within. In the terms of the previous section we may say that the artificial object imitates the real by turning the same face to the outer system, by adapting, relative to the same goals, to comparable ranges of external tasks. Imitation is possible because distinct physical systems can be organized to exhibit nearly identical behavior. The damped spring and the damped circuit obey the same second-order linear differential equation; hence we may use either one to imitate the other.
Techniques of Simulation
Because of its abstract character and its symbol manipulating generality, the digital computer has greatly extended the range of systems whose behavior can be imitated. Generally we now call the imitation "simulation," and we try to understand the imitated system by testing the simulation in a variety of simulated, or imitated, environments.
Simulation, as a technique for achieving understanding and predicting the behavior of systems, predates of course the digital computer. The model basin and the wind tunnel are valued means for studying the behavior of large systems by modeling them in the small, and it is quite certain that Ohm's law was suggested to its discoverer by its analogy with simple hydraulic phenomena.
(footnote continued from previous page)
knowledge. Instead, behavior is determined by the irrational and non-rational elements that bound the area of rationality . . . administrative theory must be concerned with the limits of rationality, and the manner in which organization affects these limits for the person making a decision." Administrative Behavior, p. 241. For a discussion of the same issue as it arises in psychology, see my "Cognitive Architectures and Rational Analysis: Comment," in Kurt Van Lehn (ed.), Architectures for Intelligence (Hillsdale, NJ: Erlbaum, 1991).

Page 14
Simulation may even take the form of a thought experiment, never actually implemented dynamically. One of my vivid memories of the Great Depression is of a large multi colored chart in my father's study that represented a hydraulic model of an economic system (with different fluids for money and goods). The chart was devised by a technocratically inclined engineer named Dahlberg. The model never got beyond the pen-and-paint stage at that time, but it could be used to trace through the imputed consequences of particular economic measures or events provided the theory was right!11
As my formal education in economics progressed, I acquired a disdain for that naive simulation, only to discover after World War II that a distinguished economist, Professor A. W. Phillips had actually built the Moniac, a hydraulic model that simulated a Keynesian economy.12 Of course Professor Phillips's simulation incorporated a more nearly correct theory than the earlier one and was actually constructed and operated two points in its favor. However, the Moniac, while useful as a teaching tool, told us nothing that could not be extracted readily from simple mathematical versions of Keynesian theory and was soon priced out of the market by the growing number of computer simulations of the economy.
Simulation As a Source of New Knowledge
This brings me to the crucial question about simulation: How can a simulation ever tell us anything that we do not already know? The usual implication of the question is that it can't. As a matter of fact, there is an interesting parallelism, which I shall exploit presently, between two assertions about computers and simulation that one hears frequently:
1. A simulation is no better than the assumptions built into it.
2. A computer can do only what it is programmed to do.
I shall not deny either assertion, for both seem to me to be true. But despite both assertions simulation can tell us things we do not already know.
11. For some published versions of this model, see A. O. Dahlberg, National Income Visualized (N.Y.: Columbia University Press, 1956). 12. A. W. Phillips, "Mechanical Models in Economic Dynamics," Economica, New Series, 17 (1950):283 305.

Page 15
There are two related ways in which simulation can provide new knowledge one of them obvious, the other perhaps a bit subtle. The obvious point is that, even when we have correct premises, it may be very difficult to discover what they imply. All correct reasoning is a grand system of tautologies, but only God can make direct use of that fact. The rest of us must painstakingly and fallibly tease out the consequences of our assumptions.
Thus we might expect simulation to be a powerful technique for deriving, from our knowledge of the mechanisms governing the behavior of gases, a theory of the weather and a means of weather prediction. Indeed, as many people are aware, attempts have been under way for some years to apply this technique. Greatly oversimplified, the idea is that we already know the correct basic assumptions, the local atmospheric equations, but we need the computer to work out the implications of the interactions of vast numbers of variables starting from complicated initial conditions. This is simply an extrapolation to the scale of modern computers of the idea we use when we solve two simultaneous equations by algebra.
This approach to simulation has numerous applications to engineering design. For it is typical of many kinds of design problems that the inner system consists of components whose fundamental laws of behavior mechanical, electrical, or chemical are well known. The difficulty of the design problem often resides in predicting how an assemblage of such components will behave.
Simulation of Poorly Understood Systems
The more interesting and subtle question is whether simulation can be of any help to us when we do not know very much initially about the natural laws that govern the behavior of the inner system. Let me show why this question must also be answered in the affirmative.
First, I shall make a preliminary comment that simplifies matters: we are seldom interested in explaining or predicting phenomena in all their particularity; we are usually interested only in a few properties abstracted from the complex reality. Thus, a NASA-launched satellite is surely an artificial object, but we usually do not think of it as "simulating" the moon or a planet. It simply obeys the same laws of physics, which relate

Page 16
only to its inertial and gravitational mass, abstracted from most of its other properties. It is a moon. Similarly electric energy that entered my house from the early atomic generating station at Shipping port did not "simulate" energy generated by means of a coal plant or a windmill. Maxwell's equations hold for both.
The more we are willing to abstract from the detail of a set of phenomena, the easier it becomes to simulate the phenomena. Moreover we do not have to know, or guess at, all the internal structure of the system but only that part of it that is crucial to the abstraction.
It is fortunate that this is so, for if it were not, the top down strategy that built the natural sciences over the past three centuries would have been infeasible. We knew a great deal about the gross physical and chemical behavior of matter before we had a knowledge of molecules, a great deal about molecular chemistry before we had an atomic theory, and a great deal about atoms before we had any theory of elementary particles if indeed we have such a theory today.
This skyhook-skyscraper construction of science from the roof down to the yet unconstructed foundations was possible because the behavior of the system at each level depended on only a very approximate, simplified, abstracted characterization of the system at the level next beneath.13 This is lucky, else the safety of bridges and airplanes might depend on the correctness of the "Eightfold Way" of looking at elementary particles.
Artificial systems and adaptive systems have properties that make them particularly susceptible to simulation via simplified models. The characterization of such systems in the previous section of this chapter
13. This point is developed more fully in "The Architecture of Complexity," chapter 8 in this volume. More than fifty years ago, Bertrand Russell made the same point about the architecture of mathematics. See the "Preface" to Principia Mathematica: ". . . the chief reason in favour of any theory on the principles of mathematics must always be inductive, i.e., it must lie in the fact that the theory in question enables us to deduce ordinary mathematics. In mathematics, the greatest degree of self-evidence is usually not to be found quite at the beginning, but at some later point; hence the early deductions, until they reach this point, give reasons rather for believing the premises because true consequences follow from them, than for believing the consequences because they follow from the premises." Contemporary preferences for deductive formalisms frequently blind us to this important fact, which is no less true today than it was in 1910.

Page 17
explains why. Resemblance in behavior of systems without identity of the inner systems is particularly feasible if the aspects in which we are interested arise out of the organization of the parts, independently of all but a few properties of the individual components. Thus for many purposes we may be interested in only such characteristics of a material as its tensile and compressive strength. We may be profoundly unconcerned about its chemical properties, or even whether it is wood or iron.
The motor control patent cited earlier illustrates this abstraction to organizational properties. The invention consisted of a ''combination" of "reversing means," of "field weakening means," that is to say, of components specified in terms of their functioning in the organized whole. How many ways are there of reversing a motor, or of weakening its field strength? We can simulate the system described in the patent claims in many ways without reproducing even approximately the actual physical device that is depicted. With a small additional step of abstraction, the patent claims could be restated to encompass mechanical as well as electrical devices. I suppose that any undergraduate engineer at Berkeley, Carnegie Mellon University, or MIT could design a mechanical system embodying reversibility and variable starting torque so as to simulate the system of the patent.
The Computer As Artifact
No artifact devised by man is so convenient for this kind of functional description as a digital computer. It is truly protean, for almost the only ones of its properties that are detectable in its behavior (when it is operating properly!) are the organizational properties. The speed with which it performs it basic operations may allow us to infer a little about its physical components and their natural laws; speed data, for example, would allow us to rule out certain kinds of "slow" components. For the rest, almost no interesting statement that one can make about an operating computer bears any particular relation to the specific nature of the hardware. A computer is an organization of elementary functional components in which, to a high approximation, only the function

Page 18
performed by those components is relevant to the behavior of the whole system.14
Computers As Abstract Objects
This highly abstractive quality of computers makes it easy to introduce mathematics into the study of their theory and has led some to the erroneous conclusion that, as a computer science emerges, it will necessarily be a mathematical rather than an empirical science. Let me take up these two points in turn: the relevance of mathematics to computers and the possibility of studying computers empirically.
Some important theorizing, initiated by John von Neumann, has been done on the topic of computer reliability. The question is how to build a reliable system from unreliable parts. Notice that this is not posed as a question of physics or physical engineering. The components engineer is assumed to have done his best, but the parts are still unreliable! We can cope with the unreliability only by our manner of organizing them.
To turn this into a meaningful problem, we have to say a little more about the nature of the unreliable parts. Here we are aided by the knowledge that any computer can be assembled out of a small array of simple, basic elements. For instance, we may take as our primitives the so-called Pitts-McCulloch neurons. As their name implies, these components were devised in analogy to the supposed anatomical and functional characteristics of neurons in the brain, but they are highly abstracted. They are formally isomorphic with the simplest kinds of switching circuits "and" "or," and "not'' circuits. We postulate, now, that we are to build a system from such elements and that each elementary part has a specified probability of functioning correctly. The problem is to arrange the elements and their interconnections in such a way that the complete system will perform reliably.
The important point for our present discussion is that the parts could as well be neurons as relays, as well relays as transistors. The natural laws governing relays are very well known, while the natural laws governing
14. On the subject of this and the following paragraphs, see M. L. Minsky, op. cit.; then John von Neumann, "Probabilistic Logics and the Synthesis of Reliable Organisms from Unreliable Components," in C. E. Shannon and J. McCarthy (eds.), Automata Studies (Princeton: Princeton University Press, 1956).

Page 19
neurons are known most imperfectly. But that does not matter, for all that is relevant for the theory is that the components have the specified level of unreliability and be interconnected in the specified way.
This example shows that the possibility of building a mathematical theory of a system or of simulating that system does not depend on having an adequate micro theory of the natural laws that govern the system components. Such a micro theory might indeed be simply irrelevant.
Computers As Empirical Objects
We turn next to the feasibility of an empirical science of computers as distinct from the solid-state physics or physiology of their componentry.15 As a matter of empirical fact almost all of the computers that have been designed have certain common organizational features. They almost all can be decomposed into an active processor (Babbage's "Mill") and a memory (Babbage's "Store") in combination with input and output devices. (Some of the larger systems, somewhat in the manner of colonial algae, are assemblages of smaller systems having some or all of these components. But perhaps I may oversimplify for the moment.) They are all capable of storing symbols (program) that can be interpreted by a program-control component and executed. Almost all have exceedingly limited capacity for simultaneous, parallel activity they are basically one-thing-at-a-time systems. Symbols generally have to be moved from the larger memory components into the central processor before they can be acted upon. The systems are capable of only simple basic actions: recoding symbols, storing symbols, copying symbols, moving symbols, erasing symbols, and comparing symbols.
Since there are now many such devices in the world, and since the properties that describe them also appear to be shared by the human central nervous system, nothing prevents us from developing a natural history of them. We can study them as we would rabbits or chipmunks and discover how they behave under different patterns of environmental stimulation. Insofar as their behavior reflects largely the broad functional
15. A. Newell and H. A. Simon, "Computer Science as Empirical Inquiry," Communications of the ACM, 19(March 1976):113 126. See also H. A. Simon, "Artificial Intelligence: An Empirical Science," Artificial Intelligence, 77(1995):95 127.

Page 20
characteristics we have described, and is independent of details of their hardware, we can build a general but empirical theory of them.
The research that was done to design computer time-sharing systems is a good example of the study of computer behavior as an empirical phenomenon. Only fragments of theory were available to guide the design of a time-sharing system or to predict how a system of a specified design would actually behave in an environment of users who placed their several demands upon it. Most actual designs turned out initially to exhibit serious deficiencies, and most predictions of performance were startlingly inaccurate.
Under these circumstances the main route open to the development and improvement of time-sharing systems was to build them and see how they behaved. And this is what was done. They were built, modified, and improved in successive stages. Perhaps theory could have anticipated these experiments and made them unnecessary. In fact it didn't, and I don't know anyone intimately acquainted with these exceedingly complex systems who has very specific ideas as to how it might have done so. To understand them, the systems had to be constructed, and their behavior observed.16
In a similar vein computer programs designed to play games or to discover proofs for mathematical theorems spend their lives in exceedingly large and complex task environments. Even when the programs themselves are only moderately large and intricate (compared, say, with the monitor and operating systems of large computers), too little is known about their task environments to permit accurate prediction of how well they will perform, how selectively they will be able to search for problem solutions.
Here again theoretical analysis must be accompanied by large amounts of experimental work. A growing literature reporting these experiments is beginning to give us precise knowledge about the degree of heuristic power of particular heuristic devices in reducing the size of the problem spaces that must be searched. In theorem proving, for example, there has
16. The empirical, exploratory flavor of computer research is nicely captured by the account of Maurice V. Wilkes in his 1967 Turing Lecture, "Computers Then and Now," Journal of the Association for Computing Machinery, 15(January 1968):1 7.

Page 21
been a whole series of advances in heuristic power based on and guided by empirical exploration: the use of the Herbrand theorem, the resolution principle, the set-of-support principle, and so on.17
Computers and Thought
As we succeed in broadening and deepening our knowledge theoretical and empirical about computers, we discover that in large part their behavior is governed by simple general laws, that what appeared as complexity in the computer program was to a considerable extent complexity of the environment to which the program was seeking to adapt its behavior.
This relation of program to environment opened up an exceedingly important role for computer simulation as a tool for achieving a deeper understanding of human behavior. For if it is the organization of components, and not their physical properties, that largely determines behavior, and if computers are organized somewhat in the image of man, then the computer becomes an obvious device for exploring the consequences of alternative organizational assumptions for human behavior. Psychology could move forward without awaiting the solutions by neurology of the problems of component design however interesting and significant these components turn out to be.
Symbol Systems: Rational Artifacts
The computer is a member of an important family of artifacts called symbol systems, or more explicitly, physical symbol systems.18 Another important member of the family (some of us think, anthropomorphically, it is the most important) is the human mind and brain. It is with this family
17. Note, for example, the empirical data in Lawrence Wos, George A. Robinson, Daniel F. Carson, and Leon Shalla, "The Concept of Demodulation in Theorem Proving," Journal of the Association for Computing Machinery, 14(October 1967):698 709, and in several of the earlier papers referenced there. See also the collection of programs in Edward Feigenbaum and Julian Feldman (eds.), Computers and Thought (New York: McGraw-Hill, 1963). It is common practice in the field to title papers about heuristic programs, "Experiments with an XYZ Program." 18. In the literature the phrase information-processing system is used more frequently than symbol system. I will use the two terms as synonyms.

Page 22
of artifacts, and particularly the human version of it, that we will be primarily concerned in this book. Symbol systems are almost the quintessential artifacts, for adaptivity to an environment is their whole raison d'être. They are goalseeking, information-processing systems, usually enlisted in the service of the larger systems in which they are incorporated.
Basic Capabilities of Symbol Systems
A physical symbol system holds a set of entities, called symbols. These are physical patterns (e.g., chalk marks on a blackboard) that can occur as components of symbol structures (sometimes called "expressions"). As I have already pointed out in the case of computers, a symbol system also possesses a number of simple processes that operate upon symbol structures processes that create, modify, copy, and destroy symbols. A physical symbol system is a machine that, as it moves through time, produces an evolving collection of symbol structures.19 Symbol structures can, and commonly do, serve as internal representations (e.g., "mental images") of the environments to which the symbol system is seeking to adapt. They allow it to model that environment with greater or less veridicality and in greater or less detail, and consequently to reason about it. Of course, for this capability to be of any use to the symbol system, it must have windows on the world and hands, too. It must have means for acquiring information from the external environment that can be encoded into internal symbols, as well as means for producing symbols that initiate action upon the environment. Thus it must use symbols to designate objects and relations and actions in the world external to the system.
Symbols may also designate processes that the symbol system can interpret and execute. Hence the programs that govern the behavior of a symbol system can be stored, along with other symbol structures, in the system's own memory, and executed when activated.
Symbol systems are called "physical" to remind the reader that they exist as realworld devices, fabricated of glass and metal (computers) or flesh and blood (brains). In the past we have been more accustomed to thinking of the symbol systems of mathematics and logic as abstract and disembodied, leaving out of account the paper and pencil and human minds that were required actually to bring them to life. Computers have
19. Newell and Simon, "Computer Science as Empirical Inquiry," p. 116.

Page 23
transported symbol systems from the platonic heaven of ideas to the empirical world of actual processes carried out by machines or brains, or by the two of them working together.
Intelligence As Computation
The three chapters that follow rest squarely on the hypothesis that intelligence is the work of symbol systems. Stated a little more formally, the hypothesis is that a physical symbol system of the sort I have just described has the necessary and sufficient means for general intelligent action.
The hypothesis is clearly an empirical one, to be judged true or false on the basis of evidence. One task of chapters 3 and 4 will be to review some of the evidence, which is of two basic kinds. On the one hand, by constructing computer programs that are demonstrably capable of intelligent action, we provide evidence on the sufficiency side of the hypothesis. On the other hand, by collecting experimental data on human thinking that tend to show that the human brain operates as a symbol system, we add plausibility to the claims for necessity, for such data imply that all known intelligent systems (brains and computers) are symbol systems.
Economics: Abstract Rationality
As prelude to our consideration of human intelligence as the work of a physical symbol system, chapter 2 introduces a heroic abstraction and idealization the idealization of human rationality which is enshrined in modern economic theories, particularly those called neoclassical. These theories are an idealization because they direct their attention primarily to the external environment of human thought, to decisions that are optimal for realizing the adaptive system's goals (maximization of utility or profit). They seek to define the decisions that would be substantively rational in the circumstances defined by the outer environment.
Economic theory's treatment of the limits of rationality imposed by the inner environment by the characteristics of the physical symbol system tends to be pragmatic, and sometimes even opportunistic. In the more formal treatments of general equilibrium and in the so-called "rational expectations" approach to adaptation, the possibilities that an information-processing system may have a very limited capability for

Page 24
adaptation are almost ignored. On the other hand, in discussions of the rationale for market mechanisms and in many theories of decision making under uncertainty, the procedural aspects of rationality receive more serious treatment.
In chapter 2 we will see examples both of neglect for and concern with the limits of rationality. From the idealizations of economics (and some criticisms of these idealizations) we will move, in chapters 3 and 4, to a more systematic study of the inner environment of thought of thought processes as they actually occur within the constraints imposed by the parameters of a physical symbol system like the brain.

Page 25
2 Economic Rationality: Adaptive Artifice
Because scarcity is a central fact of life land, money, fuel, time, attention, and many other things are scarce it is a task of rationality to allocate scarce things. Performing that task is the focal concern of economics.
Economics exhibits in purest form the artificial component in human behavior, in individual actors, business firms, markets, and the entire economy. The outer environment is defined by the behavior of other individuals, firms, markets, or economies. The inner environment is defined by an individual's, firm's, market's, or economy's goals and capabilities for rational, adaptive behavior. Economics illustrates well how outer and inner environment interact and, in particular, how an intelligent system's adjustment to its outer environment (its substantive rationality) is limited by its ability, through knowledge and computation, to discover appropriate adaptive behavior (its procedural rationality).
The Economic Actor
In the textbook theory of the business firm, an "entrepreneur" aims at maximizing profit, and in such simple circumstances that the computational ability to find the maximum is not in question. A cost curve relates dollar expenditures to amount of product manufactured, and a revenue curve relates income to amount of product sold. The goal (maximizing the difference between income and expenditure) fully defines the firm's inner environment. The cost and revenue curves define the outer environment.1 Elementary calculus shows how to find the profit-maximizing
1. I am drawing the line between outer and inner environment not at the firm's boundary but at the skin of the entrepreneur, so that the factory is part of the external technology; the brain, perhaps assisted by computers, is the internal.

Page 26
quantity by taking a derivative (rate at which profit changes with change in quantity) and setting it equal to zero.
Here are all the elements of an artificial system adapting to an outer environment, subject only to the goal defined by the inner environment. In contrast to a situation where the adaptation process is itself problematic, we can predict the system's behavior without knowing how it actually computes the optimal output. We need consider only substantive rationality.2
We can interpret this bare-bones theory of the firm either positively (as describing how business firms behave) or normatively (as advising them how to maximize profits). It is widely taught in both senses in business schools and universities, just as if it described what goes on, or could go on, in the real world. Alas, the picture is far too simple to fit reality.
Procedural Rationality
The question of maximizing the difference between revenue and cost becomes interesting when, in more realistic circumstances, we ask how the firm actually goes about discovering that maximizing quantity. Cost accounting may estimate the approximate cost of producing any particular output, but how much can be sold at a specific price and how this amount varies with price (the elasticity of demand) usually can be guessed only roughly. When there is uncertainty (as there always is), prospects of profit must be balanced against risk, thereby changing profit maximization to the much more shadowy goal of maximizing a profit-vs.-risk "utility function" that is assumed to lurk somewhere in the recesses of the entrepreneur's mind.
But in real life the business firm must also choose product quality and the assortment of products it will manufacture. It often has to invent and design some of these products. It must schedule the factory to produce a profitable combination of them and devise marketing procedures and structures to sell them. So we proceed step by step from the simple caricature of the firm depicted in the textbooks to the complexities of real firms in the real world of business. At each step toward realism, the problem
2. H. A. Simon, "Rationality as Process and as Product of Thought," American Economic Review, 68(1978):1 16.

Page 27
gradually changes from choosing the right course of action (substantive rationality) to finding a way of calculating, very approximately, where a good course of action lies (procedural rationality). With this shift, the theory of the firm becomes a theory of estimation under uncertainty and a theory of computation decidedly non-trivial theories as the obscurities and complexities of information and computation increase.
Operations Research and Management Science
Today several branches of applied science assist the firm to achieve procedural rationality.3 One of them is operations research (OR); another is artificial intelligence (AI). OR provides algorithms for handling difficult multivariate decision problems, sometimes involving uncertainty. Linear programming, integer programming, queuing theory, and linear decision rules are examples of widely used OR procedures.
To permit computers to find optimal solutions with reasonable expenditures of effort when there are hundreds or thousands of variables, the powerful algorithms associated with OR impose a strong mathematical structure on the decision problem. Their power is bought at the cost of shaping and squeezing the real-world problem to fit their computational requirements: for example, replacing the real-world criterion function and constraints with linear approximations so that linear programming can be used. Of course the decision that is optimal for the simplified approximation will rarely be optimal in the real world, but experience shows that it will often be satisfactory.
The alternative methods provided by AI, most often in the form of heuristic search (selective search using rules of thumb), find decisions that are "good enough," that satisfice. The AI models, like OR models, also only approximate the real world, but usually with much more accuracy and detail than the OR models can admit. They can do this because heuristic search can be carried out in a more complex and less well-structured problem space than is required by OR maximizing tools. The price paid
3. For a brief survey of these developments, see H. A. Simon, "On How to Decide What to Do," The Bell Journal of Economics, 9(1978):494 507. For an estimate of their impact on management, see H. A. Simon, The New Science of Management Decision, rev. ed. (Englewood Cliffs, NJ: Prentice-Hall, 1977), chapters 2 and 4.

Page 28
for working with the more realistic but less regular models is that AI methods generally find only satisfactory solutions, not optima. We must trade off satisficing in a nearly-realistic model (AI) against optimizing in a greatly simplified model (OR). Sometimes one will be preferred, sometimes the other.
AI methods can handle combinatorial problems (e.g., factory scheduling problems) that are beyond the capacities of OR methods, even with the largest computers. Heuristic methods provide an especially powerful problem-solving and decision-making tool for humans who are unassisted by any computer other than their own minds, hence must make radical simplifications to find even approximate solutions. AI methods also are not limited, as most OR methods are, to situations that can be expressed quantitatively. They extend to all situations that can be represented symbolically, that is, verbally, mathematically or diagrammatically.
OR and AI have been applied mainly to business decisions at the middle levels of management. A vast range of top management decisions (e.g., strategic decisions about investment, R&D, specialization and diversification, recruitment, development, and retention of managerial talent) are still mostly handled traditionally, that is, by experienced executives' exercise of judgment.
As we shall see in chapters 3 and 4, so-called ''judgment" turns out to be mainly a non-numerical heuristic search that draws upon information stored in large expert memories. Today we have learned how to employ AI techniques in the form of so-called expert systems in a growing range of domains previously reserved for human expertise and judgment for example, medical diagnosis and credit evaluation. Moreover, while classical OR tools could only choose among predefined alternatives, AI expert systems are now being extended to the generation of alternatives, that is, to problems of design. More will be said about these developments in chapters 5 and 6.
Satisficing and Aspiration Levels
What a person cannot do he or she will not do, no matter how strong the urge to do it. In the face of real-world complexity, the business firm turns to procedures that find good enough answers to questions whose best answers are unknowable. Because real-world optimization, with or with-

Page 29
out computers, is impossible, the real economic actor is in fact a satisficer, a person who accepts "good enough" alternatives, not because less is preferred to more but because there is no choice.
Many economists, Milton Friedman being perhaps the most vocal, have argued that the gap between satisfactory and best is of no great importance, hence the unrealism of the assumption that the actors optimize does not matter; others, including myself, believe that it does matter, and matters a great deal.4 But reviewing this old argument would take me away from my main theme, which is to show how the behavior of an artificial system may be strongly influenced by the limits of its adaptive capacities its knowledge and computational powers.
One requirement of optimization not shared by satisficing is that all alternatives must be measurable in terms of a common utility function. A large body of evidence shows that human choices are not consistent and transitive, as they would be if a utility function existed.5 But even in a satisficing theory we need some criteria of satisfaction. What realistic measures of human profit, pleasure, happiness and satisfaction can serve in place of the discredited utility function?
Research findings on the psychology of choice, indicate some properties a thermometer of satisfaction should have. First, unlike the utility function, it is not limited to positive values, but has a zero point (of minimal contentment). Above zero, various degrees of satisfaction are experienced, and below zero, various degrees of dissatisfaction. Second, if periodic readings are taken of people in relatively stable life circumstances, we only occasionally find temperatures very far from zero in either direction, and the divergent measurements tend to regress over time back toward the zero mark. Most people consistently register either slightly below zero (mild discontent) or a little above (moderate satisfaction).
4. I have argued the case in numerous papers. Two recent examples are "Rationality in Psychology and Economics," The Journal of Business, 59(1986):S209 S224 (No. 4, Pt. 2); and "The State of Economic Science," in W. Sichel (ed.), The State of Economic Science (Kalamazoo, MI: W. E. Upjohn Institute for Employment Research, 1989). 5. See, for example, D. Kahneman and A. Tversky, "On the Psychology of Prediction," Psychological Review, 80(1973):237 251, and H. Kunreuther et al., Disaster Insurance Protection (New York: Wiley, 1978).

Page 30
To deal with these phenomena, psychology employs the concept of aspiration level. Aspirations have many dimensions: one can have aspirations for pleasant work, love, good food, travel, and many other things. For each dimension, expectations of the attainable define an aspiration level that is compared with the current level of achievement. If achievements exceed aspirations, satisfaction is recorded as positive; if aspirations exceed achievements, there is dissatisfaction. There is no simple mechanism for comparison between dimensions. In general a large gain along one dimension is required to compensate for a small loss along another hence the system's net satisfactions are history-dependent and it is difficult for people to balance compensatory offsets.
Aspiration levels provide a computational mechanism for satisficing. An alternative satisfices if it meets aspirations along all dimensions. If no such alternative is found, search is undertaken for new alternatives. Meanwhile, aspirations along one or more dimensions drift down gradually until a satisfactory new alternative is found or some existing alternative satisfices. A theory of choice employing these mechanisms acknowledges the limits on human computation and fits our empirical observations of human decision making far better than the utility maximization theory.6
Markets and Organizations
Economics has been concerned less with individual consumers or business firms than with larger artificial systems: the economy and its major components, markets. Markets aim to coordinate the decisions and behavior of multitudes of economic actors to guarantee that the quantity of brussels sprouts shipped to market bears some reasonable relation to the quantity that consumers will buy and eat, and that the price at which brussels sprouts can be sold bears a reasonable relation to the cost of producing them. Any society that is not a subsistence economy, but has
6. H. A. Simon, "A Behavioral Model of Rational Choice," Quarterly Journal of Economics, 6(1955):99 118; I. N. Gallhofer and W. E. Saris, Foreign Policy Decision-Making: A Qualitative and Quantitative Analysis of Political Argumentation (New York: Praeger, in press).

Page 31
substantial specialization and division of labor, needs mechanisms to perform this coordinative function.
Markets are only one, however, among the spectrum of mechanisms of coordination on which any society relies. For some purposes, central planning based on statistics provides the basis for coordinating behavior patterns. Highway planning, for example, relies on estimates of road usage that reflect statistically stable patterns of driving behavior. For other purposes, bargaining and negotiation may be used to coordinate individual behaviors, for instance, to secure wage agreements between employers and unions or to form legislative majorities. For still other coordinative functions, societies employ hierarchic organizations business, governmental and educational with lines of formal authority running from top to bottom and networks of communications lacing through the structure. Finally, for making certain important decisions and for selecting persons to occupy positions of public authority, societies employ a wide variety of balloting procedures.
Although all of these coordinating techniques can be found somewhere in almost any society, their mix and applications vary tremendously from one nation or culture to another.7 We ordinarily describe capitalist societies as depending mostly on markets for coordination and socialist societies as depending mostly on hierarchic organizations and planning, but this is a gross oversimplification, for it ignores the uses of voting in democratic societies of either kind, and it ignores the great importance of large organizations in modern "market" societies.
The economic units in capitalist societies are mostly business firms, which are themselves hierarchic organizations, some of enormous size, that make almost negligible use of markets in their internal functioning. Roughly eighty percent of the human economic activity in the American economy, usually regarded as almost the epitome of a "market" economy, takes place in the internal environments of business and other organizations and not in the external, between-organization environments of markets.8 To avoid misunderstanding, it would be appropriate to call such
7. R. A. Dahl and C. E. Lindblom, Politics, Economics, and Welfare (New York: Harper and Brothers, 1953). 8. H. A. Simon, "Organizations and Markets," Journal of Economic Perspectives, 5(1991):25 44.

Page 32
a society an organization-&-market economy; for in order to give an account of it we have to pay as much attention to organizations as to markets.
The Invisible Hand
In examining the processes of social coordination, economics has given top billing sometimes almost exclusive billing to the market mechanism. It is indeed a remarkable mechanism which under many circumstances can bring it about that the producing, consuming, buying and selling behaviors of enormous numbers of people, each responding only to personal selfish interests, allocate resources so as to clear markets do in fact nearly balance the production with the consumption of brussels sprouts and all the other commodities the economy produces and uses.
Only relatively weak conditions need be satisfied to bring about such an equilibrium. Achieving it mainly requires that prices drop in the face of an excess supply, and that quantities produced decline when prices are lowered or when inventories mount. Any number of dynamic systems can be formulated that have these properties, and these systems will seek equilibrium and oscillate stably around it over a wide range of conditions.
There have been many recent laboratory experiments on market behavior, sometimes with human subjects, sometimes with computer programs as simulated subjects.9 Experimental markets in which the simulated traders are "stupid" sellers, knowing only a minimum price below which they should not sell, and "stupid" buyers, knowing only a maximum price above which they should not buy move toward equilibrium almost as rapidly as markets whose agents are rational in the classical sense.10
Markets and Optimality
These findings undermine the much stronger claims that are made for the price mechanism by contemporary neoclassical economics. Claims that it does more than merely clear markets require the strong assumptions of perfect competition and of maximization of
9. V. L. Smith, Papers in Experimental Economics (New York: Cambridge University Press, 1991.) 10. D. J. Gode and S. Sunder, "Allocative Efficiency of Markets with Zero Intelligence Traders," Journal of Political Economy, 101(1993):119 127.

Page 33
profit or utility by the economic actors. With these assumptions, but not without them, the market equilibrium can be shown to be optimal in the sense that it could not be altered so as to make everyone simultaneously better off. These are the familiar propositions of Pareto optimality of competitive equilibrium that have been formalized so elegantly by Arrow, Debreu, Hurwicz, and others.11
The optimality theorems stretch credibility, so far as real-world markets are concerned, because they require substantive rationality of the kinds we found implausible in our examination of the theory of the firm. Markets populated by consumers and producers who satisfice instead of optimizing do not meet the conditions on which the theorems rest. But the experimental data on simulated markets show that market clearing, the only property of markets for which there is solid empirical evidence, can be achieved without the optimizing assumptions, hence also without claiming that markets do produce a Pareto optimum. As Samuel Johnson said of the dancing dog, "The marvel is not that it dances well, but that it dances at all "the marvel is not that markets optimize (they don't) but that they often clear.
Order Without a Planner
We have become accustomed to the idea that a natural system like the human body or an ecosystem regulates itself. This is in fact a favorite theme of the current discussion of complexity which we will take up in later chapters. We explain the regulation by feedback loops rather than a central planning and directing body. But somehow, untutored intuitions about self-regulation without central direction do not carry over to the artificial systems of human society. I retain vivid memories of the astonishment and disbelief expressed by the architecture students to whom I taught urban land economics many years ago when I pointed to medieval cities as marvelously patterned systems that had mostly just "grown" in response to myriads of individual human decisions. To my students a pattern implied a planner in whose mind it had been conceived and by whose hand it had been implemented. The idea that a city could acquire its pattern as naturally as a snowflake was
11. See Gerard Debreu, Theory of Value: An Axiomatic Analysis of Economic Equilibrium (New York: Wiley, 1959).

Page 34
foreign to them. They reacted to it as many Christian fundamentalists responded to Darwin: no design without a Designer!
Marxist fundamentalists reacted in a similar way when, after World War I, they undertook to construct the new socialist economies of eastern Europe. It took them some thirty years to realize that markets and prices might play a constructive role in socialist economies and might even have important advantages over central planning as tools for the allocation of resources. My sometime teacher, Oscar Lange, was one of the pioneers who carried this heretical notion to Poland after the Second World War and risked his career and his life for the idea.
With the collapse of the Eastern European economies around 1990 the simple faith in central planning was replaced in some influential minds by an equally simple faith in markets. The collapse taught that modern economies cannot function well without smoothly operating markets. The poor performance of these economies since the collapse has taught that they also cannot function well without effective organizations.
If we focus on the equilibrating functions of markets and put aside the illusions of Pareto optimality, market processes commend themselves primarily because they avoid placing on a central planning mechanism a burden of calculation that such a mechanism, however well buttressed by the largest computers, could not sustain. Markets appear to conserve information and calculation by assigning decisions to actors who can make them on the basis of information that is available to them locally that is, without knowing much about the rest of the economy apart from the prices and properties of the goods they are purchasing and the costs of the goods they are producing.
No one has characterized market mechanisms better than Friederich von Hayek who, in the decades after World War II, was their leading interpreter and defender. His defense did not rest primarily upon the supposed optimum attained by them but rather upon the limits of the inner environment the computational limits of human beings:12
The most significant fact about this system is the economy of knowledge with which it operates, or how little the individual participants need to know in order to be able to take the right action.
12. F. von Hayek, "The Use of Knowledge in Society," American Economic Review, 35(September 1945):519 30, at p. 520.

Page 35
The experiments on simulated markets, described earlier, confirm his view. At least under some circumstances, market traders using a very small amount of mostly local information and extremely simple (and non-optimizing) decision rules, can balance supply and demand and clear markets.
It is time now that we turn to the role of organizations in an organization-&market economy and the reasons why all economic activities are not left to market forces. In preparation for this topic, we need to look at the phenomena of uncertainty and expectations.
Uncertainty and Expectations
Because the consequences of many actions extend well into the future, correct prediction is essential for objectively rational choice. We need to know about changes in the natural environment: the weather that will affect next year's harvest. We need to know about changes in social and political environments beyond the economic: the civil warfare of Bosnia or Sri Lanka. We need to know about the future behaviors of other economic actors customers, competitors, suppliers which may be influenced in turn by our own behaviors.
In simple cases uncertainty arising from exogenous events can be handled by estimating the probabilities of these events, as insurance companies do but usually at a cost in computational complexity and information gathering. An alternative is to use feedback to correct for unexpected or incorrectly predicted events. Even if events are imperfectly anticipated and the response to them less than accurate, adaptive systems may remain stable in the face of severe jolts, their feedback controls bringing them back on course after each shock that displaces them. After we fail to predict the blizzard, snow plows still clear the streets. Although the presence of uncertainty does not make intelligent choice impossible, it places a premium on robust adaptive procedures instead of optimizing strategies that work well only when finely tuned to precisely known environments.13
13. A remarkable paper by Kenneth Arrow, reprinted in The New Palgrave: A Dictionary of Economics (London: Macmillan Press, 1987), v. 2, pp. 69 74, under the title of "Economic Theory and the Hypothesis of Rationality," shows that to preserve the Pareto optimality properties of markets when there is uncertainty
(footnote continued on next page)

Page 36
Expectations
A system can generally be steered more accurately if it uses feed forward, based on prediction of the future, in combination with feedback, to correct the errors of the past. However, forming expectations to deal with uncertainty creates its own problems. Feed forward can have unfortunate destabilizing effects, for a system can overreact to its predictions and go into unstable oscillations. Feed forward in markets can become especially destabilizing when each actor tries to anticipate the actions of the others (and hence their expectations).
The standard economic example of destabilizing expectations is the speculative bubble. Bubbles that ultimately burst are observed periodically in the world's markets (the Tulip Craze being one of many well-known historical examples). Moreover, bubbles and their bursts have now been observed in experimental markets, the overbidding occurring even though subjects know that the market must again fall to a certain level on a specified and not too distant date.
Of course not all speculation blows bubbles. Under many circumstances market speculation stabilizes the system, causing its fluctuations to become smaller, for the speculator attempts to notice when particular prices are above or below their "normal" or equilibrium levels in order to sell or buy, respectively. Such actions push the prices closer to equilibrium.
Sometimes, however, a rising price creates the expectation that it will go higher yet, hence induces buying rather than selling. There ensues a game of economic "chicken," all the players assuming that they can get out just before the crash occurs. There is general consensus in economics that destabilizing expectations play an important role in monetary hyperinflation and in the business cycle. There is less consensus as to whose expectations are the first movers in the chain of reactions or what to do about it.
The difficulties raised by mutual expectations appear wherever markets are not perfectly competitive. In perfect competition, each firm assumes that market prices cannot be affected by their actions: prices are as much a part of the external environment as are the laws of the physical world.
(footnote continued from previous page)
about the future, we must impose information and computational requirements on economic actors that are exceedingly burdensome and unrealistic.

Page 37
But in the world of imperfectly competitive markets, firms need not make this assumption. If, for example, there are only a few firms in an industry, each may try to outguess its competitors. If more than one plays this game, even the definition of rationality comes into question.
The Theory of Games
A century and a half ago, Augustin Cournot undertook to construct a theory of rational choice in markets involving two firms.14 He assumed that each firm, with limited cleverness, formed an expectation of its competitor's reaction to its actions, but that each carried the analysis only one move deep. But what if one of the firms, or both, tries to take into account the reactions to the reactions? They may be led into an infinite regress of outguessing.
A major step toward a clearer formulation of the problem was taken a century later, in 1944, when von Neumann and Morgenstern published The Theory of Games and Economic Behavior.15 But far from solving the problem, the theory of games demonstrated how intractable a task it is to prescribe optimally rational action in a multiperson situation where interests are opposed.
The difficulty of defining rationality exhibits itself well in the so-called Prisoners' Dilemma game.16 In the Prisoners' Dilemma, each player has a choice between two moves, one cooperative and one aggressive. If both choose the cooperative move, both receive a moderate reward. If one chooses the cooperative move, but the other the aggressive move, the co-operator is penalized severely while the aggressor receives a larger reward. If both choose the aggressive move, both receive lesser penalties. There is no obvious rational strategy. Each player will gain from cooperation if and only if the partner does not aggress, but each will gain even more from aggression if he can count on the partner to cooperate. Treachery pays, unless it is met with treachery. The mutually beneficial strategy is unstable.
14.Researches into the Mathematical Principles of the Theory of Wealth (New York: Augustus M. Kelley, 1960), first published in 1838. 15. Princeton: Princeton University Press, 1944. 16. R. D. Luce and H. Raiffa, Games and Decisions (New York: Wiley, 1957), pp. 94 102; R. M. Axelrod, The Evolution of Cooperation, (New York: Basic Books, 1984).

Page 38
Are matters improved by playing the game repetitively? Even in this case, cleverly timed treachery pays off, inducing instability in attempts at cooperation. However, in actual experiments with the game, it turns out that cooperative behavior occurs quite frequently, and that a tit-for-tat strategy (behave cooperatively until the other player aggresses; then aggress once but return to cooperation if the other player also does) almost always yields higher rewards than other strategies. Roy Radner has shown (personal communication) that if players are striving for a satisfactory payoff rather than an optimal payoff, the cooperative solution can be stable. Bounded rationality appears to produce better outcomes than unbounded rationality in this kind of competitive situation.
The Prisoners' Dilemma game, which has obvious real-world analogies in both politics and business, is only one of an unlimited number of games that illustrates the paradoxes of rationality wherever the goals of the different actors conflict totally or partially. Classical economics avoided these paradoxes by focusing upon the two situations (monopoly and perfect competition) where mutual expectations play no role.
Market institutions are workable (but not optimal) well beyond that range of situations precisely because the limits on human abilities to compute possible scenarios of complex interaction prevent an infinite regress of mutual outguessing. Game theory's most valuable contribution has been to show that rationality is effectively undefinable when competitive actors have unlimited computational capabilities for outguessing each other, but that the problem does not arise as acutely in a world, like the real world, of bounded rationality.
Rational Expectations
A different view from the one just expressed was for a time popular in economics: that the problem of mutual outguessing should be solved by assuming that economic actors form their expectations ''rationally."17 This is interpreted to mean that the actors know (and agree on) the laws that govern the economic system and that their predic-
17. The idea and the phrase "rational expectations" originated with J. F. Muth, "Rational Expectations and the Theory of Price Movements," Econometrica, 29(1961):315 335. The notion was picked up, developed, and applied systematically to macroeconomics by R. E. Lucas, Jr., E. C. Prescott, T. J. Sargent, and others.

Page 39
tions of the future are unbiased estimates of the equilibrium defined by these laws. These assumptions rule out most possibilities that speculation will be destabilizing.
Although the assumptions underlying rational expectations are empirical assumptions, almost no empirical evidence supports them, nor is it obvious in what sense they are "rational" (i.e., utility maximizing). Business firms, investors, or consumers do not possess even a fraction of the knowledge or the computational ability required for carrying out the rational expectations strategy. To do so, they would have to share a model of the economy and be able to compute its equilibrium.
Today, most rational expectationists are retreating to more realistic schemes of "adaptive expectations," in which actors gradually learn about their environments from the unfolding of events around them.18 But most approaches to adaptive expectations give up the idea of outguessing the market, and instead assume that the environment is a slowly changing "given" whose path will not be significantly affected by the decisions of any one actor.
In sum, our present understanding of the dynamics of real economic systems is grossly deficient. We are especially lacking in empirical information about how economic actors, with their bounded rationality, form expectations about the future and how they use such expectations in planning their own behavior. Economics could do worse than to return to the empirical methods proposed (and practiced) by George Katona for studying expectation formation,19 and to an important extent, the current interest in experimental economics represents such a return. In face of the current gaps in our empirical knowledge there is little empirical basis for choosing among the competing models currently proposed by economics to account for business cycles, and consequently, little rational basis for choosing among the competing policy recommendations that flow from those models.
18. T. J. Sargent, Bounded Rationality in Macroeconomics (Oxford: Clarendon Press, 1993). Note that Sargent even borrows the label of "bounded rationality" for his version of adaptive expectations, but, regrettably, does not borrow the empirical methods of direct observation and experimentation that would have to accompany it in order to validate the particular behavioral assumptions he makes. 19. G. Katona, Psychological Analysis of Economic Behavior (New York: McGraw-Hill, 1951).

Page 40
Business Organizations
We turn now to the great mass of economic activity that takes place within the internal environments of organizations. The key question here, one much discussed in "the new institutional economics" (NIE),20 is: what determines the boundary between organizations and markets; when will one be used, and when the other, to organize economic activity?
The Organization-Market Boundary
At the outset it should be observed that the boundary is often quite movable. For example, retail sales of automobiles are usually handled by dealerships, organizations with separate ownership from the manufacturers. Many other commodities are sold directly to consumers by manufacturers, and in some industries (e.g., fast foods) there is a combination of direct outlets and franchise agencies. The franchise is an excellent example of a hybrid species, as is the sole-source vendor who supplies raw materials or parts to a manufacturer.
We take the frequent movability or indefiniteness of organizational boundaries as evidence that often there is nearly a balance between the advantages of markets and organizations. Nevertheless we recall again the vast activity that takes place inside organizations, many of them very large, as an indication that in many circumstances they offer important advantages over markets.
The NIE explanation for sometimes preferring organizations to markets is that certain kinds of market contracts incur transaction costs that can be avoided or reduced by replacing the sales contract by an employment relation. On the other hand, as all economic actors are supposed by the NIE theory to be motivated by selfish interest, organizations incur the costs of rewarding their employees for following organizational goals instead of personal interest and of supervising them to see that they do so.21
This account of the relative advantages of the two institutions misses essential parts of the story, especially the opportunities for decentralization of decision making within organizations. These opportunities de-
20. O. E. Williamson, Markets and Hierarchies (New York: The Free Press, 1975). 21. O. E. Williamson, op. cit.; O. E. Williamson, The Economic Institutions of Capitalism (New York: The Free Press, 1985).

Page 41
pend, in turn, upon the strength of the loyalties of employees to their organizations, and their identification with organizational objectives that derives from loyalty and from the local informational environment in which they find themselves.
Decentralization
Organizations are not highly centralized structures in which all the important decisions are made at the center. Organizations operating in that centralized way would exceed the limits of human procedural rationality and lose many of the advantages attainable from the use of hierarchical authority. Real-world organizations behave quite differently.22
As a single decision may be influenced by a large number of facts and criteria of choice, some fraction of these premises may be specified by superiors without implying complete centralization. Organizations can localize and minimize information demands just as markets do, by decentralizing decisions. Matters of fact can be determined wherever the most skill and information is located to determine them, and they can then be communicated to "collecting points" where all the facts relevant to an issue can be put together and a decision reached. We can think of a decision as produced by executing a large computer program, each subroutine having its special tasks and relying on local sources of information. No single person or group need be expert on all aspects of the decision.
Thus business organizations, like markets, are vast distributed computers whose decision processes are substantially decentralized. The top level of a large corporation, which is typically subdivided into specialized product groups, will perform only a few functions, most often: (1) the "investment banking" function of allocating funds for capital projects, (2) selection of top executive personnel, and (3) long-range planning for capital funds and for possible new activities outside the scope of existing divisions.
Markets and organizations, however decentralized, are not fully equivalent in their effects. None of the theorems of optimality in resource
22. J. G. March and H. A. Simon, Organizations, 2nd ed. (Cambridge, MA: Blackwell, 1993).

Page 42
allocation that are provable for ideal competitive markets can be proved for hierarchies, but this does not mean that real organizations operate inefficiently as compared to real markets.
Externalities
Economists sometimes state the case for organizations as opposed to markets in terms of externalities. Externalities arise because the price mechanism works as advertised only when all of the inputs and outputs of an activity are subject to market pricing. A traditional example of an externality is a factory that is allowed to spew smoke from its stacks without compensating the surrounding homeowners. In these circumstances the price mechanism will not secure a socially desirable level of manufacturing activity; the product, priced below its social cost, will be overused.
The economist's preferred remedy for externalities is to bring the undesired consequences within the calculus of the price system: tax the emission of smoke, for example. This raises the question of how the tax is to be set. Although the techniques of cost-benefit analysis can provide answers, they are administrative answers and not answers given by an automatic market mechanism.
Similar questions of externalities among corporate divisional operations make large corporations less than fully willing to allow transactions among their component divisions and departments to be governed wholly by internal markets. In the absence of perfect competition, internal market prices are administered or negotiated prices, not competitive prices.
Uncertainty
Uncertainty often persuades social systems to use hierarchy rather than markets in making decisions. It is not reasonable to allow the production department and the marketing department in the widget company to make independent estimates of next year's demand for widgets if the production department is to make the widgets that the marketing department is to sell. In matters like this, and also in matters of product design, it may be preferable that all the relevant departments operate on the same body of assumptions even if (or perhaps "especially if") the uncertainties might justify quite a range of different assumptions. In facing uncertainty, standardization and coordination, achieved through agreed-upon assumptions and specifications, may be more effective than prediction.

Page 43
Uncertainty calls for flexibility, but markets do not always provide the greatest flexibility in the face of uncertainty. All depends on the sources of the uncertainty. If what is uncertain is a multitude of facts about individual and separate markets, then decentralized pricing will appear attractive; if the uncertainty encompasses major events that will affect many parts of the organization in the same direction, then it may be advantageous to centralize the making of assumptions about the future and to require the decentralized units to use these assumptions in their decisions.
Uncertainty is especially troublesome when it involves expectations by one unit about what other units in the same organization will do. Left to the market, this kind of uncertainty leads directly to the dilemmas of rationality that we described earlier in terms of game theory and rational expectations. Absorption of the uncertainty by the organization through managerial coordination may be the most effective course. We see in uncertainty a frequent source of advantage of organizations over markets as decision-making mechanisms.
In a world of bounded rationality there are several ways to magnify the computing capabilities of individual human beings and enhance the possibilities of their collective survival and prosperity. With the combined use of markets and administrative hierarchies, the human species has enormously increased its capabilities for specialization and division of work. It would be too much to attribute the vast growth and spread of human populations to such mechanisms alone modern medicine and modern technology have had something to do with it too but the (perhaps temporary) dominance of our species over the globe today is witness to the augmentation of human reason applied to local, not global, concerns that has been made possible by these social artifacts.
Organizational Loyalties and Identifications
Brief mention was made earlier of a crucial reason why so much human activity takes place within organizations: people acquire loyalty, and often a large amount of loyalty, to the groups, including organizations, to which they belong.
Consequences of Identification
Organizational loyalty is perhaps better labeled identification, for it is both motivational and cognitive. The motivational component is an attachment to group goals and a willingness to

Page 44
work for them even at some sacrifice of personal goals. (In effect, the group goals become personal goals.) The ethnic conflict we observe in many parts of the world provides vivid evidence of this attachment to group goals and the differential treatment it generates between "we" and "they."
Identification with an organization also has a cognitive component, for members are surrounded by information, conceptions and frames of reference quite different from those of people outside the organization or in a different organization. As creatures of bounded rationality, incapable of dealing with the world in all of its complexity, we form a simplified picture of the world, viewing it from our particular organizational vantage point and our organization's interests and goals.
This frame of reference and information provided by an organization influence strongly the processing and outcomes of decisions. The frame of reference varies, too, from one organization unit to another and from one level to another, so that an employee may identify at one time with his department, at another with his section, at another with the whole company.
Affected by their organizational identifications, members frequently pursue organizational goals at the expense of their own interests that is to say, behave in a way that is altruistic from a personal standpoint. No organization could survive that elicited only behavior for which employees felt selfishly rewarded and that supervisors could enforce. The added effort that is elicited by identification is a major and essential source of organizational effectiveness and is a principal reason for carrying out economic activities in organizations rather than markets.
Evolutionary Basis for Identification
It may be objected that human beings are basically selfish and do not behave in this altruistic fashion. In fact, neo-Darwinian evolutionary theory has generally claimed that altruism, except to close relatives, is inconsistent with the basic postulate that organisms evolve to increase their fitness.23 However, I should like to show that this widely repeated claim is mistaken.24
23. The case is stated, for example, in R. Dawkins, The Selfish Gene (New York: Oxford University Press, 1989). 24. H. A. Simon, "A Mechanism for Social Selection and Successful Altruism," Science, 250(1990):1665 1668.

Page 45
Because of their bounded rationality, and because they can therefore greatly enhance their limited knowledge and skill by accepting information and advice from the social groups to which they belong, individuals who are docile who tend to accept such information and advice have a great advantage in fitness over those who are not docile who reject social influence. Docile people do not have to learn about hot stoves by touching them.
Most social influence does enhance the fitness of the recipient. It provides information and advice about the world that is generally valid or at least much more informative and valid than the information the recipient could generate independently. But docility can be "taxed" by influencing people also to take certain actions that are not personally beneficial but are beneficial to the group. As long as the "taxation" is not so heavy as to cancel the advantages of docility, the altruistic individual will be fitter than the non-docile individual. By this means, the fitness of the organization will be enhanced by the docility, hence altruism, of its members. Although, docility is generally rewarding to the individual, some fraction of the behavior it induces is altruistic in this sense, and this altruism is an important factor in the efficacy of organizations.
We can summarize our account of the respective roles of markets and organizations in a modern society as follows: (1) organizations find their niches wherever constellations of interdependent activities are best carried out in coordinated fashion in order to remove the need for individuals' outguessing each other; (2) the human motivation that makes organizations viable and alleviates the public goods problems that arise when individual efforts cannot be tied closely to individual rewards is provided by organizational loyalty and identification; (3) in both organizations and markets, the bounds on human rationality are addressed by arranging decisions so that the steps in decision making can depend largely on information that is locally available to individuals.
The Evolutionary Model
Evolutionary processes are significant not only for explaining organizational loyalty, but also for describing and explaining the historical development of economic institutions, including business firms. The simplest

Page 46
scheme of evolution depends on two processes: a generator and a test. The generator produces variety, new forms that have not existed previously, whereas the test culls out the generated forms so that only those that are well fitted to the environment will survive. In modern biological Darwinism, genetic mutation and crossover of chromosomes are the principal generators, and natural selection is the test.
The Alternative Theory of Economic Man
No one supposes that a modern organization-&-market economy is the product of deliberate design. Surely it evolved from earlier subsistence economies, shaped by myriads of decisions made by hosts of actors over thousands of years. By contrast, most accounts of business firms assume that actors deliberately select actions appropriate to their goals within the context of the given economic environment. Adaptation, in the latter accounts, stems from selection by rational actors, not by natural selection of those actors whose behavior happens to be adaptive. An evolutionary theory of the firm might argue that it does not matter whether people maximize or satisfice, for in a world of competitive markets only those who make decisions as if they were maximizing will survive.25 Does this evolutionary argument in fact imply optimization?
Our discussion will have implications for biology as well as economics, for evolutionary biology uses the language of optimality quite freely and in recent years has even borrowed linear programming and other OR techniques to predict the outcomes of natural selection in biological systems. This is legitimate only if optimization would lead reliably to the same equilibria as would natural selection.
Local and Global Maxima
For the question before us, the difference between local and global maxima is crucial. In the landscape of California every tiny hill is a local maxi-
25. A. A. Alchian, "Uncertainty, Evolution, and Economic Theory," Journal of Political Economy, 58(1950):211 222; M. Friedman, "The Methodology of Positive Economics," in Essays in Positive Economics (Chicago: University of Chicago Press, 1953). The identification of selection with optimization is challenged by S. G. Winter, for example, in his "Economic Natural Selection and the Theory of the Firm," Yale Economic Essays, 4(1964):225 272.

Page 47
mum of altitude, but only Mt. Whitney is a global maximum. For many purposes it makes a difference whether one finds oneself standing on Nob Hill or Mt. Whitney. Finding a local maximum is usually easy: walk uphill until there is no place to walk. Finding the global maximum, on the other hand, is usually exceedingly complex unless the terrain has very special properties (no local maxima). The world of economic affairs is replete with local maxima. It is quite easy to devise systems in which each subsystem is optimally adapted to the other subsystems around it, but in which the equilibrium is only local, and quite inferior to distant equilibria that cannot be reached by the up-hill climb of evolution.
The Myopia of Evolution
Darwinian evolution is completely myopic. At each incremental step the evolving organism becomes fitter relative to its current environment, but there is no reason for the progress to lead to a global maximum of fitness of the individuals, separately or severally. If we are considering this kind of system, whose environment has a multitude of local maxima, we cannot understand the system unless we know something of the method and history of its evolution. Nor is there any reasonable sense in which such a system can be regarded as "fittest."
This is not just an in-principle objection to confounding hill climbing with optimization. In a myopic hill-climbing system, it may be difficult or impossible to move from a local maximum to another that is in view across a deep valley. The movement from the English system of measures to the metric system is a case in point. A society starting from scratch, and familiar with both systems, would surely prefer the metric to the English system. But if future benefits are discounted at some rate of interest, it might never be economical to switch from the one system, once adopted, to the other.
Hence, from the fact that an economic system is evolving, one cannot conclude that it has reached or is likely to reach a position that bears any resemblance to the equilibria found in the theory of perfect competition. Each species in the ecosystem is adapting to an environment of other species evolving simultaneously with it. The evolution and future of such systems can only be understood from a knowledge of their histories.

Page 48
The Mechanisms of Economic Evolution
If the adaptation of both the business firm and biological species to their respective environments are instances of heuristic search, hence of local optimization or satisficing, we still have to account for the mechanisms that bring the adaptation about. In biology the mechanism is located in the genes and their success in reproducing themselves. What is the gene's counterpart in the business firm?
Nelson and Winter suggest that business firms accomplish most of their work through standard operating procedures algorithms for making daily decisions that become routinized and are handed down from one generation of executives and employees to the next.26 Evolution derives from all the processes that produce innovation and change in these algorithms. The fitness test is the profitability and growth rate of the firm. Profitable firms grow by the reinvestment of their profits and their attractiveness for new investment.
Nelson and Winter observe that in economic evolution, in contrast to biological evolution, successful algorithms may be borrowed by one firm from another. Thus the hypothesized system is Lamarkian, because any new idea can be incorporated in operating procedures as soon as its success is observed, and hence successful mutations can be transferred between firms. Transfer is of course not costless, but involves learning costs for the adopting firm. It may also be impeded by patent protection and commercial secrecy. Nevertheless, processes of the kinds just described play a large role in the gradual evolution of an economic system composed of business firms.
From these considerations, one sees that the evolution of firms and of economies does not lead to any easily predictable equilibrium, much less an optimum, but is a complex process, probably continuing indefinitely, that is probably best understood through an examination of its history. As in any dynamic system that has propensities for following diverging paths from almost identical starting points, equilibrium theories of an economy can tell us little about either its present state or its future.
26. R. R. Nelson and S. G. Winter, An Evolutionary Theory of Economic Change (Cambridge: Harvard University Press, 1982).

Page 49
Human Society
Economics has been unfairly labeled the ''gloomy" science, for in its Ricardian form, incorporating Malthusian views of the pressure of population on resources, it did not hold out much hope for human progress. The label is unfair, because economics in fact draws a romantic, almost heroic, picture of the human mind. Classical economics depicts humankind, individually and collectively, as solving immensely complex problems of optimizing the allocation of resources. The artfulness of the economic actors enables them to make the very best adaptations in their environments to their wants and needs. In this chapter, while keeping the adaptive capabilities of mind in the center of things, I have tried to suggest a more complex state of affairs. A veridical picture of economic actors and institutions must incorporate the information processing limits set by their inner environments. The picture must also accommodate both the conscious rationality of economic decision makers and the unplanned but adaptive evolutionary processes that have molded economic institutions.
Operations research and artificial intelligence have enhanced the procedural rationality of economic actors, helping them to make better decisions. On a larger scale, markets and organizations are social schemes that facilitate coordinated behavior, at the same time conserving the critical scarce resource of human ability to handle complexity and great masses of information. In this chapter I have not tried to evaluate these forms of individual and social organization, but simply to describe them as commonly used solutions to the central human problem of accommodating to our bounded rationality.
The analysis shows that a deeper understanding of the tools of procedural rationality requires a closer examination of how the human mind works, of the limits on human rationality. The next two chapters will describe what has been learned in the past half century about human information processing. Chapter 3 will focus on problem solving processes and general cognitive architecture, chapter 4 on memory and learning processes.

Page 50

Page 51
3 The Psychology of Thinking: Embedding Artifice in Nature
We watch an ant make his laborious way across a wind- and wave-molded beach. He moves ahead, angles to the right to ease his climb up a steep dune let, detours around a pebble, stops for a moment to exchange information with a compatriot. Thus he makes his weaving, halting way back to his home. So as not to anthropomorphize about his purposes, I sketch the path on a piece of paper. It is a sequence of irregular, angular segments not quite a random walk, for it has an underlying sense of direction, of aiming toward a goal.
I show the unlabelled sketch to a friend. Whose path is it? An expert skier, perhaps, slaloming down a steep and somewhat rocky slope. Or a sloop, beating upwind in a channel dotted with islands or shoals. Perhaps it is a path in a more abstract space: the course of search of a student seeking the proof of a theorem in geometry.
Whoever made the path, and in whatever space, why is it not straight; why does it not aim directly from its starting point to its goal? In the case of the ant (and for that matter the others) we know the answer. He has a general sense of where home lies, but he cannot foresee all the obstacles between. He must adapt his course repeatedly to the difficulties he encounters and often detour uncrossable barriers. His horizons are very close, so that he deals with each obstacle as he comes to it; he probes for ways around or over it, without much thought for future obstacles. It is easy to trap him into deep detours.
Viewed as a geometric figure, the ant's path is irregular, complex, hard to describe. But its complexity is really a complexity in the surface of the beach, not a complexity in the ant. On that same beach another small

Page 52
creature with a home at the same place as the ant might well follow a very similar path.
Many years ago Grey Walter built an electromechanical "turtle," having only tactile sense of its environment but capable of exploring a room, and periodically seeking its nest to recharge its batteries.1 Today, robots with modest visual sensory capabilities roam about in a number of artificial intelligence alaboratories.2 Suppose we undertook to design an automaton with the approximate dimensions of an ant, similar means of locomotion, and comparable sensory acuity. Suppose we provided it with a few simple adaptive capabilities: when faced with a steep slope, try climbing it obliquely; when faced with an insuperable obstacle, try detouring; and so on. (Except for problems of miniaturization of components, the present state of the art would readily support such a design.) How different would its behavior be from the behavior of the ant?
These speculations suggest a hypothesis, one that could as well have been derived as corollary from our previous discussion of artificial objects:
An ant, viewed as a behaving system, is quite simple. The apparent complexity of its behavior over time is largely a reflection of the complexity of the environment in which it finds itself.
We may find this hypothesis initially plausible or implausible. It is an empirical hypothesis, to be tested by seeing whether attributing quite simple properties to the ant's adaptive system will permit us to account for its behavior in the given or similar environments. For the reasons developed at length in the first chapter, the truth or falsity of the hypothesis should be independent of whether ants, viewed more microscopically, are simple or complex systems. At the level of cells or molecules ants are demonstrably complex, but these microscopic details of the inner environment may be largely irrelevant to the ant's behavior in relation to the outer
1. W. Grey Walter, "An Imitation of Life," Scientific American, 185(1950):42. 2. See, for example, R. Brooks, "A Robust-layered Control System for a Mobile Robot," IEEE Journal of Robotics and Automation, RA-2(1986):14 23. And a motor vehicle, NAVLAB, steered itself in the Summer of 1995 on public highways from Washington, D.C., to San Diego, California, and has also demonstrated strong capabilities for off-road navigation.

Page 53
environment. That is why an automaton, though completely different at the microscopic level, might nevertheless simulate the ant's gross behavior.
In this chapter I should like to explore this hypothesis but with the word "human being" substituted for "ant."
Human beings, viewed as behaving systems, are quite simple. The apparent complexity of our behavior over time is largely a reflection of the complexity of the environment in which we find ourselves.
Now I should like to hedge my bets a little. Instead of trying to consider the "whole person," fully equipped with glands and viscera, I should like to limit the discussion to Homo sapiens, "thinking person." I myself believe that the hypothesis holds even for the whole person, but it may be more prudent to divide the difficulties at the outset, and analyze only cognition rather than behavior in general.3
I should also like to hedge my bets in a second way, for a human being can store away in memory a great furniture of information that can be evoked by appropriate stimuli. Hence I would like to view this information-packed memory less as part of the organism than as part of the environment to which it adapts.
The reasons for assigning some a priori probability to the hypothesis of simplicity have already been set forth in the last two chapters. A thinking human being is an adaptive system; men's goals define the interface between their inner and outer environments, including in the latter their memory stores. To the extent that they are effectively adaptive, their behavior will reflect characteristics largely of the outer environment (in the light of their goals) and will reveal only a few limiting properties of the inner environment of the physiological machinery that enables a person to think.
3. I have sketched an extension of this hypothesis to phenomena of emotion and motivation in "Motivational and Emotional Controls of Cognition," Psychological Review, 74(1967):29 39, and to certain aspects of perception in "An Information-Processing Explanation of Some Perceptual Phenomena," British Journal of Psychology, 58(1967):1 12. Both papers are reprinted in my Models of Thought, vol. 1 (1979), chapters 1.3 and 6.1. The discussion of these issues is continued in ''Bottleneck of Attention: Connecting Thought with Motivation," in W. D. Spaulding (ed.), Integrative Views of Motivation, Cognition and Emotion. Lincoln, NE: University of Nebraska Press, 1994.

Page 54
I do not intend to repeat this theoretical argument at length, but rather I want to seek empirical verification for it in the realm of human thought processes. Specifically I should like to point to evidence that there are only a few "intrinsic" characteristics of the inner environment of thinking beings that limit the adaptation of thought to the shape of the problem environment. All else in thinking and problem-solving behavior is artificial is learned and is subject to improvement through the invention of improved designs and their storage in memory.
Psychology As a Science of the Artificial
Problem solving is often described as a search through a vast maze of possibilities, a maze that describes the environment. Successful problem solving involves searching the maze selectively and reducing it to manageable proportions. Let us take, by way of specific example, a puzzle of the kind known as crypt arithmetic problems:4
The task is to replace the letters in this array by numerals, from zero through nine, so that all instances of the same letter are replaced by the same numeral, different letters are replaced by different numerals, and the resulting numerical array is a correctly worked out problem in arithmetic. As an additional hint for this particular problem, the letter D is to be replaced by the numeral 5.
One way of viewing this task is to consider all the 10!, ten factorial, ways in which ten numerals can be assigned to ten letters. The number 10! is not so large as to strike awe in the heart of a modern computer; it is only a little more than 3 million (3,628,800, to be exact). A program designed to generate all possible assignments systematically, and requiring
4. The crypt arithmetic task was first used for research on problem solving by F. Bartlett in his Thinking (New York: Basic Books, 1958). In the present account I have drawn on his work and on my research with Allen Newell reported in our book, Human Problem Solving (Englewood Cliffs, N.J.: Prentice-Hall, 1972), chapters 8 10.

Page 55
a tenth of a second to generate and test each, would require at most about ten hours to do the job. (With the cue D = 5, only an hour would be needed.) I haven't written the program, but a tenth of a second is far longer than a computer would need to examine each possibility.
There is no evidence that a human being could do this. It might take a man as long as a minute to generate and test each assignment, and he would have great difficulty in keeping track of where he was and what assignments he had already tried. He could use paper and pencil to assist him on the latter score, but that would slow him down even more. The task, performed in this way, might call for several man-years of work I assume a forty-hour week.
Notice that in excluding exhaustive, systematic search as a possible way for a human to solve the problem, we are making only very gross assumptions about human capabilities. We are assuming that simple arithmetic operations take times that are of the order of seconds, that the operations are essentially executed serially, rather than in parallel, and that large amounts of memory are not available in which new information can be stored at split-second speeds. These assumptions say something, but not very much, about the physiology of the human central nervous system. For example, modifying the brain by incorporating in it a new subsystem with all the properties of a desk calculator would be a quite remarkable feat of brain surgery or evolution. But even such a radical alteration would change the relevant assumptions only slightly for purposes of explaining or predicting behavior in this problem environment.
Human beings do frequently solve the DONALD + GERALD = ROBERT problem. How do they do it? What are the alternative ways of representing the environment and conducting the search?
Search Strategies
One way to cut down the search drastically is to make the assignments systematically, as before, but to assign numerals to the letters one by one so that inconsistencies can be detected before an assignment is complete, and hence whole classes of possible assignments can be ruled out at one step. Let me illustrate how this works.
Suppose we start from the right, trying assignments successively for the letters D, T, L, R, A, E, N, B, O, and G, and substituting numerals in the

Page 56
order 1, 2, 3, 4, 5, 6, 7, 8, 9, 0. We already know that D = 5, so we strike 5 from the list of available numerals. We now try T = 1. Checking in the right-hand column, we detect a contradiction, for D + D = T + c, where c is 10 or 0. Hence, since (D = 5, T = 1) is not feasible, we can rule out all the remaining 8! assignments of the eight remaining numerals to the eight remaining letters. In the same way all possible assignments for T, except T = 0, can be ruled out without considering the assignments for the remaining letters.
The scheme can be improved further by the expedient of calculating directly, by addition, what assignment should be made to the sum of a column whenever the two addends are known. With this improvement we shall not need to search for the assignment for T, for T = 0 can be inferred directly from D = 5. Using this scheme, the DONALD + GERALD = ROBERT problem can be solved quite readily, with paper and pencil. Ten minutes should suffice. Figure 3 shows the search tree, in slightly simplified form. Each branch is carried to the point where a contradiction is detected. For example, after the assignments (D = 5, T = 0), the assignment L = 1 leads to the inference R = 3, which yields a contradiction since from the left-hand column of the problem array R = 3 would imply that G is negative.
Figure 3 is oversimplified in one respect. Each of the branches that terminates with a contradiction after assignment of a value to E should actually be branched one step further. For the contradiction in these cases arises from observing that no assignment for the letter O is now consistent. In each case four assignments must be examined to determine this. Thus the full search tree would have 68 branches still a far cry from 10! or even 9!.
An enormous space has been cut down to a quite small space by some relatively small departures from systematic, exhaustive search. It must be confessed that the departures are not all as simple as I have made them appear. One step in the proposed scheme requires finding the contradictions implied by an assignment. This means of course the "relatively direct" contradictions, for if we had a rapid process capable of detecting all inconsistent implications, direct or indirect, it would find the problem solution almost at once. In this problem any set of assignments other than the single correct one implies a contradiction.

Page 57
Figure 3 Possible search tree for DONALD + GERALD = ROBERT
What is meant by searching for direct contradictions is something like this: after a new assignment has been made, those columns are examined where the newly substituted letter occurs. Each such column is solved, if possible, for a stillunassigned letter, and the solution checked to see whether this numeral remains unassigned. If not, there is a contradiction. In place of brute-force search we have now substituted a combined system of search and "reason." Can we carry this process further; can we eliminate substantially all trial-and-error search from the solution method? It turns out that we can for this problem, although not for all crypt arithmetic problems.5 The basic idea that permits us to eliminate most trial-and-error search in solving the problem before us is to depart from the systematic right-to-left assignment of numerals. Instead we search for columns of the
5. For example, the method to be described does not eliminate as much search from the crypt arithmetic problem CROSS + ROADS = DANGER.

Page 58
problem array that are sufficiently determinate to allow us to make new assignments, or at least new inferences about the properties of assignments.
Let me go through the process briefly. From D = 5, we immediately infer T = 0, as before. We also infer that 1 is carried into the second column, hence that R = 2L + 1 is odd. On the extreme left, from D = 5, we infer that R is greater than 5 (for R = 5 + G). Putting together these two inferences, we have R = 7 or R = 9, but we do not try these assignments. Now we discover that the second column from the left has the peculiar structure O + E = Oa number plus another equals itself (apart from what is carried into or out of the column). Mathematical knowledge, or experiment, tells us that this can be true only if E = 0 or E = 9. Since we already have T = 0, it follows that E = 9. This eliminates one of the alternatives for R, so R = 7.
Since E = 9, it follows that A = 4, and there must be a one carried into the third column from the right; hence 2L + 1 = 17, or L = 8. All that remains now is to assign 1, 2, 3, and 6 in some order to N, B, O, and G. We get G = 1 by observing that for any assignment of O there is a number carried into the leftmost column. We are now left with only 3! = 6 possibilities, which we may be willing to eliminate by trial and error: N = 6, B = 3, and therefore O = 2.
We have traced a solution path through the problem maze on three different assumptions about the search strategy. The more sophisticated, in a certain sense, that strategy became, the less search was required. But it is important to notice that, once the strategy was selected, the course of the search depended only on the structure of the problem, not on any characteristics of the problem solver. By watching a person, or an automaton, perform in this problem environment, what could we learn about him? We might well be able to infer what strategy was followed. By the mistakes made, and the success in recovering from them, we might be able to detect certain limits of the capacity or accuracy of the individual's memory and elementary processes. We might learn something about the speed of these processes. Under favorable circumstances, we might be able to learn which among the thinkable strategies the individual was able actually to acquire and under what circumstances likely to acquire them. We should certainly be unlikely to learn anything specific about the neurological characteristics of the central nervous system, nor would the spe-

Page 59
cifics of that system be relevant to his behavior, beyond placing bounds on the possible.
The Limits on Performance
Let us undertake to state in positive fashion just what we think these bounds and limits are, as revealed by behavior in problem situations like this one. In doing so, we shall draw upon both experimental evidence and evidence derived from computer simulations of human performance. The evidence refers to a variety of cognitive tasks, ranging from relatively complex ones (crypt arithmetic, chess, theorem proving), through an intermediate one (concept attainment), to simple ones that have been favorites of the psychological laboratory (rote verbal learning, short-term memory span). It is important that with this great variety of performance only a small number of limits on the adaptability of the inner system reveal themselves and these are essentially the same limits over all the tasks. Thus the statement of what these limits are purports to provide a single, consistent explanation of human performance over this whole range of heterogeneous task environments.
Limits on Speed of Concept Attainment
Extensive psychological research has been carried out on concept attainment within the following general paradigm.6 The stimuli are a set of cards bearing simple geometric designs that vary, from card to card, along a number of dimensions: shape (square, triangle, circle), color, size, position of figure on card, and so on. A "concept" is defined extensionally by some set of cards the cards that are instances of that concept. The concept is defined intensionally by a property that all the instances have in common but that is not possessed by any of the remaining cards.
6. This account of concept attainment is based on the paper with my late colleague Lee Gregg, "Process Models and Stochastic Theories of Simple Concept Formation," Journal of Mathematical Psychology, 4(June 1967):246 276. See also A. Newell and H. A. Simon, "Overview: Memory and Process in Concept Formation," chapter 11 in B. Kleinmuntz (ed.), Concepts and the Structure of Memory (New York: Wiley, 1967), pp. 241 262. The former paper is reprinted in Models of Thought, vol. 1, chapter 5.4.

Page 60
Examples of concepts are "yellow" or "square" (simple concepts), "green triangle" or "large, red" (conjunctive concepts), ''small or yellow" (disjunctive concept), and so on.
In our discussion here I shall refer to experiments using an N-dimensional stimulus, with two possible values on each dimension, and with a single relevant dimension (simple concepts). On each trial an instance (positive or negative) is presented to the subject, who responds "Positive" or "Negative" and is reinforced by "Right" or "Wrong," as the case may be. In typical experiments of this kind, the subject's behavior is reported in terms of number of trials or number of erroneous responses before an error-free performance is attained. Some, but not all, experiments ask the subject also to report periodically the intensional concept (if any) being used as a basis for the responses.
The situation is so simple that, as in the crypt arithmetic problem, we can estimate a priori how many trials, on the average, a subject should need to discover the intended concept provided that the subject used the most efficient discovery strategy. On each trial, regardless of response, the subject can determine from the experimenters reinforcement whether the stimulus was actually an instance of the concept or not. If it was an instance, the subject knows that one of the attribute values of the stimulus its color, size, shape, for example defines the concept. If it was not an instance, the subject knows that the complement of one of its attribute values defines the concept. In either case each trial rules out half of the possible simple concepts; and in a random sequence of stimuli each new stimulus rules out, on the average, approximately half of the concepts not previously eliminated. Hence the average number of trials required to find the right concept will vary with the logarithm of the number of dimensions in the stimulus.
If sufficient time were allowed for each trial (a minute, say, to be generous), and if the subject were provided with paper and pencil, any subject of normal intelligence could be taught to follow this most efficient strategy and would do so without much difficulty. As these experiments are actually run, subjects are not instructed in an efficient strategy, are not provided with paper and pencil, and take only a short time typically four seconds, say to respond to each successive stimulus. They also use

Page 61
many more trials to discover the correct concept than the number calculated from the efficient strategy. Although the experiment has not, to my knowledge, been run, it is fairly certain that, even with training, a subject who was required to respond in four seconds and not allowed paper and pencil would be unable to apply the efficient strategy.
What do these experiments tell us about human thinking? First, they tell us that human beings do not always discover for themselves clever strategies that they could readily be taught (watching a chess master play a duffer should also convince us of that). This is hardly a very startling conclusion, although it may be an instructive one. I shall return to it in a moment.
Second, the experiments tell us that human beings do not have sufficient means for storing information in memory to enable them to apply the efficient strategy unless the presentation of stimuli is greatly slowed down or the subjects are permitted external memory aids, or both. Since we know from other evidence that human beings have virtually unlimited semi-permanent storage (as indicated by their ability to continue to store odd facts in memory over most of a lifetime), the bottleneck in the experiment must lie in the small amount of rapid-access storage (so-called short-term memory) available and the time required to move items from the limited short-term store to the large-scale long-term store.7
From evidence obtained in other experiments, it has been estimated that only some seven items can be held in the fast, short-term memory and that perhaps as many as five to ten seconds are required to transfer an item from the short-term to the long-term store. To make these statements operational, we shall have to be more precise, presently, about the meaning of "item." For the moment let us assume that a simple concept is an item.
Even without paper and pencil a subject might be expected to apply the efficient strategy if (1) he was instructed in the efficient strategy and
7. The monograph by J. S. Bruner, J. J. Goodnow, and G. A. Austin, A Study of Thinking (New York: Wiley, 1956) was perhaps the first work to emphasize the role of short-term memory limits (their term was "cognitive strain") in performance on concept-attainment tasks. That work also provided rather definite descriptions of some of the subjects' strategies.

Page 62
(2) he was allowed twenty or thirty seconds to respond to and process the stimulus on each trial. Since I have not run the experiment, this-assertion stands as a prediction by which the theory may be tested.
Again the outcome may appear obvious to you, if not trivial. If so, I remind you that it is obvious only if you accept my general hypothesis: that in large part human goal-directed behavior simply reflects the shape of the environment in which it takes place; only a gross knowledge of the characteristics of the human information-processing system is needed to predict it. In this experiment the relevant characteristics appear to be (1) the capacity of short-term memory, measured in terms of number of items (or "chunks," as I shall call them); (2) the time required to fixate an item, or chunk, in long-term memory. In the next section I shall inquire as to how consistent these characteristics appear to be over a range of task environments. Before I do so, I want to make a concluding comment about subjects' knowledge of strategies and the effects of training subjects.
That strategies can be learned is hardly a surprising fact, nor that learned strategies can vastly alter performance and enhance its effectiveness. All educational institutions are erected on these premises. Their full implication has not always been drawn by psychologists who conduct experiments in cognition. Insofar as behavior is a function of learned technique rather than "innate" characteristics of the human information-processing system, our knowledge of behavior must be regarded as sociological in nature rather than psychological that is, as revealing what human beings in fact learn when they grow up in a particular social environment. When and how they learn particular things may be a difficult question, but we must not confuse learned strategies with built-in properties of the underlying biological system.
The data that have been gathered, by Bartlett and in our own laboratory, on the crypt arithmetic task illustrate the same point. Different subjects do indeed apply different strategies in that task both the whole range of strategies I sketched in the previous section and others as well. How they learned these, or how they discover them while performing the task, we do not fully know (see chapter 4), although we know that the sophistication of the strategy varies directly with a subject's previous exposure to and comfort with mathematics. But apart from the strategies

Page 63
the only human characteristic that exhibits itself strongly in the crypt arithmetic task is the limited size of short-term memory. Most of the difficulties the subjects have in executing the more combinatorial strategies (and perhaps their general aversion to these strategies also) stem from the stress that such strategies place on short-term memory. Subjects get into trouble simply because they forget where they are, what assignments they have made previously, and what assumptions are implicit in assignments they have made conditionally. All of these difficulties would necessarily arise in a processor that could hold only a few chunks in short-term memory and that required more time than was available to transfer them to long-term memory.
The Parameters of Memory Eight Seconds per Chunk
If a few parameters of the sort we have been discussing are the main limits of the inner system that reveal themselves in human cognitive behavior, then it becomes an important task for experimental psychology to estimate the values of these parameters and to determine how variable or constant they are among different subjects and over different tasks.
Apart from some areas of sensory psychology, the typical experimental paradigms in psychology are concerned with hypothesis testing rather than parameter estimating. In the reports of experiments one can find many assertions that a particular parameter value is or is not "significantly different" from another but very little comment on the values themselves. As a matter of fact the pernicious practice is sometimes followed of reporting significance levels, or results of the analysis of variance, without reporting at all the numerical values of the parameters that underlie these inferences.
While I am objecting to publication practices in experimental psychology, I shall add another complaint. Typically little care is taken in choosing measures of behavior that are the most relevant to theory. Thus in learning experiments "rate of learning" is reported, almost indifferently, in terms of "number of trials to criterion," "total number of errors," "total time to criterion," and perhaps other measures as well. Specifically the practice of reporting learning rates in terms of trials rather than time, prevalent through the first half of this century, and almost up to the

Page 64
present time, not only hid from view the remarkable constancy of the parameter I am about to discuss but also led to much meaningless dispute over "one-trial" versus "incremental" learning.8
Ebbinghaus knew better. In his classic experiments on learning nonsense syllables, with himself as subject, he recorded both the number of repetitions and the amount of time required to learn sequences of syllables of different length. If you take the trouble to calculate it, you find that the time per syllable in his experiments works out to about ten to twelve seconds.9
I see no point in computing the figure to two decimal places or even to one. The constancy here is a constancy to an order of magnitude, or perhaps to a factor of two more nearly comparable to the constancy of the daily temperature, which in most places stays between 263° and 333° Kelvin, than to the constancy of the speed of light. There is no reason to be disdainful of a constancy to a factor of two. Newton's original estimates of the speed of sound contained a fudge factor of 30 per cent (eliminated only a hundred years later), and today some of the newer physical "constants" for elementary particles are even more vague. Beneath any approximate, even very rough, constancy, we can usually expect to find a genuine parameter whose value can be defined accurately once we know what conditions we must control during measurement.
If the constancy simply reflected a parameter of Ebbinghausalbeit one that held steady over several years it would be more interesting to biography than psychology. But that is not the case. When we examine some of the HullHovland experiments of the 1930s, as reported, for ex-
8. The evidence of the constancy of the fixation parameter is reviewed in L. W. Gregg and H. A. Simon, "An Information-Processing Explanation of One-Trial and Incremental Learning," Journal of Verbal Learning and Verbal Behavior, 6(1967):780 787; H. A. Simon and E. A. Feigenbaum, "An Information-Processing Theory of Verbal Learning," ibid., 3(1964):385 396; Feigenbaum and Simon, "A Theory of the Serial Position Effect," British Journal of Psychology, 53(1962):307 320; E. A. Feigenbaum, "An Information-Processing Theory of Verbal Learning," unpublished doctoral dissertation, Pittsburgh: Carnegie Institute of Technology, 1959; and references cited therein. All these papers save the last are reprinted in Models of Thought, vol. 1 9. Herman Ebbinghaus, Memory (New York: Dover Publications, 1964), translated from the German edition of 1885, especially pp. 35 36, 40, 51.

Page 65
ample, in Carl Hovland's chapter in S. S. Stevens's Handbook, we find again (after we calculate them, for trials are reported instead of times) times in the neighborhood of ten or fifteen seconds for college sophomores to fixate nonsense syllables of low meaningfulness by the serial anticipation method. When the drum speed increases (say from four seconds per syllable to two seconds per syllable), the number of trials to criterion increase proportionately, but the total learning time remains essentially constant.
There is a great deal of gold in these hills. If past nonsense-syllable experiments are re-examined from this point of view, many are revealed where the basic learning parameter is in the neighborhood of fifteen seconds per syllable. You can make the calculation yourself from the experiments reported, for example in J. A. McGeoch's Psychology of Human Learning. B. R. Bugelski, however, seems to have been the first to make this parameter constancy a matter of public record and to have run experiments with the direct aim of establishing it.10
I have tried not to exaggerate how constant is "constant." On the other hand, efforts to purify the parameter measurement have hardly begun. We do know about several variables that have a major effect on the value, and we have a theoretical explanation of these effects that thus far has held up well.
We know that meaningfulness is a variable of great importance. Nonsense syllables of high association value and unrelated one-syllable words are learned in about one-third the time required for nonsense syllables of low association value. Continuous prose is learned in about one-third the time per word required for sequences of unrelated words. (We can get the latter figure also from Ebbinghaus' experiments in memorizing Doll Juan. The times per symbol are roughly 10 percent of the corresponding times for nonsense syllables.)
We know that similarity particularly similarity among stimuli has an effect on the fixation parameter somewhat less than the effect of meaningfulness, and we can also estimate its magnitude on theoretical grounds.
10. B. R. Bugelski, "Presentation Time, Total Time, and Mediation in Paired-Associate Learning," Journal of Experimental Psychology, 63(1962):409 412.

Page 66
The theory that has been most successful in explaining these and other phenomena reported in the literature on rote verbal learning is an informationprocessing theory, programmed as a computer simulation of human behavior, dubbed EPAM.11 Since EPAM has been reported at length in the literature, I shall not discuss it here, except for one point that is relevant to our analysis. The EPAM theory gives us a basis for understanding what a "chunk" is. A chunk is a maximal familiar substructure of the stimulus. Thus a nonsense syllable like "QUV" consists of the chunks "Q," "U," "V,''; but the word "CAT" consists of a single chunk, since it is a highly familiar unit. EPAM postulates constancy in the time required to fixate a chunk. Empirically the constant appears to be about eight seconds per chunk, or perhaps a little more. Virtually all the quantitative predictions that EPAM makes about the effects of meaningfulness, familiarity, and similarity upon learning speed follow from this conception of the chunk and of the constancy of the time required to fixate a single chunk.
In fixation of new information, EPAM first adds new branches to its discrimination net then adds information to images at terminal nodes of the branches. There is growing evidence that the eight seconds for fixation in longterm memory is required only for expanding the net, and that information can be added in a second or two to locations (variable-places) in images that are already present in an expert's long-term memory. Such images are called retrieval structures or templates. We will return to this point in discussing expert memory. EPAM's architecture and memory processes are described in H. B. Richman, J. J. Staszewski and H. A. Simon, "Simulation of Expert Memory Using EPAM IV," Psychological Review, 102 (1995):305 330.
The Parameters of Memory Seven Chunks, or Is It Two?
The second limiting property of the inner system that shows up again and again in learning and problem-solving experiments is the amount of
11. For a survey of the range of phenomena for which EPAM has been tested, see E. A. Feigenbaum and H. A. Simon, "EPAM-like Models of Recognition and Learning" Cognitive Science, 8(1984): 305 336, reprinted in Models of Thought, vol. 2 (1989), chapter 3.4.

Page 67
information that can be held in short-term memory. Here again the relevant unit appears to be the chunk, where this term has exactly the same meaning as in the definition of the fixation constant.
Attention was attracted to this parameter, known previously from digit span, numerosity-judging, and discrimination tasks, by George Miller's justly celebrated paper on "The Magical Number Seven, Plus or Minus Two."12 It is no longer as plausible as it was when he wrote his paper that a single parameter is involved in the three kinds of task, rather than three different parameters: we shall consider here only tasks of the digit-span variety. Today we would express the parameter as the amount of information that can be rehearsed in about two seconds, which is, in fact, about seven syllables or short words.
The facts that appear to emerge from recent experiments on short-term memory are these. If asked to read a string of digits or letters and simply to repeat them back, a subject can generally perform correctly on strings up to seven or even ten items in length. If almost any other task, however simple, is interposed between the subject's hearing the items and repeating them, the number retained drops to two. From their familiarity in daily life we could dub these numbers the "telephone directory constants." We can generally retain seven numbers from directory to phone if we are not interrupted in any way not even by our own thoughts.
Where experiments appear to show that more than two chunks are retained across an interruption, the phenomena can almost always be explained parsimoniously by mechanisms we have already discussed in the previous section. In some of these experiments the explanation as already pointed out by Milleris that the subject recodes the stimulus into a smaller number of chunks before storing it in short-term memory. If ten items can be recoded as two chunks, then ten items can be retained. In the other experiments where "too much" appears to be retained in short-term memory, the times allowed the subjects permit them in fact to fixate the excess of items in long-term memory. For experts who have acquired retrieval structures or templates in their domain of expertise into which the new information can be inserted, these times can be quite short a second or two per item.
12.Psychological Review, 63(1956):81 97.

Page 68
Putting aside expert performance for the moment, I shall cite just two examples from the literature. N. C. Waugh and D. A. Norman report experiments, their own and others', that show that only the first two of a sequence of items is retained reliably across interruption, but with some residual retention of the remaining items.13 Computation of the fixation times available to the subjects in these experiments shows that a transfer rate to long-term memory of one chunk per five seconds would explain most of the residuals. (This explanation is entirely consistent with the theoretical model that Waugh and Norman themselves propose.)
Roger Shepard has reported that subjects shown a very long sequence of photographs mostly landscape scan remember which of these they have seen (when asked to choose from a large set) with high reliability.14 When we note that the task is a recognition task, requiring storage only of differentiating cues, and that the average time per item was about six seconds, the phenomenon becomes entirely understandable indeed predictable within the framework of the theory that we are proposing.
The Organization of Memory
I have by no means exhausted the list of experiments I could cite in support of the fixation parameter and the short-term capacity parameter and in support of the hypothesis that these parameters are the principal, and almost only, characteristics of the information-processing system that are revealed, or could be revealed, by these standard psychological experiments.
This does not imply that there are not other parameters, and that we cannot find experiments in which they are revealed and from which they can be estimated. What it does imply is that we should not look for great complexity in the laws governing human behavior, in situations where the behavior is truly simple and only its environment is complex.
In our laboratory we have found that mental arithmetic tasks, for instance, provide a useful environment for teasing out other possible pa-
13. N. C. Waugh and D. A. Norman, "Primary Memory," Psychological Review, 72(1965):89 104. 14. Roger N. Shepard, "Recognition Memory for Words, Sentences, and Pictures," Journal of Verbal Learning and Verbal Behavior, 6(1957):156 163.

Page 69
rameters. Work that Dansereau has carried forward shows that the times required for elementary arithmetic operations and for fixation of intermediate results account for only part perhaps one-half of the total time for performing mental multiplications of four digits by two. Much of the remaining time appears to be devoted to retrieving numbers from the memory where they have been temporarily fixated, and "placing" them in position in short-term memory where they can be operated upon.15
Stimulus Chunking
I should like now to point to another kind of characteristic of the inner system more "structural" and also less quantitative that is revealed in certain experiments. Memory is generally conceived to be organized in an "associative" fashion, but it is less clear just what that term is supposed to mean. One thing it means is revealed by McLean and Gregg. They gave subjects lists to learn specifically 24 letters of the alphabet in scrambled order. They encouraged, or induced, chunking of the lists by presenting the letters either one at a time, or three, four, six, or eight on a single card. In all of the grouped conditions, subjects learned in about half the time required in the one-at-a-time condition.16
McLean and Gregg also sought to ascertain whether the learned sequence was stored in memory as a single long list or as a hierarchized list of chunks, each of which was a shorter list. They determined this by measuring how subjects grouped items temporally when they recited the list, and especially when they recited it backwards. The results were clear: the alphabets were stored as sequences of short sub sequences; the sub sequences tended to correspond to chunks presented by the experimenter, or sub lengths of those chunks; left to his own devices, the subject tended to prefer chunks of three or four letters. (Recall the role of chunks of this length in the experiments on effects of meaningfulness in rote learning.)
15. See Donald F. Dansereau and Lee W. Gregg, "An Information Processing Analysis of Mental Multiplication," Psychonomic Science, 6(1966):71 72. The parameters of memory are discussed in more detail in Models of Thought, vol. 1, chapters 2.2, and 2.3; and vol. 2, chapter 2.4; and in Richman, Staszewski and Simon, op. cit. 16. R. S. McLean and L. W. Gregg, "Effects of Induced Chunking on Temporal Aspects of Serial Recitation," Journal of Experimental Psychology, 74(1967): 455 459.

Page 70
Visual Memory
The materials in the McLean-Gregg experiments were strings of symbols. We might raise similar questions regarding the form of storage of information about two-dimensional visual stimuli.17 In what sense do memory and thinking represent the visual characteristics of stimuli? I do not wish to revive the debate on "imageless thought "certainly not in the original form that debate took. But perhaps the issue can now be made more operational than it was at the turn of the century.
As I enter into this dangerous ground, I am comforted by the thought that even the most fervent opponents of mentalism have preceded me. I quote, for example, from B. F. Skinner's Science and Human Behavior (1952, p. 266):
A man may see or hear "stimuli which are not present" on the pattern of the conditioned reflexes: he may see X, not only when X is present, but when any stimulus which has frequently accompanied X is present. The dinner bell not only makes our mouth water, it makes us see food.
I do not know exactly what Professor Skinner meant by "seeing food," but his statement gives me courage to say what an information-processing theory might mean by it. I shall describe in a simplified form one kind of experiment that has been used to throw light on the question. Suppose we allow a subject to memorize the following visual stimulus a magic square:
4 9 2
3 5 7
8 1 6
Now we remove the stimulus and ask the subject a series of questions about it, timing his or her answers. What numeral lies to the right of 3, to the right of 1? What numeral lies just below 5? What numeral is diagonally above and to the right of 3? The questions are not all of the same difficulty in fact I have arranged them in order of increasing difficulty
17. The letters in the stimuli of the McLean-Gregg experiment are, of course, also two-dimensional visual stimuli. Since they are familiar chunks, however, and can be immediately recognized and recoded, there is no reason to suppose that their two-dimensional character plays any role in the subject's behavior in the experiment. Again this is "obvious" but only if we already have a general theory of how stimuli are processed "inside."

Page 71
and would expect a subject to take substantially longer to answer the last question than the first.
Why should this be? If the image stored in memory were isomorphic to a photograph of the stimulus, we should expect no large differences in the times required to answer the different questions. We must conclude that the stored image is organized quite differently from a photograph. An alternative hypothesis is that it is a list structure a hypothesis that is consistent, for example, with the data from the McLean-Gregg experiment and that is much in the spirit of information-processing models of cognition.
For example, if what was stored were a list of lists: "TOP," "MIDDLE," "BOTTOM," where "TOP" is 4-9-2, ''MIDDLE" is 3-5-7, and "BOTTOM" is 81-6; the empirical results would be easy to understand. The question "What numeral lies to the right of 3?" is answered by searching down lists. The question "What numeral lies just below 5?" is answered, on the other hand, by matching two lists, item by item a far more complex process than the previous one.
There is no doubt, of course, that a subject could learn the up-down relations or the diagonal relations as well as the left-right relations. An EPAM-like theory would predict that it would take the subject about twice as long to learn both leftright and up-down relations as the former alone. This hypothesis can be easily tested, but, to the best of my knowledge, it has not been.
Evidence about the nature of the storage of "visual" images, pointing in the same direction as the example I have just given, is provided by the well-known experiments of A. de Groot and others on chess perception.18 De Groot put chess positions taken from actual games before subjects for, say, five seconds; then he removed the positions and asked the subjects to reconstruct them. Chess grandmasters and masters could reconstruct the positions (with perhaps 20 to 24 pieces on the board) almost without error, while duffers were able to locate hardly any of the pieces correctly, and the performance of players of intermediate skill fell somewhere
18. Adriaan D. de Groot, "Perception and Memory versus Thought: Some Old Ideas and Recent Findings," in B. Kleinmuntz (ed.), Problem Solving (New York: Wiles; 1966), pp. 19 50. See also the work by Chase and Simon reported in chapters 6.4 and 6.5 of Models of Thought vol. 1.

Page 72
between masters and duffers. But the remarkable fact was that, when masters and grandmasters were shown other chessboards with the same numbers of pieces arranged at random, their abilities to reconstruct the boards were only marginally better than the duffers' with the boards from actual games, while the duffers performed as well or poorly as they had before.
What conclusion shall we draw from the experiment? The data are inconsistent with the hypothesis that the chess masters have some special gift of visual imagery or else why the deterioration of their performance? What the data suggest strongly is that the information about the board is stored in the form of relations among the pieces, rather than a "television scan" of the 64 squares. It is inconsistent with the parameters proposed earlier seven chunks in short-term memory and five seconds to fixate a chunk to suppose that anyone, even a grandmaster, can store 64 pieces of information (or 24) in ten seconds. It is quite plausible that he can store (in short-term and long-term memory) information about enough relations (supposing each one to be a familiar chunk) to permit him to reproduce the board of figure 4:
1. Black has castled on the K's side, with a fianchettoed K's bishop defending the K's Knight.
2. White has castled on the Q's side, with his Queen standing just before his King.
3. A Black pawn on his K5 and a White pawn on his Q5 are attacked and defended by their respective K's and Q's Knights, the White Queen also attacking the Black pawn on the diagonal.
4. White's Q-Bishop attacks the Knight from KN5.
5. The Black Queen attacks the White K's position from her QN3.
6. A Black pawn stands on its QB4.
7. A White pawn on K3 blocks that advance of the opposing Black pawn.
8. Each side has lost a pawn and a Knight.
9. White's K-Bishop stands on K2.
Pieces not mentioned are assumed to be in their starting positions. Since some of the relations as listed are complex, I shall have to provide reasons for considering them unitary "chunks." I think most strong chess players would regard them as such. Incidentally I wrote down these relations from my own memory of the position, in the order in which they

Page 73
Figure 4 Chess position used in memory experiment
occurred to me. Eye-movement data for an expert chess player looking at this position tend to support this analysis of how the relations are analyzed and stored.19 The eye-movement data exhibit with especial clarity the relations 3 and 5. The expert can store the information about the position even more rapidly if he or she recognizes the standard opening to which it belongs in this case, the Gruenfeld Defense there by accessing a familiar template that gives the positions of about a dozen pieces. The implication of this discussion of visual memory for my main theme is that many of the phenomena of visualization do not depend in any
19. O. K. Tikhomirov and E. D. Poznyanskaya, "An Investigation of Visual Search as a Means of Analysing Heuristics," English translation from Voprosy psikhologii, 1966, vol. 12, in Soviet Psychology, 2(Winter 1966 1967):3 15. See also Models of Thought, vol. 1, chapters 6.2 and 6.3.

Page 74
detailed way upon underlying neurology but can be explained and predicted on the basis of quite general and abstract features of the organization of memory features which are essentially the same ones that were postulated in order to build information-processing theories of rote learning and of concept attainment phenomena.
Specifically, we are led to the hypothesis that memory is an organization of list structures (lists whose components can also be lists), which include descriptive components (two-termed relations) and short (three-element or four-element) component lists. A memory with this form of organization appears to have the right properties to explain storage phenomena in both visual and auditory modalities, and of pictorial and diagrammatic as well as propositional (verbal and mathematical) information.
The Mind's Eye
The experiments we have been discussing relate not only to visual long-term memory, but also to the Mind's Eye, the short-term memory where we hold and process mental images. In the mind's eye we can often substitute "seeing" for reasoning. Consider the economist's common supply-and-demand diagram, which shows, by one curve, the quantity of a commodity that will be supplied to the market at each price, and by another curve, the quantity that will be demanded at each price. If we notice that the two curves intersect, we can interpret the intersection as the point at which the supply and demand quantities are equal, a point of market equilibrium; and we can read off directly from the xaxis and y-axis of the diagram the equilibrium quantity and price (the x and y coordinates of the intersection). All this processing goes on in the mind's eye, using the information read from the diagram.
Alternatively, we could write down the equations for the two lines and solve them simultaneously to find the same equilibrium quantity and price. Using visual processes and algebraic ones we attain the same knowledge, but by completely different computational paths (and perhaps with vastly different amounts of labor and insight). In many scientific fields, inferences are made with a combination of verbal, mathematical and diagrammatic reasoning certain inferences being reached more easily in one form, others in another. In Alfred Marshall's famous

Page 75
text book, Principles of Economics, the text is wholly verbal, the diagrams are provided in footnotes, and the corresponding algebra is given in a mathematical appendix, thus allowing readers full freedom to adopt their preferred representation in each instance.
To understand the interplay of these and other modes of human inference, we need to study the computational processes required to reach conclusions in each representation. Currently, this is a very active area of cognitive research.20
Processing Natural Language
A theory of human thinking cannot and should not avoid reference to that most characteristic cognitive skill of human beings the use of language. How does language fit into the general picture of cognitive processes that I have been sketching and into my general thesis that psychology is a science of the artificial?
Historically the modern theory of transformational linguistics and the information-processing theory of cognition were born in the same matrix the matrix of ideas produced by the development of the modern digital computer, and in the realization that, though the computer was embodied in hardware, its soul was a program. One of the initial professional papers on transformational linguistics and one of the initial professional papers on information-processing psychology were presented, the one after the other, at a meeting at MIT in September 1956.21 Thus the two bodies of theory have had cordial relations from an early date, and quite rightly, for they rest conceptually on the same view of the human mind.
20. J. Larkin and H. A. Simon, "Why a Diagram is (Sometimes) Worth 10,000 Words," Cognitive Science, 11(1987):65 100; A. M. Leonardo, H. J. M. Tabachneck and H. A. Simon, "A Computational Model of Diagram Reading and Reasoning" Proceedings of the 17th Annual Conference of the Cognitive Science Society (1995); Y. Qin and H. A. Simon, "Imagery and Mental Models of Problem Solving," in J. Glasgow, N. H. Narayanan and B. Chandrasekaran (eds.), Diagrammatic Reasoning: Computational and Cognitive Perspectives (Menlo Park, CA: AAAI/The MIT Press, 1995). 21. N. Chomsky, "Three Models for the Description of Language," and A. Newell and H. A. Simon, "The Logic Theory Machine," both in IRE Transactions on Information Theory, IT-2, no. 3 (September 1956).

Page 76
Now some may object that this is not correct and that they rest on almost diametrically opposed views of the human mind. For I have stressed the artificial character of human thinking how it adapts itself, through individual learning and social transmission of knowledge, to the requirements of the task environment. The leading exponents of the formal linguistic theories, on the other hand, have taken what is sometimes called a "nativist" position. They have argued that a child could never acquire any skill so complex as speaking and understanding language if he did not already have built into him at birth the basic machinery for the exercise of these skills.
The issue is reminiscent of the debate on language universals on whether there are some common characteristics shared by all known tongues. We know that the commonalities among languages are not in any sense specific but that they relate instead to very broad structural characteristics that all languages seem to share in some manner. Something like the distinction between noun and verb between object and action or relation appears to be present in all human languages. All languages appear to have the boxes-within-boxes character called phrase structure. All languages appear to derive certain strings from others by transformation.22
Now if we accept these as typical of the universals to which the nativist argument appeals, there are still at least two different possible interpretations of that argument. The one is that the language competence is purely linguistic, that language is sui generis, and that the human faculties it calls upon are not all employed also in other performances.
An alternative interpretation of the nativist position is that producing utterances and understanding the utterances of others depend on some characteristics of the human central nervous system which are common in all languages but also essential to other aspects of human thinking besides speech and listening.
The former interpretation does not, but the latter does, provide an explanation for the remarkable parallelism holding between the underlying
22. On language universals see Joseph H. Greenberg (ed.), Universals of Language (Cambridge: The MIT Press, 1963), particularly Greenberg's own chapter, pp. 58 90. On the "nativist" position, see Jerrold J. Katz, The Philosophy of Language (New York: Harper and Row, 1966), pp. 240 282.

Page 77
assumptions about human capabilities embedded in modern linguistic theory and the assumptions embedded in information-processing theories of human thinking. The kinds of assumptions that I made earlier about the structure of human memory are just the kinds of assumptions one would want to make for a processing system capable of handling language. Indeed there has been extensive borrowing back and forth between the two fields. Both postulate hierarchically organized list structures as a basic principle of memory organization. Both are concerned with how a serially operating processor can convert strings of symbols into list structures or list structures into strings. In both fields the same general classes of computer-programming languages have proved convenient for modeling and simulating the phenomena.
Semantics in Language Processing
Let me suggest one way in which the relation between linguistic theories and information-processing theories of thinking is going to be even closer in the future than it was in the past. Linguistic theory has thus far been largely a theory of syntax, of grammar. In practical application to such tasks as automatic translation, it has encountered difficulties when translation depended on more than syntactic cues when it depended on context and meaning. It seems pretty clear that one of the major directions that progress in linguistics will have to take is toward development of an adequate semantics to complement syntax.
The theory of thinking I have been outlining can already provide an important part of such a semantic component. The principles of memory organization I have described can be used as a basis for discussing the internal representation of both linguistic strings and two-dimensional visual stimuli, or other non-linguistic stimuli. Given these comparable bases for the organization of the several kinds of stimuli, it becomes easier to conceptualize the cooperation of syntactic and semantic cues in the interpretation of language.
Several research projects have been carried out at Carnegie Mellon University that bear on this point. I should like to mention just two of these, which illustrate how this approach might be used to explain the resolution of syntactic ambiguities by use of semantic cues.

Page 78
L. Stephen Coles, in a dissertation completed in 1967, described a computer program that uses pictures on a cathode ray tube to resolve syntactic ambiguities.23 I shall paraphrase his procedure with an example that is easier to visualize than any he actually used. Consider the sentence:
I saw the man on the hill with the telescope.
This sentence has at least three acceptable interpretations; a linguist could, no doubt, discover others. Which of the three obvious ones we pick depends on where we think the telescope is: Do I have it? Does the man on the hill have it? Or is it simply on the hill, not in his hands?
Now suppose that the sentence is accompanied by figure 5. The issue is no longer in doubt. Clearly it is I who have the telescope. Coles's program is capable of recognizing objects in a picture and relations among objects; and it is capable of representing the picture as a list structure, which, in the example before us, we might describe thus:
SAW ((I, WITH (telescope)), (man, ON (hill))).
I have not tried to reproduce the actual details of the scheme he used, but I have simply shown that a picture, so represented, could readily be matched against alternate parsings of a verbal string and thus used to resolve the ambiguity of the latter.
Another program, completed by Laurent Siklóssy, illustrates how semantic information can aid in the acquisition of a language.24 The reader may be familiar with the "Language through Pictures" books developed by Professor I. A. Richards and his associates. These books have been prepared for a large number of languages. On each page is a picture and beneath it one or more sentences that say something about the picture in the language to be learned. The sequence of pictures and accompanying sentences is arranged to proceed from very simple situations ("I am here," "That is a man") to more complex ones (''The book is on the shelf"). Siklóssy's program takes as its input an analogue to one of the "Language through Pictures" books. The picture is assumed to have already
23. L. Stephen Coles, Syntax Directed Interpretation of Natural Language, doctoral dissertation, Carnegie Institute of Technology, 1967. A slightly abridged version is reprinted in H.A. Simon and L. Siklóssy (eds.), Representation and Meaning (Englewood Cliffs, N.J.: Prentice-Hall, 1972). 24. Also reprinted in Representation and Meaning.

Page 79
Figure 5 A syntactically ambiguous sentence; "I saw the man on the hill with the telescope"
been transformed into a list structure (not unlike the one illustrated earlier for Coles's system) as its internal representation. The program's task is to learn, when confronted with such a picture, to utter the appropriate sentence in the natural language it is learning a sentence that says what the picture shows. In the case of the sentence about the telescope (somewhat more complicated than any on which the scheme has actually been tested), one would hope that the program would respond to the picture with "I saw the man on the hill with the telescope," if it were learning English, or Ich habe den Mann auf dem Berg mit dem Fernglas gesehen, if it were learning German. Of course the program could respond correctly only if it had learned earlier, in the context of other sentences, the lexical and syntactical components required for the translation. A child trying to understand the sentence must meet the same requirement. In other cases the program would use the sentence associated with the picture to add to its vocabulary and syntax.25
25. I may mention in passing that Siklóssy's system refutes John Searle's notorious "Chinese Room Paradox," which purports to prove that a computer cannot understand language. As Siklóssy's program shows, if the room has windows on the world (which Searle's room doesn't) the system matches words, phrases and sentences to their meanings by comparing sentences with the scenes they denote.

Page 80
I do not wish to expand some pioneering experiments into a comprehensive theory of semantics. The point of these examples is that they show that the kind of memory structure that has been postulated, for other reasons, to explain human behavior in simpler cognitive tasks is suitable for explaining how linguistic strings might be represented internally, how other kinds of stimuli might be similarly represented, and how the communalities in representation the use of hierarchically organized list structures for both may explain how language and "meanings" come together in the human head.
There is no contradiction, then, between the thesis that a human being possesses, at birth, a competence for acquiring and using language and the thesis that language is the most artificial, hence also the most human of all human constructions. The former thesis is an assertion that there is an inner environment and that it does place limits on the kinds of information processing of which the organism is capable. The structure of language reveals these limits; and these limits in turn account for such commonality as exists among the Babel of human tongues.
The latter thesis, of the artificiality of language, is an assertion that the limits on adaptation, on possible languages, imposed by the inner environment are very broad limits on organization, not very specific limits on syntax. Moreover, according to the thesis, they are limits imposed not only on language but also on every other mode of representing internally experience received through stimuli from outside.
Such a view of the relation of language and thinking puts a new cast on the "Whorfian" hypothesis that stating it in over strong form only the expressible is thinkable. If the view is valid, it would be as correct to say. "Only the thinkable is expressible "a view that, I suppose, Kant would have found quite congenial.
Conclusion
The thesis with which I began this chapter was the following:
Human beings viewed as behaving systems, are quite simple. The apparent complexity of our behavior over time is largely a reflection of the complexity of the environment in which we find ourselves.

Page 81
That hypothesis was based in turn on the thesis of the first chapter: that behavior is adapted to goals, hence is artificial, hence reveals only those characteristics of the behaving system that limit the adaptation.
To illustrate how we have begun to test these theses and at the same time to build up a theory of the simple principles that underlie human behavior, I have surveyed some of the evidence from a range of human performances, particularly those that have been studied in the psychological laboratory.
The behavior of human subjects in solving crypt arithmetic problems, in attaining concepts, in memorizing, in holding information in short-term memory, in processing visual stimuli, and in performing tasks that use natural languages provides strong support for these theses. The artificiality hence variability of human behavior hardly calls for evidence beyond our observation of everyday life. The experiments are therefore mostly significant in what they show about the broad commonalities in organizations of the human information-processing system as it engages in different tasks.
The evidence is overwhelming that the system is basically serial in its operation: that it can process only a few symbols at a time and that the symbols being processed must be held in special, limited memory structures whose content can be changed rapidly. The most striking limits on subjects' capacities to employ efficient strategies arise from the very small capacity of the short-term memory structure (seven chunks) and from the relatively long time (eight seconds) required to transfer a chunk of information from short-term to long-term memory.
The claim that the human cognitive system is basically serial has been challenged in recent years by advocates of neural nets and parallel connectionist models of the nervous system. I would make the following cautionary observations. Although there is clearly a lot of parallelism in the sensory organs (especially eyes and ears), after stimuli have been recognized seriality is enforced by the small capacity of the short-term memory that is employed in the subsequent stages of processing. There is also a moderate degree of parallelism in the processing of motor signals, but again, only after the initial signals have passed through the STM bottleneck. Third, seriality of processing at the symbolic level, the level with which we are concerned here, says nothing, one way or the other, about

Page 82
the extent of seriality or parallelism in the neural implementation of the symbolic processing at the next level below. (By an ironic reverse twist, parallel connectionist networks are routinely simulated by programs run on serial computers of standard von Neumann architecture.)
Finally, a large part of the discernible parallel neural activity in the brain may well consist only in passive maintenance of memory, the active processes being largely localized and serial. (Evidence now coming from magnetic resonance imaging [MRI] of the brain is consistent with this view.) The speeds at which people can perform cognitive tasks and the usual limits on the numbers of tasks they can perform concurrently do not provide much evidence for (or need for) parallel processing capacity. Until connectionism has demonstrated, which it has not yet done, that complex thinking and problem-solving processes can be modeled as well with parallel connectionist architectures as they have been with serial architectures, and that the experimentally observed limits on concurrent cognitive activity can be represented in the connectionist models, the case for massive parallelism outside the sensory functions remains dubious.
When we turn from tasks that exercise mainly the short-term memory and serialprocessing capabilities of the central nervous system to tasks that involve retrieval of stored information, we encounter new limits of adaptation, and through these limits we acquire new information about the organization of mind and brain. Studies of visual perception and of tasks requiring use of natural language show with growing clarity that memory is indeed organized in associative fashion, but that the "associations" have the properties of what, in the computer trade, are usually called "list structures." I have indicated briefly what those properties are, and more will be said about them in the next chapter.
These are the sorts of generalizations about human thinking that are emerging from the experimental evidence. They are simple things, just as our hypothesis led us to expect. Moreover, though the picture will continue to be enlarged and clarified, we should not expect it to become essentially more complex. Only human pride argues that the apparent intricacies of our path stem from a quite different source than the intricacy of the ant's path.
One of the curious consequences of my approach of my thesis is that I have said almost nothing about physiology. But the mind is usually

Page 83
thought to be located in the brain. I have discussed the organization of the mind without saying anything about the structure of the brain.
The main reason for this disembodiment of mind is of course the thesis that I have just been discussing. The difference between the hardware of a computer and the "hardware" of the brain has not prevented computers from simulating a wide spectrum of kinds of human thinking just because both computer and brain, when engaged in thought, are adaptive systems, seeking to mold themselves to the shape of the task environment.
It would be unfortunate if this conclusion were altered to read that neurophysiology has nothing to contribute to the explanation of human behavior. That would be of course a ridiculous doctrine. But our analysis of the artificial leads us to a particular view of the form that the physiological explanation of behavior must take. Neurophysiology is the study of the inner environment of the adaptive system called Homo sapiens. It is to physiology that we must turn for an explanation of the limits of adaptation: Why is short-term memory limited to seven chunks; what is the physiological structure that corresponds to a "chunk"; what goes on during the eight seconds that a chunk is being fixated; how are associational structures realized in the brain?
As our knowledge increases, the relation between physiological and informationprocessing explanations will become just like the relation between quantummechanical and physiological explanations in biology (or the relation between solid-state physics and programming explanations in computer science). They constitute two linked levels of explanation with (in the case before us) the limiting properties of the inner system showing up at the interface between them.
Finally, we may expect also that, as we link information-processing psychology to physiology on the inner side, we shall also be linking psychology to the general theory of search through large combinatorial spaces on the outer side the side of the task environment. But that is the topic of my fifth chapter, for the theory of design is that general theory of search. Before we take up that topic we must say more about how the large bodies of information used by designers are stored in the human mind and accessed.

Page 84

Page 85
4 Remembering and Learning: Memory As Environment for Thought
In developing the proposition in chapter 3 that human thought processes are simple, the cards were perhaps stacked by the examples selected to illustrate the thesis. A task like DONALD + GERALD = ROBERT is difficult enough for an intelligent adult, but it does not call on much information stored in one's memory. The solver must know the numbers, how to add and subtract them, and perhaps a few facts about parity, but that is about all. Contrast this with the task of driving a taxi in Pittsburgh or in the East Bay. No amount of intelligence will take the cab driver from here to there unless he has stored in memory an enormous amount of information about the names of streets, their locations, and mutual intersections. (The street index of my Pittsburgh atlas contains about 8,500 entries.) If this information is available in memory, however, choosing a route probably does not call for a very complex strategy.1
The hypothesis that human thought processes are simple emerged from the information-processing research of the 1950s and 1960s. Most of that research employed puzzle like tasks, similar to the crypt arithmetic problems and concept attainment tasks discussed in the last chapter, which could be performed without great dependence on memory or skills previously learned. Additional examples are the Missionaries and Cannibals puzzle, the Tower of Hanoi puzzle, and problems of logical inference, all of which have been studied extensively in the psychological laboratory
1. I believe this is true, but it is not obvious. Exercise for the reader: write a computer program that, given the street map of an area and some knowledge of which streets are trunk routes, will choose a reasonable path to deliver a passenger from one point to another.

Page 86
and support the picture of human thinking that was drawn in the last chapter.
It is reasonable that research on human thinking should begin with relatively content less tasks of these kinds but not that it should end there. And so in recent decades research in both cognitive psychology and artificial intelligence has been turning more and more to semantically rich domains domains that have substantial, meaningful content, where skillful performance calls upon large amounts of specialized knowledge retrieved from memory. Does human thinking still look simple in such domains?
In pursuing this question, we shall be interested especially in high-level performance of the kinds of tasks that confront professionals in their everyday work or college students who are preparing for professional practice. Among the professional-level domains that have been studied fairly extensively in the laboratory, and hence some of whose parameters are known, are chess playing, making medical diagnoses, solving college physics problems, and discovering regularities in empirical data. We will use these and others as examples.
Except for chess playing, long-term memory played only a modest role in performance of the tasks examined in chapter 3. The simplicity we discovered there was largely a simplicity of process (only a few basic symbol manipulating processes had to be postulated to account for the behavior) and a simplicity of the architecture of the mind (its seriality and its limited short-term memory). A few parameters, especially the chunk capacity of STM and the storage time for new chunks in LTM, played a dominant role in fixing the limits of the system's performance.
As we move to semantically rich domains, new questions of simplicity and complexity arise. Does the richness of the contents of long-term memory imply complexity of structure, or can that richness be accommodated by the simple organizations of list structures that were described briefly in chapter 3? Is a higher level of complexity required for programs that exploit these large stores of memory, or are the same processes in evidence as those that account for problem solving in the puzzle like tasks of chapter 3? Do the learning programs required to store new data and processes in long-term memory introduce new levels of complexity? We will see that the evidence from studies of human performance and from

Page 87
its simulation by computer generally supports the hypothesis of simplicity. More memory does not necessarily mean more complexity.
Semantically Rich Domains
There is a certain arbitrariness in drawing the boundary between inner and outer environments of artificial systems. In our discussion of economic behavior in chapter 2, we might well have considered the business firm's cost function to be part of the inner environment. Instead we abstracted the decision-making process from the production technology and regarded only the limits on rational calculation as inner constraints on adaptivity. The cost function was treated, along with the demand function, as part of the outer environment to which the firm was seeking to adapt.
We can adopt a similar viewpoint toward the human problem solver, whose basic tool for solving problems is a small repertory of information processes of the sorts described in the last chapter. This processor operates on an outer environment that has two major components: the "real world," sensed through eye, ear, and touch, and acted upon by leg, hand, and tongue, and a large store of (correct and incorrect) information about that world, held in long-term memory and retrievable by recognition or by association. When the processor is solving puzzle like problems, the memory plays a limited role. The structure of the problem rather than the organization of memory steers the problem-solving search. When it is solving problems in semantically rich domains, a large part of the problem-solving search takes place in long-term memory and is guided by information discovered in that memory. Hence an explanation of problem solving in such domains must rest on an adequate theory of memory.
Long-Term Memory
Certain facts about human long-term memory (LTM) were set forth in the last chapter. It is of essentially unlimited size no one appears ever to have been able to fill his memory to overflowing, although in senility new items cannot be stored. About eight seconds are required to store a new chunk in LTM, except when an expert has an already stored template with which the chunk can be associated, in which case only a second or

Page 88
two is required."2 A rather shorter time (a few hundred milliseconds to a couple of seconds) is needed to retrieve information previously stored. The memory is usually described as "associative" because of the way in which one thought retrieved from it leads to another. Information is stored in linked list structures.
In terms of our present-day knowledge of LTM, we can extend this description a bit. We can think of the memory as a large encyclopedia or library, the information stored by topics (nodes), liberally cross-referenced (associational links), and with an elaborate index (recognition capability) that gives direct access through multiple entries to the topics. Long-term memory operates like a second environment, parallel to the environment sensed through eyes and ears, through which the problem solver can search and to whose contents he can respond.
Medical diagnosis is a semantically rich domain that has now been investigated extensively, with the aims both of understanding the diagnostic processes used by physicians and of building systems for diagnosis by computer. The thickness of medical textbooks and reference books attests to the large volume of information that is required for accurate diagnosis. When the diagnostic strategies of physicians are studied, two kinds of processes are prominent in their thinking-aloud protocols: processes of direct recognition, where presence of a symptom leads almost immediately to hypothesizing a disease that might be its cause, and processes of search quite like those identified in the simpler problemsolving tasks that were described in chapter 3.3 The diagnosis generally proceeds from symptoms to hypothesized disease entities, to tests for resolving doubts and weeding out alternatives, to new symptoms, and so on. Thus the search is conducted alternately in each of two environments: the physician's mental library of medical knowledge and the patient's body. Information gleaned from one environment is used to guide the next step of search in the other.
2. But see the discussion of retrieval structures in chapter 3. 3. Arthur Elstein et al., Medical Problem Solving (Cambridge, Mass.: Harvard University Press, 1978). There is now on the market a fully automated diagnostic system for internal medicine, Doctor's Assistant, that is based largely on this model of the diagnostic process and that has performed well in clinical trials.

