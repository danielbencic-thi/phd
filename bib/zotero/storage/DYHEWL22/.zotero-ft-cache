IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Typesetting math: 100%

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Conferences > 2021 International Conference...
Quadrotor UAV 3D Path Planning with Optical-Flow-based Obstacle Avoidance
Publisher: IEEE
Cite This
PDF
  << Results   
Giancarlo Allasia ; Alessandro Rizzo ; Kimon Valavanis
All Authors
View Document
129
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Proposed Solution
    III.
    Results
    IV.
    Conclusions

Authors
Figures
References
Keywords
Metrics
More Like This

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: A real-time waypoint-based 3D local path planning algorithm is proposed for obstacle avoidance using the optical flow obtained by a frontal monocular camera mounted on a ... View more
Metadata
Abstract:
A real-time waypoint-based 3D local path planning algorithm is proposed for obstacle avoidance using the optical flow obtained by a frontal monocular camera mounted on a quadrotor UAV. The algorithm accounts for vertical and horizontal obstacle avoidance, as well as for avoidance of frontally approaching obstacles. Implementation and testing are carried out in the ROS environment and the algorithm effectiveness is demonstrated via Gazebo simulations. Realtime algorithm performance is also assessed through software profiling and in terms of worst case execution time using the NVIDIA Jetson TX1 and RaspberryPi 4 for hardware-in-the-loop (HIL) tests.
Published in: 2021 International Conference on Unmanned Aircraft Systems (ICUAS)
Date of Conference: 15-18 June 2021
Date Added to IEEE Xplore : 19 July 2021
ISBN Information:
ISSN Information:
INSPEC Accession Number: 20916286
DOI: 10.1109/ICUAS51884.2021.9476762
Publisher: IEEE
Conference Location: Athens, Greece
Contents
SECTION I.
Introduction

Unmanned aerial vehicles (UAVs) in general, and quadrotors in particular, have been used in a wide spectrum of applications due to their versatility, flexibility, and their ability to fly at very low altitudes. However, a major issue to be overcome is real-time obstacle avoidance, hence the path planning strategy used to carry out a specific application should encompass the ability for the UAV to be aware of its surroundings. This ability is enabled by the inclusion of multiple sensors aboard. Sonars, radars, laser scanners (LIDAR), cameras or any combination of them are being used to deal with this issue [1] . LIDAR provides the most accurate data / information for environment mapping and for determining an obstacle-free path [2] . Even if LIDAR sensors in last few years became cheaper, compared to cameras they still have an economical disadvantage, not to mention their greater weight and power consumption, making cameras more advantageous particularly for small-scale UAVs. Information gathered from cameras, however, needs to be extracted in real-time by means of computer vision algorithms, already widely used in robotics for perception. Among the several available computer vision techniques, the biologically inspired optical flow algorithm has been investigated widely in literature as a solution to support UAVs' local path planning strategy and endowing UAVs with obstacle avoidance capability by means of just a monocular camera. The majority of optical flow-based path planning algorithms, unfortunately, suffer from some common limitations. Table I shows a classification of aforementioned limitation found in most recent related literature.

Therefore, this paper proposes an optical-flow-based local 3D path planning algorithm for obstacle avoidance, that exploits also third dimension enabling the avoidance of ground and floating obstacle and is able to avoid frontally approaching obstacle by considering the expansion of the 2D optical flow velocity vector field generated by the obstacles in the field of view (FOV) of the camera. The algorithm is real-time implementable on the quadrotor's on-board hardware, and a thorough analysis of the algorithm's worst case execution time (WCET) is carried out along with code profiling for performance evaluation in a HIL test using NVIDIA Jetson TX1 and RaspberryPi 4 boards. In the problem stated in this paper, the quadrotor is assumed to navigate based on a provided list of waypoints and be endowed, beyond standard sensors used for control purposes, only with a cheap frontal monocular camera, from which information from surrounding obstacles is extracted by exploiting optical flow computer vision algorithm. The algorithm is tested in a simulated environment implemented using Robotic Operating System (ROS) [3] in combination with Gazebo open source simulator [4] and OpenCV Python library for optical flow implementation, in particular Farnebäck's dense optical flow method [5] , [6] . The effectiveness of the obstacle avoidance strategy will be tested in Gazebo in three scenarios, shown in Figure 1 .
SECTION II.
Proposed Solution
A. Rationale of Proposed Solution

Assuming that the overall mission is expressed in terms of a list of waypoints to follow assigned by a global path planning algorithm, the strategy consists in computing in real time an intermediate waypoint to avoid the obstacle based on the optical flow vector field produced by the frames coming from the onboard camera, as is shown in Figure 2 .

In Figure 3 , an example of what the onboard camera sees and the respective brightness-coded optical flow field magnitude (the brighter the pixel, the faster the optical movement of that pixel among subsequent frames) with OSD superimposed is shown.

Closer objects to the moving observer (i.e. the UAV) appear to move faster in the FOV with respect to the farthest ones. Hence, areas in which the OF field is stronger are considered high collision risk areas, therefore the intermediate waypoint should be placed away from those areas. For instance, if obstacles on the right are at a smaller distance from us they seem to move faster in FOV, appearing as strong OF field on the right, hence a movement to the left is required for avoidance, so an intermediate waypoint will be appropriately placed on the left. The same holds for vertical obstacle avoidance. To translate this optical motion into a number, two scalar signals called horizontal and vertical optical flow unbalance signals are created, where sign and magnitude at a certain time instant indicate respectively where optical flow is more intense in the frame (left vs. right, up vs. down) and how strong is this unbalance. These signals are used to trigger an emergency situation whether the obstacle is approaching and avoid it by computing the intermediate waypoint. As far as frontally approaching obstacles are concerned, given that these seem expanding in the FOV as they approach the observer, an appropriate signal called Expansion of Optical Flow (EOF) is defined, which is related to the divergence of the OF vector field. When one of these signals exceeds a threshold the obstacle avoidance strategy is triggered, with higher priority for frontal avoidance, and an intermediate waypoint is computed by using these signals to appropriately place it, by exploiting the spherical coordinates representation of the point to be computed in the vehicle reference frame. The produced waypoint is then prepended to the waypoint's list and the quadrotor heads to it. Once reached, the waypoint is deleted from the list and the quadrotor heads to the next one. The mathematical formalism of the algorithm is explained in the following sections with a bottom-up approach. After defining optical flow vector field in Section II-B , how horizontal and vertical optical flow unbalance and EOF signals are built is explained respectively in Section II-C and Section II-D . Then coordinates of the intermediate waypoint starting from aforementioned signals are derived in Section II-E . Software implementation is finally covered in Section II-G .
Figure 1: - Test scenarios in gazebo for horizontal avoidance (la), vertical avoidance (1c) and frontal avoidance (1b). The blue circle represents the goal waypoint that the drone has to reach overcoming obstacles.
Figure 1:

Test scenarios in gazebo for horizontal avoidance (la), vertical avoidance (1c) and frontal avoidance (1b). The blue circle represents the goal waypoint that the drone has to reach overcoming obstacles.

Show All
Figure 2: - Here is shown the strategy's working principle. First the UAV is heading towards the next waypoint in the list (blue). When a lateral obstacle approaches the drone, from the right in this example, the obstacle's movement in drone's FOV leads to strong optical flow field on the right side of the FOV, triggering the obstacle avoidance, which computes an intermediate waypoint (green) that will become the next waypoint in the list, allowing the UAV to successfully avoid the obstacle.
Figure 2:

Here is shown the strategy's working principle. First the UAV is heading towards the next waypoint in the list (blue). When a lateral obstacle approaches the drone, from the right in this example, the obstacle's movement in drone's FOV leads to strong optical flow field on the right side of the FOV, triggering the obstacle avoidance, which computes an intermediate waypoint (green) that will become the next waypoint in the list, allowing the UAV to successfully avoid the obstacle.

Show All
Figure 3: - Figure 3a shows the onboard view of a lateral obstacle during forward motion is shown, while Figure 3b shows the computed OF in that scene with OSD superimposed, where brighter pixels correspond to faster moving points.
Figure 3:

Figure 3a shows the onboard view of a lateral obstacle during forward motion is shown, while Figure 3b shows the computed OF in that scene with OSD superimposed, where brighter pixels correspond to faster moving points.

Show All
Table I: Classification of developed monocular optical-flow based obstacle avoidance methods. Columns represent, respectively, the referenced work with its publication year (ref.), the type of robot intended for algorithm application (robot), the optical flow algorithm used (of method), the dimensionality of obstacle avoidance control commands produced (2D/3D), validation of achieved results (result) and main limitations (limitations) classified as follows: A - 2D obstacle avoidance (left/right control commands); B - offboard computation C - restricted applicability (e.g. Specifically structured environment, intrinsically limited); D - real-time implementable (or not specified), E: Does not deal with frontal obstacles.
Table I:- Classification of developed monocular optical-flow based obstacle avoidance methods. Columns represent, respectively, the referenced work with its publication year (ref.), the type of robot intended for algorithm application (robot), the optical flow algorithm used (of method), the dimensionality of obstacle avoidance control commands produced (2D/3D), validation of achieved results (result) and main limitations (limitations) classified as follows: A - 2D obstacle avoidance (left/right control commands); B - offboard computation C - restricted applicability (e.g. Specifically structured environment, intrinsically limited); D - real-time implementable (or not specified), E: Does not deal with frontal obstacles.

B. Optical Flow

Optical flow in computer vision can be generally defined as the motion of objects in images between subsequent frames of image sequences. Such a motion is caused by the relative movement between the object and the camera expressed as a 2D velocity field, representing for each pixel its displacement between the two consecutive frames. Figure 4 shows how optical flow is defined.
Figure 4: - The displacement of a pixel obtained among subsequent frame defines the optical flow vector for that specific pixel. Image adapted from [12].
Figure 4:

The displacement of a pixel obtained among subsequent frame defines the optical flow vector for that specific pixel. Image adapted from [12] .

Show All

I ( x , y , t − 1 ) and I ( x , y , t ) are the gray scale versions of the two subsequent frames sampled from the camera, modeled as 2D matrix containing the brightness of pixels ( x , y ) respectively at time t − 1 and t . These are matrices with dimensions H × W , where H = 240 and W = 320 are respectively the height and width of the image. Given that the gray scale values are sampled over 8 bits, pixel brightness is an integer number ranging from 0 to 255. Time t   ∈   N is discretized, with a sample rate of 10 Hz in the case of simulated onboard camera. Considering a single pixel ( x i , y i ), by solving an optimization problem that takes into account also the behaviour of that pixel's neighborhood among frames, it is possible to estimate its displacement from time t − 1 to time t as a velocity vector O F ( x i , y i , t ) = u i i ^ + v i j ^ . The solution to that optimization problem extended to all the pixels in the frame outputs a 2D velocity vector field, which represents the optical flow vector field and can be expressed as in Equation 1 .
O F ( x , y , t ) = u ( x , y , t ) i ^ + v ( x , y , t ) j ^ (1)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \mathbf{OF}(x, y, t)=u(x, y, t)\hat{\mathbf{i}}+v(x, y, t)\hat{\mathbf{j}} \tag{1} \end{equation*}

Optical flow algorithms can be divided in sparse and dense, depending on whether flow is computed only for a subset of pixels (sparse), usually corresponding to feature detected by corner detecting methods or similar feature detection algorithms, or for all pixels in the image (dense). Sparse methods are computationally lighter but less accurate, while dense ones require more computation but manage to estimate the flow more correctly. In this paper the Gunnar Farnebäck dense optical flow method is used, implemented by OpenCV Python library. Farnebäck's method is not only more accurate than classic Lukas-Kanade method, as proved by results in [6] from a test on Yosemite sequence, but it gets faster on smaller images as tested in [16] . This last feature becomes important given that this algorithm applies the pyramid method, i.e. the optical flow computation is applied at different levels of frames' downsizing to capture larger optical movement, improving robustness.
C. Optical Flow Unbalance Computation

In order to obtain a measure of which areas in the FOV experience a stronger optical flow, the frames sampled from the onboard camera are divided into different templates, as can be seen in Figure 5 , of which dimensions are determined by a trial and error procedure.
Figure 5: - Templates geometry notation.
Figure 5:

Templates geometry notation.

Show All

Not the whole frame area may be considered important for triggering collision avoidance. For instance, optical flow contributions from corners can be considered negligible for that purpose, as well as the most external strip, because, based on the assumption that that the camera is always facing forward motion direction, objects in those areas won't likely enter in collision trajectory. This is why a region of interest (ROI) smaller than frame's dimensions is defined, and templates are defined inside ROI. Templates are designed to cover a vertical strip (VU and VD) and an horizontal strip (HL and HR) for vertical and horizontal unbalance computation and obstacle avoidance, respectively. The same reasoning holds for the central template (FR) used for EOF computation. Vertical and horizontal optical flow unbalance signals are defined as:
e V ( t ) e H ( t ) = σ V D ( t ) − σ V U ( t ) = σ H R ( t ) − σ H L ( t ) (2a) (2b)
View Source Right-click on figure for MathML and additional features. \begin{align*} e_{V}(t) &= \sigma_{VD}(t)- \sigma_{VU}(t)\tag{2a}\\ e_{H}(t) &= \sigma_{HR}(t)- \sigma_{HL}(t)\tag{2b} \end{align*} where the signal σ T ( t ) associated to generic template T is the sum of compensated OF field magnitude for all pixels in T, defined as:
σ T ( t ) = ∑ ( x , y ) ∈ T ∥ O F C ( x , y , t ) ∥ (3)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \sigma_{T}(t)=\sum_{(x, y)\in T}\Vert \mathbf{OF}_{\mathbf{C}}(x, y, t)\Vert \tag{3} \end{equation*}

O F C is the field OF computed from the camera with adequate compensation for self-motion produced OF (more about this in Section II-F ). In order to smooth the noisy unbalance signals and avoiding peaks that could trigger avoidance unnecessarily, a Moving Mean Filter (MMF) is applied. Filtered versions of unbalance signals are produced:
e V ¯ ¯ ¯ ¯ ¯ ¯ ( t ) e H ¯ ¯ ¯ ¯ ¯ ¯ ( t ) = 1 M ∑ i = 0 M − 1 e V ( t − i ) , M = 3 = 1 M ∑ i = 0 M − 1 e H ( t − i ) , M = 3 (4a) (4b)
View Source Right-click on figure for MathML and additional features. \begin{align*} \overline{e_{V}}(t) &=\frac{1}{M} \sum\limits_{i=0}^{M-1} e_{V}(t-i),\quad M=3\tag{4a}\\ \overline{e_{H}}(t) &=\frac{1}{M} \sum\limits_{i=0}^{M-1} e_{H}(t-i),\quad M=3\tag{4b} \end{align*}

Length of filter's memory M has been set by trial and error procedure.
D. Expansion of Optical Flow (EOF) Computation

When an object is approaching frontally, it seems for perspective reasons expanding in the FOV. In order to quantify this expansion, the concept of Expansion of Optical Flow is introduced. First, the Focus Of Expansion (FOE), that is the fixed point of the expansion, is considered. It is the point through which (theoretically) all directions of the vectors in a purely expanding field pass. Usually is not the case and its coordinates are obtained by solving an LSE problem, that leads to unreliable estimation. In this paper, FOE is assumed fixed in frame's center. Given that, ideally, FOE indicates the direction in which the observer is moving and in the presented work the drone always moves forward straight toward next waypoint, this assumption is reasonable. Then, for each pixel in the frontal template, the divergent component of OF from FOE is computed and divided by its distance from FOE to give more importance to points directly in front of the quadrotor, as shown in following equations and in Figure 6 . Finally, all contributions from template's pixels are summed up to obtain the EOF signal.
O F D I V , i ( t ) = O F i ( t ) ⋅ u ^ r , i , u ^ r , i = r i ∥ r i ∥ E O F i ( t ) = O F D I V , i ( t ) ∥ r i ∥ = O F i ⋅ r i ∥ r i ∥ 2 E O F ( t ) = ∑ i N E O F i ( t ) (5a) (5b) (5c)
View Source Right-click on figure for MathML and additional features. \begin{gather*} OF_{DIV, i}(t)=\mathbf{OF}_{\mathbf{i}}(t)\cdot\hat{\mathbf{u}}_{\mathbf{r},\mathbf{i}},\quad \hat{\mathbf{u}}_{\mathbf{r},\mathbf{i}}=\frac{\mathbf{r}_{\mathbf{i}}}{\Vert \mathbf{r}_{\mathbf{i}}\Vert}\tag{5a}\\ EOF_{i}(t)= \frac{OF_{DIV, i}(t)}{\Vert \mathbf{r}_{\mathbf{i}}\Vert}=\frac{\mathbf{OF}_{\mathbf{i}}\cdot \mathbf{r}_{\mathbf{i}}}{\Vert \mathbf{r}_{\mathbf{i}}\Vert^{2}}\tag{5b}\\ EOF(t)= \sum\limits_{i}^{N} EOF_{i}(t)\tag{5c} \end{gather*}

Figure 6: - Computation of OF divergence component $OF_{DIV, i}$ for the $i^{th}$ pixel.
Figure 6:

Computation of OF divergence component O F D I V , i for the i t h pixel.

Show All

E. Intermediate Waypoint Computation

By means of the obtained signals, the situation can be evaluated and action can be taken. Thresholds are defined and if any of the three signals exceeds them the obstacle avoidance strategy is triggered and the adequate intermediate waypoint is computed. This is done by using the signals to compute the spherical coordinates in the mobile (vehicle) reference frame and then translating it in cartesian coordinate in the fixed (world) reference frame, as shown in Figure 7 .
Figure 7: - Intermediate waypoint computation for obstacle avoidance.
Figure 7:

Intermediate waypoint computation for obstacle avoidance.

Show All

Vertical and horizontal unbalance signals and EOF signal are compared to respective thresholds τ V , τ H , τ F to trigger the obstacle avoidance strategy. Depending on which kind if avoidance is required, horizontal/vertical or frontal, different equations for computing spherical coordinates in vehicle reference frame are used. The frontal avoidance has priority over horizontal and vertical avoidance. For instance, if both E O F ( t )   >   τ F and e V ¯ ¯ ¯ ¯ ¯ ¯ ( t ) > τ V at a certain time instant t , only the frontal avoidance is applied and the second set of equations will be used. As far as horizontal or vertical avoidance is concerned, when E O F ( t ) ≤ τ F , the Equations 6 for computing r , θ and ψ at time instant t are applied:
r = r V , H θ ( t ) = { K P , V   e V ¯ ¯ ¯ ¯ ¯ ¯ ( t ) , 0 , if   | e V ¯ ¯ ¯ ¯ ¯ ¯ ( t ) | > τ V if   | e V ¯ ¯ ¯ ¯ ¯ ¯ ( t ) | ≤ τ V ψ ( t ) = { K P , H   e H ¯ ¯ ¯ ¯ ¯ ¯ ( t ) , 0 , if   | e H ¯ ¯ ¯ ¯ ¯ ¯ ( t ) | > τ H if   | e H ¯ ¯ ¯ ¯ ¯ ¯ ( t ) | ≤ τ H (6a) (6b) (6c)
View Source Right-click on figure for MathML and additional features. \begin{gather*} \quad r= r_{V, H}\tag{6a}\\ \theta(t)=\begin{cases} K_{P, V}\ \overline{e_{V}}(t), & \text{if}\ \vert \overline{e_{V}}(t)\vert > \tau_{V}\\ 0, & \text{if}\ \vert \overline{e_{V}}(t)\vert \leq \tau_{V} \end{cases}\tag{6b}\\ \psi(t)=\begin{cases} K_{P, H}\ \overline{e_{H}}(t), & \text{if}\ \vert \overline{e_{H}}(t)\vert > \tau_{H}\\ 0, & \text{if}\ \vert \overline{e_{H}}(t)\vert \leq \tau_{H} \end{cases}\tag{6c} \end{gather*}

Angles are then saturated to ± π 2 in case the values exceed the range. For frontal avoidance instead, i.e. E O F ( t ) > τ F , a right angle turn to be avoided is required. Equations 7 are used for computing intermediate waypoint's spherical coordinates.
r = r F θ ( t ) = 0 ψ ( t ) = { − π 2 , + π 2 , e H ¯ ¯ ¯ ¯ ¯ ¯ ( t ) ≤ 0 e H ¯ ¯ ¯ ¯ ¯ ¯ ( t ) > 0 (7a) (7b) (7c)
View Source Right-click on figure for MathML and additional features. \begin{gather*} r= r_{F} \tag{7a}\\ \\ \theta(t)=0\tag{7b}\\ \\ \psi(t)=\begin{cases} -\frac{\pi}{2}, &\quad \overline{e_{H}}(t)\leq 0\\ +\frac{\pi}{2}, &\quad \overline{e_{H}}(t) > 0 \end{cases}\tag{7c} \end{gather*}

Finally, the translation in cartesian coordinates in the world reference frame is needed in order to feed this point as a reference in position to the quadrotor's controller. Defining η ( t ) as the heading of the quadrotor with respect to world reference frame's x axis, p I as the intermediate waypoint in world reference frame and p Q as the quadrotor's position, coordinates of p I are defined by Equations 8 .
x I ( t ) = r cos θ ( t ) cos ( ψ ( t ) + η ( t ) ) + x Q ( t ) y I ( t ) = r cos θ ( t ) sin ( ψ ( t ) + η ( t ) ) + y Q ( t ) z I ( t ) = r sin θ ( t ) + z Q ( t ) (8a) (8b) (8c)
View Source Right-click on figure for MathML and additional features. \begin{gather*} x_{I}(t)=r\cos\theta(t)\cos(\psi(t)+\eta(t))+ x_{Q}(t) \tag{8a}\\ y_{I}(t)=r\cos\theta(t)\sin(\psi(t)+\eta(t))+ y_{Q}(t)\tag{8b}\\ z_{I}(t)=r\sin\theta(t)+ z_{Q}(t) \tag{8c} \end{gather*}

The computed waypoint is now prepended to the stored waypoints list and will be used as the next reference in order to avoid the obstacle. The values for thresholds, gains and radii working for the simulation environment used in this work has been found by trial and error procedure.
F. Self-Motion Generated Optical Flow Compensation

Given that the OF field generated is the result of the relative motion between the camera fixed to the UAV and the surrounding environment, quadrotor's tilting movements are influencing the generated optical motion. Given that only the OF generated by forward motion is useful to avoid obstacles, these unwanted OF must be compensated. To compute signals the compensated OF field O F C is therefore taken into account, where compensation is operated by multiplying OF horizontal and vertical components for a time-varying coefficient as shown in Equations 9 .
O F C ( x , y , t ) = u C ( x , y , t ) i ^ + v C ( x , y , t ) j ^ u C ( x , y , t ) = u ( x , y , t ) 1 1 + K C , y a w | ψ ˙ ( t ) | v C ( x , y , t ) = v ( x , y , t ) 1 1 + K C , l i n   z | h ˙ ( t ) | + K C , p i t c h | q ( t ) | (9a) (9b) (9c)
View Source Right-click on figure for MathML and additional features. \begin{gather*} \mathbf{OF}_{\mathbf{C}}(x, y, t)= u_{C}(x, y, t)\hat{\mathbf{i}}+ v_{C}(x, y, t)\hat{\mathbf{j}}\tag{9a}\\ \\ u_{C}(x, y, t)=u(x, y, t)\frac{1}{1+ K_{C, yaw}\vert \dot{\psi}(t)\vert}\tag{9b}\\ \\ v_{C}(x, y, t)=v(x, y, t)\frac{1}{1+ K_{C, lin\ z}\vert \dot{h}(t)\vert + K_{C, pitch}\vert q(t)\vert}\tag{9c} \end{gather*} where ψ ˙ ( t ) is yaw rate at time t ,   h ˙ ( t ) is the climbing rate and q ( t ) is the pitch rate. Coefficients' values have been tuned by means of a trial and error procedure as well.

G. Software Implementation

To implement and test the algorithm, ROS/Gazebo quadrotor simulator package hector_quadrotor is used [17] . The algorithm is implemented as a node in ROS Melodic (Ubuntu 18.04 LTS) and set to run at a fixed rate of 5 Hz , even if higher rates can be achieved, as shown in Section III . In Figure 8 , main topics used by the node are shown.

The algorithm at each iteration passes through three main phases: Status Evaluation, Control Command Computation, Outputs Publication. In Status Evaluation OF field is computed as well as OF unbalances and EOF in order to compare signal to thresholds and set flags. In the Control Command Computation the previously evaluated flags are used to decide if and which kind of avoidance is required. In the last phase, Outputs Publication, produced outputs are published over respective topics.
Figure 8: - Graph of inputs and outputs for node /ofoa_manager implementing optical-flow-based obstacle avoidance (ofoa) strategy.
Figure 8:

Graph of inputs and outputs for node /ofoa_manager implementing optical-flow-based obstacle avoidance (ofoa) strategy.

Show All

SECTION III.
Results
A. Obstacle Avoidance Effectiveness in Simulated Test Scenarios

For each scenario in Figure 1 , 20 simulations are run with randomly generated starting points in the plane of interest according to a 2D Gaussian distribution; a further one with the origin (0, 0, 0) as starting point is considered. Two kind of plots are produced. First one is a plot of all simulations' trajectories with obstacle profiles. An ellipse which principal semi-axes are 3 σ of the randomly distributed starting point displacement along that direction is drawn. In the second plot, the minimum distance of the drone from the nearest obstacle for each run is shown. In order to compute success rate, the size of the quadrotor is considered. A certain threshold distance (dashed line) is set as the half of the size of the quadrotor along x or y in horizontal plane avoidance scenarios or along z in vertical plane avoidance scenarios, incremented of 30% as safety clearance. If the nearest obstacle is below aforementioned threshold, that run is labeled as a fail. Result plots for each scenario are shown in Figures 9 , 10 and 11 and performance measurements in Table II . As can be seen from results, success rate is 100% for both horizontal and vertical avoidance and 95.2% for frontal avoidance.
B. HIL Simulations

To prove that the algorithm can run in real-time on onboard integrable hardware, two boards are tested in a HIL setup, the NVIDIA Jetson TX1 and RaspberryPi 4. The experimental setup is shown in Figure 12 with Jetson TX1 as an example.
1) Worst Case Execution Time (WCET) Analysis for Real-Time Performances

To estimate WCET, a statistical analysis of algorithm's execution time is carried out on both platforms. In order to produce the measurements, a timer starts at the beginning of main algorithm function and it's stopped at the end of its execution, evaluating elapsed time and, by its inverse, the frequency of execution ideally achieved in that iteration. Here operating system time is considered, not simulation time. Measurements are collected over 60 seconds of simulation. The algorithm execution rate is fixed to 5 Hz , hence 200 ms period, that is a safe enough value in order to let all the operations be executed without excessively compromising obstacle avoidance reactivity. Results are shown in Table III .
Figure 9: - Plots of results obtained from simulations run in lateral obstacle avoidance scenario in horizontal plane.
Figure 9:

Plots of results obtained from simulations run in lateral obstacle avoidance scenario in horizontal plane.

Show All
Figure 10: - Plots of results obtained from simulations run in vertical obstacle avoidance scenario in vertical plane.
Figure 10:

Plots of results obtained from simulations run in vertical obstacle avoidance scenario in vertical plane.

Show All

As shown, both boards are able to run the algorithm in real-time keeping up with the requested 5 Hz rate. Jetson TX1 shows a WCET of 159.4 ms (average 124.8 ms) corresponding to a theoretical minimum guaranteed rate of 6.27 Hz , while RaspberryPi 4 shows better performances with WCET of 78.4 ms (average 66.8 Hz) , corresponding to a minimum guaranteed rate of 12.75 Hz , being more than double the NVIDIA board's rate. Furthermore, RaspberryPi 4 weights around 52% of Jetson TX1's weight and its maximum consumption is around 41.7% in comparison. In conclusion, the RaspberryPi 4 board not only can handle the real-time requirement better than the Jetson TX1 module, but it is more power efficient and lighter in weight, making it suitable to be mounted onboard on a quadrotor.
Figure 11: - Plots of results obtained from simulations run in frontal obstacle avoidance scenario in horizontal plane.
Figure 11:

Plots of results obtained from simulations run in frontal obstacle avoidance scenario in horizontal plane.

Show All
Figure 12: - Setup for HIL simulation. On the left, the jetson tx1 development kit is running ubuntu 14.04 and OFOA ROS node. On the right, the laptop with virtualized ubuntu 18.04 which hosts ROS master node and gazebo simulator. The jetson TX1 and the PC are connected by means of the wifi router acting as a gateway, in the middle.
Figure 12:

Setup for HIL simulation. On the left, the jetson tx1 development kit is running ubuntu 14.04 and OFOA ROS node. On the right, the laptop with virtualized ubuntu 18.04 which hosts ROS master node and gazebo simulator. The jetson TX1 and the PC are connected by means of the wifi router acting as a gateway, in the middle.

Show All
Table II: Summary of results from simulations run.
Table II:- Summary of results from simulations run.

2) Code Profiling

An analysis of where in the code the most part of execution time is spent to identify bottlenecks and try to optimize slower parts of code has to be carried out by means of Yappi profiler and results are visualized as treemaps in KCacheGrind. Experiment is carried out as HIL simulation with NVIDIA Jetson TX1 and results are shown in Figure 13 .
Figure 13: - Treemap profiling representation by means of KCacheGrind of algorithm run on NVIDIA jetson TX1.
Figure 13:

Treemap profiling representation by means of KCacheGrind of algorithm run on NVIDIA jetson TX1.

Show All
Table III: Statistics of main algorithm computation time performance run on NVIDIA jetson TX1 and RaspberryPi 4 embedded systems. The equivalent rate is obtained as the inverse of execution time.
Table III:- Statistics of main algorithm computation time performance run on NVIDIA jetson TX1 and RaspberryPi 4 embedded systems. The equivalent rate is obtained as the inverse of execution time.

What can be noticed is that the majority of time is spent in OF field computation, actually 84.79% of the overall time spent in caller function OFOAMainAlgorithm , making it the real bottleneck of the whole algorithm. OF field computation is implemented by an OpenCV function that recalls an optimized compiled function in C++, therefore cannot be optimized anymore. The second most expensive function with a time cost of 6.16% of main function execution time, hence already negligible, is the one used to publish outputs by means of standard non optimizable ROS Python library's functions. The remaining functions have a negligible impact and contain already optimized or non-optimizable operations.
SECTION IV.
Conclusions

Effectiveness of proposed real-time waypoint-based 3D local path planning strategy in avoiding lateral, floating, ground and frontal obstacles has been demonstrated with high success rate. Capability of running in real-time at a rate of at least 5 Hz on onboard integrable embedded hardware has been shown. The proposed algorithm, even if proved effective, has some limitations such as dependence on visibility (e.g. dark environment, poorly textured objects, limited FOV angle not perceiving obstacles on the sides such as wall corners) and lack of depth information due to the use of a monocular camera.
ACKNOWLEDGMENT

This work is partially supported by Amazon Science through a 2019 Amazon Research Award granted to Dr. A. Rizzo.

Authors
Figures
References
Keywords
Metrics
   Back to Results   
More Like This
Dominant plane detection using a RGB-D camera for autonomous navigation

2015 6th International Conference on Automation, Robotics and Applications (ICARA)

Published: 2015
Neural controller of autonomous driving mobile robot by an embedded camera

2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)

Published: 2018
Show More
References
1. J. N. Yasin, S. A. S. Mohamed, M. Haghbayan, J. Heikkonen, H. Tenhunen and J. Plosila, "Unmanned aerial vehicles (uavs): Collision avoidance systems and approaches", IEEE Access , vol. 8, pp. 105139-105155, 2020.
Show in Context View Article Full Text: PDF (6606) Google Scholar
2. Lidar wikipedia page , [online] Available: https://en.wikipedia.org/wiki/Lidar.
Show in Context Google Scholar
3. Robotic operating system (ros) , [online] Available: https://www.ros.org/.
Show in Context Google Scholar
4. Gazebo simulator tool , [online] Available: http://gazebosim.org/.
Show in Context Google Scholar
5. Opencv: an open source computer vision library , [online] Available: https://opencv.org/.
Show in Context Google Scholar
6. G. Farnebäck, Two-frame motion estimation based on polynomial expansion , vol. 2749, no. 06, pp. 363-370, 2003.
Show in Context Google Scholar
7. B. Richards, S. Bhandari, M. Gan, J. Dayton, M. Enriquez, J. Liu, et al., Obstacle Avoidance System for UAVs using Computer Vision, 2015.
Google Scholar
8. H. Miao and Y. Wang, Optical flow based obstacle avoidance and path planning for quadrotor flight , pp. 631-638, 2018.
Google Scholar
9. P. Gao, D. Zhang, Q. Fang and S. Jin, "Obstacle avoidance for micro quadrotor based on optical flow", 2017 29th Chinese Control And Decision Conference (CCDC) , pp. 4033-4037, 2017.
View Article Full Text: PDF (509) Google Scholar
10. P. Agrawal, A. Ratnoo and D. Ghose, "A composite guidance strategy for optical flow based uav navigation", IFAC Proceedings Volumes , vol. 47, no. 1, pp. 1099-1103, 2014.
CrossRef Google Scholar
11. A. Eresen, N. Imamoglu and M. Efe, "Autonomous quadrotor flight with vision-based obstacle avoidance in virtual environment", Expert Systems with Applications , vol. 39, pp. 894-905, 2012.
CrossRef Google Scholar
12. G. Cho, J. Kim and H. Oh, "Vision-based obstacle avoidance strategies for mavs using optical flows in 3-d textured environments", Sensors(Basel) , 2019.
Show in Context CrossRef Google Scholar
13. D.-W. Yoo, D.-Y. Won and M.-J. Tahk, Optical flow based collision avoidance of multi-rotor uavs in urban environments , 2011.
Google Scholar
14. W. Aguilar, L. Alvarez, S. Grijalva and I. Rojas, Monocular Vision-Based Dynamic Moving Obstacles Detection and Avoidance , pp. 386-398, 2019.
Google Scholar
15. C. Wang, W. Liu and M. Q. Meng, "Obstacle avoidance for quadrotor using improved method based on optical flow", 2015 IEEE International Conference on Information and Automation , pp. 1674-1679, 2015.
View Article Full Text: PDF (1249) Google Scholar
16. J. de Boer and M. Kalksma, Choosing between optical flow algorithms for uav position change measurement , 2015.
Show in Context Google Scholar
17. Hector quadrotor ros wiki page , [online] Available: http://wiki.ros.org/hectorquadrotor.
Show in Context Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
