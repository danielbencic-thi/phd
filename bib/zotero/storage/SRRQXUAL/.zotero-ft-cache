IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathMenu.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Journals & Magazines > IEEE Signal Processing Letters > Volume: 28
Seeing in the Dark by Component-GAN
Publisher: IEEE
Cite This
PDF
  << Results   
Ning Rao ; Tao Lu ; Qiang Zhou ; Yanduo Zhang ; Zhongyuan Wang
All Authors
View Document
1
Paper
Citation
258
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    The Proposed Method
    III.
    Experimental Results
    IV.
    Conclusion

Authors
Figures
References
Citations
Keywords
Metrics
More Like This

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: Recently, Retinex theory based low-light image enhancement (LLIE) algorithms have achieved impressive results in controlled environment. However, the majority of deep lea... View more
Metadata
Abstract:
Recently, Retinex theory based low-light image enhancement (LLIE) algorithms have achieved impressive results in controlled environment. However, the majority of deep learning based LLIE algorithms leverage relighting by enhancing the illumination components that directly determines the image brightness, regretfully, they ignore the information of reflectance components, which may cause problems such as image noise and color distortion in reconstructed images. To tackle this problem, in this letter, we propose a component enhancement network based on Generative Adversarial Network (Component-GAN) for recovering clear images from low-light ones. Specifically, the network is composed of the decomposition part for dividing the paired low/normal-light images into illumination components and reflectance components, and the enhancement part for generating high-quality images. It is worth to note that we provide two branches of component enhancement network, which are parallel to improve the two components simultaneously. Hereby, we treat the reconstruction part as the generative network and adopt discriminative network to boost image reconstruction performance. Through extensive experiments, the proposed approach outperforms some state-of-the-art LLIE methods in terms of visual and subjective qualities.
Published in: IEEE Signal Processing Letters ( Volume: 28 )
Page(s): 1250 - 1254
Date of Publication: 12 May 2021
ISSN Information:
INSPEC Accession Number: 21015044
DOI: 10.1109/LSP.2021.3079848
Publisher: IEEE
Funding Agency:
Contents
SECTION I.
Introduction

Image captured in the dark suffers from poor visibility, low contrast and high ISO noise. Low-light images not only affect people's ability to perform visual tasks, but also bring great challenges in various machine-learning tasks such as face recognition, pedestrian detection and automatic driving   [1] . Therefore, it is necessary to improve the quality of low-light images. In the past few decades, there have been a lot low-light images enhancement (LLIE) algorithms devoted to solve this problem. Generally, it can be divided into two categories: mathematical model based and deep learning based methods.

Mathematical model based methods use statistical model to adjust the light intensity distribution for better visual results. There are two categories of the most representative methods: one is based on Retinex theory  [2] , [3] and another is based on histogram equalization and its variants  [4] – [5] [6] . Based on the logarithmic transformation, Fu et al.   [7] proposed a weighted variational model to estimate both the reflectance and illumination component from the observed image with imposed regularization terms. Guo et al.   [8] enhanced the illumination component by imposing priors. Fu et al.   [9] proposed a fusion-based method to enhance the illlumination component by fusing original illuminance with luminance-improved and contrast-enhanced versions. Park et al.   [10] presented an algorithm that enhances undesirably illuminated images by generating and fusing multi-level illuminations from a single image. The final image is obtained by multiplying the equalized illumination and enhanced reflectance. However, due to the nonlinearity across color channels and data complexity, existing methods have limited capabilities to enhance color, since color is easy to be distorted locally.

Deep learning based methods developed Convolutional Neural Networks (CNNs) to view low-light images as image reconstruction problems. Gharbi et al.   [11] proposed HDR-Net, which incorporated deep networks with the ideas of bilateral grid processing and local affine color transforms with pairwise supervision. RetinexNet  [12] combined Retinex theory with CNNs to estimate illuminance map and enhance low illumination images by enhancing illuminance map. Li et al.   [13] proposed a new Retinex model that takes noise into consideration. Inspired by GAN, Jiang et al.   [14] used unsupervised manner to learn a GAN to yield good results. Chunle et al.   [15] proposed a zero-reference deep curve estimation to recover low-light images.

As we known, the illumination component represents the various lightness on objects. While for low-light images, it usually suffers from darkness and unbalanced illumination distributions. So most LLIE approaches heavily rely on enhancing the illuminance channel. In order to suppress the low-light image noise, some methods just cascade denoising and lighting enhancement modules to achieve better performance. Neither denoising before enhancement (may result in blurring) nor enhancement before denoising (may cause noise amplification) own perfect relighting performance.

Taking into account the above issues, we discover that reflection component always benefits texture reconstruction. Because reflecction component describes the intrinsic property of the observed objects, which is considered to be consistent under any lightness conditions. Details of the low-light images are retained on the reflectance and some texture parts of images may degrade by noise from reflectance channel. So enhancing reflectance also matters.

Based on this observation, in this letter, we propose a component enhancement network based on Generative Adversarial Network (Component-GAN) to cope with these problems. First, we input the paired low/normal-light images into the decomposition network, in which the images will be divided into reflectance components and illumination components. Second, we design two branch enhancement networks for the retinex components, and then fuse them together to reconstruct a light enhancement image. Finally, discriminator is adopted to judge whether the rendered images meet the reconstruction criterion. To summarize the major contributions of this work: 1) We propose two branch component enhancement networks to optimize both illumination and reflectance simultaneously. We further develop the power of reflectance. The combination of these two components successfully reduces most noise and achieves impressive performance. 2) In order to make full use of paired low/normal-light images, we propose a component-GAN method to extend the Retinex network into GAN scheme. The designed discriminator further reduces noise and artifacts. 3) We perform evaluations on the proposed method compared with some popular datasets, which demonstrates the superiority of the proposed method qualitatively and quantitatively.
SECTION II.
The Proposed Method
A. Network Architecture

As shown in Fig. 1 , the network architecture of component-GAN has three parts, which will be described as follows:
Fig. 1. - The framework of Component-GAN. Component-GAN contains decomposition network, generator network and discriminator network. Decomposition network decomposes the low/normal-light images into reflectance and illumination channels. Generator network enhances two components respectively, and then fuse these two channels together for rendering an enhanced output. Here, U-Net is adopted as the backbone. Then we use the discriminator of VGG-19 architecture to distinguish the generated image according to the normal light image to obtain better image reconstruction quality.
Fig. 1.

The framework of Component-GAN. Component-GAN contains decomposition network, generator network and discriminator network. Decomposition network decomposes the low/normal-light images into reflectance and illumination channels. Generator network enhances two components respectively, and then fuse these two channels together for rendering an enhanced output. Here, U-Net is adopted as the backbone. Then we use the discriminator of VGG-19 architecture to distinguish the generated image according to the normal light image to obtain better image reconstruction quality.

Show All
1) Decomposition Network

The network consists of two sub-networks with same structure and same parameters. Each sub-network has only 5 convolutional layers. S n o r m a l and S l o w represent the normal-light input image and the low-light input image. Through the sub-networks, they will be decomposed into the reflectance and illumination components, which means we will get R n o r m a l , I n o r m a l from S n o r m a l and R l o w , I l o w from S l o w .
2) Generator Network

It is composed of reflectance and illumination enhancement module. Different from RetinexNet  [12] , we add U-Net in illumination enhancement module to reduce noise and artifacts. Considering U-Net  [16] has achieved huge success on image enhancement, we adopt it as our enhancement network backbone. We also find that the reflection component R n o r m a l has the effect of constraining its paired low-light reflection component R l o w . Actually, both components as reflection and illumination channels from normal light images can boost the low-light components reconstruction performance. Finally, we fuse the components of two branches to generate a reconstructed image, which is the input of discriminator.
3) Discriminator Network

We use VGG19 network with FC layers to distinguish the reconstructed image S ˜ l o w in line with the real normal light image S n o r m a l . The discriminator further improves the reconstruction quality.
B. Loss Function

In order to learn the mapping function of low/normal-light images through N image pairs { S l o w _ i , S n o r m a l _ i } N i = 1 , in which i is the index of sample pairs, we design a loss function L . The loss L consists of four components: illumination components loss L I , invariable reflectance components loss L R , reconstruction loss L R e c and GAN loss L G . It is represented as:
L = λ I L I + λ R L R + λ R e c L R e c + λ G L G A N , (1)
View Source Right-click on figure for MathML and additional features. \begin{equation*} L = {\lambda _I}{L_I} + {\lambda _{\rm {R}}}{L_R} + {\lambda _{\rm {Rec}}}{L_{Rec}}+{\lambda _{\rm {G}}}{L_{GAN}}, \tag{1} \end{equation*} where λ I , λ R , λ R e c , λ G are the balance parameters for controlling the contributions of each loss term.

A good decomposition method should consider the decomposed illumination components preserving the detailed texture and overall structure. In order to make that comes true, we adopt an improved TV loss  [12] as the illumination component loss. Total variation minimization  [17] minimizes the gradient of the whole image. It is often used to preserve the overall structure and texture information. The L I is formulates as:
L I = ∑ i = 1 N ∥ ∇ I l o w _ I , n o r m a l _ I ∘ exp ( − λ g ∇ R l o w _ i , n o r m a l _ i ) ∥ , (2)
View Source Right-click on figure for MathML and additional features. \begin{equation*} L_I = \sum \limits _{i = 1}^N {\left\Vert {\nabla {I_{low\_I,normal\_I}} \circ \exp (- \lambda g\nabla {R_{low\_i,normal\_i}})} \right\Vert }, \tag{2} \end{equation*} where ∇ denotes the gradient including ∇ x (horizontal) and ∇ y (vertical), λ g represents structure perception coefficient. From Retinex theory, the illumination component becomes steep when it encounters the reflection component of the object, where image structures locate and its illumination channel should be discontinuous. So, the weight exp ( − λ g ∇ R l o w _ i , n o r m a l _ i ) denotes illumination component gradient becomes steep.

Considering the reflection component is determined by the nature of object, the reflection component should be consistent under any condition. So L R can be defined as:
L R = ∑ i = 1 N ∥ R l o w _ i − R n o r m a l _ i ∥ . (3)
View Source Right-click on figure for MathML and additional features. \begin{equation*} L_R{\rm { = }}\sum \limits _{{\rm {i = 1}}}^N {\left\Vert {{R_{low\_i}} - {R_{normal\_i}}} \right\Vert }.\tag{3} \end{equation*}

Using reconstructed S ~ l o w and I ~ l o w , the reconstruction loss L R e c can be formulated as:
L Re c = ∑ i = 1 N ∥ ∥ R ~ l o w _ i ∗ I ~ l o w _ i − S n o r m a l _ i ∥ ∥ 2 (4)
View Source Right-click on figure for MathML and additional features. \begin{equation*} L_{{\mathop {\rm Re}\nolimits } c} = {\sum \limits _{i = 1}^N {\left\Vert {{{\tilde{R}}_{low\_i}} * {{\tilde{I}}_{low\_i}} - {S_{normal\_i}}} \right\Vert } ^2} \tag{4} \end{equation*} where R ~ l o w _ i ∗ I ~ l o w _ i denotes the enhanced image S ~ l o w . Here, with the pixel MSE loss function, the reconstructed S ~ l o w should be similar with its normal light version. But the loss function of the MSE optimization usually lacks the high-frequency content which results in perceptual unsatisfactory images with over-smooth texture. This is one reason why the reconstructed image is blurry.

To achieve more realistic reconstructed image, we introduce adversarial loss  [18] to the objective loss, which is defined as follows:
L a d v = 1 N ∑ i = 1 N log ( 1 − D θ ( G ω ( I L R i ) ) ) , (5)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \begin{array}{l}L_{adv} = \frac{1}{N}\sum \limits _{i = 1}^N {\log (1 - {D_\theta }} ({G_\omega }(I_i^{LR}))), \end{array} \tag{5} \end{equation*} where D θ ( G ω ( I L R i ) ) is probability that the reconstruction image G ω ( I L R i ) is a natural relighted image, and the funtion G ω ( ) means the output of the generator. The adversarial loss encourages the generator network to generate shaper high-frequency details. We combine the adversarial loss function with the pixel-wise MSE loss function for getting better natural reconstruction quality. So the generator network loss function L G is defined as follows:
L G = 1 N ∑ i = 1 N log ( 1 − D θ ( G ω ( R ~ l o w _ i ∗ I ~ l o w _ i ) ) ) + ( ∥ ∥ G ω ( R ~ l o w _ i ∗ I ~ l o w _ i ) − S n o r m a l _ i ∥ ∥ 2 ) (6)
View Source Right-click on figure for MathML and additional features. \begin{align*} L_G =& \frac{1}{N}\sum \limits _{i = 1}^N {\log (1 - {D_\theta }({G_\omega }({{\tilde{R}}_{low\_i}} * {{\tilde{I}}_{low\_i}})))} \\ & + \left({\left\Vert {{G_\omega }({{\tilde{R}}_{low\_i}} * {{\tilde{I}}_{low\_i}}) - {S_{normal\_i}}} \right\Vert ^2}\right) \tag{6} \end{align*} where D θ ( G ω ( R ~ l o w _ i ∗ I ~ l o w _ i ) ) denotes the probability that the reconstruction image G ω ( R ~ l o w _ i ∗ I ~ l o w _ i ) is a natural normal light image S n o r a m l . It enforces the reconstructed light images to be similar with the real ones on the pixel, high-frequency details and semantic level.

We also introduce classification loss to classify whether the reconstructed normal light images are fake or real. The loss function is defined as follows:
L D = 1 N ∑ i = 1 N − ( ( log ( 1 − D θ ( G ω ( R ~ l o w _ i ∗ I ~ l o w _ i ) ) ) + log D θ ( S n o r m a l _ i ) ) , (7)
View Source Right-click on figure for MathML and additional features. \begin{align*} L_D =& \frac{1}{N}\sum \limits _{i = 1}^N { - ((\log (1 - {D_\theta }({G_\omega }({{\tilde{R}}_{low\_i}} * {{\tilde{I}}_{low\_i}})))} \\ & + \log {D_\theta }({S_{normal\_i}})), \tag{7} \end{align*} by introducing the classification loss, discriminator network makes images generated by generator more realistic. Therefore, the GAN network can be trained by the objective function as:
L G A N = max θ min ω 1 N ∑ i = 1 N log ( 1 − D θ ( G ω ( R ~ l o w _ i ∗ I ~ l o w _ i ) ) ) + log D θ ( S n o r m a l _ i ) + ( ∥ ∥ G ω ( R ~ l o w _ i ∗ I ~ l o w _ i ) − S n o r m a l _ i ∥ ∥ 2 ) (8)
View Source Right-click on figure for MathML and additional features. \begin{align*} L_{GAN} =& \mathop {\max }\limits _\theta \mathop {\min }\limits _\omega \frac{1}{N}\sum \limits _{i = 1}^N {\log (1 - {D_\theta }({G_\omega }({{\tilde{R}}_{low\_i}} * {{\tilde{I}}_{low\_i}})))} \\ & + \log {D_\theta }({S_{normal\_i}})\\ & + \left({\left\Vert {{G_\omega }({{\tilde{R}}_{low\_i}} * {{\tilde{I}}_{low\_i}}) - {S_{normal\_i}}} \right\Vert ^2}\right) \tag{8} \end{align*}

SECTION III.
Experimental Results
A. Datasets and Implementation Details

Component-GAN requires paired low/normal-light images for training. Considering it is difficult to collect paired low/normal-light images, we use the LOL dataset  [12] and convert the normal-light image into a low-light version as training dataset. LOL is the first dataset containing low-light/normal image pairs from a real scene. It contains 500 pairs of low/normal-light images. We select 485 pairs as the training dataset, and the remaining 15 pairs as the testing dataset. To test the performance of Component-GAN, we also choose the standard images used in previous work as the testing datasets: (NPE  [19] , LIME  [8] , MEF  [20] , DICM  [21] , etc.). All samples are converted to PNG format and resized to 256 × 256 pixels. We use the random gradient descent method for back propagation and batch size is set to be 32. λ I , λ R and λ g are set to be 0.1, 0.01 and 10 respectively.
B. Comparisons With Some State-of-The-Art Algorithms

In this section, we compare Component-GAN with current state-of-the-art methods including quantitative comparison, visual quality comparison, and no-referenced image quality assessment (IQA).
1) Objective Evaluation

In this subsection, we select three evaluation indexes: PSNR, SSIM, and FSIM to show the objective reconstruction quality. 15 pairs of low/normal-light images from LOL dataset are used as testing samples, in which low-light images are used as input and normal light images are used as groundtruth images. Six state-of-the-art LLIE methods: PLE  [22] , SRIE  [7] , LIME  [8] , GladNet  [23] , RetinexNet  [12] , EnlightenGAN  [14] are selected as benchmarks for comparing. The average of indexes are listed in Table I .
TABLE I The Average of PSNR(dB), SSIM and FSIM Values With Different LLIE Methods
Table I- The Average of PSNR(dB), SSIM and FSIM Values With Different LLIE Methods
2) Visual Quality Comparison

In order to show the visual results of Component-GAN, we list some reconstructed normal light images from the selected benchmarks. The results are demonstrated in Fig. 2 and Fig. 3 , where the first column is the input low-light images, the second to seventh columns are the images enhanced by: PLE  [22] , SRIE  [7] , LIME  [8] , GladNet  [23] , RetinexNet  [12] , EnlightenGAN  [14] . The last column shows the results produced by our Component-GAN.
Fig. 2. - Comparison with other state-of-the-art methods. We randomly selected three images from the LOL test dataset as input images, and reconstructed the results on the current excellent light enhancement algorithms.
Fig. 2.

Comparison with other state-of-the-art methods. We randomly selected three images from the LOL test dataset as input images, and reconstructed the results on the current excellent light enhancement algorithms.

Show All
Fig. 3. - Comparison with other state-of-the-art methods. We randomly selected three images as input images, and reconstructed the results on the current excellent light enhancement algorithms.
Fig. 3.

Comparison with other state-of-the-art methods. We randomly selected three images as input images, and reconstructed the results on the current excellent light enhancement algorithms.

Show All

In Fig. 2 and Fig. 3 , the results from Component-GAN are brighter than EnlightenGAN. EnlightenGAN is an unsupervised learning method that does not require paired low/normal-light images training, thus its visual results are not as good as Component-GAN. The image reconstructed by Component-GAN is more real and clearer than the image reconstructed by GladNet and RetinexNet. The main reason is that GladNet and RetinexNet lose high-frequency information during the process of enhancing light intensity, resulting in too smooth images.
3) No-Referenced Image Quality Assessment

Moreover, we adopt Natural Image Quality Evaluator (NIQE)  [24] , a well-known no-reference image quality assessment for evaluating real image restoration. The NIQE results on five publicly available image datasets used by previous works (MEF, NPE, LIME, VV, and DICM) are reported in Table II : a lower NIQE value indicates better visual quality. Specifically, MEF, NPE, LIME, VV and DICM contain 17,8,10,24 and 69 images respectively. Component-GAN wins two out of five and is the best in terms of overall averaged NIQE. This further testifies the superiority of Component-GAN in generating high-quality visual results.
TABLE II NIQE Scores on Five Testing Datasets (MEF, LIME, NPE, VV, DICM)
Table II- NIQE Scores on Five Testing Datasets (MEF, LIME, NPE, VV, DICM)
SECTION IV.
Conclusion

In this letter, we propose a novel low-light enhancement method named Component-GAN. By combining generative adversarial network with Retinex theory, the component enhancement networks do boost the LLIE performance. The convincing tests and satisfactory results have confirmed the effectiveness of Component-GAN. In the future, our work will focus on the super-resolution research of low-light images.

Authors
Figures
References
Citations
Keywords
Metrics
   Back to Results   
More Like This
Two-Dimensional Compact Variational Mode Decomposition-Based Low-Light Image Enhancement

IEEE Access

Published: 2019
DSLR: Deep Stacked Laplacian Restorer for Low-Light Image Enhancement

IEEE Transactions on Multimedia

Published: 2021
Show More
References
1. F. Yu et al., "Bdd100 k: A diverse driving video database with scalable annotation tooling", arXiv:1805.04687 , 2018.
Show in Context Google Scholar
2. E. H. Land, "The retinex theory of color vision", Sci. Amer. , vol. 237, no. 6, pp. 108-129, 1977.
Show in Context CrossRef Google Scholar
3. D. J. Jobson, Z. Rahman and G. A. Woodell, "A multiscale retinex for bridging the gap between color images and the human observation of scenes", IEEE Trans. Image Proces. , vol. 6, no. 7, pp. 965-976, Jul. 1997.
Show in Context View Article Full Text: PDF (424) Google Scholar
4. S.-C. Huang, F.-C. Cheng and Y.-S. Chiu, "Efficient contrast enhancement using adaptive gamma correction with weighting distribution", IEEE Trans. Image Process. , vol. 22, no. 3, pp. 1032-1041, Mar. 2013.
Show in Context View Article Full Text: PDF (4979) Google Scholar
5. K. Zuiderveld, "Contrast limited adaptive histogram equalization", Proc. Graph. Gems IV , pp. 474-485, 1994.
Show in Context CrossRef Google Scholar
6. M. Kaur, J. Kaur and J. Kaur, "Survey of contrast enhancement techniques based on histogram equalization", Int. J. Adv. Comput. Sci. Appl. , vol. 2, no. 7, 2011.
Show in Context CrossRef Google Scholar
7. X. Fu, D. Zeng, Y. Huang, X.-P. Zhang and X. Ding, "A weighted variational model for simultaneous reflectance and illumination estimation", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , pp. 2782-2790, 2016.
Show in Context View Article Full Text: PDF (4554) Google Scholar
8. X. Guo, Y. Li and H. Ling, "Lime: Low-light image enhancement via illumination map estimation", IEEE Trans. Image Proces. , vol. 26, no. 2, pp. 982-993, Feb. 2017.
Show in Context View Article Full Text: PDF (9438) Google Scholar
9. X. Fu, D. Zeng, Y. Huang, Y. Liao, X. Ding and J. Paisley, "A fusion-based enhancing method for weakly illuminated images", Signal Process. , vol. 129, pp. 82-96, 2016, [online] Available: https://www.sciencedirect.com/science/article/pii/S0165168416300949.
Show in Context CrossRef Google Scholar
10. J. S. Park and N. Cho, "Generation of high dynamic range illumination from a single image for the enhancement of undesirably illuminated images", Multimedia Tools Appl. , vol. 78, no. 14, pp. 20263-20283, Jul. 2019.
Show in Context CrossRef Google Scholar
11. M. Gharbi, J. Chen, J. Barron, S. Hasinoff and F. Durand, "Deep bilateral learning for real-time image enhancement", ACM Trans. Graph. , vol. 36, no. 4, pp. 1-12, Jul. 2017.
Show in Context CrossRef Google Scholar
12. W. Y. J. L. Chen Wei and W. Wang, "Deep retinex decomposition for low-light enhancement", Proc. Brit. Mach. Vis. Conf. , 2018.
Show in Context Google Scholar
13. M. Li, J. Liu, W. Yang, X. Sun and Z. Guo, "Structure-revealing low-light image enhancement via robust retinex model", IEEE Trans. Image Process. , vol. 27, no. 6, pp. 2828-2841, Jun. 2018.
Show in Context View Article Full Text: PDF (8528) Google Scholar
14. Y. Jiang et al., "Enlightengan: Deep light enhancement without paired supervision", IEEE Trans. Image Process. , vol. 30, pp. 2340-2349, 2021.
Show in Context View Article Full Text: PDF (7052) Google Scholar
15. G. Chunle et al., "Zero-reference deep curve estimation for low-light image enhancement", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , pp. 1780-1789, 2020.
Show in Context Google Scholar
16. O. Ronneberger, P. Fischer and T. Brox, "U-net: Convolutional networks for biomedical image segmentation", Proc. Int. Conf. Med. Image Comput. Computer-assisted Interv , pp. 234-241, 2015.
Show in Context Google Scholar
17. B. Goldluecke and D. Cremers, "An approach to vectorial total variation based on geometric measure theory", Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. , pp. 327-333, 2010.
Show in Context View Article Full Text: PDF (2064) Google Scholar
18. C. Ledig et al., "Photo-realistic single image super-resolution using a generative adversarial network", Proc. IEEE Conf. Computer Vis. Pattern Recognit. , pp. 4681-4690, 2017.
Show in Context View Article Full Text: PDF (1523) Google Scholar
19. S. Wang, J. Zheng, H.-M. Hu and B. Li, "Naturalness preserved enhancement algorithm for non-uniform illumination images", IEEE Trans. Image Process. , vol. 22, no. 9, pp. 3538-3548, Sep. 2013.
Show in Context View Article Full Text: PDF (1847) Google Scholar
20. K. Ma, K. Zeng and Z. Wang, "Perceptual quality assessment for multi-exposure image fusion", IEEE Trans. Image Process. , vol. 24, no. 11, pp. 3345-3356, Nov. 2015.
Show in Context View Article Full Text: PDF (3362) Google Scholar
21. C. Lee, C. Lee and C.-S. Kim, "Contrast enhancement based on layered difference representation", Proc. 19th IEEE Int. Conf. Image Process. , pp. 965-968, 2012.
Show in Context View Article Full Text: PDF (787) Google Scholar
22. X. Fu, Y. Liao, D. Zeng, Y. Huang, X.-P. Zhang and X. Ding, "A probabilistic method for image enhancement with simultaneous illumination and reflectance estimation", IEEE Trans. Image Process. , vol. 24, no. 12, pp. 4965-4977, Dec. 2015.
Show in Context View Article Full Text: PDF (6459) Google Scholar
23. W. Wang, C. Wei, W. Yang and J. Liu, "Gladnet: Low-light enhancement network with global awareness", Proc. 13th IEEE Int. Conf. Autom. Face Gesture Recognit. , pp. 751-755, 2018.
Show in Context View Article Full Text: PDF (1517) Google Scholar
24. A. Mittal, R. Soundararajan and A. C. Bovik, "Making a “completely blind” image quality analyzer", IEEE Signal Process. Lett. , vol. 20, no. 3, pp. 209-212, Mar. 2013.
Show in Context View Article Full Text: PDF (565) Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
