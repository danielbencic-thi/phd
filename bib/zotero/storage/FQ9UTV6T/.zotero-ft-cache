IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathZoom.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Journals & Magazines > IEEE Transactions on Automati... > Volume: 17 Issue: 4
Neural RRT*: Learning-Based Optimal Path Planning
Publisher: IEEE
Cite This
PDF
Jiankun Wang ; Wenzheng Chi ; Chenming Li ; Chaoqun Wang ; Max Q.-H. Meng
All Authors
39
Paper
Citations
5081
Full
Text Views

    Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Preliminaries
    III.
    NRRT* Algorithm
    IV.
    Simulation Results
    V.
    Conclusion and Future Work

Authors
Figures
References
Citations
Keywords
Metrics
Abstract:
Rapidly random-exploring tree (RRT) and its variants are very popular due to their ability to quickly and efficiently explore the state space. However, they suffer sensitivity to the initial solution and slow convergence to the optimal solution, which means that they consume a lot of memory and time to find the optimal path. It is critical to quickly find a short path in many applications such as the autonomous vehicle with limited power/fuel. To overcome these limitations, we propose a novel optimal path planning algorithm based on the convolutional neural network (CNN), namely the neural RRT* (NRRT*). The NRRT* utilizes a nonuniform sampling distribution generated from a CNN model. The model is trained using quantities of successful path planning cases. In this article, we use the A* algorithm to generate the training data set consisting of the map information and the optimal path. For a given task, the proposed CNN model can predict the probability distribution of the optimal path on the map, which is used to guide the sampling process. The time cost and memory usage of the planned path are selected as the metric to demonstrate the effectiveness and efficiency of the NRRT*. The simulation results reveal that the NRRT* can achieve convincing performance compared with the state-of-the-art path planning algorithms. Note to Practitioners-The motivation of this article stems from the need to develop a fast and efficient path planning algorithm for practical applications such as autonomous driving, warehouse robot, and countless others. Sampling-based algorithms are widely used in these areas due to their good scalability and high efficiency. However, the quality of the initial path is not guaranteed and it takes much time to converge to the optimal path. To quickly obtain a high-quality initial path and accelerate the convergence speed, we propose the NRRT*. It utilizes a nonuniform sampling distribution and achieves better performance. The NRRT* can be also applied t...
(Show More)
Published in: IEEE Transactions on Automation Science and Engineering ( Volume: 17 , Issue: 4 , Oct. 2020 )
Page(s): 1748 - 1758
Date of Publication: 16 March 2020
ISSN Information:
INSPEC Accession Number: 20023419
DOI: 10.1109/TASE.2020.2976560
Publisher: IEEE
Funding Agency:
SECTION I.
Introduction

Robotic path planning is to plan a collision-free path for some specific tasks among a number of static or moving obstacles [1] . Many well-established algorithms have been proposed to solve the path planning problems. The artificial potential field (APF) method [2] utilizes a potential function over the whole configuration space to direct the movement of the robot. However, it often ends up in a local minimum. The grid-based algorithms including the A* [3] and D* [4] can guarantee finding an optimal path if it exists. Here, the generated optimal path is called “resolution optimal” because the map discretization under different resolutions will lead to different optimal paths. But their time cost and memory usage increase exponentially with the map size and the dimensionality of the state space. The sampling-based algorithms, such as probabilistic roadmap (PRM) [5] and rapidly random-exploring tree (RRT) [6] , have become very popular for efficiently solving high-dimensional and multiconstrained path planning problems. It is noted that the sampling-based algorithms only provide a weaker form of completeness (also called probabilistic completeness) and the generated paths are often nonoptimal. An essential factor in determining the performance of the sampling-based planner is the sampling distribution because all sampling-based planners iteratively construct trees to connect samples drawn from the given sampling distributions. Usually, planners probabilistically or deterministically draw random state samples from a uniform distribution. Such a sampling strategy can guarantee the probabilistic completeness and asymptotic optimality [7] . However, due to environment types, clearance requirements, robot dynamics, and other constraints, robots often work in the small subsets of the state space and drawing state samples in the other parts of the state space is not necessary. For example, in environments with sparse obstacles, it is reasonable to bias the samples toward the goal region because the final path will be relatively straight. Therefore, the placement of the samples in these promising regions is closely related to the performance of the sampling-based path planners.

In [8] , Brunner et al. propose the A*-RRT* algorithm, where an initial path generated from the A* algorithm is used to guide the sampling process of the RRT* planner. The convergence speed can be accelerated significantly as the A* algorithm always provides an optimal path. However, as the map size or step size used in the search process increases, the A* algorithm consumes much more time and memory to find the optimal path, as shown in Fig. 1 . It affects the algorithm performance, especially in complex dynamic environments. The test map is shown in Fig. 2(a) , and the resolution of map discretization is 1. In this article, it means that the minimum distance between two states is 1 pixel.
Fig. 1. - Time cost of the A* algorithm on finding the optimal path under different step size and map size. (a) Time cost of the A* algorithm increases as the step size becomes larger. S2 means that the step size is 2. (b) Time cost of the A* algorithm increases significantly as the map size becomes double and triple.
Fig. 1.

Time cost of the A* algorithm on finding the optimal path under different step size and map size. (a) Time cost of the A* algorithm increases as the step size becomes larger. S2 means that the step size is 2. (b) Time cost of the A* algorithm increases significantly as the map size becomes double and triple.

Show All
Fig. 2. - Probability distribution of the optimal path given different clearance and step size. The red and green rectangles denote the start and goal states, respectively. As the color changes from blue to magenta, the existence probability of the optimal path in current region becomes larger and larger. (a) Clearance $= 1$ , Step size $= 2$ . (b) Clearance $= 6$ , Step size $= 2$ . (c) Clearance $= 1$ , Step size $= 1$ . (d) Clearance $= 1$ , Step size $= 4$ .
Fig. 2.

Probability distribution of the optimal path given different clearance and step size. The red and green rectangles denote the start and goal states, respectively. As the color changes from blue to magenta, the existence probability of the optimal path in current region becomes larger and larger. (a) Clearance = 1 , Step size = 2 . (b) Clearance = 6 , Step size = 2 . (c) Clearance = 1 , Step size = 1 . (d) Clearance = 1 , Step size = 4 .

Show All

In order to overcome the aforementioned limitations and further take advantage of the A* and the RRT* algorithms, in this article, we propose a novel optimal path planning algorithm through learning from the A* algorithm. We train a convolutional neural network (CNN) model by learning a large number of optimal paths generated from the A* algorithm. Given a new path planning problem, our trained model can rapidly provide a predicted probability distribution of the optimal path, which is used to guide the sampling process of the RRT* planner. In the A* algorithm, the generated path has little clearance to the obstacles so that it is dangerous for a robot to track due to the uncertainty in sensor measurement or robot dynamics [9] . It means that we will get different optimal paths if we set a different clearance value in the path planning problem. In addition, when the step size is different in the search process in the A* algorithm, the final optimal path is different for the same environment. Therefore, we take into account these two factors in the training process and our CNN model captures the differences so that it can output different probability distributions of the optimal path for the same environment with different parameter settings, as shown in Fig. 2 . Then, the predicted sampling distribution is used to guide the sampling process. The other components of the Neural RRT*(NRRT*) are similar to those of the RRT* [7] algorithm.
A. Related Work

Recently, many researchers have proposed different heuristics to accelerate the convergence of the RRT algorithms to an optimal path.

A common method is to combine the discrete search with a sampling-based path planner. Bekris and Kavraki [10] utilize the informed subdivision tree as a heuristic to guide the growth of the RRT tree. As a two-phase motion planner, the A*-RRT* algorithm uses a geometrically feasible path to bias the sampling process of the RRT*. Rickert et al. [11] propose the exploring/exploiting tree (EET) algorithm to utilize the workspace information to bias the tree growth, but the probabilistic completeness is not guaranteed. By using any-angle path biasing, Palmieri et al. [12] propose the Thete*-RRT* algorithm, which can find a shorter path faster than the RRT* and the A*-RRT* algorithms. However, these aforementioned methods become inefficient when the map size or the dimensionality of the state space increases.

There are also other good heuristics used in the RRTs algorithms. For example, Qureshi and Ayaz [13] use the potential function as the sampling heuristic. In [14] , Wang et al. implement a 2-D Gaussian mixture model to find a high-quality initial solution and adjust the sampling behavior of the RRT algorithms. Yershova et al. [15] present dynamic-domain RRT, which ignores the samples that are too far from the current tree. In [16] , the obstacle boundary information is used to guide the sampling process. Gammell et al. [17] present the informed RRT* (IRRT*) to accelerate the convergence to an optimal path using an admissible ellipsoidal heuristic. Urmson and Simmons [18] compute a heuristic quality that is used to evaluate the quality of a path passing through the node. The sampling process will focus on the regions containing high-quality nodes. By limiting the sampling region, the planner can avoid unnecessary exploration in these regions where the feasible path probably does not exist and as a result, the performance becomes better. In [19] , the generalized Voronoi graph and multiple potential functions are used to achieve efficient path planning. While the previous work is effective for robots in certain environments, they are not generally applicable. Sometimes the heuristic can increase the planning time dramatically if it is not suited to the concerned environment.

Recently, reinforcement learning and deep learning methods are widely used in sampling-based planners. Baldwin and Newman [20] use semantic information to learn the sampling distribution. Zucker et al. use the reinforcement learning method to optimize the sampling process in the discretized workspace. Another reinforcement learning technique called value iteration networks (VIN) is shown capable of effectively finding near-optimal trajectories in 2-D mazes and 3-D landscapes. However, VIN suffers from several disadvantages such as the training instability and the random seed sensitivity. Ichter et al. [21] propose a conditional variational autoencoder (CVAE) to learn explicit sampling distribution for robot motion planning. In [22] , Qureshi and Yip propose to use a contractive autoencoder to achieve deeply informed neural sampling. They take into consideration the workspace encoding, the initial and the goal states to generate an end-to-end feasible path. Zhang et al. [23] present a policy-search based method as an adaptive way to learn implicit sampling distribution through a rejection sampling strategy. Li et al. [24] utilize the neural network to predict the cost function in the RRT algorithm.

Different from the previous work, our methodology does not propose a human-designed heuristic or require any discretization of the state space. Moreover, it learns the probability distribution of the optimal path with different attributes and can be easily combined with other sampling-based algorithms to further improve the algorithm performance.
B. Original Contributions

The contributions of this article are 1) to propose a CNN model to predict a probability distribution of the optimal path for the environment with different parameter settings, 2) present a novel optimal path planning algorithm, the NRRT*, and 3) provide the proof of the probabilistic completeness and the asymptotic optimality of the NRRT*. In fact, the predicted probability distribution helps the path planner achieve nonuniform sampling in the current environment. This method can be generalized to learn the probability distribution of the optimal path from the other path planners with different constraints and it can be also easily applied to other sampling-based algorithms.

The rest of this article is organized as follows. We first formulate the path planning problem in Section II and then explain the details of the proposed NRRT* algorithm in Section III . The simulation results are reported in Section IV . At last, we draw conclusions and discuss the future work in Section V .
SECTION II.
Preliminaries

In this section, we first formulate the path planning algorithm and then provide an overview of the RRT* algorithm.
A. Problem Formulation

The basic path planning problem can be defined as follows. Let X ∈ R n be the state space. The obstacle space and the free space are denoted as X obs and X free , respectively. Let x init be the initial state and x goal be the goal state. Usually, a goal region G ( x goal ) = { x ∈ X ∣ ∣ | | x − x goal | | < r } is used. The aim is to compute a path σ : [ 0 , T ] → X free such that σ ( 0 ) = x init and σ ( T ) ∈ G ( x goal ) .

Let Σ be the set of all feasible paths. With a cost function, c ( σ ) , by mapping each feasible path σ → X free to a positive real number R , the optimal path planning problem is defined as
σ ∗ = arg min σ ∈ Σ   c ( σ ) s.t.   σ ( 0 ) = x init σ ( T ) ∈ G ( x goal ) σ ( t ) ∈ X free ( t ) ∀ t ∈ [ 0 , T ] . (1)
View Source Right-click on figure for MathML and additional features. \begin{align*} \sigma ^{*}=&\arg \underset {\sigma \in \Sigma }{\min } ~c\left ({\sigma }\right) \\&\text {s.t.} ~\sigma \left ({0}\right) = x_{\text {init}} \\&\hphantom {\text {s.t.} ~}\sigma \left ({T}\right) \in \mathcal {G}\left ({x_{\text {goal}}}\right) \\&\hphantom {\text {s.t.} ~} \sigma \left ({t}\right) \in \mathcal {X}_{\text {free}}\left ({t}\right)\quad \forall t \in \left [{0,T}\right].\tag{1}\end{align*}

In this article, the cost function between two states is defined as follows:
c ( x 1 , x 2 ) = α 1 | | x 2 − x 1 | | + α 2 arccos v 1 → ⋅ x 1 x 2 − → − | v 1 → | | x 1 x 2 − → − | (2)
View Source Right-click on figure for MathML and additional features. \begin{equation*} c\left ({x_{1}, x_{2}}\right) = \alpha _{1} ||x_{2} - x_{1}|| + \alpha _{2} \arccos \frac {\overrightarrow {v_{1}} \cdot \overrightarrow {x_{1} x_{2}}}{|\overrightarrow {v_{1}}||\overrightarrow {x_{1} x_{2}}|}\tag{2}\end{equation*} where | | ⋅ | | is the Euclidean distance between x 1 and x 2 , v 1 → is the robot linear velocity at x 1 , and x 1   x 2 − → − − is the vector from x 1 to x 2 . α 1 and α 2 are two parameters to balance the effect of the Euclidean distance and angle difference, respectively. So the cost function c takes into account the Euclidean distance and angle difference between two states. For an arbitrary path σ , a smaller value of c ( σ ) means that the robot takes less time to move from the start position to the goal position.

B. RRT and RRT*

Generally, sampling-based path planners like the RRT and the PRM continuously draw samples x rand from the state space using a uniform sampling distribution to construct the feasible path σ ∗ in terms of a cost function c ( σ ) . The RRT algorithm aims at single-query application while the PRM algorithm focuses on multiquery application. Although multiquery approaches are valuable in highly structured environments, most online planning problems do not require multiple queries since the environment changes dynamically. Moreover, in some applications, computing a roadmap for the PRM algorithm is computationally challenging. However, incremental sampling-based algorithms such as RRTs have become popular and efficient approaches for many applications.

The RRT algorithm is initialized with a tree including the initial state as one vertex and no edges. At each iteration, the sampled state x rand from a uniform sampling distribution attempts to connect the nearest vertex x nearest in the tree. If this connection is successful, x rand will be adjusted to x new through a steering function. Then, x new is added to the vertex set and ( x nearest , x new ) is added to the edge set, respectively. The algorithm will stop until the tree contains a state in the goal region or the number of iterations reaches the threshold.

Compared with the RRT, the RRT*, as shown in Algorithm 1 , has two extra procedures, ChooseParent in the Extend ( T , x new ) function and Rewire process. In the ChooseParent procedure, when the RRT* tries to connect x new to x nearest , the vertices around x new within a certain radius will be searched to determine the optimal parent in terms of the cost. After connecting x new to the tree, the RRT* will rewire the neighbor vertices to check if one path through x new has a lower cost than the current path. The details of the RRT* can be found in [7] . Therefore, the asymptotic optimality can be guaranteed as the number of iterations goes to infinity. But it consumes much time and memory usage to reach the optimal solution. In the real-world implementation, especially for mobile robots or flying robots, it is necessary to compute high-quality paths efficiently.
Algorithm 1 - RRT*.
Algorithm 1

RRT*.

Show All

SECTION III.
NRRT* Algorithm

In this section, we first describe the proposed CNN model formulation in Sections III-A and III-B . Then we introduce the NRRT* and analyze its probabilistic completeness and asymptotic optimality in Sections III-C and III-D .
A. CNN Model

In this section, we describe our model formulation. We use 2-D images to represent the 2-D state space, which is denoted as I . Each pixel in the 2-D image has an assigned value. 0 means empty free space and 1 indicates occupied by the obstacles. 2 labels start position and 3 means goal position. Besides we also consider the robotic attributes like clearance denoted as C and robot step size denoted as S .

The input I is fed into our neural network together with these robotic attributes C and S . Our neural planning model outputs the corresponding image denoted as O . Each pixel receives a probability p ^ ∈ [ 0 , 1 ] indicating the chance that it will be used to construct the optimal path. It means that if the value of p ^ of a state is large, the optimal path probably goes through this state.

Our proposed model is shown in Fig. 3 . It has an encoding and decoding stage with atrous convolution [25] . This type of deep convolutional networks has been widely used in artificial intelligence-related fields like computer vision and machine learning and gains great success.
Fig. 3. - Illustration of the proposed CNN model.
Fig. 3.

Illustration of the proposed CNN model.

Show All

During the encoding stage, 2-D residual convolution network [26] , [27] is applied into the input I to extract features of the input map in a hierarchical fashion.

Take an image I u v with a size of ( W , H , 3 ) as an example. Each 2-D convolutional layer has its own small kernel K i j . The convolution operation is defined as follows:
( I ∗ K ) ( u , v ) = ∑ i = − I I ∑ j = − J J I ( u − i , v − j ) × K ( i , j ) (3)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \left ({I * K}\right)\left ({u, v}\right) = \sum ^{I}_{i=-I}\sum ^{J}_{j=-J} I\left ({u - i, v - j}\right) \times K\left ({i,j}\right)\tag{3}\end{equation*} where I and J are the size of the convolutional kernel K .

The convolution operation is applied to the whole image and the output of the convolution operation is called the feature map. The feature map is fed into a higher convolution layer to extract another feature map. Low-level feature maps F l and high level features maps F h are convolution results at low convolutional layers and high convolutional layers, respectively. Low-level feature maps encode local information while high-level feature maps represent high-level description of the map.

We adopt ResNet50 [26] as the backbone of our network encoder. We extract feature map C 1 and C 4 in ResNet50 as the low-level feature and high-level feature. Feature map C 1 has a dimension of ( W / 8 , H / 8 , 256 ) , and feature map C 4 has a dimension ( W / 32 , H / 32 , 2048 ) .

Atrous convolution is a general form of convolution and is defined in the following way:
( I ∗ K ) ( u , v ; r ) = ∑ i = − I I ∑ j = − J J I ( u − i × r , v − j × r ) × K ( i , j ) (4)
View Source Right-click on figure for MathML and additional features. \begin{align*} \left ({I * K}\right)\left ({u, v; r}\right) \!= \!\sum ^{I}_{i=-I}\sum ^{J}_{j=-J} I(u \!- \!i \times r, v {-} j \times r) \times K(i,j)\!\! \\\tag{4}\end{align*} where r is the dilated rate. Atrous convolution allows us to explicitly control the resolution of feature maps computed by deep CNNs and adjust kernel’s field-of-view in order to capture multiscale information. Atrous spatial pyramid pooling (ASPP) [28] module is applied to the high-level features to extract multiscale information in our network. The final feature map F h has a dimension of ( W / 32 , H / 32 , 256 ) .

On the other input branch, these input robotic attributes S and C are fed into fully connected neural networks. The actual inputs of these attributes are float numbers. The corresponding low-level and high-level feature maps are extracted and denoted as F a l and F a h . F a l has a dimension of ( 1 , 1 , 32 ) and F a h has a dimension of ( 1 , 1 , 64 ) .

Features of the map and features of these robotic attributes are concatenated together because we want our neural network model to be able to adjust the optimal path according to these robotic attributes. Note that feature maps at the same level are concatenated together. F a h is repeated along the image width and height axis by ( W / 32 ) and ( H / 32 ) . The corresponding feature map is ( W / 32 , H / 32 , 64 ) and is concatenated along the third dimension element-wise. Low-level feature concatenation follows the same principles. The low- and high-level feature maps after the concatenation operation are denoted as F c l and F c h , respectively. F c l has a dimension of ( W / 8 , H / 8 , 256 + 32 ) and F c h has a dimension of ( W / 32 , H / 32 , 256 + 64 ) . The F c h is linearly resized to have a dimension of ( W / 8 , H / 8 , 256 + 64 ) and is concatenated with F c l element-wise. The final encoding feature is denoted as F e has a dimension of ( W / 8 , H / 8 , 608 ) .

In the decoding stage, the feature map F e is fed into the decoding convolutional network to get the final output O . The final output has a dimension of ( W , H , 1 ) with the same resolution as the input I . Each pixel at ( u , v ) contains a probability p ^ indicating the chance that the pixel is inside the optimal path.

In our proposed neural network, the fully convolutional network fashion [29] is adopted to process the input I and the output O . A fully convolutional network can handle input I with different resolutions.
B. Loss Function

We define the loss function as the cross-entropy of predicted probability and ground truth binary labeling per pixel. The cross-entropy loss is summed among all possible locations on the map
L = ∑ ce ( O , G ) + λ ce ( I , R ) (5)
View Source Right-click on figure for MathML and additional features. \begin{equation*} L = \sum {\textrm {ce}\left ({\mathcal {O},\mathcal {G}}\right)} + \lambda \textrm {ce}\left ({\mathcal {I},\mathcal {R}}\right)\tag{5}\end{equation*} where λ is a parameter to weigh the loss of reconstruction. O represents the predicted probability image, and G denotes the ground truth probability image. The generation procedure of ground truth G is described in Section IV-A .

C. Neural RRT*

In the NRRT*, the trained neural network model is used to initialize the sampling process at first. The model outputs a pretty sharp probability distribution of the optimal path for the current map. If we directly use this probability distribution, the probability that quantities of states are selected in the sampling process will approach zero. Then the probability of completeness of the algorithm is hardly guaranteed. In order to maintain the ability of the NRRT* to represent the environment with arbitrarily high fidelity, and thus maintain the theoretical completeness guarantee, we also sample from a uniform sampler. So there are two samplers in the NRRT*, a nonuniform sampler from the neural network model and a uniform sampler. We denote the fraction of uniform samples as α , i.e., α N samples are generated from the uniform sampler and ( 1 − α ) N from the nonuniform sampler. In fact, as shown in Section IV , sometimes the predicted probability distribution is not continuous. If we set α = 0 (i.e., all samples from the nonuniform sampler), the path planner will never find a solution. In addition, if we set α = 1 (i.e., all samples from the uniform sampler), the performance of the algorithm is the same as the conventional path planner. Through a number of simulations, we find that α = 0.5 offers the best balance between ensuring full coverage of the environment and leveraging the learned sample regions. Generally, the nonuniform sampler can quickly find a feasible path with a few samples. However, if the nonuniform sampler does not completely identify the solution region, the uniform sampler needs to effectively fill in the gaps, i.e., the nonuniform sampler needs many more samples so that it can find these regions.

The details of the NRRT* is shown in Algorithm 2 . First, We initialize a tree T = ( V , E ) consisting of a vertex set V ⊂ X free and edge set E ⊆ V × V . Then, in terms of the current map and parameter settings, NeuralModel( M a p , S , C ) outputs the predicted sampling distribution O . We find that the value of the state in the predicted sampling distribution O is polarized, i.e., the value of the state located on the predicted optimal path approaches 1 while the value of the other states approaches 0. Therefore, a simple rejection sampling method is implemented where the value of the state less than 0.5 is rejected during the nonuniform sampling process. In the whole sampling process, a random number Rand() ∈ ( 0 , 1 ) is used to determine which sampling method to use. If Rand() > 0.5 , the nonuniform sampler is used. Otherwise, the uniform sampler will be used. The following procedure is similar to the pipeline of the RRT* [7] . If the newest sample belongs to the goal region, i.e., x new ∈ G ( x goal ) , the algorithm will return a tree T consisting of a feasible path σ connecting x init and G ( x goal ) . Or the number of iterations reaches the threshold, the algorithm will return f a i l u r e .
Algorithm 2 - NRRT*.
Algorithm 2

NRRT*.

Show All

It is noted that the difference between the NRRT* and the RRT* is the sampling technique, which can be easily used in other sampling-based algorithms.
D. Probabilistic Completeness and Asymptotic Optimality

In the NRRT*, the nonuniform sampler samples states from the promising region with the probability 1 − α while the uniform sampler samples states from the whole state space with the probability α . It means that as the number of iterations goes to infinity, each state in the state space will be sampled. Therefore, the probabilistic completeness is naturally guaranteed.

In reference to the theorem 38 in [7] , the theoretical guarantee of the asymptotic optimality of the RRT* algorithm holds if the following inequation holds:
γ > ( 2 ( 1 + 1 / d ) ) 1 / d ( μ ( X free ) ξ d ) 1 / d (6)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \gamma \gt \left ({2 \left ({1 + 1 / d}\right)}\right)^{1 / d} \left ({\frac {\mu \left ({\mathcal {X}_{\text {free}}}\right)}{\xi _{d}}}\right)^{1 / d}\tag{6}\end{equation*} where γ represents the search radius in Extend ( T , x new ) , d the dimensionality of the state space, μ ( X free ) the Lebesgue measure of X free , and ξ d the volume of unit ball in the d -dimensional Euclidean space. As the same procedures happen in the RRT*, it can be stated that the NRRT* is also asymptotically optimal according to Lemma 56, 71, and 72 in [7] .

SECTION IV.
Simulation Results

In this section, we demonstrate the performance and generality of the NRRT* through several numerical simulations. In Section IV-A , we describe the details of network training. In the following, we compare the NRRT* with two state-of-the-art algorithms, the RRT* and the IRRT* under different clearance and different step sizes. Because the map size does not affect the distribution of the optimal path, the simulation under different map size is not provided.
A. Details of Network Training

The neural network experiments are conducted on NVIDIA Quadro P5000 with TensorFlow. For training, we use the Adam optimizer [30] with its suggested default parameters of β 1 = 0.9 and β 2 = 0.999 along with a batch size of 20 training pairs. The learning rate starts at ϵ = 0.0001 and is consistent across the experiments. It is noted that the proposed neural network only takes 50 ms to output a predicted probability distribution of the optimal path whatever the map size, the clearance or the step size. It saves a lot of time cost compared with those which use the A* algorithm to find a prior heuristic path as we have mentioned in Fig. 1 .

To obtain a reasonable and generalized data set, we first generate 5576 different 2-D random maps. The size of each map is 201 × 201 pixels. As shown in Table I , for each map with corresponding parameter settings ( c 1 means that the clearance is 1 and s 1 means that the step size is 1), we randomly set 12 different start and goal states. So there are 10 70 592 images in the training data set. We use the A* algorithm [3] to find the optimal path.
TABLE I Path Planning Data Set
Table I- Path Planning Data Set

The width of the optimal path is one pixel. In order to make the learning easier, we widen the optimal path by one pixel on each side, which means that the final width of the optimal path is three pixels.
B. Path Planning Under Different Clearance

In this section, we test the performance of three algorithms on the path planning problem under different clearance. Four kinds of clearance ( 1 , 2 , 4 , 6 ) are used. The input of the step size to the neural network is always 2. A larger clearance means that the planned path is far away from the obstacles while a smaller clearance means that the planned path is close to the obstacles. For the RRT* and the IRRT*, the clearance is a predefined parameter. However, in the NRRT*, the neural network model outputs the corresponding sampling distribution to feed the nonuniform sampler according to different clearance settings.

Fig. 4(a)–(h) shows the illustrations of the predicted sampling distribution for two different maps [ Fig. 4(a)–(d) for Map 1 and Fig. 4(e)–(h) for Map 2] under different clearance settings. The red and green rectangles denote the start and goal states, respectively. The magenta indicates that the corresponding region has a higher potential to contain the optimal path and the blue indicates a lower potential. It can be seen that the optimal path lies in the predicted probability distribution. Therefore, using the prediction from the neural network as a nonuniform sampler, the NRRT* can quickly find an initial solution and converge to the optimal solution. However, it is noted that some prediction results are not perfect. For example, in Fig. 4(b) and (f) , some regions where the optimal solution cannot lie in are considered as part of the prediction result. In Fig. 4(c), (f), and (g) , the predicted sampling regions are not continuous. Through a series of simulations, we find that these imperfect predictions have little effect on the performance of the algorithm. The reason is that RRTs algorithms have the ability to explore the whole state space. For example, in Fig. 4(c) , beginning from the start state, the tree grows to the end of the left part of the predicted probability distribution. When the state is sampled from the right part of the predicted probability distribution, the tree can gradually grow to the right. When the tree arrives at the right part of the predicted probability distribution, it can quickly grow toward the goal state. This is also the reason that we use both the uniform sampler and nonuniform sampler in the path planning process, as mentioned in Section III-C .
Fig. 4. - Predicted sampling distribution under different clearance settings. The magenta indicates that the corresponding region has a higher potential to contain the optimal path and the blue indicates a lower potential. (a) Map 1, C1. (b) Map 1, C2. (c) Map 1, C4. (d) Map 1, C6. (e) Map 2, C1. (f) Map 2, C2. (g) Map 2, C4. (h) Map 2, C6.
Fig. 4.

Predicted sampling distribution under different clearance settings. The magenta indicates that the corresponding region has a higher potential to contain the optimal path and the blue indicates a lower potential. (a) Map 1, C1. (b) Map 1, C2. (c) Map 1, C4. (d) Map 1, C6. (e) Map 2, C1. (f) Map 2, C2. (g) Map 2, C4. (h) Map 2, C6.

Show All

In addition, we can see that the prediction changes a lot for different clearance settings, especially from Fig. 4(b), (c), (f), and (g) . The reason is that the optimal path actually changes under different clearance settings. When the required clearance becomes larger, the feasible path will be far away from the obstacles and the optimal path changes accordingly. So the predictions for different clearance settings reveal that the proposed neural network is capable of dealing with the clearance requirement in the path planning process.

Two different maps (Map 1 and Map 2) are used to test the performance of the RRT*, the IRRT* and the NRRT*. The simulation results under different clearance settings are provided in Fig. 5 . The “Time” denotes the sum of CPU time cost and GPU time cost in the path planning process since the GPU is used to execute the CNN. The “Node” denotes the number of the node when the optimal solution is found. The number of the node indicates the memory usage in the path planning process because different algorithms use almost the same memory usage in other places. Fig. 5(a), (b) and (c), (d) shows the performance of three algorithms on finding the optimal path in Map 1 and Map 2, respectively. First, the NRRT* uses the least time cost and nodes to find the optimal path. It means that the NRRT* can save a lot of computation resources compared with the RRT* and the IRRT*. A statistical result is provided in Table II to show the performance improvement compared with the RRT* and the IRRT*. Second, the standard deviation of the time cost and nodes of the NRRT* are both much smaller, which reveals the better stability of the NRRT*. Third, as the clearance changes, the RRT* and the IRRT* requires different time cost and nodes to find the optimal path. But the time cost and nodes used in the NRRT* do not change a lot, which shows the good robustness of the NRRT*. The simulation results demonstrate that the predicted sampling distribution significantly improves the algorithm performance. In fact, the nonuniform sampling technique provides a strong bias to the optimal path and the initial path generated by the NRRT* is very close to the optimal path. Table III shows the length of the initial path generated from three algorithms. We can see that under different clearance settings, the NRRT* always finds a good initial path. It is noted that in Map 2, when the clearance is set to 4, the algorithm performance is not very good because there are many discontinuous parts in the predicted probability distribution of the optimal path. In this case, if the value of α mentioned in Section III-C increases, the performance will be better. So it is still an open problem to measure the quality of the prediction result and choose the appropriate parameter.
TABLE II Comparison of Algorithm Performance
Table II- Comparison of Algorithm Performance
TABLE III Initial Path Length Under Different Clearance
Table III- Initial Path Length Under Different Clearance
Fig. 5. - Simulation results under different clearance settings. (a), (b) and (c), (d) show the performance of three algorithms on finding the optimal path on two different maps, respectively. The time cost and the number of node are selected as the metric to evaluate the performance.
Fig. 5.

Simulation results under different clearance settings. (a), (b) and (c), (d) show the performance of three algorithms on finding the optimal path on two different maps, respectively. The time cost and the number of node are selected as the metric to evaluate the performance.

Show All

In short, the NRRT* achieves better performance compared with the RRT* and the IRRT*. The reason is that the NRRT* uses a nonuniform sampler to bias samples to the predicted sampling region. This region generated from the neural network contains the optimal path with a large probability, so such a biased sampling process naturally achieves better results.
C. Path Planning Under Different Step Size

In this section, we test the performance of three algorithms on the path planning problem under different step sizes. Four kinds of step size (1, 2, 4, 6) are used. The input of the clearance to the neural network is always 1. The step size means the search range in the A* algorithm; for example, step size 2 means that the A* planner will search two grids in each direction. Different step size settings in the A* algorithm lead to different optimal paths. Since the optimal paths used in the training data set are from the A* algorithm, the NRRT* is also sensitive to the step size.

Fig. 6(a)–(h) shows the illustrations of the predicted sampling distribution of two maps (Maps 3 and 4) under different step size settings. The red and green rectangles denote the start and goal states, respectively. The magenta indicates that the corresponding region has a higher potential to contain the optimal path and the blue indicates a lower potential. Again, we find that the predicted region contains the optimal path. It is noted that the prediction in Fig. 6(a) is obviously different from those in Fig. 6(b)–(d) . It is because the optimal path under step size 1 is different from those under step size 2,4,6, which results in a different prediction result. This is also a limitation of the A* algorithm. If the step size is set to a small value, the final path is not optimal compared with the final path under a large step size setting. The following simulation results will also support this argument. However, if the step size is set to a large value, the time cost of the A* algorithm increases exponentially. But the NRRT* always quickly predicts the sampling distribution under different step size settings.
Fig. 6. - Predicted sampling distribution under different step size settings. The magenta indicates that the corresponding region has a higher potential to contain the optimal path and the blue indicates a lower potential. (a) Map 3, S1. (b) Map 3, S2. (c) Map 3, S4. (d) Map 3, S6. (e) Map 4, S1. (f) Map 4, S2. (g) Map 4, S4. (h) Map 4, S6.
Fig. 6.

Predicted sampling distribution under different step size settings. The magenta indicates that the corresponding region has a higher potential to contain the optimal path and the blue indicates a lower potential. (a) Map 3, S1. (b) Map 3, S2. (c) Map 3, S4. (d) Map 3, S6. (e) Map 4, S1. (f) Map 4, S2. (g) Map 4, S4. (h) Map 4, S6.

Show All

The simulation results under different step size settings are provided in Fig. 7 . The NRRT*-1 means that the NRRT* uses the predicted sampling distribution generated from the neural network under step size 1. For Fig. 6(a)–(d) , the NRRT*-2, the NRRT*-4, and the NRRT*-6 use much less time cost and nodes to find the optimal path because the generated initial path is already very close to the optimal path. However, the predicted sampling distribution of the NRRT*-1 does not contain the optimal path and the nonuniform sampler cannot contribute a lot to finding the optimal path. This is also the big limitation of a series of algorithms that use the A* algorithm to find a heuristic path. A large step size means that the preprocessing consumes a lot of time while a small step size cannot guarantee a good heuristic path. However, in the NRRT*, we can set a larger step size to guarantee a good heuristic path without extra time cost. We can also find that the IRRT* performs better than the RRT* and the NRRT*-1 on finding the optimal path in the Map 3. This is because the IRRT* uses an admissible ellipsoidal heuristic to optimize the initial path. The details can be found in [17] . For Fig. 6(e)–(h) , the simulation results shown in Fig. 7(c) and (d) demonstrate that the NRRT* can quickly find the optimal path in a “trap” environment compared with the RRT* and the IRRT*.
Fig. 7. - Simulation results under different step size settings. (a), (b) and (c), (d) show the performance of three algorithms on finding the optimal path on two different maps, respectively. The time cost and the number of node are selected as the metric to evaluate the performance.
Fig. 7.

Simulation results under different step size settings. (a), (b) and (c), (d) show the performance of three algorithms on finding the optimal path on two different maps, respectively. The time cost and the number of node are selected as the metric to evaluate the performance.

Show All

In summary, the NRRT* achieves convincing performance again under different step size settings.
SECTION V.
Conclusion and Future Work

In this article, based on the CNN model and the RRT* algorithm, we propose the NRRT* to achieve nonuniform sampling in the path planning process by learning quantities of successful planning cases from the A* algorithm. To satisfy the different constraints in the path planning, the clearance and step size are both considered in the designed CNN model. The simulation results show that the proposed algorithm performs much better in comparison with conventional algorithms. In fact, the proposed algorithm is a novel sampling technique and can be readily applied to other sampling-based algorithms for improved results.

There are also many possible avenues for future work. First, the semantic information of the environment can be taken into account to better understand the task specification and interact with humans, which is also called semantic-aware path planning [31] . Second, a challenging extension is how to directly encode the environment using raw point-cloud data. It is crucial for achieving real-time optimal path planning while satisfying kinodynamic constraints [24] in practice. Furthermore, an interesting topic is to achieve socially compliant path planning [32] with a neural network model.

Authors
Figures
References
Citations
Keywords
Metrics
More Like This
Enhanced Ant Colony Optimization Algorithm for Global Path Planning of Mobile Robots

2013 International Conference on Computational and Information Sciences

Published: 2013
An improved ant colony optimization algorithm in mobile robot path planning

2021 40th Chinese Control Conference (CCC)

Published: 2021
Show More
References
1. B. Siciliano and O. Khatib, Springer Handbook Robotics, Berlin, Germany:Springer, 2016.
Show in Context CrossRef Google Scholar
2. O. Khatib, "Real-time obstacle avoidance for manipulators and mobile robots", Int. J. Robot. Res. , vol. 5, no. 1, pp. 90-98, 1986.
Show in Context CrossRef Google Scholar
3. P. Hart, N. Nilsson and B. Raphael, "A formal basis for the heuristic determination of minimum cost paths", IEEE Trans. Syst. Sci. Cybern. , vol. 4, no. 2, pp. 100-107, Jul. 1968.
Show in Context View Article Full Text: PDF (2390) Google Scholar
4. A. Stentz, "Optimal and efficient path planning for partially known environments" in Intelligent Unmanned Ground Vehicles, Boston, MA, USA:Springer, pp. 203-220, 1997.
Show in Context CrossRef Google Scholar
5. L. Kavraki, P. Svestka and M. H. Overmars, "Probabilistic roadmaps for path planning high-dimensional configuration spaces", IEEE Trans. Robot. Autom. , vol. 12, no. 4, pp. 566-580, Aug. 1996.
Show in Context View Article Full Text: PDF (1960) Google Scholar
6. S. M. LaValle and J. J. Kuffner, "Randomized kinodynamic planning", Int. J. Robot. Res. , vol. 20, no. 5, pp. 378-400, Jul. 2016.
Show in Context CrossRef Google Scholar
7. S. Karaman and E. Frazzoli, "Sampling-based algorithms for optimal motion planning", Int. J. Robot. Res. , vol. 30, no. 7, pp. 846-894, Jun. 2011.
Show in Context CrossRef Google Scholar
8. M. Brunner, B. Bruggemann and D. Schulz, "Hierarchical rough terrain motion planning using an optimal sampling-based method", Proc. IEEE Int. Conf. Robot. Autom. , pp. 5539-5544, May 2013.
Show in Context View Article Full Text: PDF (3468) Google Scholar
9. S. Thrun, W. Burgard and D. Fox, Probabilistic Robotics, Cambridge, MA, USA:MIT Press, 2005.
Show in Context Google Scholar
10. K. Bekris and L. Kavraki, "Informed and probabilistically complete search for motion planning under differential constraints", Proc. 1st Int. Symp. Search Techn. Artif. Intell. Robot. (STAIR) , pp. 1-8, 2008.
Show in Context Google Scholar
11. M. Rickert, A. Sieverling and O. Brock, "Balancing exploration and exploitation in sampling-based motion planning", IEEE Trans. Robot. , vol. 30, no. 6, pp. 1305-1317, Dec. 2014.
Show in Context View Article Full Text: PDF (1076) Google Scholar
12. L. Palmieri, S. Koenig and K. O. Arras, "RRT-based nonholonomic motion planning using any-angle path biasing", Proc. IEEE Int. Conf. Robot. Autom. (ICRA) , pp. 2775-2781, May 2016.
Show in Context View Article Full Text: PDF (2504) Google Scholar
13. A. H. Qureshi and Y. Ayaz, "Potential functions based sampling heuristic for optimal path planning", Auton. Robots , vol. 40, no. 6, pp. 1079-1093, Nov. 2015.
Show in Context CrossRef Google Scholar
14. J. Wang, W. Chi, M. Shao and M. Q.-H. Meng, "Finding a high-quality initial solution for the RRTs algorithms in 2D environments", Robotica , vol. 37, no. 10, pp. 1677-1694, Feb. 2019.
Show in Context CrossRef Google Scholar
15. A. Yershova, L. Jaillet, T. Simeon and S. M. LaValle, "Dynamic-domain RRTs: Efficient exploration by controlling the sampling domain", Proc. IEEE Int. Conf. Robot. Autom. , pp. 3856-3861, Apr. 2005.
Show in Context View Article Full Text: PDF (190) Google Scholar
16. J. Wang, X. Li and M. Q.-H. Meng, "An improved RRT algorithm incorporating obstacle boundary information", Proc. IEEE Int. Conf. Robot. Biomimetics (ROBIO) , pp. 625-630, Dec. 2016.
Show in Context View Article Full Text: PDF (549) Google Scholar
17. J. D. Gammell, S. S. Srinivasa and T. D. Barfoot, " Informed RRT * : Optimal sampling-based path planning focused via direct sampling of an admissible ellipsoidal heuristic ", Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. , pp. 2997-3004, Sep. 2014.
Show in Context Google Scholar
18. C. Urmson and R. Simmons, "Approaches for heuristically biasing RRT growth", Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS) , vol. 2, pp. 1178-1183, Oct. 2003.
Show in Context View Article Full Text: PDF (425) Google Scholar
19. J. Wang and M. Q.-H. Meng, "Optimal path planning using generalized Voronoi graph and multiple potential functions", IEEE Trans. Ind. Electron. .
Show in Context View Article Full Text: PDF (5508) Google Scholar
20. I. Baldwin and P. Newman, "Non-parametric learning for natural plan generation", Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. , pp. 4311-4317, Oct. 2010.
Show in Context View Article Full Text: PDF (1121) Google Scholar
21. B. Ichter, J. Harrison and M. Pavone, "Learning sampling distributions for robot motion planning", Proc. IEEE Int. Conf. Robot. Autom. (ICRA) , pp. 7087-7094, May 2018.
Show in Context View Article Full Text: PDF (1725) Google Scholar
22. A. H. Qureshi and M. C. Yip, "Deeply informed neural sampling for robot motion planning", Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS) , pp. 6582-6588, Oct. 2018.
Show in Context View Article Full Text: PDF (1074) Google Scholar
23. C. Zhang, J. Huh and D. D. Lee, "Learning implicit sampling distributions for motion planning", arXiv:1806.01968 , 2018, [online] Available: http://arxiv.org/abs/1806.01968.
Show in Context View Article Full Text: PDF (1337) Google Scholar
24. Y. Li, R. Cui, Z. Li and D. Xu, "Neural network approximation based near-optimal motion planning with kinodynamic constraints using RRT", IEEE Trans. Ind. Electron. , vol. 65, no. 11, pp. 8718-8729, Nov. 2018.
Show in Context View Article Full Text: PDF (1827) Google Scholar
25. L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff and H. Adam, "Encoder-decoder with atrous separable convolution for semantic image segmentation", Proc. Eur. Conf. Comput. Vis. (ECCV) , pp. 801-818, 2018.
Show in Context CrossRef Google Scholar
26. K. He, X. Zhang, S. Ren and J. Sun, "Deep residual learning for image recognition", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 770-778, Jun. 2016.
Show in Context View Article Full Text: PDF (282) Google Scholar
27. C. B. Choy, D. Xu, J. Gwak, K. Chen and S. Savarese, "3D-R2N2: A unified approach for single and multi-view 3D object reconstruction", Proc. Eur. Conf. Comput. Vis. , pp. 628-644, 2016.
Show in Context CrossRef Google Scholar
28. L.-C. Chen, G. Papandreou, F. Schroff and H. Adam, "Rethinking atrous convolution for semantic image segmentation", arXiv:1706.05587 , 2017, [online] Available: http://arxiv.org/abs/1706.05587.
Show in Context Google Scholar
29. J. Long, E. Shelhamer and T. Darrell, "Fully convolutional networks for semantic segmentation", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 3431-3440, Jun. 2015.
Show in Context View Article Full Text: PDF (373) Google Scholar
30. D. Kingma and J. Ba, "Adam: A method for stochastic optimization", Proc. Int. Conf. Learn. Representations (ICLR) , pp. 1-15, 2015.
Show in Context Google Scholar
31. C. Wang, J. Cheng, W. Chi, T. Yan and M. Q.-H. Meng, "Semantic-aware informative path planning for efficient object search using mobile robot", IEEE Trans. Syst. Man Cybern. Syst. .
Show in Context CrossRef Google Scholar
32. J. Wang and M. Q.-H. Meng, "Socially compliant path planning for robotic autonomous luggage trolley collection at airports", Sensors , vol. 19, no. 12, pp. 2759, Jun. 2019.
Show in Context CrossRef Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
