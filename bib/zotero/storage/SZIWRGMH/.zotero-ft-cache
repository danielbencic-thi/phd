IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Processing math: 0%

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Journals & Magazines > IEEE Internet of Things Journal > Volume: 7 Issue: 7
Deep-Reinforcement-Learning-Based Autonomous UAV Navigation With Sparse Rewards
Publisher: IEEE
Cite This
PDF
Chao Wang ; Jian Wang ; Jingjing Wang ; Xudong Zhang
All Authors
15
Paper
Citations
1819
Full
Text Views

    Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Problem Formulation and MDP Modeling
    III.
    Method
    IV.
    Experiments
    V.
    Conclusion

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
Footnotes
Abstract:
Unmanned aerial vehicles (UAVs) have the potential in delivering Internet-of-Things (IoT) services from a great height, creating an airborne domain of the IoT. In this article, we address the problem of autonomous UAV navigation in large-scale complex environments by formulating it as a Markov decision process with sparse rewards and propose an algorithm named deep reinforcement learning (RL) with nonexpert helpers (LwH). In contrast to prior RL-based methods that put huge efforts into reward shaping, we adopt the sparse reward scheme, i.e., a UAV will be rewarded if and only if it completes navigation tasks. Using the sparse reward scheme ensures that the solution is not biased toward potentially suboptimal directions. However, having no intermediate rewards hinders the agent from efficient learning since informative states are rarely encountered. To handle the challenge, we assume that a prior policy (nonexpert helper) that might be of poor performance is available to the learning agent. The prior policy plays the role of guiding the agent in exploring the state space by reshaping the behavior policy used for environmental interaction. It also assists the agent in achieving goals by setting dynamic learning objectives with increasing difficulty. To evaluate our proposed method, we construct a simulator for UAV navigation in large-scale complex environments and compare our algorithm with several baselines. Experimental results demonstrate that LwH significantly outperforms the state-of-the-art algorithms handling sparse rewards and yields impressive navigation policies comparable to those learned in the environment with dense rewards.
Published in: IEEE Internet of Things Journal ( Volume: 7 , Issue: 7 , July 2020 )
Page(s): 6180 - 6190
Date of Publication: 11 February 2020
ISSN Information:
INSPEC Accession Number: 19768296
DOI: 10.1109/JIOT.2020.2973193
Publisher: IEEE
Funding Agency:
SECTION I.
Introduction

Over the past few years, we have seen an unprecedented growth of unmanned aerial vehicles (UAVs) applications, such as goods delivery, emergency aid, and intelligent transportation [1] – [2] [3] . When combined with techniques, such as 5G networks [4] – [5] [6] and vehicular ad hoc networks [7] – [8] [9] [10] , UAVs have the potential in delivering Internet-of-Things (IoT) services from a great height, creating an airborne domain of the IoT. One of the key techniques behind these applications is the autonomous navigation of UAVs in large-scale complex environments. Conventional methods address the problem based on simultaneously localization-and-mapping techniques [11] – [12] [13] and sensing-and-avoidance techniques [14] – [15] [16] [17] . Though these nonlearning-based methods have shown satisfactory performance, they are generally limited to simple and small environments, such as the countryside or indoor areas. It is difficult to directly generalize them to large-scale complex environments since passively avoiding dense obstacles or actively constructing maps are inefficient when the environment is large and complex.

Observing that the UAV navigation problem is a sequential decision-making problem, researchers turn to reinforcement learning (RL)-based methods [18] – [19] [20] [21] . For example, Imanberdiyev et al. [18] developed a model-based RL algorithm as a high-level control method for UAV navigation in a grid map. Ross et al. [20] built an imitation learning (IL)-based controller using a small set of human expert demonstrations and achieved a good performance in natural forest environments. This article is mainly based on [21] that formulates UAV navigation in large-scale complex environments as a partially observable Markov decision process (MDP). While [21] focuses on addressing the problem of partial observability of states, in this article, we aim at solving the reward shaping problem raised by [21] , where manually designed reward functions are prone to yielding unexpected control policies. Specifically, the navigation problem is formulated as an MDP with sparse rewards. In contrast to the dense reward setting, where the UAV can get a reward each time it interacts with the environment by executing an action (i.e., control command), in the sparse-reward setting, the UAV can get a nonzero reward if and only if it successfully completes the navigation task. While defining sparse rewards is simple and straightforward, the underlying learning problem becomes hard to solve. As we all know, effective RL relies on collecting informative reward signals, so the agent has to discover a long sequence of “correct” actions so as to reach the target that yields sparse rewards. While the environment is scattered with dense obstacles (e.g., high buildings) and has a large coverage area, discovering sparse reward signals via random search is highly unlikely.

A magnitude of methods has been proposed to deal with the challenge raised by sparse rewards. The most basic approach is to reshape the reward function by utilizing intrinsic curiosity [22] , [23] or information gain [24] that encourages the agent to explore states that are rarely seen before. These methods often require fitting models of the environment dynamics so as to measure curiosity. However, either a poorly fitted environment model or less predictable environment dynamics can significantly degenerate their performance. Besides, since these intuitively designed rewards generally do not satisfy the policy invariance condition [25] that the optimal policy of an MDP is preserved if the added rewards for transitions between states can be expressed as the difference in the value of an arbitrary potential function, they may lead to suboptimal policies. A more common way is to use expert-demonstrated trajectories to guide the learning procedure by IL or Learning from Demonstrations (LfD). 1 IL aims at mimicking expert behaviors. As an IL method, inverse RL [26] derives control policies by recovering the reward function from expert demonstrations. The most recent progress of IL is generative adversarial IL [27] , which uses a discriminator to distinguish whether a state-action pair is similar to those given by experts. The confidence score is then utilized by the generator to learn a policy to confuse the discriminator. The major drawback of IL is that the performance of the learned policy is upper bounded by the expert policy and usually suffers from drastic performance degradation if expert demonstrations are imperfect. LfD overcomes such limitations by leveraging valuable feedback given by expert demonstrations as well as the environment. Deep Q-LfDs [28] and deep deterministic policy gradient from demonstrations (DDPGfD) [29] , for example, add potentially imperfect demonstrations into a replay buffer and never flush them out so that the learning agent can continuously learn from the data sampled from both demonstrations and the environment. Policy optimization from demonstrations (POfD) [30] , instead, leverages demonstrations by enforcing stationary state distribution matching between the learned policy and expert data. Though these LfD methods have shown state-of-the-art results in several simple Atari games [31] , their performance in environments with extremely sparse rewards is unclear. Besides, for real applications, we generally have strong prior information on the problem to be solved. However, such knowledge is rarely explored and exploited.

In this article, we formulate the problem of UAV navigation in large-scale complex environments as an MDP with extremely sparse rewards. UAVs would be rewarded if and only if it completes the navigation task. To overcome the challenge raised by sparse rewards, we propose an algorithm named DRL with nonexpert helpers (LwH). It is mainly based on three principles.

    A prior control policy (nonexpert helper) is assumed available to the learning agent. The policy could be of poor performance as long as it can assist the agent in achieving goals (completing navigation tasks) with some probability.

    The prior policy plays the role of helping the agent explore the environment as well as assisting the agent in achieving goals.

    The influence of the prior policy is reduced as learning progresses for the purpose of enabling the agent to achieve goals on its own.

LwH operates in a way similar to a baby learning to run. First, he learns to stand up with his mom’s help, then he struggles to walk by himself, and finally, he can walk and even run by himself.

It is reasonable to assume that a prior policy is available since for real applications, such as self-driving cars and UAV delivery, human-crafted policies (e.g., planning, sensing, and avoidance) have been applied at scale. In our UAV navigation scenario, supposing the destination is northeast of the UAV, “heading north while avoiding obstacles until the target is on the east, then heading east while avoiding obstacles until arriving at the destination” could be a feasible prior policy.

LwH deviates from previous methods in two aspects: 1) LwH requires a prior control policy (which could be of poor performance) instead of a set of demonstrations and 2) the learning objective of LwH changes over time while most of the existing work only focuses on static objectives. We investigate the LwH algorithm in the context of UAV navigation in large-scale complex environments. Our main contributions can be summarized as follows.

    To the best of our knowledge, this is the first work modeling UAV navigation in large-scale complex environments as an MDP with extremely sparse rewards.

    To address the sparse-reward problem, a nonexpert prior policy is used to guide the agent in exploring the state space and to set dynamical learning goals. Motivated by this, a learning algorithm is developed based on rigorous mathematical formulation and derivations.

    Extensive experimental results demonstrate that our method yields excellent navigation policies and significantly outperforms the state-of-the-art RL algorithms in terms of solving MDPs with sparse rewards.

The remainder of this article is outlined as follows. The problem formulation is detailed in Section II . Several baseline algorithms along with our proposed LwH addressing RL problems with sparse rewards are elaborated in Section III . In Section IV , extensive simulation results are provided for evaluating our proposed LwH algorithm followed by our conclusion and future work in Section V .
SECTION II.
Problem Formulation and MDP Modeling
A. Preliminaries: MDP and DRL

We model the UAV navigation problem as an MDP. An MDP is defined by the tuple ( \mathcal {S} , \mathcal {A} , \mathcal {P} , r , \gamma , \rho _{0} ) [32] , where \mathcal {S} is the state space, \mathcal {A} is the action space, \mathcal {P}\,\,:\,\,\mathcal {S}\times \mathcal {A}\times \mathcal {S}\rightarrow \mathbb {R} is the state transition probability, r\,\,:\,\,\mathcal {S}\times \mathcal {A} \rightarrow \mathbb {R} is the reward function, \gamma \in (0, 1) is the discount factor, and \rho _{0}~:~\mathcal {S}\rightarrow \mathbb {R} is the distribution of the initial state s_{0} . The goal of RL is to learn a parameterized control policy a\sim \pi _{\theta }(\cdot |s) (for ease of denotation, we make it implicit in all cases that \pi in fact represents \pi _{\theta } ) that maximizes an objective (or its surrogates) defined by long-term discounted cumulative rewards \begin{equation*} \max _{\theta } \eta \left ({\pi }\right) = \mathbb {E}_{s_{0}}\left [{V_{\pi }\left ({s_{0}}\right)}\right]\tag{1}\end{equation*} View Source \begin{equation*} \max _{\theta } \eta \left ({\pi }\right) = \mathbb {E}_{s_{0}}\left [{V_{\pi }\left ({s_{0}}\right)}\right]\tag{1}\end{equation*} where V_{\pi }(s) is referred as the value function of state s and defined as \begin{equation*} V_{\pi }\left ({s}\right)=\mathbb {E}_{s_{t\geq 1}\sim \mathcal {P}, a_{t\geq 0}\sim \pi } \left [{\sum \limits _{t=0}^{\infty } \gamma ^{t} r\left ({s_{t}, a_{t}}\right)\left |{s_{0}=s, \pi }\right.}\right]\tag{2}\end{equation*} View Source \begin{equation*} V_{\pi }\left ({s}\right)=\mathbb {E}_{s_{t\geq 1}\sim \mathcal {P}, a_{t\geq 0}\sim \pi } \left [{\sum \limits _{t=0}^{\infty } \gamma ^{t} r\left ({s_{t}, a_{t}}\right)\left |{s_{0}=s, \pi }\right.}\right]\tag{2}\end{equation*} RL sometimes involves in estimating the action-value function \begin{equation*} Q_{\pi }\left ({s, a}\right) = \mathbb {E}_{s^{\prime }\sim \mathcal {P}}\left [{r\left ({s,a}\right) + \gamma V_{\pi }\left ({s^{\prime }}\right)}\right]\tag{3}\end{equation*} View Source \begin{equation*} Q_{\pi }\left ({s, a}\right) = \mathbb {E}_{s^{\prime }\sim \mathcal {P}}\left [{r\left ({s,a}\right) + \gamma V_{\pi }\left ({s^{\prime }}\right)}\right]\tag{3}\end{equation*} which describes the expected return of taking action a_{t} at state s_{t} and thereafter following policy \pi . By definition, V_{\pi }(s) = \sum _{a} \pi (a|s) Q_{\pi }(s,a) . Besides, their difference A_{\pi }(s,a) = Q_{\pi }(s,a) - V_{\pi }(s) is referred as the advantage function. Apparently, we have \begin{equation*} \mathbb {E}_{a\sim \pi \left ({a|s}\right)}\left [{A_{\pi }\left ({s,a}\right)}\right] = 0.\tag{4}\end{equation*} View Source \begin{equation*} \mathbb {E}_{a\sim \pi \left ({a|s}\right)}\left [{A_{\pi }\left ({s,a}\right)}\right] = 0.\tag{4}\end{equation*}

Equation (1) is generally solved within the actor–critic framework [32] . The gradient of the objective function with respect to policy parameters \theta is given by [32] \begin{equation*} \nabla _{\theta } \eta \left ({\pi }\right) = \mathbb {E}_{s\sim d_{\pi }\left ({s}\right), a\sim \pi \left ({\cdot |s}\right)}\left [{\nabla _{\theta }\log \pi \left ({a|s}\right) \hat Q_{\pi }\left ({s,a}\right)}\right]\tag{5}\end{equation*} View Source \begin{equation*} \nabla _{\theta } \eta \left ({\pi }\right) = \mathbb {E}_{s\sim d_{\pi }\left ({s}\right), a\sim \pi \left ({\cdot |s}\right)}\left [{\nabla _{\theta }\log \pi \left ({a|s}\right) \hat Q_{\pi }\left ({s,a}\right)}\right]\tag{5}\end{equation*} where d_{\pi }(s) is the unnormalized state distribution induced by the policy \pi , and \hat Q_{\pi }(s,a) is an estimate of the action–value function Q_{\pi }(s,a) , which is often parameterized by a function Q_{\omega }(s,a) [or V_{\omega }(s) ]. The critic is adjusted by minimizing the n -step time-difference (TD) error [32] \begin{align*}&\mathbb {E}_{a\sim \pi \left ({\cdot |s}\right)}\left \|{\sum _{\tau =0}^{n} r\left ({s_{t+\tau },a_{t+\tau }}\right) }\right. \\&\qquad \qquad \left.{+\,\,\gamma Q_{\omega }\left ({s_{t+\tau +1},a_ {t+\tau +1}}\right) - Q_{\omega }\left ({s_{t},a_{t}}\right)\vphantom {\left \|{\sum _{\tau =0}^{n} r\left ({s_{t+\tau },a_{t+\tau }}\right) }\right.}}\right \|_{2}\tag{6}\end{align*} View Source \begin{align*}&\mathbb {E}_{a\sim \pi \left ({\cdot |s}\right)}\left \|{\sum _{\tau =0}^{n} r\left ({s_{t+\tau },a_{t+\tau }}\right) }\right. \\&\qquad \qquad \left.{+\,\,\gamma Q_{\omega }\left ({s_{t+\tau +1},a_ {t+\tau +1}}\right) - Q_{\omega }\left ({s_{t},a_{t}}\right)\vphantom {\left \|{\sum _{\tau =0}^{n} r\left ({s_{t+\tau },a_{t+\tau }}\right) }\right.}}\right \|_{2}\tag{6}\end{align*} where Q_{\omega }(s,a) can be replaced with V_{\omega }(s) if we use V_{\omega }(s) to estimate Q_{\pi }(s,a) . In the actor–critic framework, the policy and the action–value function are also called the actor and the critic.
B. MDP Modeling of UAV Navigation With Sparse Rewards

Here, we give a formal description of the problem of UAV navigation in large-scale complex environments. Suppose a UAV is located at some point (departure position) in a 3-D environment, which is denoted as (x_{0}, y_{0}, z_{0}) in the Earth-fixed coordinate frame, and targets at flying to a destination that is denoted as (x_{d}, y_{d}, z_{d}) . Dense obstacles are scattered in the environment. The environment map, departure positions, and destinations are randomly generated across different navigation tasks. The UAV can only have access to its sensor readings of observations of the local environment as well as its relative position to the destination. The goal of the UAV is to fly from the departure place to the destination by controlling its speed, direction, and height based on limited sensor observations.

For simplicity, we assume that the UAV flies at a fixed height (i.e., z_{0}=z_{d}=H , where H is a positive constant), and suppose that control commands can take effect in no time. Then, the dynamics of the UAV can be formulated as \begin{align*} \begin{cases} v_{t+1} = v_{t} + \rho _{t}\\ \phi _{t+1} = \phi _{t} + \varphi _{t}\\ x_{t+1} = x_{t} + v_{t+1}\times \cos \left ({{\phi }_{t+1}}\right)\\ y_{t+1} = y_{t} + v_{t+1}\times \sin \left ({{\phi }_{t+1}}\right) \end{cases}\tag{7}\end{align*} View Source \begin{align*} \begin{cases} v_{t+1} = v_{t} + \rho _{t}\\ \phi _{t+1} = \phi _{t} + \varphi _{t}\\ x_{t+1} = x_{t} + v_{t+1}\times \cos \left ({{\phi }_{t+1}}\right)\\ y_{t+1} = y_{t} + v_{t+1}\times \sin \left ({{\phi }_{t+1}}\right) \end{cases}\tag{7}\end{align*} where v_{t} and \phi _{t} denote the speed and the first-perspective direction of the UAV at time step t , and \rho _{t} and \varphi _{t} denote the throttle and the steering commands. In this article, we assume that the UAV’s observation of the local environment is achieved by range finders, as illustrated in Fig. 1 , and its relative position to the destination is measured by the GPS. The overall observations can be denoted as o_{t}=[d_{1}^{t},\ldots, d_{16}^{t}, d_{r}^{t}, \xi ^{t}, v^{t}, \phi ^{t}] , where d_{r}^{t} and \xi ^{t} denote the distance and the angle (relative to the north direction) between the UAV’s current position and the destination at time step t . To model the navigation problem, we regard the UAV’s overall observations as the system’s states and the throttle as well as the steering commands as users’ actions. The state transition probability is automatically determined by the environment, which is unknown.
Fig. 1.

Illustration of a UAV’s observation of the local environment. The UAV and buildings (obstacles) are represented as an orange sphere and gray cylinders, respectively. The red arrow represents the UAV’s first-perspective direction and the green lines represent signals emitted from range finders.

Show All

In this article, we use the sparse reward scheme, i.e., the UAV would be rewarded if and only if its distance to the destination is less than a prefixed threshold. Specifically, we define the sparse reward as \begin{align*} r_{sp}\left ({s_{t},a_{t}}\right) = \begin{cases} 1,&\quad {\left \|{\left ({x_{t}, y_{t}}\right) - \left ({x_{d}, y_{d}}\right)}\right \|_{2}\leq r_{d}}\\ 0,&\quad \text {otherwise} \end{cases}\tag{8}\end{align*} View Source \begin{align*} r_{sp}\left ({s_{t},a_{t}}\right) = \begin{cases} 1,&\quad {\left \|{\left ({x_{t}, y_{t}}\right) - \left ({x_{d}, y_{d}}\right)}\right \|_{2}\leq r_{d}}\\ 0,&\quad \text {otherwise} \end{cases}\tag{8}\end{align*} where r_{d} is a prefixed constant and (x_{t}, y_{t}) denotes the UAV’s horizontal position at time step t .
SECTION III.
Method
A. Baselines: A3C, DDPGfD, and POfD

A3C: A3C [33] constructs approximations to the policy \pi (a|s) and the value V_{\pi }(s) using two neural networks with parameters \theta and \omega . Besides, the policy network and the value network generally share the same hidden architecture and parameters. Parameters \theta are adjusted by \begin{equation*} \Delta \theta = \sum \limits _{t} \nabla _{\theta } \log \left ({\pi \left ({a_{t}|s_{t}}\right)}\right)\left ({R_{t}\left ({n}\right) - V_{\omega }\left ({s_{t}}\right)}\right)\tag{9}\end{equation*} View Source \begin{equation*} \Delta \theta = \sum \limits _{t} \nabla _{\theta } \log \left ({\pi \left ({a_{t}|s_{t}}\right)}\right)\left ({R_{t}\left ({n}\right) - V_{\omega }\left ({s_{t}}\right)}\right)\tag{9}\end{equation*} and parameters \omega are adjusted by minimizing \begin{equation*} J_{V} = \sum \limits _{t} \left [{\left ({R_{t}\left ({n}\right) - V_{\omega }\left ({s_{t}}\right)}\right)^{2}}\right]\tag{10}\end{equation*} View Source \begin{equation*} J_{V} = \sum \limits _{t} \left [{\left ({R_{t}\left ({n}\right) - V_{\omega }\left ({s_{t}}\right)}\right)^{2}}\right]\tag{10}\end{equation*} where \begin{equation*} R_{t}\left ({n}\right) = \sum \limits _{i=t}^{t+n-1} \gamma ^{i-t}r_{i} + \gamma ^{n} V_{\omega }\left ({s_{t+n}}\right).\tag{11}\end{equation*} View Source \begin{equation*} R_{t}\left ({n}\right) = \sum \limits _{i=t}^{t+n-1} \gamma ^{i-t}r_{i} + \gamma ^{n} V_{\omega }\left ({s_{t+n}}\right).\tag{11}\end{equation*} In A3C, many instances of the agent interact in parallel with many instances of the environment and update parameters of the policy network and the value network asynchronously, which stabilizes and accelerates the learning procedure.

DDPGfD: DDPGfD [29] , built upon DDPG [34] , settles DRL problems with sparse rewards by taking advantage of potentially imperfect demonstrations. DDPG aims at learning a deterministic policy that directly maps states to actions. The actor and the critic are modeled as two neural networks a = \mu _{\theta }(s) and Q_{\omega }(s,a) . The actor parameters \theta are updated by \begin{equation*} \Delta _{\theta } = \mathbb {E}_{s\sim d_{\mu \left ({s}\right)}} \left [{\nabla _{\theta }\mu _{\theta }\left ({s}\right) \left.{\nabla _{a} Q_{\omega }\left ({s,a}\right)}\right |_{a=\mu _{\theta }\left ({s}\right)}}\right]\tag{12}\end{equation*} View Source \begin{equation*} \Delta _{\theta } = \mathbb {E}_{s\sim d_{\mu \left ({s}\right)}} \left [{\nabla _{\theta }\mu _{\theta }\left ({s}\right) \left.{\nabla _{a} Q_{\omega }\left ({s,a}\right)}\right |_{a=\mu _{\theta }\left ({s}\right)}}\right]\tag{12}\end{equation*} and the critic parameters \omega are updated by \begin{equation*} J_{Q} = \mathbb {E}_{\mathcal {D}} \left [{\left ({R_{t}\left ({n}\right) - Q_{\omega }\left ({s_{t}, a_{t}}\right)}\right)^{2}}\right]\tag{13}\end{equation*} View Source \begin{equation*} J_{Q} = \mathbb {E}_{\mathcal {D}} \left [{\left ({R_{t}\left ({n}\right) - Q_{\omega }\left ({s_{t}, a_{t}}\right)}\right)^{2}}\right]\tag{13}\end{equation*} where \begin{equation*} R_{t}\left ({n}\right) = \sum \limits _{i=t}^{t+n-1} \gamma ^{i-t}r_{i} + Q_{\omega }\left ({s_{t+n}, \mu _{\theta }\left ({s_{t+n}}\right)}\right)\tag{14}\end{equation*} View Source \begin{equation*} R_{t}\left ({n}\right) = \sum \limits _{i=t}^{t+n-1} \gamma ^{i-t}r_{i} + Q_{\omega }\left ({s_{t+n}, \mu _{\theta }\left ({s_{t+n}}\right)}\right)\tag{14}\end{equation*} and n generally takes the value 1. The critic parameters \omega and the actor parameters \theta are updated by sampling a minibatch of transition tuples (s_{t}, a_{t}, s_{t+1}, r_{t}) from a replay memory \mathcal {D} , which, as a tool of stabilizing the learning procedure, caches history transitions within a prefixed time window. To handle the sparse-reward problem, DDPGfD puts demonstrations into a separate replay memory \mathcal {D}^{E} and keeps them throughout the entire learning process. Compared to DDPG, DDPGfD updates the actor and the critic by sampling data from both \mathcal {D} and \mathcal {D}^{E} . The objective for optimizing Q_{\omega } becomes \begin{align*} J_{Q}=&\mathbb {E}_{\mathcal {D}} \left [{\left ({R_{t}\left ({n}\right) - Q_{\omega }\left ({s_{t}, a_{t}}\right)}\right)^{2}}\right] \\&+\,\,\alpha \mathbb {E}_{\mathcal {D}^{E}}\left [{\left ({R_{t}\left ({n}\right) - Q_{\omega }\left ({s_{t}, a_{t}}\right)}\right)^{2}}\right]\tag{15}\end{align*} View Source \begin{align*} J_{Q}=&\mathbb {E}_{\mathcal {D}} \left [{\left ({R_{t}\left ({n}\right) - Q_{\omega }\left ({s_{t}, a_{t}}\right)}\right)^{2}}\right] \\&+\,\,\alpha \mathbb {E}_{\mathcal {D}^{E}}\left [{\left ({R_{t}\left ({n}\right) - Q_{\omega }\left ({s_{t}, a_{t}}\right)}\right)^{2}}\right]\tag{15}\end{align*} where \alpha denotes the ratio of samples from \mathcal {D} and \mathcal {D}^{E} .

POfD: In contrast to DDPGfD that places demonstrations into a replay memory, POfD [30] leverages demonstrations through enforcing occupancy measure (also known as state distribution) matching between the learned policy and demonstrations. This gives a surrogate learning objective \begin{align*}&\min _{\theta }\max _{\nu } -\mathbb {E}_{\pi }\left [{r^{\prime }\left ({s,a}\right)}\right] - \lambda _{2} H\left ({\pi }\right) \\&+\,\,\lambda _{1} \mathbb {E}_{\pi _{E}}\left [{\log \left ({1-D_{\nu }\left ({s,a}\right)}\right)}\right]\tag{16}\end{align*} View Source \begin{align*}&\min _{\theta }\max _{\nu } -\mathbb {E}_{\pi }\left [{r^{\prime }\left ({s,a}\right)}\right] - \lambda _{2} H\left ({\pi }\right) \\&+\,\,\lambda _{1} \mathbb {E}_{\pi _{E}}\left [{\log \left ({1-D_{\nu }\left ({s,a}\right)}\right)}\right]\tag{16}\end{align*} where \pi _{E} is the policy generating demonstrations, D_{\nu }(s,a) is a discriminator judging whether a state–action pair (s,a) is from demonstrations, r^{\prime }(s,a) = r(s,a) - \lambda _{1}\log (D_{\nu }(s,a)) is a virtual reshaped reward function, which augments the sparse reward r(s,a) with demonstration information, H(\pi) is the policy entropy, and \lambda _{1} and \lambda _{2} are two hyperparameters balancing the strength of different parts of the loss function. The above objective is similar with generative-adversarial networks (GANs) [35] and is optimized by alternately updating policy parameters \theta and discriminator parameters \nu . First, the discriminator is optimized using demonstration data as well as trajectories given by the current policy \pi (a|s) . The gradient is computed by \begin{equation*} \mathbb {E}_{\pi } \left [{\nabla _{\nu } \log \left [{D_{\nu }\left ({s,a}\right)}\right]}\right] + \mathbb {E}_{\pi _{E}} \left [{\log \left ({1-D_{\nu }\left ({s,a}\right)}\right)}\right].\tag{17}\end{equation*} View Source \begin{equation*} \mathbb {E}_{\pi } \left [{\nabla _{\nu } \log \left [{D_{\nu }\left ({s,a}\right)}\right]}\right] + \mathbb {E}_{\pi _{E}} \left [{\log \left ({1-D_{\nu }\left ({s,a}\right)}\right)}\right].\tag{17}\end{equation*} Given the virtual reshaped reward, the policy parameters \theta can be optimized by standard DRL methods, such as A3C [33] and TRPO [36] .
B. DRL With Nonexpert Helpers

LwH assumes that the learning agent can have access to a prior control policy \pi _{h}(a|s) that may be of poor performance. The prior policy can help the agent in achieving goals with some probability and is used for exploring the state space and reshaping the learning objective.

First, the prior policy plays the role of guiding the agent in efficiently exploring the state space, which is achieved by \begin{equation*} \pi _{b}\left ({a|s}\right)\propto \pi \left ({a|s}\right)\cdot \pi _{h}\left ({a|s}\right).\tag{18}\end{equation*} View Source \begin{equation*} \pi _{b}\left ({a|s}\right)\propto \pi \left ({a|s}\right)\cdot \pi _{h}\left ({a|s}\right).\tag{18}\end{equation*} where \pi _{b}(a|s) refers to the behavior policy that is used by the agent for environment interaction and \pi (a|s) is the policy learned by the agent (see Appendix A ). Equation (18) encodes that the behavior policy \pi _{b} explores the state space by synthesizing information from two independent sources: one from the prior policy \pi _{h} and the other from the learned policy \pi . With such a relationship, it is apparent that the behavior policy is mainly controlled by the source policy with lower variance. If the variance of the learned policy is smaller than that of the prior policy, the behavior policy is mainly controlled by the learned policy. On the contrary, if the variance of the prior policy is smaller, the behavior policy is mainly determined by the prior policy. As a consequence, though the learned policy is similar to a uniform distribution (because it is randomly initialized) in the early training stage, the behavior policy can still perform efficient exploration of the state space since the prior policy has a lower variance.

The prior policy also plays the role of assisting the learning agent in achieving goals. We see that if the behavior policy \pi _{b}(a|s) is used for environment interaction, importance sampling is needed for an unbiased estimate of the stochastic gradient in (5) of the original DRL objective in (1) , since the expectation in (5) is with respect to the state distribution d_{\pi }(s) induced by the learned policy \pi (a|s) instead of the behavior policy \pi _{b}(a|s) and the expectation in (6) is also with respect to the learned policy. While in the early learning stage, the behavior policy may significantly deviate from the learned policy, the variance of the importance sampling ratio could be tremendously large and thus might hinder the agent from learning useful policies. In light of this, we propose to use \eta (\pi _{b}) as the learning objective to avoid using importance sampling. Changing the learning objective from \eta (\pi) to \eta (\pi _{b}) indicates that we require the learned policy together with the prior policy instead of only the learned policy to achieve goals.

Given the behavior policy is computed by (18) , the policy gradient of \eta (\pi _{b}) is given by \begin{equation*} \nabla _{\theta } \eta \left ({\pi _{b}}\right) = \mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right)}\left [{\nabla _{\theta }\log \pi \left ({a|s}\right) A_{\pi _{b}}\left ({s,a}\right)}\right]\tag{19}\end{equation*} View Source \begin{equation*} \nabla _{\theta } \eta \left ({\pi _{b}}\right) = \mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right)}\left [{\nabla _{\theta }\log \pi \left ({a|s}\right) A_{\pi _{b}}\left ({s,a}\right)}\right]\tag{19}\end{equation*} where d_{\pi _{b}}(s) denotes the state distribution induced by the behavior policy \pi _{b}(a|s) , and A_{\pi _{b}}(s,a) is the corresponding advantage function (see Appendix B ). Apparently, data [i.e., state transition tuples (s_{t}, a_{t}, s_{t+1}, r_{t}) ] collected by the behavior policy can be directly utilized to estimate the gradient in (19) without importance sampling, since the expectation in (19) is exactly with respect to the state distribution d_{\pi _{b}}(s) induced by the behavior policy \pi _{b}(a|s) instead of the state distribution d_{\pi }(s) induced by the learned policy \pi (a|s) . Additionally, the advantage function A_{\pi _{b}}(s,a) is also with respect to the behavior policy \pi _{b} rather than the learned policy \pi .

Compared to (5) that uses the action–value function Q_{\pi }(s,a) of the learned policy to guide the policy learning, (19) uses the advantage function A_{\pi _{b}}(s,a) of the behavior policy. In this respect, the learned policy can leverage the prior policy to improve its performance (since the behavior policy is constructed by both the learned policy and the prior policy) even though the environment gives sparse feedback.

As the ultimate goal is to learn a policy that can achieve goals independently, we need to bridge the gap between \eta (\pi _{b}) and \eta (\pi) . The two objectives hold the following relationship (in fact, it holds true for any two policies [36] ): \begin{equation*} \eta \left ({\pi _{b}}\right) = \eta \left ({\pi }\right) + \mathbb {E}_{s_{t\geq 0}, a_{t \geq 0},\ldots, \sim \pi _{b}}\left [{\sum \limits _{t=0}^{\infty }\gamma ^{t} A_{\pi }\left ({s,a}\right)}\right].\tag{20}\end{equation*} View Source \begin{equation*} \eta \left ({\pi _{b}}\right) = \eta \left ({\pi }\right) + \mathbb {E}_{s_{t\geq 0}, a_{t \geq 0},\ldots, \sim \pi _{b}}\left [{\sum \limits _{t=0}^{\infty }\gamma ^{t} A_{\pi }\left ({s,a}\right)}\right].\tag{20}\end{equation*} By (4) , the second term in the left-hand side of (20) would be equal to zero if the behavior policy \pi _{b} approaches the learned policy \pi , which gives \begin{equation*} \lim _{\pi _{b}\rightarrow \pi } \eta \left ({\pi _{b}}\right) = \eta \left ({\pi }\right).\tag{21}\end{equation*} View Source \begin{equation*} \lim _{\pi _{b}\rightarrow \pi } \eta \left ({\pi _{b}}\right) = \eta \left ({\pi }\right).\tag{21}\end{equation*} In view of this, we propose to reduce the influence of the prior policy on the behavior policy as learning progresses (which would be detailed in Section IV-A ). This is equivalent to gradually shifting the learning objective from \eta (\pi _{b}) to \eta (\pi) , or in other words, we require the learned policy to achieve goals together with the prior policy in the early learning stage, and as learning progresses, we hope the learned policy can achieve goals on its own.

Although our proposed LwH is compatible with most DRL algorithms handling stochastic policies (e.g., A3C [33] , PPO [37] , and TRPO [36] ), we implement it based on A3C in this article. The pseudocode of the algorithm is demonstrated in Algorithm 1 .
Algorithm 1 LwH—Pseudocode for Each Thread

Assuming parameters of the policy and the value nets \pi _\theta and V_{\omega } and the counter T=0 are shared across different threads; Assuming the tread ID is j\in [1, \cdots, J]

Initialize thread-specific step counter t_{j}\leftarrow 1

repeat

Synchronize thread-specific parameters \theta _{j}\leftarrow \theta and \omega _{j}\leftarrow \omega

Set t_{\mathrm {init} = t} and observe state s_{t}

repeat

Reduce the influence of the prior policy (by (25) )

Compute the behavior policy \pi _{b}(a|s) (by (22) )

Execute the action a_{t}\sim \pi _{b}(\cdot |s)

Receive reward r_{t} and new state s_{t+1}

Update step counters: t\leftarrow t+1 , T\leftarrow T+1

until terminal s_{t} or t - t_{\mathrm {init}} == t_{\max }

for i=t_{\mathrm {init}}, t-1 do

Estimate the expected return \hat R_{i} of \pi _{b} in each state s_{i} using generalized advantage estimation [38]

end for

Compute the gradient with respect to \theta by

\Delta \theta _{j} = \sum \limits _{i=t_{\mathrm {init}}}^{t-1} \left [{\nabla _{\theta _{j}} \log \pi _{\theta _{j}}(a_{i}|s_{i}) (\hat R_{i} - V_{\omega _{j}}(s_{i}))}\right]

Compute the gradient with respect to \omega by

\Delta \omega _{j} = \sum \limits _{i=t_{\mathrm {init}}}^{t-1} \left [{\nabla _{\omega _{j}} \partial (R_{i} - V_{\omega _{j}}(s_{i}))^{2}/ \partial {\omega _{j}}}\right]

Asynchronously update \theta and \omega using \Delta _{\theta _{j}} and \Delta _{\omega _{j}}

until T\geq T_{\max }

SECTION IV.
Experiments
A. Virtual Environment and Experimental Setting

We construct a virtual environment by the simulation to implement UAV navigation in large-scale complex environments as well as evaluate the performance of our proposed method. The virtual environment covers around 4 km 2 . Buildings are represented as cylinders with random heights and the UAV is represented as a sphere. As a stochastic environment, each time a navigation task ends, the environment map, the departure position, and the destination are randomly reinitialized again. The maximum speed of the UAV is restricted to 50 m/s. Details of the virtual environment can be found in Appendix C . Source codes for the virtual environment as well as our proposed algorithm LwH has been open sourced at https://github.com/DennisWangCW/LwH and https://github.com/DennisWangCW/gym-uav , respectively.

The implementation of LwH is based on A3C [33] . The hyperparameters of LwH are summarized in Table II in Appendix D . The policy and the value function are approximated by two neural networks \pi (\cdot |s) and V_{\omega }(s) with parameters \theta and \omega , as illustrated in Fig. 2 . As most existing methods do, the stochastic policy is approximated by Gaussian distributions. The policy network outputs the mean and the standard deviation, i.e., \mu _{\theta }(s) and \sigma _{\theta }(s) . Given the relationship demonstrated in (18) , if we denote the mean and the standard deviation of the prior policy as \mu _{h}(s) and \sigma _{h}(s) , then the behavior policy can be computed by \begin{equation*} \pi _{b}\left ({a|s}\right) = \frac {1}{\sqrt {2\pi }\sigma _{b}\left ({s}\right)}e^{-\frac {\left ({a-\mu _{b}\left ({s}\right)}\right)^{2}}{2\sigma ^{2}_{b}\left ({s}\right)}}\tag{22}\end{equation*} View Source \begin{equation*} \pi _{b}\left ({a|s}\right) = \frac {1}{\sqrt {2\pi }\sigma _{b}\left ({s}\right)}e^{-\frac {\left ({a-\mu _{b}\left ({s}\right)}\right)^{2}}{2\sigma ^{2}_{b}\left ({s}\right)}}\tag{22}\end{equation*} where \begin{equation*} \sigma _{b}^{2}\left ({s}\right) = \frac {\sigma ^{2}_{h}\left ({s}\right)\sigma ^{2}_{\theta }\left ({s}\right)}{{\sigma ^{2}_{h}\left ({s}\right)}+{\sigma ^{2}_{\theta }\left ({s}\right)}}\tag{23}\end{equation*} View Source \begin{equation*} \sigma _{b}^{2}\left ({s}\right) = \frac {\sigma ^{2}_{h}\left ({s}\right)\sigma ^{2}_{\theta }\left ({s}\right)}{{\sigma ^{2}_{h}\left ({s}\right)}+{\sigma ^{2}_{\theta }\left ({s}\right)}}\tag{23}\end{equation*} and \begin{equation*} \mu _{b}\left ({s}\right) = \frac {\sigma ^{2}_{\theta }\left ({s}\right)\mu _{h}\left ({s}\right) + \sigma ^{2}_{h}\left ({s}\right)\mu _{\theta }\left ({s}\right)}{\sigma ^{2}_{\theta }\left ({s}\right) + \sigma ^{2}_{h}\left ({s}\right)}.\tag{24}\end{equation*} View Source \begin{equation*} \mu _{b}\left ({s}\right) = \frac {\sigma ^{2}_{\theta }\left ({s}\right)\mu _{h}\left ({s}\right) + \sigma ^{2}_{h}\left ({s}\right)\mu _{\theta }\left ({s}\right)}{\sigma ^{2}_{\theta }\left ({s}\right) + \sigma ^{2}_{h}\left ({s}\right)}.\tag{24}\end{equation*} In this article, the influence of the prior policy is reduced by linearly increasing the standard deviation of the prior policy as learning progresses, i.e., \begin{equation*} \sigma _{h}^{t+1}\left ({s}\right) = \sigma _{h}^{t}\left ({s}\right) \times \beta T\tag{25}\end{equation*} View Source \begin{equation*} \sigma _{h}^{t+1}\left ({s}\right) = \sigma _{h}^{t}\left ({s}\right) \times \beta T\tag{25}\end{equation*} where T refers to the global time step and \beta is the decay rate.
Fig. 2.

Network structures of the policy \pi _{\theta }(\cdot |s) and the value function V_{\omega }(s) . The policy network takes a state as input and outputs the mean and the standard deviation of the policy, and the value network takes a state as input and outputs its value. (a) Policy network. (b) Value network.

Show All

Two prior policies are used to evaluate the performance of LwH. The first prior policy follows a sensing-and-avoidance strategy while the second follows a relatively naive strategy. For ease of reference, we denote the two prior policies as SenAvo-Pri and Naive-Pri, respectively (schematic views of the two prior policies appear in Fig. 3 ). For each prior policy, we restrict the expected speed to 2.5 m/s. The two prior policies are in poor performance since: 1) the speed is low and cannot be dynamically adjusted according to the structure of the local environment and 2) the direction is controlled by simple handcrafted policies and cannot be generalized to complex environments. Codes for the two prior policies have been packed into the source code.
Fig. 3.

Schematic view of the two prior control policies. (a) SenAvo-Pri. (b) Naive-Pri.

Show All

B. Comparison With the Baselines

Below, we evaluate the effectiveness of LwH in the virtual UAV navigation environment with sparse rewards. The standard deviation of the two prior policies is set to \sigma _{h}(s) = 0.4 and the prior decay rate is set to \beta = 5e^{-5} . For a better understanding of LwH, we first evaluate the performance of the two prior policies by running each for 100 navigation tasks. The results show that the success rates (of navigation tasks) of the two prior policies are only 0.36 and 0.42, respectively, suggesting that they are indeed in poor performance. For purposes of comparisons, we also reimplement the standard A3C algorithm, DDPGfD, and POfD in the virtual environment. As DDPGfD and POfD require demonstrations instead of prior policies, we generate 1e^{6} transition tuples (around 1500 episodes) by running the two prior policies ( [30] claims that POfD can learn with only a single demonstrated episode). The success rates of navigation tasks are given in Table I . We see that no matter which prior policy is employed, our proposed LwH can achieve a success rate of more than 0.96. In contrast, POfD gives a success rate of 0.76 when trained with demonstrations given by SenseAvo-Pri and 0.41 when trained with demonstrations given by Naive-Pri. DDPGfD and A3C just fail to complete any navigation tasks. The learning curves are plotted in Fig. 4 . Our LwH again converges much faster than the baseline algorithms in all cases. Interestingly, POfD does learn to navigate to some extent in the early training stage, but as learning progresses, the performance degrades unexpectedly. In contrast, both DDPGfD and A3C fail to bring any performance improvement to the learned policy during the entire training process.
TABLE I Success Rates of Policies Given by Different Algorithms
TABLE II Hyperparameters of LwH
Fig. 4.

Mean success rate versus the training step curves for different algorithms. The success rate is obtained by evaluating each learned navigation policy over 100 randomly generated navigation tasks. The mean success rate is estimated by taking the average of the success rates of three independent trials of each algorithm with different random seeds. (a) Trained with SenAvo-Pri. (b) Trained with Naive-Pri.

Show All

We believe the success of LwH contributes to three aspects: 1) the structure of the behavior policy enables the agent to explore the state space by synthesizing information from different sources; 2) the prior policy helps the agent efficiently explore the state space especially in the early training stage, which enables the agent to effectively update policy parameters even though the gradient is biased [in terms of the ultimate learning objective (1) ]; and 3) the mechanism of reducing the influence of the prior policy as learning progresses set dynamic learning objectives with increasing difficulty for the agent in different training stages. It eliminates the situation that the learning objective is too difficult while the learning agent is too weak to achieve such goals and makes it possible for the agent to derive useful policies step by step.
C. Evaluating the Learned Policy

Below, we evaluate the performance of the learned policy given by our LwH trained with different prior policies in detail. The trajectories of several navigation tasks along with the corresponding speed versus time curves appear in Fig. 5 and videos can be found at https://youtu.be/m575rkcacio . We can observe from Fig. 5 that: 1) the policy can dynamically adjust the UAV’s speed and direction according to the structure of the local environment as well as its distance to the destination, which satisfies our expectation that the UAV can learn to autonomously control its speed and direction to adapt to the complex environment and in the meantime perform navigation tasks and 2) the learned policy significantly deviates from the prior policy. For example, the learned policies can adjust the UAV’s speed within the range of [0 ~\mathrm {m/s}, 50~\mathrm {m/s}] while the expected speed is prefixed to 2.5 m/s if the prior policy is employed. Besides, when the Naive-Pri policy is used to guide the learning procedure, LwH yields a policy that can control the UAV’s direction based on its relative position to the destination as well as the structure of the local environment rather than simply mimic the Naive-Pri policy. In summary, LwH is able to learn human-like navigation behaviors by leveraging prior policies to boost self-learning in environments with sparse rewards and avoids overfitting to the prior policy.
Fig. 5.

Navigation trajectories and the corresponding speed versus time curves. Red and blue spheres denote departure places and destinations. The navigation trajectory is denoted as a sequence of orange spheres. Building lower than the UAV’s flight altitude is not shown in the figure. (a) Trajectory I (LwH trained with SenAvo-Pri). (b) Trajectory II (LwH trained with SenAvo-Pri). (c) Trajectory III (LwH trained with Naive-Pri). (d) Trajectory IV (LwH trained with Naive-Pri).

Show All

D. Robustness to Hyperparameter Configurations

The performance of LwH can be affected by the standard deviation of the prior policy \sigma _{h}(s) and its decay rate \beta . To see whether the LwH is robust to these hyperparameters, we implement LwH with a range of hyperparameter configurations. Specifically, \beta \in [5e^{-6}, 1e^{-5}, 5e^{-5}, 1e^{-4}] and \sigma _{h}(s)\in [{0.2, 0.3, 0.4, 0.5}] . Fig. 6 illustrates the success rates of policies given by LwH with different hyperparameter configurations. As we can see, though LwH does not converge under several hyperparameter configurations, in general, it shows robustness to the two hyperparameters. We also notice that if the combination of the standard deviation of the prior policy and its decay rate yields fast decay of the influence of the prior policy, LwH converges much slower (especially when the Naive-Pri is employed). That is because the learning agent can rarely benefit from the prior policy in efficiently exploring the state space. Besides, fast decay of the influence of the prior policy also makes the dynamic learning objective too difficult for the agent to achieve. In contrast, we see that if the combination of the two hyperparameters yields too slow decay of the influence of the prior policy, the agent may fail to learn useful behaviors. The reason is that the objective of LwH is to learn a policy that can complete tasks under the assistant of the prior policy. If the influence of the prior policy decays too slowly during the training phase, the learned policy will have little incentive to learn useful behaviors on its own.
Fig. 6.

Mean success rate versus the training step curves for our proposed algorithm LwH with different configurations of the standard deviation of the prior policy \sigma _{h}(s) and the prior decay rate \beta . The success rate is obtained in the same way as those in Fig. 4. (a) SenAvo-Pri, \sigma _{h}(s) =0.2 . (b) SenAvo-Pri, \sigma _{h}(s) =0.3 . (c) SenAvo-Pri, \sigma _{h}(s) =0.4 . (d) SenAvo-Pri, \sigma _{h}(s) =0.5 . (e) Naive-Pri, \sigma _{h}(s) =0.2 . (f) Naive-Pri, \sigma _{h}(s) =0.3 . (g) Naive-Pri, \sigma _{h}(s) =0.4 . (h) Naive-Pri, \sigma _{h}(s) =0.5 .

Show All

E. Ablation Study: Learning Without Prior Decay

We have seen in Section IV-B that without the assistance of the prior policy, the agent (i.e., the A3C baseline) cannot derive any useful navigation behaviors. Below, we investigate another extreme case that implementing LwH without reducing the influence of the prior policy. In this case, LwH implemented with the two prior policies with different standard deviation configurations \sigma _{h}(s)\in [{0.2, 0.3, 0.4, 0.5}] . The simulation results are demonstrated in Fig. 7 . As we see, no matter which prior policy is employed, the learned policy only achieves a maximum success rate of around 0.15, significantly lower than those in the standard setting in Section IV-B . Additionally, similar to the phenomenon observed in Section IV-D , we see that the stronger the influence of the prior policy on the behavior policy, the lower the final performance of the learned policy. The reason is that if the influence of the prior policy is not reduced, the learning objective cannot shift from \eta (\pi _{b}) to \eta (\pi) , the ultimate goal of DRL. As a consequence, the agent only learns to achieve goals under the assistance of the prior policy instead of on its own. This demonstrates that the mechanism of reducing the influence of the prior policy as learning progresses is key to the success of LwH.
Fig. 7.

Mean success rate versus the training step curves for the algorithm LwH implemented without prior decay. The success rate is obtained in the same way as those in Fig. 4. (a) SenAvo-Pri, no prior decay. (b) Naive-Pri, no prior decay.

Show All

F. Learning in Environments With Nonsparse Rewards

To have a more comprehensive evaluation of LwH and to investigate whether it can yield policies comparable to those learned from the environment with dense rewards, we reimplement it in the environment that gives dense feedback (for a meaningful comparison, we have made tremendous efforts on fine-tuning the nonsparse reward function so that it can yield good control policies; details of the nonsparse reward scheme see Appendix E ). Besides, we also reimplement A3C in the environment as a baseline. The simulation results appear in Fig. 8 . We can see that in most cases, the learning dynamics of LwH in the environment with dense rewards is similar to that of the baseline algorithm A3C. But if the influence of the prior policy is too strong, LwH converges relatively slower than A3C. That is because in environments with dense rewards, the rewards already provide the agent with rich information in deriving optimal control policies. If the prior policy is employed, it may confuse the agent since the prior policy is suboptimal. We can also notice from the learning curves that, in most cases, LwH is more stable than A3C. We think that contributes to the fact that LwH can set dynamic learning objectives with increasing difficulties for the agent. When compared with the results in the sparse-reward environment (as shown Fig. 4 ), LwH shows even more stable learning dynamics and yields policies with high success rates comparable to those given by A3C implemented in the dense-reward environment.
Fig. 8.

Mean success rate and expected return versus the training step curves for the algorithm LwH and the baseline algorithm A3C implemented in the environment with dense rewards. (a) and (b) Success rates. (c) and (d) Corresponding expected returns. (a) Success rate, SenAvo-Pri. (b) Success rate, Naive-Pri. (c) Expected return, SenAvo-Pri. (d) Expected return, Naive-Pri.

Show All

In summary, our LwH outperforms the state-of-the-art baselines by a large margin in environments with sparse rewards in terms of convergence speed, success rate, and final performance, and is robust to hyperparameter configurations. Additionally, it exhibits more stable learning dynamics and gives impressive results comparable to those of the baseline algorithm implemented in the environment with dense rewards.
SECTION V.
Conclusion

We formulate the problem of autonomous navigation of UAVs in large-scale complex environments as an MDP with sparse rewards and propose an efficient learning algorithm LwH, which guides the agent in exploring informative states especially in the early training stage and enables the agent to learn to achieve goals with different difficulty in the different training stage. Specifically, a prior control policy with potentially poor performance is assumed available to the agent. A behavior policy, which is constructed by synthesizing information from both the currently learned policy and the prior policy, is used for environment interaction. As learning progresses, the prior policy exerts less influence on the learned policy by linearly decaying the standard deviation of the prior policy, which is equivalent to setting dynamic learning objectives with increasing difficulty for the agent as learning progresses. Extensive experimental results demonstrate that LwH is efficient in learning high-performance control policies for UAV navigation in large-scale complex environments, robust to a range of hyperparameter configurations and significantly outperforms several state-of-the-art methods. Nonetheless, there are still some works to do in the future. For example, the influence of the prior policy is reduced by linearly decreasing its standard deviation. As a result, we must fine-tune the initial value of the standard deviation and the decay rate, though experimental results demonstrate that LwH is robust to them to some extent. We believe a more principled way is to let the learning algorithm itself determine the extent of influence of the prior policy in different training stages. This may be achieved by automated machine learning methods [39] and we leave it as our future work.
Appendix A Derivation of (18)

To obtain (18) , we first introduce a lemma [40] .
Lemma 1:

Consider that we have two sets of information D_{A} and D_{B} , and we use each set to independently estimate some parameter C . Denote the two estimators as P(C|D_{A}) and P(C|D_{B}) , then the best estimate of C based on the two sets is given by \begin{equation*} P\left ({C|D_{A}, D_{B}}\right)\propto \frac {P\left ({C|D_{A}}\right)P\left ({C|D_{B}}\right)}{P\left ({C}\right)}\tag{26}\end{equation*} View Source \begin{equation*} P\left ({C|D_{A}, D_{B}}\right)\propto \frac {P\left ({C|D_{A}}\right)P\left ({C|D_{B}}\right)}{P\left ({C}\right)}\tag{26}\end{equation*} where P(C) denotes the prior distribution of C .

The construction of the behavior policy (18) is inspired by Lemma 1 . By rewriting \pi _{h}(a|s) as \pi _{h}(a|s, H) and \pi _{\theta }(a|s) as \pi _{\theta }(a|s, L) , where \pi _{\theta }(a|s, h) denote the human estimator that estimates the action in state s and \pi _{\theta }(a|s, l) is the estimator learned by the agent, the final estimate of the action a at state s is given by \begin{equation*} \pi _{b}\left ({a|s,L,H}\right) \propto \frac {\pi _{\theta }\left ({a|s, L}\right)\pi _{h}\left ({a|s, H}\right)}{\pi _{p}\left ({a|s}\right)}\tag{27}\end{equation*} View Source \begin{equation*} \pi _{b}\left ({a|s,L,H}\right) \propto \frac {\pi _{\theta }\left ({a|s, L}\right)\pi _{h}\left ({a|s, H}\right)}{\pi _{p}\left ({a|s}\right)}\tag{27}\end{equation*} where \pi _{p}(a|s) is the prior action distribution. Since, generally, we have no prior information of the action at each state, we assume that \pi _{p}(a|s) complies with a uniform distribution. Then, (27) can be reformulated as \begin{equation*} \pi _{b}\left ({a|s,L,H}\right) = \frac {\pi _{\theta }\left ({a|s, L}\right)\pi _{h}\left ({a|s, H}\right)}{Z_{\theta }\left ({s}\right)}\tag{28}\end{equation*} View Source \begin{equation*} \pi _{b}\left ({a|s,L,H}\right) = \frac {\pi _{\theta }\left ({a|s, L}\right)\pi _{h}\left ({a|s, H}\right)}{Z_{\theta }\left ({s}\right)}\tag{28}\end{equation*} where Z_{\theta }(s) is a normalization constant independent of the action a . To keep consistency, we can rewrite (28) as \begin{equation*} \pi _{b}\left ({a|s}\right) = \frac {\pi _{\theta }\left ({a|s}\right)\pi _{h}\left ({a|s}\right)}{Z_{\theta }\left ({s}\right)}\propto \pi _{\theta }\left ({a|s}\right)\pi _{h}\left ({a|s}\right).\tag{29}\end{equation*} View Source \begin{equation*} \pi _{b}\left ({a|s}\right) = \frac {\pi _{\theta }\left ({a|s}\right)\pi _{h}\left ({a|s}\right)}{Z_{\theta }\left ({s}\right)}\propto \pi _{\theta }\left ({a|s}\right)\pi _{h}\left ({a|s}\right).\tag{29}\end{equation*}
Appendix B Derivation of (19)

To obtain (19) , we first introduce an equality (which has been proved true by [32] and [41] ) that \begin{align*} \nabla _{\theta } \eta \left ({\pi _{b}}\right)=&\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), s\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{b}\left ({a|s}\right)Q_{\pi _{b}}\left ({s,a}\right)}\right] \\=&\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), s\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{b}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right].\tag{30}\end{align*} View Source \begin{align*} \nabla _{\theta } \eta \left ({\pi _{b}}\right)=&\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), s\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{b}\left ({a|s}\right)Q_{\pi _{b}}\left ({s,a}\right)}\right] \\=&\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), s\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{b}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right].\tag{30}\end{align*} The relation holds true since we have \begin{equation*} Q_{\pi _{b}}\left ({s,a}\right) = A_{\pi _{b}}\left ({s,a}\right) + V_{\pi _{b}}\left ({s}\right)\tag{31}\end{equation*} View Source \begin{equation*} Q_{\pi _{b}}\left ({s,a}\right) = A_{\pi _{b}}\left ({s,a}\right) + V_{\pi _{b}}\left ({s}\right)\tag{31}\end{equation*} and \begin{align*}&\mathbb {E}_{a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{b}\left ({a|s}\right) V_{\pi _{b}}\left ({s}\right)}\right] \\&=V_{\pi _{b}}\left ({s}\right)\mathbb {E}_{a\sim \pi _{b}}\left [{\log \pi _{b}\left ({a|s}\right)}\right] \\&=V_{\pi _{b}}\left ({s}\right)\cdot 0 = 0.\tag{32}\end{align*} View Source \begin{align*}&\mathbb {E}_{a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{b}\left ({a|s}\right) V_{\pi _{b}}\left ({s}\right)}\right] \\&=V_{\pi _{b}}\left ({s}\right)\mathbb {E}_{a\sim \pi _{b}}\left [{\log \pi _{b}\left ({a|s}\right)}\right] \\&=V_{\pi _{b}}\left ({s}\right)\cdot 0 = 0.\tag{32}\end{align*} By substituting (18) into the right-hand side of (30) , the gradient of \eta _{\pi _{b}} can be rewritten as \begin{align*}&\nabla _{\theta } \eta \left ({\pi _{b}}\right) \\&=\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{b}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&=\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \left ({\frac {1}{Z_{\theta }\left ({s}\right)}\pi _{\theta }\left ({a|s}\right)\pi _{h}\left ({a|s}\right)}\right) A_{\pi _{b}}\left ({s,a}\right)}\right] \\&=\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{\theta }\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&~\,\,+\,\,\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{h}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&~\,\,-\,\,\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log Z_{\theta }\left ({s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right].\tag{33}\end{align*} View Source \begin{align*}&\nabla _{\theta } \eta \left ({\pi _{b}}\right) \\&=\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{b}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&=\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \left ({\frac {1}{Z_{\theta }\left ({s}\right)}\pi _{\theta }\left ({a|s}\right)\pi _{h}\left ({a|s}\right)}\right) A_{\pi _{b}}\left ({s,a}\right)}\right] \\&=\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{\theta }\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&~\,\,+\,\,\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{h}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&~\,\,-\,\,\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log Z_{\theta }\left ({s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right].\tag{33}\end{align*} It is clear that \begin{equation*} \mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{h}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right) = 0}\right].\tag{34}\end{equation*} View Source \begin{equation*} \mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log \pi _{h}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right) = 0}\right].\tag{34}\end{equation*} Besides, since the normalization term Z_{\theta }(s) is independent of the action a , we have \begin{align*}&\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log Z_{\theta }\left ({s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&=\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right)}\left [{\nabla _{\theta } \log Z_{\theta }\left ({s}\right)\sum _{a}\pi _{b}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&=0.\tag{35}\end{align*} View Source \begin{align*}&\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right), a\sim \pi _{b}}\left [{\nabla _{\theta } \log Z_{\theta }\left ({s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&=\mathbb {E}_{s\sim d_{\pi _{b}}\left ({s}\right)}\left [{\nabla _{\theta } \log Z_{\theta }\left ({s}\right)\sum _{a}\pi _{b}\left ({a|s}\right)A_{\pi _{b}}\left ({s,a}\right)}\right] \\&=0.\tag{35}\end{align*} Substituting (34) and (35) into (33) gives us (19) , the final result.
Appendix C The Virtual Environment

We build a simulator to implement UAV navigation in large-scale complex environments. For simplicity, we omit physical constraints on the UAV dynamics in the real situation and assume that control commands can take effect in no time. Buildings (obstacles) in the environment are abstracted as cylinders and the UAV is abstracted as a sphere. Overview of the virtual environment and the layout of buildings are demonstrated in Fig. 9 . Heights of the cylinders comply with a discrete uniform distribution \mathrm {unif}\{30\,\,\mathrm {m} + i\times 23\,\,\mathrm {m}\}_{i=0}^{10} . To ensure that the reward is sparse, the minimum distance between the departure position and the destination is larger than 200 m. Besides, a navigation task is completed if the distance between the UAV’s current position and the destination is less than 10 m [i.e., r_{d}=10 m in (8) ]. The UAV’s flight altitude is set to 100 m and the maximum speed is 50 m/s. UAV’s observation of the environment is achieved by range finders, as depicted in Fig. 1 . The simulator supports the flexible setup of the number and the direction of range finders, the layout of buildings, and the flight altitude of the UAV. The simulator is built upon the gym environment [42] and thus compatible with most DRL algorithms. Therefore, anyone who wants to test their RL algorithms handling the sparse-reward challenge, our simulator can serve as an alternative.
Fig. 9.

Overview of the (a) virtual large-scale complex environment and (b) building layout in the environment.

Show All

Appendix D Hyperparameters of LwH

The hyperparameters of LwH are shown in Table II .
Appendix E Nonsparse Reward Scheme for UAV Navigation

The reward function evaluates how good is it when executing a certain action at a certain state. So we design the nonsparse reward function as follows. First, the UAV would be rewarded if it approaches the destination and be penalized in reverse, which can be formulated as \begin{align*} r_{d}\left ({s_{t}, a_{t}}\right)=&\|\left ({x_{t-1}, y_{t-1}}\right)-\left ({x_{d}, y_{d}}\right)\|_{2} \\&\,\,- \|\left ({x_{t}, y_{t}}\right)-\left ({x_{d}, y_{d}}\right)\|_{2}.\tag{36}\end{align*} View Source \begin{align*} r_{d}\left ({s_{t}, a_{t}}\right)=&\|\left ({x_{t-1}, y_{t-1}}\right)-\left ({x_{d}, y_{d}}\right)\|_{2} \\&\,\,- \|\left ({x_{t}, y_{t}}\right)-\left ({x_{d}, y_{d}}\right)\|_{2}.\tag{36}\end{align*} The UAV would be penalized if it is too close to any obstacles. The penalty is formulated as \begin{equation*} r_{\mathrm {o}}\left ({s_{t}, a_{t}}\right) = \max \left ({\min \left ({d_{1}^{t},\ldots, d_{16}^{t}}\right) - d_{s}, 0}\right)\tag{37}\end{equation*} View Source \begin{equation*} r_{\mathrm {o}}\left ({s_{t}, a_{t}}\right) = \max \left ({\min \left ({d_{1}^{t},\ldots, d_{16}^{t}}\right) - d_{s}, 0}\right)\tag{37}\end{equation*} where d_{s} denotes the minimum safety distance between the UAV and any obstacles. We set its value to 10.0 m in this article. The UAV would also get a reward s_{sp}(s_{t}, a_{t}) [formulated as (8) ] if it arrives at the destination. Finally, to encourage the UAV to move to the destination as soon as possible, it would get a constant penalty r_{s}(s_{t}, a_{t})=-1 . The overall nonsparse reward function can be formulated as \begin{align*} r_{nsp}\left ({s_{t}, a_{t}}\right)=&\beta _{d} r_{d}\left ({s_{t}, a_{t}}\right) + \beta _{o} r_{o}\left ({s_{t}, s_{t}}\right) \\&+\,\,\beta _{s} r_{sp}\left ({s_{t}, a_{t}}\right) + \beta _{c} r_{c}\left ({s_{t}, a_{t}}\right)\tag{38}\end{align*} View Source \begin{align*} r_{nsp}\left ({s_{t}, a_{t}}\right)=&\beta _{d} r_{d}\left ({s_{t}, a_{t}}\right) + \beta _{o} r_{o}\left ({s_{t}, s_{t}}\right) \\&+\,\,\beta _{s} r_{sp}\left ({s_{t}, a_{t}}\right) + \beta _{c} r_{c}\left ({s_{t}, a_{t}}\right)\tag{38}\end{align*} where \beta _{d} , \beta _{o} , \beta _{s} , and \beta _{c} are four positive hyperparameters that take the value 1.0, 10.0, 5.0, and 1.0, respectively. (We have fine-tuned the four hyperparameters so that the baseline algorithm yields the best results.)

Authors
Figures
References
Citations
Keywords
Metrics
Footnotes
More Like This
Dynamic Motion Planning for Conducting Obstacle Avoidance Maneuver of Fixed Wing Autonomous Aerial Vehicle

2019 4th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)

Published: 2019
Intellectual method of guiding mobile robot navigation using reinforcement learning algorithm

2015 IEEE International Conference on Engineering and Technology (ICETECH)

Published: 2015
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
