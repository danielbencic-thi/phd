Sampling-based Algorithms for Optimal Motion Planning

Sertac Karaman

Emilio Frazzoli∗

arXiv:1105.1186v1 [cs.RO] 5 May 2011

Abstract
During the last decade, sampling-based path planning algorithms, such as Probabilistic RoadMaps (PRM) and Rapidly-exploring Random Trees (RRT), have been shown to work well in practice and possess theoretical guarantees such as probabilistic completeness. However, little eﬀort has been devoted to the formal analysis of the quality of the solution returned by such algorithms, e.g., as a function of the number of samples. The purpose of this paper is to ﬁll this gap, by rigorously analyzing the asymptotic behavior of the cost of the solution returned by stochastic sampling-based algorithms as the number of samples increases. A number of negative results are provided, characterizing existing algorithms, e.g., showing that, under mild technical conditions, the cost of the solution returned by broadly used sampling-based algorithms converges almost surely to a non-optimal value. The main contribution of the paper is the introduction of new algorithms, namely, PRM∗ and RRT∗, which are provably asymptotically optimal, i.e., such that the cost of the returned solution converges almost surely to the optimum. Moreover, it is shown that the computational complexity of the new algorithms is within a constant factor of that of their probabilistically complete (but not asymptotically optimal) counterparts. The analysis in this paper hinges on novel connections between stochastic sampling-based path planning algorithms and the theory of random geometric graphs.
Keywords: Motion planning, optimal path planning, sampling-based algorithms, random geometric graphs.
1 Introduction
The robotic motion planning problem has received a considerable amount of attention, especially over the last decade, as robots started becoming a vital part of modern industry as well as our daily life (Latombe, 1991; LaValle, 2006; Choset et al., 2005). Even though modern robots may possess signiﬁcant diﬀerences in sensing, actuation, size, workspace, application, etc., the problem of navigating through a complex environment is embedded and essential in almost all robotics applications. Moreover, this problem is relevant to other disciplines such as veriﬁcation, computational biology, and computer animation (Latombe, 1999; Bhatia and Frazzoli, 2004; Branicky et al., 2006; Cortes et al., 2007; Liu and Badler, 2003; Finn and Kavraki, 1999).
Informally speaking, given a robot with a description of its dynamics, a description of the environment, an initial state, and a set of goal states, the motion planning problem is to ﬁnd a sequence of control inputs so as the drive the robot from its initial state to one of the goal states while obeying the rules of the environment, e.g., not colliding with the surrounding obstacles. An algorithm to address this problem is said to be complete if it terminates in ﬁnite time, returning a valid solution if one exists, and failure otherwise.
Unfortunately, the problem is known to be very hard from the computational point of view. For example, a basic version of the motion planning problem, called the generalized piano movers
∗The authors are with the Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA.
1

problem, is PSPACE-hard (Reif, 1979). In fact, while complete planning algorithms exist (see, e.g., Lozano-Perez and Wesley, 1979; Schwartz and Sharir, 1983; Canny, 1988), their complexity makes them unsuitable for practical applications.
Practical planners came around with the development of cell decomposition methods (Brooks and Lozano-Perez, 1983) and potential ﬁelds (Khatib, 1986). These approaches, if properly implemented, relaxed the completeness requirement to, for instance, resolution completeness, i.e., the ability to return a valid solution, if one exists, if the resolution parameter of the algorithm is set ﬁne enough. These planners demonstrated remarkable performance in accomplishing various tasks in complex environments within reasonable time bounds (Ge and Cui, 2002). However, their practical applications were mostly limited to state spaces with up to ﬁve dimensions, since decomposition-based methods suﬀered from large number of cells, and potential ﬁeld methods from local minima (Koren and Borenstein, 1991). Important contributions towards broader applicability of these methods include navigation functions (Rimon and Koditschek, 1992) and randomization (Barraquand and Latombe, 1993).
The above methods rely on an explicit representation of the obstacles in the conﬁguration space, which is used directly to construct a solution. This may result in an excessive computational burden in high dimensions, and in environments described by a large number of obstacles. Avoiding such a representation is the main underlying idea leading to the development of sampling-based algorithms (Kavraki and Latombe, 1994; Kavraki et al., 1996; LaValle and Kuﬀner, 2001). See Lindemann and LaValle (2005) for a historical perspective. These algorithms proved to be very eﬀective for motion planning in high-dimensional spaces, and attracted signiﬁcant attention over the last decade, including very recent work (see, e.g., Prentice and Roy, 2009; Tedrake et al., 2010; Luders et al., 2010; Berenson et al., 2008; Yershova and LaValle, 2008; Stilman et al., 2007; Koyuncu et al., 2010). Instead of using an explicit representation of the environment, samplingbased algorithms rely on a collision checking module, providing information about feasibility of candidate trajectories, and connect a set of points sampled from the obstacle-free space in order to build a graph (roadmap) of feasible trajectories. The roadmap is then used to construct the solution to the original motion-planning problem.
Informally speaking, sampling-based methods provide large amounts of computational savings by avoiding explicit construction of obstacles in the state space, as opposed to most complete motion planning algorithms. Even though these algorithms are not complete, they provide probabilistic completeness guarantees in the sense that the probability that the planner fails to return a solution, if one exists, decays to zero as the number of samples approaches inﬁnity (Barraquand et al., 1997) (see also Hsu et al., 1997; Kavraki et al., 1998; Ladd and Kavraki, 2004). Moreover, the rate of decay of the probability of failure is exponential, under the assumption that the environment has good “visibility” properties (Barraquand et al., 1997). More recently, the empirical success of samplingbased algorithms was argued to be strongly tied to the hypothesis that most practical robotic applications, even though involving robots with many degrees of freedom, feature environments with such good visibility properties (Hsu et al., 2006).
1.1 Sampling-Based Algorithms
Arguably, the most inﬂuential sampling-based motion planning algorithms to date include Probabilistic RoadMaps (PRMs) (Kavraki et al., 1996, 1998) and Rapidly-exploring Random Trees (RRTs) (Kuﬀner and LaValle, 2000; LaValle and Kuﬀner, 2001; LaValle, 2006). Even though the idea of connecting points sampled randomly from the state space is essential in both approaches, these two algorithms diﬀer in the way that they construct a graph connecting these points.
The PRM algorithm and its variants are multiple-query methods that ﬁrst construct a graph
2

(the roadmap), which represents a rich set of collision-free trajectories, and then answer queries by computing a shortest path that connects the initial state with a ﬁnal state through the roadmap. The PRM algorithm has been reported to perform well in high-dimensional state spaces (Kavraki et al., 1996). Furthermore, the PRM algorithm is probabilistically complete, and such that the probability of failure decays to zero exponentially with the number of samples used in the construction of the roadmap (Kavraki et al., 1998). During the last two decades, the PRM algorithm has been a focus of robotics research: several improvements were suggested by many authors and the reasons to why it performs well in many practical cases were better understood (see, e.g., Branicky et al., 2001; Hsu et al., 2006; Ladd and Kavraki, 2004, for some examples).
Even though multiple-query methods are valuable in highly structured environments, such as factory ﬂoors, most online planning problems do not require multiple queries, since, for instance, the robot moves from one environment to another, or the environment is not known a priori. Moreover, in some applications, computing a roadmap a priori may be computationally challenging or even infeasible. Tailored mainly for these applications, incremental sampling-based planning algorithms such as RRTs have emerged as an online, single-query counterpart to PRMs (see, e.g., Kuﬀner and LaValle, 2000; Hsu et al., 2002). The incremental nature of these algorithms avoids the necessity to set the number of samples a priori, and returns a solution as soon as the set of trajectories built by the algorithm is rich enough, enabling on-line implementations. Moreover, tree-based planners do not require connecting two states exactly and more easily handle systems with diﬀerential constraints. The RRT algorithm has been shown to be probabilistically complete (Kuﬀner and LaValle, 2000), with an exponential rate of decay for the probability of failure (Frazzoli et al., 2002). The basic version of the RRT algorithm has been extended in several directions, and found many applications in the robotics domain and elsewhere (see, for instance, Frazzoli et al., 2002; Bhatia and Frazzoli, 2004; Cortes et al., 2007; Branicky et al., 2006, 2003; Zucker et al., 2007). In particular, RRTs have been shown to work eﬀectively for systems with diﬀerential constraints and nonlinear dynamics (LaValle and Kuﬀner, 2001; Frazzoli et al., 2002) as well as purely discrete or hybrid systems (Branicky et al., 2003). Moreover, the RRT algorithm was demonstrated in major robotics events on various experimental robotic platforms (Bruce and Veloso, 2003; Kuwata et al., 2009; Teller et al., 2010; Shkolnik et al., 2011; Kuﬀner et al., 2002).
Other sampling-based planners of note include Expansive Space Trees (EST) (Hsu et al., 1997, 1999) and Sampling-based Roadmap of Trees (SRT) (Plaku et al., 2005). The latter combines the main features of multiple-query algorithms such as PRM with those of single-query algorithms such as RRT and EST.
1.2 Optimal Motion Planning
In most applications, the quality of the solution returned by a motion planning algorithm is important. For example, one may be interested in solution paths of minimum cost, with respect to a given cost functional, such as the length of a path, or the time required to execute it. The problem of computing optimal motion plans has been proven in Canny and Reif (1987) to be very challenging even in basic cases.
In the context of sampling-based motion planning algorithms, the importance of computing optimal solutions has been pointed out in early seminal papers (LaValle and Kuﬀner, 2001). However, optimality properties of sampling-based motion planning algorithms have not been systematically investigated, and most of the relevant work relies on heuristics. For example, in many ﬁeld implementations of sampling-based planning algorithms (see, e.g., Kuwata et al., 2009), it is often the case that since a feasible path is found quickly, additional available computation time is devoted to improving the solution with heuristics until the solution is executed. Urmson and Simmons
3

(2003) proposed heuristics to bias the tree growth in RRT towards those regions that result in lowcost solutions. They have also shown experimental results evaluating the performance of diﬀerent heuristics in terms of the quality of the solution returned. Ferguson and Stentz (2006) considered running the RRT algorithm multiple times in order to progressively improve the quality of the solution. They showed that each run of the algorithm results in a path with smaller cost, even though the procedure is not guaranteed to converge to an optimal solution. Criteria for restarting multiple RRT runs, in a diﬀerent context, were also proposed in Wedge and Branicky (2008). A more recent approach is the transition-based RRT (T-RRT) designed to combine rapid exploration properties of the RRT with stochastic global optimization methods (Jaillet et al., 2010; Berenson et al., 2011).
A diﬀerent approach that also oﬀers optimality guarantees is based on graph search algorithms, such as A∗, applied over a ﬁnite discretization (based, e.g., on a grid, or a cell decomposition of the conﬁguration space) that is generated oﬄine. Recently, these algorithms received a large amount of attention. In particular, they were extended to run in an anytime fashion (Likhachev et al., 2004, 2008), deal with dynamic environments (Stentz, 1995; Likhachev et al., 2008), and handle systems with diﬀerential constraints (Likhachev and Ferguson, 2009). These have also been successfully demonstrated on various robotic platforms (Likhachev and Ferguson, 2009; Dolgov et al., 2009). However, optimality guarantees of these algorithms are only ensured up to the grid resolution. Moreover, since the number of grid points grows exponentially with the dimensionality of the state space, so does the (worst-case) running time of these algorithms.
1.3 Statement of Contributions
To the best of the author’s knowledge, this paper provides the ﬁrst systematic and thorough analysis of optimality and complexity properties of the major paradigms for sampling-based path planning algorithms, for multiple- or single-query applications, and introduces the ﬁrst algorithms that are both asymptotically optimal and computationally eﬃcient, with respect to other algorithms in this class. A summary of the contributions can be found below, and is shown in Table 1.
As a ﬁrst set of results, it is proven that the standard PRM and RRT algorithms are not asymptotically optimal, and that the “simpliﬁed” PRM algorithm is asymptotically optimal, but computationally expensive. Moreover, it is shown that the k-nearest variant of the (simpliﬁed) PRM algorithm is not necessarily probabilistically complete (e.g., it is not probabilistically complete for k = 1), and is not asymptotically optimal for any ﬁxed k.
In order to address the limitations of sampling-based path planning algorithms available in the literature, new algorithms are proposed, i.e., PRM∗, RRG, and RRT∗, and proven to be probabilistically complete, asymptotically optimal, and computationally eﬃcient. Of these, PRM∗ is a batch variable-radius PRM, applicable to multiple-query problems, in which the radius is scaled with the number of samples in a way that provably ensures both asymptotic optimality and computational eﬃciency. RRG is an incremental algorithm that builds a connected roadmap, providing similar performance to PRM∗ in a single-query setting, and in an anytime fashion (i.e., a ﬁrst solution is provided quickly, and monotonically improved if more computation time is available). The RRT∗ algorithm is a variant of RRG that incrementally builds a tree, providing anytime solutions, provably converging to an optimal solution, with minimal computational and memory requirements.
In this paper, the problem of planning a path through a connected bounded subset of a ddimensional Euclidean space is considered. As in the early seminal papers on incremental samplingbased motion planning algorithms such as Kuﬀner and LaValle (2000), no diﬀerential constraints are considered (i.e., the focus of the paper is on path planning problems), but our methods can be easily extended to planning in conﬁguration spaces and applied to several practical problems of interest.
4

Table 1: Summary of results. Time and space complexity are expressed as a function of the number of samples n, for a ﬁxed environment.

Algorithms Algorithms

Existing

Algorithm Probabilistic Completeness

PRM sPRM k-sPRM RRT
PRM∗ k-PRM∗
RRG k-RRG RRT∗ k-RRT∗

Yes Yes Conditional Yes
Yes
Yes
Yes

Asymptotic Optimality
No Yes No No
Yes
Yes
Yes

Monotone Convergence
Yes Yes No Yes
No
Yes
Yes

Time Complexity

Processing

Query

O(n log n) O(n2)
O(n log n) O(n log n)

O(n log n) O(n2)
O(n log n) O(n)

Space Complexity
O(n) O(n2) O(n) O(n)

O(n log n) O(n log n) O(n log n)

O(n log n) O(n log n) O(n log n)

O(n log n) O(n)

O(n)

Proposed

The extension to systems with diﬀerential constraints is deferred to future work (see Karaman and Frazzoli (2010a) for preliminary results).
Finally, the results presented in this article, and the techniques used in the analysis of the algorithms, hinge on novel connections established between sampling-based path planning algorithms in robotics and the theory of random geometric graphs, which may be of independent interest.
A preliminary version of this article has appeared in Karaman and Frazzoli (2010b). Since then a variety of new algorithms based on the the ideas behind PRM∗, RRG, and RRT∗ have been proposed in the literature. For instance, a probabilistically complete and probabilistically sound algorithm for solving a class of diﬀerential games has appeared in Karaman and Frazzoli (2010c). Algorithms based on the RRG were used to solve belief-space planning problems in Bry and Roy (2011). The RRT∗ algorithm was used for anytime motion planning in Karaman et al. (2011), where it was also demonstrated experimentally on a full-size robotic fork truck. In Alterovitz et al. (2011), the analysis given in Karaman and Frazzoli (2010b) was used to guarantee computational eﬃciency and asymptotic optimality of a new algorithm that can trade oﬀ between exploration and optimality during planning.
A software library implementing the new algorithms introduced in this paper has been released as open-source software by the authors, and is currently available at http://ares.lids.mit.edu/ software/
1.4 Paper Organization
This paper is organized as follows. Section 2 lays the ground in terms of notation and problem formulation. Section 3 is devoted to the discussion of the algorithms that are considered in the paper: ﬁrst, the main paradigms for sampling-based motion planning algorithms available in the literature are presented, together with their main variants. Then, the new proposed algorithms are presented and motivated. In Section 4 the properties of these algorithms are rigorously analyzed, formally establishing their probabilistic completeness and asymptotically optimality (or lack thereof), as well as their computational complexity as a function of the number of samples and of the number of obstacles in the environment. Experimental results are presented in Section 5, to illustrate and
5

validate the theoretical ﬁndings. Finally, Section 6 contains conclusions and perspectives for future work. In order not to excessively disrupt the ﬂow of the presentation, a summary of notation used throughout the paper, as well as lengthy proofs of important results are presented in the Appendix.

2 Preliminary Material
This section contains some preliminary material that will be necessary for the discussion in the remainder of the paper. Namely, the problems of feasible and optimal motion planning is introduced, and some important results from the theory of random geometric graphs are summarized. The notation used in the paper is summarized in Appendix A.

2.1 Problem Formulation

In this section, the feasible and optimal path planning problems are formalized. Let X = (0, 1)d be the conﬁguration space, where d ∈ N, d ≥ 2. Let Xobs be the obstacle region,
such that X \ Xobs is an open set, and denote the obstacle-free space as Xfree = cl(X \ Xobs), where cl(·) denotes the closure of a set. The initial condition xinit is an element of Xfree, and the goal region Xgoal is an open subset of Xfree. A path planning problem is deﬁned by a triplet (Xfree, xinit, Xgoal).
Let σ : [0, 1] → Rd; the total variation of σ is deﬁned as

n

TV(σ) =

sup

|σ(τi) − σ(τi−1)|.

{n∈N,0=τ0<τ1<···<τn=s} i=1

A function σ with TV(σ) < ∞ is said to have bounded variation.

Deﬁnition 1 (Path) A function σ : [0, 1] → Rd of bounded variation is called a
• Path, if it is continuous;
• Collision-free path, if it is a path, and σ(τ ) ∈ Xfree, for all τ ∈ [0, 1];
• Feasible path, if it is a collision-free path, σ(0) = xinit, and σ(1) ∈ cl(Xgoal).
The total variation of a path is essentially its length, i.e., the Euclidean distance traversed by the path in Rd. The feasibility problem of path planning is to ﬁnd a feasible path, if one exists, and report failure otherwise:

Problem 2 (Feasible path planning) Given a path planning problem (Xfree, xinit, Xgoal), ﬁnd a feasible path σ : [0, 1] → Xfree such that σ(0) = xinit and σ(1) ∈ cl(Xgoal), if one exists. If no such path exists, report failure.
Let Σ denote the set of all paths, and Σfree the set of all collision-free paths. Given two paths σ1, σ2 ∈ Σ, such that σ1(1) = σ2(0), let σ1|σ2 ∈ Σ denote their concatenation, i.e., (σ1|σ2)(τ ) := σ1(2 τ ) for all τ ∈ [0, 1/2] and (σ1|σ2)(τ ) := σ2(2 τ − 1) for all τ ∈ (1/2, 1]. Both Σ and Σfree are closed under concatenation. Let c : Σ → R≥0 be a function, called the cost function, which assigns a strictly positive cost to all non-trivial collision-free paths (i.e., c(σ) = 0 if and only if σ(τ ) = σ(0), ∀τ ∈ [0, 1]). The cost function is assumed to be monotonic, in the sense that for all σ1, σ2 ∈ Σ, c(σ1) ≤ c(σ1|σ2), and bounded, in the sense that there exists kc such that c(σ) ≤ kcTV(σ), ∀σ ∈ Σ.
The optimality problem of path planning asks for ﬁnding a feasible path with minimum cost:

6

Problem 3 (Optimal path planning) Given a path planning problem (Xfree, xinit, Xgoal) and a cost function c : Σ → R≥0, ﬁnd a feasible path σ∗ such that c(σ∗) = min{c(σ) : σ is feasible}. If no
such path exists, report failure.

2.2 Random Geometric Graphs
The objective of this section is to summarize some of the results on random geometric graphs that are available in the literature, and are relevant to the analysis of sampling-based path planning algorithms. In the remainder of this article, several connections are made between the theory of random geometric graphs and path-planning algorithms in robotics, providing insight on a number of issues, including, e.g., probabilistic completeness and asymptotic optimality, as well as technical tools to analyze the algorithms and establish their properties. In fact, it turns out that the data structures constructed by most sampling-based motion planning algorithms in the literature coincide, in the absence of obstacles, with standard models of random geometric graphs.
Random geometric graphs are in general deﬁned as stochastic collections of points in a metric space, connected pairwise by edges if certain conditions (e.g., on the distance between the points) are satisﬁed. Such objects have been studied since their introduction by Gilbert (1961); see, e.g., Penrose (2003) and Balister et al. (2009a) for an overview of recent results. From the theoretical point of view, the study of random geometric graphs makes a connection between random graphs (Bollob´as, 2001) and percolation theory (Bollob´as and Riordan, 2006). On the application side, in recent years, random geometric graphs have attracted signiﬁcant attention as models of ad hoc wireless networks (Gupta and Kumar, 1998, 2000).
Much of the literature on random geometric graphs deals with inﬁnite graphs deﬁned on unbounded domains, with vertices generated as a homogeneous Poisson point process. Recall that a Poisson random variable of parameter λ ∈ R>0 is an integer-valued random variable Poisson(λ) : Ω → N0 such that P(Poisson(λ) = k) = e−λλk/k!. A homogeneous Poisson point process of intensity λ on Rd is a random countable set of points Pλd ⊂ Rd such that, for any disjoint measurable sets S1, S2 ⊂ Rd, S1 ∩ S2 = ∅, the numbers of points of Pλd in each set are independent Poisson variables, i.e., card Pλd ∩ S1 = Poisson(µ(S1)λ) and card Pλd ∩ S2 = Poisson(µ(S2)λ). In particular, the intensity of a homogeneous Poisson point process can be interpreted as the expected number of points generated in the unit cube, i.e., E(card Pλd ∩ (0, 1)d ) = E(Poisson(λ)) = λ.
Perhaps the most studied model of inﬁnite random geometric graph is the following, introduced in Gilbert (1961), and often called Gilbert’s disc model, or Boolean model:

Deﬁnition 4 (Inﬁnite random r-disc graph) Let λ, r ∈ R>0, and d ∈ N. An inﬁnite random r-disc graph Gd∞isc(λ, r) in d dimensions is an inﬁnite graph with vertices {Xi}i∈N = Pλd, and such that (Xi, Xj), i, j ∈ N, is an edge if and only if Xi − Xj < r.

A fundamental issue in inﬁnite random graphs is whether the graph contains an inﬁnite con-

nected component, with non-zero probability. If it does, the random graph is said to percolate.

Percolation is an important paradigm in statistical physics, with many applications in disparate

ﬁelds such as material science, epidemiology, and microchip manufacturing, just to name a few (see,

e.g., Sahimi, 1994).

Consider the inﬁnite random r-disc graph, for r = 1, i.e., Gd∞isc(λ, 1), and assume, without loss

of generality, that the origin is one of the vertices of this graph. Let pk(λ) denote the probability

that the connected component of Gd∞isc(λ, 1) containing the origin contains k vertices, and deﬁne

p∞(λ) as p∞(λ) = 1 −

∞ k=1

pk

(λ).

The function p∞

: λ → p∞(λ) is monotone,

and p∞(0) = 0

and limλ→∞ p∞(λ) = 1 (Penrose, 2003). A key result in percolation theory is that there exists a

non-zero critical intensity λc deﬁned as λc := sup{λ : p∞(λ) = 0}. In other words, for all λ > λc,

7

there is a non-zero probability that the origin is in an inﬁnite connected component of Gd∞isc(λ, 1); moreover, under these conditions, the graph has precisely one inﬁnite connected component, almost
surely (Meester and Roy, 1996). The function p∞ is continuous for all λ = λc: in other words, the graph undergoes a phase transition at the critical density λc, often also called the continuum percolation threshold (Penrose, 2003). The exact value of λc is not known; Meester and Roy provide 0.696 < λc < 3.372 for d = 2 (Meester and Roy, 1996), and simulations suggest that λc ≈ 1.44 (Quintanilla et al., 2000).
For many applications, including the ones in this article, models of ﬁnite graphs on a bounded
domain are more relevant. Penrose introduced the following model (Penrose, 2003):

Deﬁnition 5 (Random r-disc graph) Let r ∈ R>0, and n, d ∈ N. A random r-disc graph Gdisc(n, r) in d dimensions is a graph whose n vertices, {X1, X2, . . . , Xn}, are independent, uniformly distributed random variables in (0, 1)d, and such that (Xi, Xj), i, j ∈ {1, . . . , n}, i = j, is an edge if and only if Xi − Xj < r.
For ﬁnite random geometric graph models, one is typically interested in whether a random geometric graph possesses certain properties asymptotically as n increases. Since the number of vertices is ﬁnite in random graphs, percolation can not be deﬁned easily. In this case, percolation is studied in terms of the scaling of the number of vertices in the largest connected component with respect to the total number of vertices; in particular, a ﬁnite random geometric graph is said to percolate if it contains a “giant” connected component containing at least a constant fraction of all the nodes. As in the inﬁnite case, percolation in ﬁnite random geometric graphs is often a phase transition phenomenon. In the case of random r-disc graphs,

Theorem 6 (Percolation of random r-disc graphs (Penrose, 2003)) Let Gdisc(n, r) be a random r-disc graph in d ≥ 2 dimensions, and let Nmax(Gdisc(n, r)) be the number of vertices in its largest connected component. Then, almost surely,

lim Nmax(Gdisc(n, rn)) = 0,

n→∞

n

if rn < (λc/n)1/d ,

and

lim Nmax(Gdisc(n, r)) > 0,

n→∞

n

where λc is the continuum percolation threshold.

if rn > (λc/n)1/d ,

A random r-disc graph with limn→∞ nrnd = λ ∈ (0, ∞) is said to operate in the thermodynamic limit. It is said to be in subcritical regime when λ < λc and supercritical regime when λ > λc.
Another property of interest is connectivity. Clearly, connectivity implies percolation. Interest-
ingly, emergence of connectivity in random geometric graphs is a phase transition phenomenon, as
percolation. The following result is available in the literature:

Theorem 7 (Connectivity of random r-disc graphs (Penrose, 2003)) Let Gdisc(n, r) be a random r-disc graph in d dimensions. Then,

lim P {Gdisc(n, r) is connected } =
n→∞

1, if ζdrd > log(n)/n, 0, if ζdrd < log(n)/n,

where ζd is the volume of the unit ball in d dimensions.

8

Another model of random geometric graphs considers edges between k nearest neighbors. (Note that there are no ties, almost surely.) Both inﬁnite and ﬁnite models are considered, as follows.

Deﬁnition 8 (Inﬁnite random k-nearest neighbor graph) Let λ ∈ R>0, and d, k ∈ N. An inﬁnite random k-nearest neighbor graph Gn∞ear(λ, k) in d dimensions is an inﬁnite graph with vertices {Xi}i∈N = Pλd, and such that (Xi, Xj), i, j ∈ N, is an edge if Xj is among the k nearest neighbors of Xi, or if Xi is among the k nearest neighbors of Xj.
Deﬁnition 9 (Random k-nearest neighbor graph) Let d, k, n ∈ N. A random k-nearest neighbor graph Gnear(n, k) in d dimensions is a graph whose n vertices, {X1, X2, . . . , Xn}, are independent, uniformly distributed random variables in (0, 1)d, and such that (Xi, Xj), i, j ∈ {1, . . . , n}, i = j, is an edge if Xj is among the k nearest neighbors of Xi, or if Xi is among the k nearest neighbors of Xj.
Percolation and connectivity for random k-nearest neighbor graphs exhibit phase transition phenomena, as in the random r-disc case. However, the results available in the literature are more limited. Results on percolation are only available for inﬁnite graphs:

Theorem 10 (Percolation in inﬁnite random k-nearest graphs (Balister et al., 2009a)) Let Gn∞ear(λ, k) be an inﬁnite random k-nearest neighbor graph in d ≥ 2 dimensions. Then, there exists a constant kdp > 0 such that

P ({Gn∞ear(1, k) has an inﬁnite component }) =

1, if k ≥ kdp, 0, if k < kdp.

The value of kdp is not known. However, it is believed that k2p = 3, and kdp = 2 for all d ≥ 3 (Balister et al., 2009a). It is known that percolation does not occur for k = 1 (Balister et al., 2009a).
Regarding connectivity of random k-nearest neighbor graphs, the only available results in the
literature are not stated in terms of a given number of vertices: rather, the results are stated
in terms of the restriction of a homogeneous Poisson point process to the unit cube. In other words, the vertices of the graph are obtained as {X1, X2, . . .} = Pλd ∩ (0, 1)d. This is equivalent to setting the number of vertices as a Poisson random variable of parameter n, and then sampling the Poisson(n) vertices independently and uniformly in (0, 1)d:

Lemma 11 (Stoyan et al. (1995)) Let {Xi}i∈N be a sequence of points drawn independently and uniformly from S ⊆ X . Let Poisson(n) be a Poisson random variable with parameter n. Then, {X1, X2, . . . , XPoisson(n)} is the restriction to S of a homogeneous Poisson point process with intensity n/µ(S).
The main advantage in using such a model to generate the vertices of a random geometric graph is independence: in the Poisson case, the numbers of points in any two disjoint measurable regions S1, S2 ⊂ [0, 1]d, S1 ∩ S2 = ∅, are independent Poisson random variables, with mean µ(S1)λ and µ(S2)λ, respectively. These two random variables would not be independent if the total number of vertices were ﬁxed a priori (also called a binomial point process). With some abuse of notation, such a random geometric graph model will be indicated as Gnear(Poisson(n), k).

Theorem 12 (Connectivity of random k-nearest graphs (Balister et al., 2009b; Xue and Kumar, 20 Let Gnear(Poisson(n), k) indicate a k-nearest neighbor graph model in d = 2 dimensions, such that

9

its vertices are generated using a Poisson point process of intensity n. Then, there exists a constant k2c > 0 such that

lim P ({Gnear(Poisson(n), k log(n) ) is connected }) =
n→∞

1, if k ≥ k2c, 0, if k < k2c.

The value of k2c is not known; the current best estimate is 0.3043 ≤ k2c ≤ 0.5139 (Balister et al., 2005).
Finally, the last model of random geometric graph that will be relevant for the analysis of the algorithms in this paper is the following:

Deﬁnition 13 (Online nearest neighbor graph) Let d, n ∈ N. An online nearest neighbor graph GONN(n) in d dimensions is a graph whose n vertices, (X1, X2, . . . , Xn), are independent, uniformly distributed random variables in (0, 1)d, and such that (Xi, Xj), i, j ∈ {1, . . . , n}, j > 1, is an edge if and only if Xi − Xj = min1≤k<j Xk − Xj .
Clearly, the online nearest neighbor graph is connected by construction, and trivially percolates. Recent results for this random geometric graph model include estimates of the total power-weighted edge length and an analysis of the vertex degree distribution, see, e.g., Wade (2009).

3 Algorithms
In this section, a number of sampling-based motion planning algorithms are introduced. First, some common primitive procedures are deﬁned. Then, the PRM and the RRT algorithms are outlined, as they are representative of the major paradigms for sampling-based motion planning algorithms in the literature. Then, new algorithms, namely PRM∗ and RRT∗, are introduced, as asymptotically optimal and computationally eﬃcient versions of their “standard” counterparts.
3.1 Primitive Procedures
Before discussing the algorithms, it is convenient to introduce the primitive procedures that they rely on.
Sampling: Let Sample : ω → {Samplei(ω)}i∈N0 ⊂ X be a map from Ω to sequences of points in X , such that the random variables Samplei, i ∈ N0, are independent and identically distributed (i.i.d.). For simplicity, the samples are assumed to be drawn from a uniform distribution, even though results extend naturally to any absolutely continuous distribution with density bounded away from zero on X . It is convenient to consider another map, SampleFree : ω → {SampleFreei(ω)}i∈N0 ⊂ Xfree that returns sequences of i.i.d. samples from Xfree. For each ω ∈ Ω, the sequence {SampleFreei(ω)}i∈N0 is the subsequence of {Samplei(ω)}i∈N0 containing only the samples in Xfree, i.e., {SampleFreei(ω)}i∈N0 = {Samplei(ω)}i∈N0 ∩ Xfree.
Nearest Neighbor: Given a graph G = (V, E), where V ⊂ X , a point x ∈ X , the function Nearest : (G, x) → v ∈ V returns the vertex in V that is “closest” to x in terms of a given distance function. In this paper, the Euclidean distance is used (see, e.g., LaValle and Kuﬀner (2001) for alternative choices), and hence
Nearest(G = (V, E), x) := argminv∈V x − v .

10

A set-valued version of this function is also considered, kNearest : (G, x, k) → {v1, v2, . . . , vk}, returning the k vertices in V that are nearest to x, according to the same distance function as above. (By convention, if the cardinality of V is less than k, then the function returns V .)
Near Vertices: Given a graph G = (V, E), where V ⊂ X , a point x ∈ X , and a positive real number r ∈ R>0, the function Near : (G, x, r) → V ⊆ V returns the vertices in V that are contained in a ball of radius r centered at x, i.e.,
Near(G = (V, E), x, r) := {v ∈ V : v ∈ Bx,r} .
Steering: Given two points x, y ∈ X , the function Steer : (x, y) → z returns a point z ∈ X such that z is “closer” to y than x is. Throughout the paper, the point z returned by the function Steer will be such that z minimizes z − y while at the same time maintaining z − x ≤ η, for a prespeciﬁed η > 0,1 i.e.,
Steer(x, y) := argminz∈Bx,η z − y .
Collision Test: Given two points x, x ∈ X , the Boolean function CollisionFree(x, x ) returns True if the line segment between x and x lies in Xfree, i.e., [x, x ] ⊂ Xfree, and False otherwise.
3.2 Existing Algorithms
Next, some of the sampling-based algorithms available in the literature are outlined. For convenience, inputs and outputs of the algorithms are not shown explicitly, but are as follows. All algorithms take as input a path planning problem (Xfree, xinit, Xgoal), an integer n ∈ N, and a cost function c : Σ → R≥0, if appropriate. These inputs are shared with functions and procedures called within the algorithms. All algorithms return a graph G = (V, E), where V ⊂ Xfree, card (V ) ≤ n+1, and E ∈ V × V. The solution of the path planning problem can be easily computed from such a graph, e.g., using standard shortest-path algorithms.
Probabilistic RoadMaps (PRM): The Probabilistic RoadMaps algorithm is primarily aimed at multi-query applications. In its basic version, it consists of a pre-processing phase, in which a roadmap is constructed by attempting connections among n randomly-sampled points in Xfree, and a query phase, in which paths connecting initial and ﬁnal conditions through the roadmap are sought. “Expansion” heuristics for enhancing the roadmap’s connectivity are available in the literature (Kavraki et al., 1996) but have no impact on the analysis in this paper, and will not be discussed.
The pre-processing phase, outlined in Algorithm 1, begins with an empty graph. At each iteration, a point xrand ∈ Xfree is sampled, and added to the vertex set V . Then, connections are attempted between xrand and other vertices in V within a ball of radius r centered at xrand, in order of increasing distance from xrand, using a simple local planner (e.g., straight-line connection). Successful (i.e., collision-free) connections result in the addition of a new edge to the edge set E. To avoid unnecessary computations (since the focus of the algorithm is establishing connectivity), connections between xrand and vertices in the same connected component are avoided. Hence, the roadmap constructed by PRM is a forest, i.e., a collection of trees.
1This steering procedure is used widely in the robotics literature, since its introduction in Kuﬀner and LaValle (2000). Our results also extend to the Rapidly-exploring Random Dense Trees (see, e.g., LaValle, 2006), which are slightly modiﬁed versions of the RRTs that do not require tuning any prespeciﬁed parameters such as η in this case.
11

Algorithm 1: PRM (preprocessing phase)

1 V ← ∅; E ← ∅;

2 for i = 0, . . . , n do

3 xrand ← SampleFreei; 4 U ← Near(G = (V, E), xrand, r) ;

5 V ← V ∪ {xrand};

6 foreach u ∈ U , in order of increasing u − xrand , do

7

if xrand and u are not in the same connected component of G = (V, E) then

8

if CollisionFree(xrand, u) then E ← E ∪ {(xrand, u), (u, xrand)};

9 return G = (V, E);

Analysis results in the literature are only available for a “simpliﬁed” version of the PRM algorithm (Kavraki et al., 1998), referred to as sPRM in this paper. The simpliﬁed algorithm initializes the vertex set with the initial condition, samples n points from Xfree, and then attempts to connect points within a distance r, i.e., using a similar logic as PRM, with the diﬀerence that connections between vertices in the same connected component are allowed. Notice that in the absence of obstacles, i.e., if Xfree = X , the roadmap constructed in this way is a random r-disc graph.

Algorithm 2: sPRM

1 V ← {xinit} ∪ {SampleFreei}i=1,...,n; E ← ∅; 2 foreach v ∈ V do

3 U ← Near(G = (V, E), v, r) \ {v};

4 foreach u ∈ U do

5

if CollisionFree(v, u) then E ← E ∪ {(v, u), (u, v)}

6 return G = (V, E);

Practical implementation of the (s)PRM algorithm have often considered diﬀerent choices for the set U of vertices to which connections are attempted (i.e., line 4 in Algorithm 1, and line 3 in Algorithm 2). In particular, the following criteria are of particular interest:
• k-Nearest (s)PRM: Choose the nearest k neighbors to the vertex under consideration, for a given k (a typical value is reported as k = 15 (LaValle, 2006)). In other words, U ← kNearest(G = (V, E), xrand, k) in line 4 of Algorithm 1 and U ← kNearest(G = (V, E), v, k)\ {v} in line 3 of Algorithm 2. The roadmap constructed in this way in an obstacle-free environment is a random k-nearest graph.
• Bounded-degree (s)PRM: For any ﬁxed r, the average number of connections attempted at each iteration is proportional to the number of vertices in V , and can result in an excessive computational burden for large n. To address this, an upper bound k can be imposed on the cardinality of the set U (a typical value is reported as k = 20 (LaValle, 2006)). In other words, U ← Near(G, xrand, r) ∩ kNearest(G, xrand, k) in line 4 of Algorithm 1, and U ← (Near(G, v, r) ∩ kNearest(G, v, k)) \ {v} in line 3 of Algorithm 2.
• Variable-radius (s)PRM: Another option to maintain the degree of the vertices in the roadmap small is to make the connection radius r a function of n, as opposed to a ﬁxed

12

parameter. However, there are no clear indications in the literature on the appropriate functional relationship between r and n.

Rapidly-exploring Random Trees (RRT): The Rapidly-exploring Random Tree algorithm is primarily aimed at single-query applications. In its basic version, the algorithm incrementally builds a tree of feasible trajectories, rooted at the initial condition. An outline of the algorithm is given in Algorithm 3. The algorithm is initialized with a graph that includes the initial state as its single vertex, and no edges. At each iteration, a point xrand ∈ Xfree is sampled. An attempt is made to connect the nearest vertex v ∈ V in the tree to the new sample. If such a connection is successful, xrand is added to the vertex set, and (v, xrand) is added to the edge set. In the original version of this algorithm, the iteration is stopped as soon as the tree contains a node in the goal region. In this paper, for consistency with the other algorithms (e.g., PRM), the iteration is performed n times. In the absence of obstacles, i.e., if Xfree = X , the tree constructed in this way is an online nearest neighbor graph.

Algorithm 3: RRT

1 V ← {xinit}; E ← ∅;

2 for i = 1, . . . , n do

3 xrand ← SampleFreei; 4 xnearest ← Nearest(G = (V, E), xrand);

5 xnew ← Steer(xnearest, xrand) ;

6 if ObtacleFree(xnearest, xnew) then

7

V ← V ∪ {xnew}; E ← E ∪ {(xnearest, xnew)} ;

8 return G = (V, E);

A variant of RRT consists of growing two trees, respectively rooted at the initial state, and at a state in the goal set. To highlight the fact that the sampling procedure must not necessarily be stochastic, the algorithm is also referred to as Rapidly-exploring Dense Trees (RDT) (LaValle, 2006).
3.3 Proposed algorithms
In this section, the new algorithms considered in this paper are presented. These algorithms are proposed as asymptotically optimal and computationally eﬃcient versions of their “standard” counterparts, as will be made clear through the analysis in the next section. Input and output data are the same as in the algorithms introduced in Section 3.2.
Optimal Probabilistic RoadMaps (PRM∗): In the standard PRM algorithm, as well as in its simpliﬁed “batch” version considered in this paper, connections are attempted between roadmap vertices that are within a ﬁxed radius r from one another. The constant r is thus a parameter of PRM. The proposed algorithm—shown in Algorithm 4—is similar to sPRM, with the only diﬀerence being that the connection radius r is chosen as a function of n, i.e., r = r(n) := γPRM(log(n)/n)1/d, where γPRM > γP∗RM = 2(1 + 1/d)1/d (µ(Xfree)/ζd)1/d, d is the dimension of the space X , µ(Xfree) denotes the Lebesgue measure (i.e., volume) of the obstacle-free space, and ζd is the volume of the unit ball in the d-dimensional Euclidean space. Clearly, the connection radius decreases with the number of samples. The rate of decay is such that the average number of connections attempted from a roadmap vertex is proportional to log(n).

13

Note that in the discussion of variable-radius PRM in LaValle (2006), it is suggested that the
radius be chosen as a function of sample dispersion. (Recall that the dispersion of a point set contained in a bounded set S ⊂ Rd is the radius of the largest empty ball centered in S.) Indeed,
the dispersion of a set of n random points sampled uniformly and independently in a bounded set is O((log(n)/n)1/d) (Niederreiter, 1992), which is precisely the rate at which the connection radius is scaled in the PRM∗ algorithm.

Algorithm 4: PRM∗

1 V ← {xinit} ∪ {SampleFreei}i=1,...,n; E ← ∅; 2 foreach v ∈ V do

3 U ← Near(G = (V, E), v, γPRM(log(n)/n)1/d) \ {v};

4 foreach u ∈ U do

5

if CollisionFree(v, u) then E ← E ∪ {(v, u), (u, v)}

6 return G = (V, E);

Another version of the algorithm, called k-nearest PRM∗, can be considered, motivated by the k-nearest PRM implementation previously mentioned, whereby the number k of nearest neighbors to be considered is not a constant, but is chosen as a function of the cardinality of the roadmap n. More precisely, k(n) := kPRM log(n), where kPRM > kP∗ RM = e (1 + 1/d), and U ← kNearest(G = (V, E), v, kPRM log(n)) \ {v} in line 3 of Algorithm 4.
Note that kP∗ RM is a constant that only depends on d, and does not otherwise depend on the problem instance, unlike γP∗RM. Moreover, kPRM = 2e is a valid choice for all problem instances.
Rapidly-exploring Random Graph (RRG): The Rapidly-exploring Random Graph algorithm was introduced as an incremental (as opposed to batch) algorithm to build a connected roadmap, possibly containing cycles. The RRG algorithm is similar to RRT in that it ﬁrst attempts to connect the nearest node to the new sample. If the connection attempt is successful, the new node is added to the vertex set. However, RRG has the following diﬀerence. Every time a new point xnew is added to the vertex set V , then connections are attempted from all other vertices in V that are within a ball of radius r(card (V )) = min{γRRG(log(card (V ))/ card (V ))1/d, η}, where η is the constant appearing in the deﬁnition of the local steering function, and γRRG > γR∗ RG = 2 (1 + 1/d)1/d (µ(Xfree)/ζd)1/d. For each successful connection, a new edge is added to the edge set E. Hence, it is clear that, for the same sampling sequence, the RRT graph (a directed tree) is a subgraph of the RRG graph (an undirected graph, possibly containing cycles). In particular, the two graphs share the same vertex set, and the edge set of the RRT graph is a subset of that of the RRG graph.
Another version of the algorithm, called k-nearest RRG, can be considered, in which connections are sought to k nearest neighbors, with k = k(card (V )) := kRRG log(card (V )), where kRRG > kR∗ RG = e (1 + 1/d), and Xnear ← kNearest(G = (V, E), xnew, kRRG log(card (V ))), in line 7 of Algorithm 5.
Note that kR∗ RG is a constant that depends only on d, and does not depend otherwise on the problem instance, unlike γR∗ RG. Moreover, kRRG = 2e is a valid choice for all problem instances.
Optimal RRT (RRT∗): Maintaining a tree structure rather than a graph is not only economical in terms of memory requirements, but may also be advantageous in some applications, due to, for instance, relatively easy extensions to motion planning problems with diﬀerential constraints, or

14

Algorithm 5: RRG

1 V ← {xinit}; E ← ∅;

2 for i = 1, . . . , n do

3 xrand ← SampleFreei;

4 xnearest ← Nearest(G = (V, E), xrand);

5 xnew ← Steer(xnearest, xrand) ;

6 if ObtacleFree(xnearest, xnew) then

7

Xnear ← Near(G = (V, E), xnew, min{γRRG(log(card (V ))/ card (V ))1/d, η}) ;

8

V ← V ∪ {xnew}; E ← E ∪ {(xnearest, xnew), (xnew, xnearest)} ;

9

foreach xnear ∈ Xnear do

10

if CollisionFree(xnear, xnew) then E ← E ∪ {(xnear, xnew), (xnew, xnear)}

11 return G = (V, E);

to cope with modeling errors. The RRT∗ algorithm is obtained by modifying RRG in such a way that formation of cycles is avoided, by removing “redundant” edges, i.e., edges that are not part of a shortest path from the root of the tree (i.e., the initial state) to a vertex. Since the RRT and RRT∗ graphs are directed trees with the same root and vertex set, and edge sets that are subsets of that of RRG, this amounts to a “rewiring” of the RRT tree, ensuring that vertices are reached through a minimum-cost path.
Before discussing the algorithm, it is necessary to introduce a few new functions. Given two points x1, x2 ∈ Rd, let Line(x1, x2) : [0, s] → X denote the straight-line path from x1 to x2. Given a tree G = (V, E), let Parent : V → V be a function that maps a vertex v ∈ V to the unique vertex u ∈ V such that (u, v) ∈ E. By convention, if v0 ∈ V is the root vertex of G, Parent(v0) = v0. Finally, let Cost : V → R≥0 be a function that maps a vertex v ∈ V to the cost of the unique path from the root of the tree to v. For simplicity, in stating the algorithm we will assume an additive cost function, so that Cost(v) = Cost(Parent(v)) + c(Line(Parent(v), v)), although this is not necessary for the analysis in the next section. By convention, if v0 ∈ V is the root vertex of G, then Cost(v0) = 0.
The RRT∗ algorithm, shown in Algorithm 6, adds points to the vertex set V in the same way as RRT and RRG. It also considers connections from the new vertex xnew to vertices in Xnear, i.e., other vertices that are within distance r(card (V )) = min{γRRT∗(log(card (V ))/ card (V ))1/d, η} from xnew. However, not all feasible connections result in new edges being inserted in the edge set E. In particular, (i) an edge is created from the vertex in Xnear that can be connected to xnew along a path with minimum cost, and (ii) new edges are created from xnew to vertices in Xnear, if the path through xnew has lower cost than the path through the current parent; in this case, the edge linking the vertex to its current parent is deleted, to maintain the tree structure.
Another version of the algorithm, called k-nearest RRT∗, can be considered, in which connections are sought to k nearest neighbors, with k(card (V )) = kRRG log(card (V )), and Xnear ← kNearest(G = (V, E), xnew, kRRG log(i)), in line 7 of Algorithm 6.
4 Analysis
In this section, a number of results concerning the probabilistic completeness, asymptotic optimality, and complexity of the algorithms in Section 3 are presented.
The return value of Algorithms 1-6 is a graph. Since the sampling procedure SampleFree is
15

Algorithm 6: RRT∗

1 V ← {xinit}; E ← ∅;

2 for i = 1, . . . , n do

3 xrand ← SampleFreei; 4 xnearest ← Nearest(G = (V, E), xrand);
5 xnew ← Steer(xnearest, xrand) ;

6 if ObtacleFree(xnearest, xnew) then

7

Xnear ← Near(G = (V, E), xnew, min{γRRT∗(log(card (V ))/ card (V ))1/d, η}) ;

8

V ← V ∪ {xnew};

9

xmin ← xnearest; cmin ← Cost(xnearest) + c(Line(xnearest, xnew));

10

foreach xnear ∈ Xnear do

// Connect along a minimum-cost path

11

if CollisionFree(xnear, xnew) ∧ Cost(xnear) + c(Line(xnear, xnew)) < cmin then

12

xmin ← xnear; cmin ← Cost(xnear) + c(Line(xnear, xnew))

13

E ← E ∪ {(xmin, xnew)};

14

foreach xnear ∈ Xnear do

// Rewire the tree

15

if CollisionFree(xnew, xnear) ∧ Cost(xnew) + c(Line(xnew, xnear)) < Cost(xnear)

then xparent ← Parent(xnear);

16

E ← (E \ {(xparent, xnear)}) ∪ {(xnew, xnear)}

17 return G = (V, E);

stochastic, the returned graph is in fact a random variable.2 Since the sampling procedure is modeled as a map from the sample space Ω to inﬁnite sequences in X , sets of vertices and edges of the graphs maintained by the algorithms can be deﬁned as functions from the sample space Ω to appropriate sets. More precisely, let ALG be a label indicating one of the algorithms in Section 3, and let {ViALG(ω)}i∈N and {EiALG(ω)}i∈N be, respectively, the sets of vertices and edges in the graph returned by algorithm ALG, indexed by the number of samples, for a particular realization of the sample sequence. (In other words, these are sequences of functions deﬁned from Ω into ﬁnite subsets of Xfree or Xfree × Xfree.) Similarly, let GAi LG = (ViALG, EiALG). (The label ALG will be at times omitted when the algorithm being used is clear from the context.)
All algorithms considered in the paper are sound, in the sense that they only return graphs with vertices and edges representing points and paths in Xfree.This statement can be easily veriﬁed by inspection of the algorithms in Section 3.
4.1 Probabilistic Completeness
In this section, the feasibility problem is considered, and the (probabilistic) completeness properties of the algorithms in Section 3 are analyzed. First, some preliminary deﬁnitions are given, followed by a deﬁnition of probabilistic completeness. Then, completeness properties of various samplingbased motion planning algorithms are stated.
Let δ > 0 be a real number. A state x ∈ Xfree is said to be a δ-interior state of Xfree, if the closed ball of radius δ centered at x lies entirely inside Xfree. The δ-interior of Xfree, denoted as intδ(Xfree), is deﬁned as the collection of all δ-interior states, i.e., intδ(Xfree) := {x ∈ Xfree | Bx,δ ⊆ Xfree}. In
2We will not address the case in which the sampling procedure is deterministic, but refer the reader to LaValle et al. (2004), which contains an in-depth discussion of the relative merits of randomness and determinism in sampling-based motion planning algorithms.
16

 

Figure 1: An illustration of the δ-interior of Xfree. The obstacle region Xobs is shown in dark grey and the δ-interior of Xfree is shown in light grey. The distance between the dashed boundary of intδ(Xfree) and the solid boundary of Xfree is precisely δ.

other words, the δ-interior of Xfree is the set of all states that are at least a distance δ away from any point in the obstacle set (see Figure 1). A collision-free path σ : [0, 1] → Xfree is said to have strong δ-clearance, if σ lies entirely inside the δ-interior of Xfree, i.e., σ(τ ) ∈ intδ(Xfree) for all τ ∈ [0, 1]. A path planning problem (Xfree, xinit, Xgoal) is said to be robustly feasible if there exists a path with strong δ-clearance, for some δ > 0, that solves it. In terms of the notation used in this paper, the
notion of probabilistic completeness can be stated as follows.

Deﬁnition 14 (Probabilistic Completeness) An algorithm ALG is probabilistically complete, if, for any robustly feasible path planning problem (Xfree, xinit, Xgoal),

lim inf P
n→∞

{∃xgoal ∈ VnALG ∩ Xgoal such that xinit is connected to xgoal in GAn LG }

= 1.

If an algorithm is probabilistically complete, and the path planning problem is robustly feasible, the limit

limn→∞ P {∃xgoal ∈ VnALG ∩ Xgoal such that xinit is connected to xgoal in GAn LG }

exists and is equal to 1. On the other hand, the same limit is equal to zero for any sampling-based algorithm (including probabilistically complete ones) if the problem is not robustly feasible, unless the samples are drawn from a singular distribution adapted to the problem.
It is known from the literature that the sPRM and RRT algorithms are probabilistically complete, and that the probability of ﬁnding a solution if one exists approaches one exponentially fast with the number of vertices in the graph returned by the algorithms. In other words,

Theorem 15 (Probabilistic completeness of sPRM (Kavraki et al., 1998)) Consider a robustly feasible path planning problem (Xfree, xinit, Xgoal). There exist constants a > 0 and n0 ∈ N, dependent only on Xfree and Xgoal, such that
P ∃ xgoal ∈ VnsPRM ∩ Xgoal : xgoal is connected to xinit in GsnPRM > 1 − e−a n, ∀n > n0.
Theorem 16 (Probabilistic Completeness of RRT (LaValle and Kuﬀner, 2001)) Consider a robustly feasible path planning problem (Xfree, xinit, Xgoal). There exist constants a > 0 and n0 ∈ N, both dependent only on Xfree and Xgoal, such that
P VnRRT ∩ Xgoal = ∅ > 1 − e−a n, ∀n > n0.
On the other hand, the probabilistic completeness results do not necessarily extend to the heuristics used in practical implementations of the (s)PRM algorithm, as detailed in Section 3. For

17

example, consider the k-nearest sPRM algorithm, where k = 1. That is, each vertex is connected to its nearest neighbor and the resulting undirected graph is returned as the output. This sPRM algorithm will be called the 1-nearest sPRM, and indicated with the label 1PRM. The RRT algorithm can be thought of as the incremental version of the 1-nearest sPRM algorithm: the RRT algorithm also connects each sample to its nearest neighbor, but forces connectivity of the graph by an incremental construction. The following theorem shows that the 1-nearest sPRM algorithm is not probabilistically complete, although the RRT is (see Theorem 16). Furthermore, the probability that it fails to ﬁnd a path converges to one as the number of samples approaches inﬁnity.

Theorem 17 (Incompleteness of k-nearest sPRM for k = 1) The k-nearest sPRM algorithm is not probabilistically complete for k = 1. Furthermore,

lim P
n→∞

{∃xgoal ∈ Vn1PRM ∩ Xgoal such that xinit is connected to xgoal in GAn LG }

= 0.

The proof of this theorem requires two intermediate results that are provided below. For simplicity of presentation, consider the case when Xfree = X . Let G1nPRM = (Vn1PRM, En1PRM) denote the graph returned by the 1-nearest sPRM algorithm, when the algorithm is run with n samples. Let Ln denote the total length of all the edges present in G1nPRM. Recall that ζd denotes the volume of the unit ball in the d-dimensional Euclidean space. Let ζd denote the volume of the union of two unit balls whose centers are a unit distance apart.

Lemma 18 (Total length of the 1-nearest neighbor graph (Wade, 2007)) For all d ≥ 2, Ln/n1−1/d converges to a constant in mean square, i.e.,

lim E
n→∞

Ln n1−1/d

−

1 1+
d

1 ζd

−

ζd 2 (ζd)1+1/d

2
= 0.

Proof This lemma is a direct consequence of Theorem 3 of Wade (2007).

Let Nn denote the number of connected components of G1nPRM.

Lemma 19 (Number of connected components of the 1-nearest neighbor graph) For all d ≥ 2, Nn/n converges to a constant in mean square, i.e.,

lim E
n→∞

Nn − ζd

2
= 0.

n 2 ζd

Proof A reciprocal pair is a pair of vertices each of which is the other one’s nearest neighbor. In a graph formed by connecting each vertex to its nearest neighbor, any connected component includes exactly one reciprocal pair whenever the number of vertices is greater than 2 (see, e.g., Eppstein et al., 1997). The number of reciprocal pairs in such a graph was shown to converge to ζd/(2ζd) in mean square in Henze (1987) (see also Remark 2 in Wade (2007)).

Proof of Theorem 17 Let Ln denote the average length of a connected component in G1nPRM, i.e., Ln = Ln/Nn. Let Ln denote the length of the connected component that includes xinit. Since the samples are drawn independently and uniformly, the random variables Ln and Ln have the same distribution (although they are clearly dependent). Let γL denote the constant that Ln/n1−1/d converges to (see Lemma 18). Similarly, let γN denote the constant that Nn/n converges to (see Lemma 19).

18

Recall that convergence in mean square implies convergence in probability and hence conver-

gence in distribution (Grimmett and Stirzaker, 2001). Since both Ln/n1−1/d and Nn/n converge

in mean square to constants and P({Nn = 0}) = 0 for all n ∈ N, by Slutsky’s theorem (Resnick,

1999), n1/d Ln =

Ln /n1−1/d Nn/n

converges

to

γ

:= γL/γN

in

distribution.

In this

case, it also

converges

in probability, since γ is a constant (Grimmett and Stirzaker, 2001). Then, n1/d Ln also converges

to γ in probability, since Ln and Ln are identically distributed for all n ∈ N. Thus, Ln converges

to 0 in probability, i.e., limn→∞ P ({Ln > }) = 0, for all > 0.

Let > 0 be such that < infx∈Xgoal x−xinit . Let An denote the event that the graph returned

by the 1-nearest sPRM algorithm contains a feasible path, i.e., one that starts from xinit and reaches

the goal region Clearly, the event {Ln > } occurs whenever An does, i.e., An ⊆ {Ln > }. Then,

P(An) ≤ P({Ln > }). Taking the limit superior of both sides

lim inf
n→∞

P(An)

≤

lim sup P(An)
n→∞

≤

lim sup P({Ln >
n→∞

})

=

0.

In other words, the limit limn→∞ P(An) exists and is equal zero.

Consider the variable-radius sPRM algorithm. The following theorem asserts that variableradius sPRM algorithm is not probabilistically complete in the subcritical regime.

Theorem 20 (Incompleteness of variable-radius sPRM with r(n) = γn−1/d) There exists a constant γ > 0 such that the variable radius sPRM with connection radius r(n) = γn−1/d is not probabilistically complete.
The proof of this result requires some intermediate results from random geometric graph theory. Recall that λc is the critical density, or continuum percolation threshold (see Section 2.2). Given a Borel set Γ ⊆ Rd, let GdΓisc(n, r) denote the random r-disc graph formed with vertices independent and uniformly sampled from Γ and edges connecting two vertices, v and v , whenever v − v < rn.
Lemma 21 (Penrose (2003)) Let λ ∈ (0, λc) and Γ ⊂ Rd be a Borel set. Consider a sequence {rn}n∈N that satisﬁes n rnd ≤ λ, ∀n ∈ N. Let Nmax(GdΓisc(n, rn)) denote the size of the largest component in GdΓisc(n, rn). Then, there exist constants a, b > 0 and m0 ∈ N such that for all m ≥ m0,
P Nmax(GdΓisc(n, rn)) ≥ m ≤ n e−a m + e−b n .

Proof of Theorem 20 Let > 0 such that < infx∈Xgoal x − xinit and that the 2 -ball centered at xinit lies entirely within the obstacle-free space. Let GPnRM = (VnPRM, EnPRM) denote the graph returned by this variable radius sPRM algorithm, when the algorithm is run with n samples. Let Gn = (Vn, En) denote the the restriction of GPnRM to the 2 -ball centered at xinit deﬁned as Vn = VnPRM ∩ Bxinit,2 and En = (Vn × Vn) ∩ EnPRM.
Clearly, Gn is equivalent to the random r-disc graph on Γ = Bxinit,2 . Let Nmax(Gn) denote the number of vertices in the largest connected component of Gn. By Lemma 21, there exists constants a, b > 0 and m0 ∈ N such that
P({Nmax(Gn) ≥ m}) ≤ n e−a m + e−b n ,

for all m ≥ m0. Then, for all m = λ−1/d ( /2) n1/d > m0,

P Nmax(Gn) ≥ λ−1/d 2 n1/d

≤ n e−a λ−1/d ( /2) n1/d + e−b n .

19

Let Ln denote the total length of all the edges in the connected component that includes xinit. Since rn = λ1/dn−1/d,

P Ln ≥ 2

≤ n e−a λ−1/d ( /2) n1/d + e−b n .

Since the right hand side is summable, by the Borel-Cantelli lemma the event {Ln ≥ /2} occurs inﬁnitely often with probability zero, i.e., P(lim supn→∞{Ln ≥ /2}) = 0.
Given a graph G = (V, E) deﬁne the diameter of this graph as the distance between the farthest
pair of vertices in V , i.e., maxv,v ∈V v − v . Let Dn denote the diameter of the largest component in Gn. Clearly, Dn ≤ Ln holds surely. Thus, P (lim supn→∞ {Dn ≥ /2}) = 0.
Let I ∈ N be the smallest number that satisﬁes rI ≤ /2. Notice that the edges connected to the vertices VnPRM ∩ Bxinit, coincide with those connected to Vn ∩ Bxinit, , for all n ≥ I. Let Rn denote distance of the farthest vertex v ∈ VnPRM to xinit in the component that contains xinit in GPnRM. Notice also that Rn ≥ only if Dn ≥ /2, for all n ≥ I. That is, for all n ≥ I, {Rn ≥ } ⊆ {Dn ≥ /2}, which implies P (lim supn→∞ {Rn ≥ }) = 0.
Let An denote the event that the graph returned by this variable radius sPRM algorithm includes a path that reaches the goal region. Clearly, {Rn ≥ } holds, whenever An holds. Hence, P(An) ≤ P({Rn ≥ }). Taking the limit superior of both sides yields

lim inf
n→∞

P(An)

≤

lim sup P(An)
n→∞

≤

lim sup P ({Rn ≥
n→∞

})

≤

P

lim sup {Rn ≥
n→∞

}

= 0.

Hence, limn→∞ P(An) = 0.

Finally, the probabilistic completeness of the new algorithms proposed in Section 3 is established. Probabilistic completeness of PRM∗ is implied by its asymptotic optimality, proved in Section 4.2.

Theorem 22 (Completeness of PRM∗) The PRM∗ algorithm is probabilistically complete.
Probabilistic completeness of RRG and RRT∗ is a straightforward consequence of the probabilistic completeness of RRT:

Theorem 23 (Probabilistic completeness of RRG and RRT∗) The RRG and RRT∗ algo-

rithms are probabilistically complete. Furthermore, for any robustly feasible path planning problem

(Xfree, xinit, Xgoal), there exist constants a > 0 and n0 ∈ N, both dependent only on Xfree and Xgoal,

such that

P VnRRG ∩ Xgoal = ∅ > 1 − e−a n, ∀n > n0,

and P VnRRT∗ ∩ Xgoal = ∅ > 1 − e−a n, ∀n > n0.

Proof By construction, VnRRG(ω) = VnRRT∗(ω) = VnRRT(ω), for all ω ∈ Ω and n ∈ N. Moreover, the RRG and RRT∗ algorithms return connected graphs. Hence the result follows directly from the probabilistic completeness of RRT.

In particular, note that if the RRT algorithm returns a feasible solution by iteration n, so will the RRG and RRT∗ algorithms, assuming the same sample sequence.

20

4.2 Asymptotic Optimality
In this section, the optimality problem of path planning is considered. The algorithms presented in Section 3 are analyzed, in terms of their ability to return solutions whose cost converge to the global optimum. First, a deﬁnition of asymptotic optimality is provided as almost-sure convergence to optimal paths. Second, it is shown that the RRT algorithm lacks the asymptotic optimality property. Third, the PRM∗, RRG, and RRT∗ algorithms, as well as their k-nearest implementations, are shown to be asymptotically optimal.
Recall from Section 4.1 that an algorithm is probabilistically complete if the algorithm ﬁnds with high probability a solution to path planning problems that are robustly feasible, i.e., for which feasible path exists with strong δ-clearance. A similar approach is used to deﬁne asymptotic optimality, relying on a notion of weak δ-clearance and on a continuity property for the cost of paths, which will be introduced below.
Let σ1, σ2 ∈ Σfree be two collision-free paths with the same end points. A path σ1 is said to be homotopic to σ2, if there exists a continuous function ψ : [0, 1] → Σfree, called the homotopy, such that ψ(0) = σ1, ψ(1) = σ2, and ψ(τ ) is a collision-free path in for all τ ∈ [0, 1]. Intuitively, a path that is homotopic to σ can be continuously transformed to σ through Xfree (see Munkres, 2000). A collision-free path σ : [0, s] → Xfree is said to have weak δ-clearance, if there exists a path σ that has strong δ-clearance and there exist a homotopy ψ, with ψ(0) = σ, ψ(1) = σ , and for all α ∈ (0, 1] there exists δα > 0 such that ψ(α) has strong δα-clearance. See Figure 2 for an illustration of the weak δ-clearance property. A path that violates the weak δ-clearance property is shown in Figure 3. Weak δ-clearance does not require points along a path to be at least a distance δ away from the obstacles (see Figure 4). In fact, a collision-free path with uncountably many points lying on the boundary of an obstacle can still have weak δ-clearance.


Figure 2: An illustration of a path σ with weak δ-clearance. The path σ that lies inside intδ(Xfree) and is in the same homotopy class as σ is also shown in the ﬁgure. Note that σ does not have strong δ-clearance.
  
Figure 3: An illustration of an example path σ that does not have weak δ-clearance. For any positive value of δ, there is no path in intδ(Xfree) that is in the same homotopy class as σ.
Next, the set of all paths with bounded length is introduced as a normed space, which allows taking the limit of a sequence of paths. Recall that Σ is the set of all paths, and T V (·) denotes the
21

Front view

Side view

Figure 4: An illustration of a path that has weak δ-clearance. The path passes through a point where two spheres representing the obstacle region are in contact. Clearly, the path does not have strong δ-clearance.

total variation, i.e., the length, of a path (see Section 2.1). Given σ1, σ2 ∈ Σ with σ1 : [0, 1] → X

and σ2 : [0, 1] → X , the addition operation is deﬁned as (σ1 + σ2)(τ ) = σ1(τ ) + σ2(τ ) for all

τ ∈ [0, 1]. The set of paths Σ is closed under addition. Given a path σ : [0, 1] → X and a scalar

α ∈ R, the multiplication by a scalar operation is deﬁned as (ασ)(τ ) := α σ(τ ) for all τ ∈ [0, 1].

With these addition and multiplication by a scalar operations, the function space Σ is, in fact, a

vector space. On the vector space Σ, deﬁne the norm

σ BV :=

1 0

|σ(τ )|

dτ

+

TV(σ),

and

denote

the function space Σ endowed with the norm · BV by BV(X ). The norm · BV induces the

following distance function:

1

dist(σ1, σ2) = σ1 − σ2 BV =

(σ1 − σ2)(τ ) dτ + TV(σ1 − σ2)

0

where · is the usual Euclidean norm. A sequence {σn}n∈N of paths is said to converge to a path
σ¯, denoted as limn→∞ σn = σ¯, if the norm of the diﬀerence between σn and σ¯ converges to zero,
i.e., limn→∞ σn − σ¯ BV = 0. A feasible path σ∗ ∈ Xfree that solves the optimality problem (Problem 3) is said to be a robustly
optimal solution if it has weak δ-clearance and, for any sequence of collision-free paths {σn}n∈N, σn ∈ Xfree, ∀n ∈ N, such that limn→∞ σn = σ∗, limn→∞ c(σn) = c(σ∗). Clearly, a path planning problem that has a robustly optimal solution is necessarily robustly feasible. Let c∗ = c(σ∗) be the cost of an optimal path, and let YnALG be the extended random variable corresponding to the cost of the minimum-cost solution included in the graph returned by ALG at the end of iteration n.

Deﬁnition 24 (Asymptotic Optimality) An algorithm ALG is asymptotically optimal if, for
any path planning problem (Xfree, xinit, Xgoal) and cost function c : Σ → R≥0 that admit a robustly optimal solution with ﬁnite cost c∗,

P lim sup YnALG = c∗ = 1.
n→∞
Note that, since YnALG ≥ c∗, ∀n ∈ N, asymptotic optimality of ALG implies that the limit limn→∞ YnALG exists, and is equal to c∗. Clearly, probabilistic completeness is necessary for asymptotic optimality. Moreover, the probability that a sampling-based algorithm converges to an optimal solution almost surely has probability either zero or one. That is, a sampling-based algorithm either converges to the optimal solution in almost all runs, or the convergence does not occur in almost all runs.

Lemma 25 Given that lim supn→∞ YnALG < ∞, i.e., ALG ﬁnds a feasible solution eventually, the probability that lim supn→∞ YnALG = c∗ is either zero or one.

22

Proof Conditioning on the event {lim supn→∞ YnALG < ∞} ensures that YnALG is ﬁnite, thus a random variable, for all large n. Given a sequence {Yn}n∈N of random variables, let Fm denote the σ-ﬁeld generated by the sequence {Yn}∞ n=m of random variables. The tail σ-ﬁeld T is deﬁned as T = n∈N Fn. An event A is said to be a tail event if A ∈ T . Any tail event occurs with probability either zero or one by the Kolmogorov zero-one law (Resnick, 1999). Consider the sequence {YnALG}n∈N of random variables. Let Fm denote the σ-ﬁelds generated by {YnALG}∞ n=m. Then, lim supn→∞ YnALG = c∗ = lim supn→∞, n≥m YnALG = c∗ ∈ Fm for all n ∈ N. Hence,
YnALG = c∗ ∈ n∈N Fm is a tail event. The result follows by the Kolmogorov zero-one law.
Among the ﬁrst steps in assessing the asymptotic optimality properties of an algorithm ALG is determining whether the limit limn→∞ YnALG exists. It turns out that if the graphs returned by ALG satisfy a monotonicity property, then the limit exists, and is in general a random variable, indicated with Y∞ALG.
Lemma 26 If GAi LG(ω) ⊆ GAi+L1G(ω), ∀ω ∈ Ω and ∀i ∈ N, then limn→∞ YnALG(ω) = Y∞ALG(ω).
Proof Since GAi LG(ω) ⊆ GAi+L1G(ω), then YiA+L1G(ω) ≤ YiALG(ω), for all ω ∈ Ω. Since YiALG ≥ c∗, then the sequence converges to some limiting value, dependent on ω, i.e., Y∞ALG(ω).
Of the algorithms presented in Section 3, it is easy to check that PRM, sPRM, RRT, RRG, and RRT∗ satisfy the monotonicity property in Lemma 26. On the other hand, k-nearest sPRM and PRM∗ do not: in these cases, the random variable YiA+L1G is not necessarily dominated by YiALG. This is evident in numerical experiments, e.g., see Figures 10 and 11 in Section 5.
In order to avoid trivial cases of asymptotic optimality, it is necessary to rule out problems in which optimal solutions can be computed after a ﬁnite number of samples. Let Σ∗ denote the set of all optimal paths, i.e., the set of all paths that solve the optimal planning problem (Problem 3), and Xopt denote the set of states that an optimal path in Σ∗ passes through, i.e.,
Xopt = {x ∈ Xfree | ∃σ∗ ∈ Σ∗, τ ∈ [0, 1] such that x = σ∗(τ )}.
Assumption 27 (Zero-measure Optimal Paths) The set of all points traversed by an optimal trajectory has measure zero, i.e., µ (Xopt) = 0.
Most cost functions and problem instances of interest satisfy this assumption, including, e.g., the Euclidean length of the path when the goal region is convex. This assumption does not imply that there is a single optimal path; indeed, there are problem instances with uncountably many optimal paths, for which Assumption 27 holds. (A simple example is the motion planning problem in three dimensional Euclidean space where a ball shaped obstacle is placed between the initial state and the goal region.) Assumption 27 implies that no sampling-based planning algorithm can ﬁnd a solution to the optimality problem in a ﬁnite number of iterations.
Lemma 28 If Assumption 27 holds, the probability that a sampling-based algorithm ALG returns a graph containing an optimal path at a ﬁnite iteration n ∈ N is zero, i.e.,
P ∪n∈N{YnALG = c∗} = 0.
Proof Let Bn denote the event that ALG constructs a graph containing a path with cost exactly equal to c∗ at the end of iteration i, i.e., Bn = {YnALG = c∗}. Let B denote the event that ALG returns a graph containing a path that costs exactly c∗ at some ﬁnite iteration i. Then, B can be
23

written as B = ∪n∈NBn. Since Bn ⊆ Bn+1, by monotonocity of measures, limi→∞ P(Bn) = P(B). By Assumption 27 and the deﬁnition of the sampling procedure, P(Bn) = 0 for all n ∈ N, since the probability that the set ni=1{SampleFree(i)} of points contains a point from a zero-measure set is zero. Hence, P(B) = 0.
In the remainder of the paper, it will be tacitly assumed that Assumption 27, and hence Lemma 28, hold.

4.2.1 Existing algorithms
The algorithms in Section 3.2 were originally introduced to eﬃciently solve the feasibility problem, relaxing the completeness requirement to probabilistic completeness. Nevertheless, it is of interest to establish whether these algorithms are asymptotically optimal in addition to being probabilistically complete. (The ﬁrst two results in this section rely on results that will be proven in Section 4.2.2, i.e., the fact that the RRT algorithm is not asymptotically optimal, and the PRM∗ algorithm is asymptotically optimal)
First, consider the PRM algorithm and its variants. The PRM algorithm, in its original form, is not asymptotically optimal.
Theorem 29 (Non-optimality of PRM) The PRM algorithm is not asymptotically optimal.
Proof The proof is based on a counterexample, establishing a form of equivalence between PRM and RRT, which in turn will be proven not to be asymptotically optimal in Theorem 33. Consider a convex obstacle-free environment, e.g., Xfree = X , and choose the connection radius for PRM and the steering parameter for RRT such that r, η > diam(X ). At each iteration, exactly one vertex and one edge is added to the graph, since (i) all connection attempts using the local planner (e.g., straight line connections as considered in this paper) are collision-free, and (ii) at the end of each iteration, the graph is connected (i.e., it contains only one connected component). In particular, the graph returned by the PRM algorithm in this case is a tree, and the arborescence obtained by choosing as the root the ﬁrst sample point, i.e., SampleFree0, is an online nearest-neighbor graph (see Section 2.2) coinciding with the graph returned by RRT with the random initial condition xinit = SampleFree0.
Recall that the PRM algorithm is applicable for multiple-query planning problems: in other words, the graph returned by the PRM algorithm is used to solve path planning problems from arbitrary xinit ∈ Xfree and Xgoal ⊂ Xfree. (Note that all such problems admit robust optimal solutions.) In particular, for xinit = SampleFree0, and any Xgoal, then YnPRM(ω) = YnRRT(ω), for all ω ∈ Ω, n ∈ N. In particular, since both PRM and RRT satisfy the monotonicity condition in Lemma 26, Theorem 33 implies that

P

lim sup YnPRM = c∗
n→∞

=P

lim
n→∞

YnPRM

=

c∗

=P

lim
n→∞

YnRRT

=

c∗

= 0.

The lack of asymptotic optimality of PRM is due to its incremental construction, coupled with the constraint eliminating edges making unnecessary connections within a connected component. Such a constraint is not present in the batch construction of the sPRM algorithm, which is indeed asymptotically optimal (at the expense of computational complexity, see Section 4.3).

24

Theorem 30 (Asymptotic Optimality of sPRM) The sPRM algorithm is asymptotically optimal.
Proof By construction, VnsPRM(ω) = VnPRM∗(ω), and EnsPRM(ω) ⊇ EnPRM∗(ω) for all ω ∈ Ω. Hence, the graph returned by sPRM includes all the paths that are present in the graph returned by PRM∗. Then, asymptotic optimality of sPRM follows from that of PRM∗, which will be proven in Theorem 34.

On the other hand, as in the case of probabilistic completeness, the heuristics that are often used in the practical implementation of (s)PRM are not asymptotically optimal.

Theorem 31 (Non-optimality of k-nearest sPRM) The k-nearest sPRM algorithm is not asymptotically optimal, for any constant k ∈ N.

This theorem will be proven under the assumption that the underlying point process is Poisson. More precisely, the algorithm is analyzed when it is run with Poisson(n) samples. That is, the realization of the random variable Poisson(n) determines the number of points sampled independently and uniformly in Xfree. Hence, the expected number of samples is equal to n, although its realization may slightly diﬀer. However, since the Poisson random variable has exponentially-decaying tails, its large deviations from its mean is unlikely (see, e.g., Grimmett and Stirzaker (2001) for a more precise statement). With a slight abuse of notation, the cost of the best path in the graph returned by the k-nearest sPRM algorithm when the algorithm is run with Poisson(n) number of samples is denoted by YnkPRM, and it is shown that P({lim supn→∞ YnkPRM = c∗}) = 0.

Proof of Theorem 31 Let σ∗ denote an optimal path and s∗ denote its length, i.e., s∗ = T V (σ∗).

For each n, consider a tiling of σ∗ with disjoint open hypercubes, each with edge length 2 n−1/d,

such that the center of each cube is a point on σ∗. See Figure 5. Let Mn denote the maximum

number

of

tiles

that

can

be

generated

in

this

manner

and

note

Mn

≥

s∗ 2

n1/d.

Partition

each

tile

into several open cubes as follows: place an inner cube with edge length n−1/d at the center of the

tile

and

place

several

outer

cubes

each

with

edge

length

1 2

n−1/d

around

the

cube

at

the

center

as

shown in Figure 5. Let Fd denote the number of outer cubes. The volumes of the inner cube and

each of the outer cubes are n−1 and 2−d n−1, respectively.

Inner cube Outer cubes

. . .

Figure 5: An illustration of the tiles mention in the proof of Theorem 31. A single tile is shown in the left; a tiling of the optimal trajectory σ∗ is shown on the right.

For n ∈ N and m ∈ {1, 2, . . . , Mn}, consider the tile m when the algorithm is run with Poisson(n)

samples. Let In,m denote the indicator random variable for the event that the center cube of this

tile contains no samples, whereas every outer cube contains at least k + 1 samples, in tile m.

The probability that the inner cube contains no samples is e−1/µ(Xfree). The probability that

an outer cube contains at least k + 1 samples is 1 − P {Poisson(2−d/µ(Xfree)) ≥ k + 1} = 1 −

P({Poisson(2−d/µ(Xfree))

≤

k})

=

1

−

Γ(k+1,2−d/µ(Xfree)) k!

,

where

Γ(·, ·)

is

the

incomplete

gamma

25

function (Abramowitz and Stegun, 1964). Then, noting that the cubes in a given tile are disjoint and using the independence property of the Poisson process (see Lemma 11),

E [In,m] = e−1/µ(Xfree)

1 − Γ(k + 1, 2−d/µ(Xfree)) Fd > 0, k!

which is a constant that is independent of n; denote this constant by α.

Let Gn = (Vn, En) denote the graph returned by the k-nearest PRM algorithm by the end of

Poisson(n) iterations. Observe that if In,m = 1, then there is no edge of Gn crossing the cube of

side

length

1 2

n−1/d

that

is

centered

at

the

center

of

the

inner

cube

in

tile

m

(shown

as

the

white

cube in Figure 6). To prove this claim, note the following two facts. First, no point that is outside

of the cubes can have an edge that crosses the i√nner cube. Second, no point in one of the outer

cubes has an edge that has length greater than

d 2

i−1/d.

Thus,

no

edge

can

cross

the

white

cube

illustrated in Figure 6.

Figure 6: The event that the inner cube contains no points and each outer cube contains at least

k

points

of

the

point

process

is

illustrated.

The

cube

of

side

length

1 2

n−1/d

is

shown

in

white.

Let σn denote the path in Gn that is closest to σ∗ in terms of the bounded variation norm. Let

Un :=

σn − σ∗

BV.

Notice that Un ≥

1 2

n−1/d

Mn m=1

In,m

=

1 2

n−1/d

Mn

In,1

=

s∗ 4

In,1

.

Then,

s∗

α s∗

E lim sup Un
n→∞

≥

lim sup E [Un]
n→∞

≥

lim sup
n→∞

4

E [In,m]

≥

4

> 0,

where the ﬁrst inequality follows from Fatou’s lemma (Resnick, 1999). This implies P({lim supn→∞ Un > 0}) > 0. Since Ui > 0 implies Yn > c∗ surely,

P ({lim supn→∞ Yn > c∗}) ≥ P ({lim supn→∞ Un > 0}) > 0. That is, P ({lim supn→∞ Yn = c∗}) < 1. In fact, by Lemma 25, P ({lim supn→∞ Yn = c∗}) = 0.

Second, asymptotic optimality of a large class of variable radius sPRM algorithms is considered. Consider a variable radius sPRM in which connection radius satisﬁes r(n) ≤ γ n−1/d for some γ > 0
and for all n ∈ N. The next theorem shows that this algorithm lacks the asymptotic optimality property.

Theorem 32 (Non-optimality of variable radius sPRM with r(n) = γ n−1/d) Consider a variable radius sPRM algorithm with connection radius r(n) = γ n−1/d. This sPRM algorithm is not asymptotic optimal for any γ ∈ R≥0.
Proof Let σ∗ denote a path that is a robust solution to the optimality problem. Let n denote the number of samples that the algorithm is run with. For all n, construct a set Bn = {Bn,1, Bn,2, . . . , Bn,Mn} of openly disjoint balls as follows. Each ball in Bn has radius rn = γ n−1/d,

26

and lies entirely inside Xfree. Furthermore, the balls in Bn “tile” σ∗ such that the center of each

ball lies on σ∗ (see Figure 7). Let Mn denote the maximum number of balls, s¯ denote the length of

the portion of σ∗ that lies within the δ-interior of Xfree, and n0 ∈ N denote the number for which

rn ≤ δ for all n ≥ n0.

Then, for all n ≥ n0,

Mn ≥ 2γ

s¯

= s¯ n1/d.

1 1/d 2 γ

n

Figure 7: An illustration of the covering of the optimal path, σ∗, with openly disjoint balls. The balls cover only a portion of σ∗ that lies within the δ-interior of Xfree.

Indicate the graph returned by this sPRM algorithm as Gn = (Vn, En). Denote the event that the ball Bn,m contains no vertex in Vn by An,m. Denote the indicator random variable for the event An,m by In,m, i.e., In,m = 1 when An,m holds and In,m = 0 otherwise. Then, for all n ≥ n0,

E[In,m] = P(An,m) =

1 − µ(Bn,m)

n
=

µ(Xfree)

1 − ζd γd 1 n µ(Xfree) n

Let Nn be the random variable that denotes the total number of balls in Bn that contain no

vertex in Vn, i.e., Nn =

Mn m=1

In,m.

Then,

for

all

n

≥ n0,

E[Nn] = E

Mn m=1

In,m

=

Mn
E[In,m] = Mn E[In,1] ≥

s¯ n1/d 2γ

m=1

1−

ζd γd

1

n
.

µ(Xfree) n

Consider

a

ball

Bn,m

that

contains

no

vertices

of

this sPRM
√

algorithm.

Then,

no

edges

of

the

graph returned by this algorithm cross the ball of radius

3 2

rn

centered

at

the

center

of

Bn,m.

See

Figure 8.

Let Pn denote the (ﬁnite) set of all acyclic paths that reach the goal region in the graph returned

by this sPRM algorithm when the algorithm is run with n samples. Let Un denote the total variation of the path that is closest to σ∗ among all paths in Pn, i.e., Un := minσn∈Pn σn − σ∗ BV. Then,

1 1/d

E[Un] ≥ E γ n

Nn

s¯ ≥
2

1−

ζd γd

1

n
.

µ(Xfree) n

Taking the limit superior of both sides, the following inequality can be established:

E lim sup Un
n→∞

s¯

≥

lim sup E [Un]
n→∞

≥

lim sup n→∞ 2

1−

ζd γd

1

n

=

s¯ e−

ζd γd µ(Xfree

)

>

0,

µ(Xfree) n

2

where the ﬁrst inequality follows from Fatou’s lemma (Resnick, 1999). Hence, P({lim supn→∞ Un > 0}) > 0, which implies that P lim supn→∞ YnALG > c∗ > 0. That is, P lim supn→∞ YnALG = c∗ < 1. In fact, P lim supn→∞ YnALG = c∗ = 0 by the Kolmogorov zero-one law (see Lemma 25).

27

Figure 8: If the outer ball does not contain vertices of the PRM graph, then no edge of the graph corresponds to a path crossing the inner ball.

Rapidly-exploring Random Trees In this section, it is shown that the minimum-cost path in the RRT algorithm converges to a certain random variable, however, under mild technical assumptions, this random variable is not equal to the optimal cost, with probability one.

Theorem 33 (Non-optimality of RRT) The RRT algorithm is not asymptotically optimal.
The proof of this theorem can be found in Appendix B. Note that, since at each iteration the RRT algorithm either adds a vertex and an edge, or leaves the graph unchanged, GRi RT(ω) ⊆ GRi+R1T(ω), for all i ∈ N and all ω ∈ Ω, and hence the limit limn→∞ YnRRT exists and is equal to the random variable Y∞RRT. In conjunction with Lemma 25, Theorem 33 implies that this limit is strictly greater than c∗ almost surely, i.e., P {limn→∞ YnRRT > c∗} = 1. In other words, the cost of the best solution returned by RRT converges to a suboptimal value, with probability one. In fact, it is possible to construct problem instances such that the probability that the ﬁrst solution returned by the RRT algorithm has arbitrarily high cost is bounded away from zero (Nechushtan et al., 2010).
Since the cost of the best path returned by the RRT algorithm converges to a random variable, Theorem 33 provides new insight explaining the eﬀectiveness of approaches as in Ferguson and Stentz (2006). In fact, running multiple instances of the RRT algorithm amounts to drawing multiple samples of Y∞RRT.

4.2.2 Proposed algorithms
In this section, the proposed algorithms are analyzed for asymptotic optimality, i.e., almost sure convergence to optimal solutions. It is shown that the PRM∗, RRG, and RRT∗ algorithms, as well as their k-nearest implementations, are all asymptotically optimal. The proofs of the following theorems are quite lengthy, and will be provided in the appendix.
Recall that d denotes the dimensionality of the conﬁguration space, µ(Xfree) denotes the Lebesgue measure of the obstacle-free space, and ζd denotes the volume of the unit ball in the d-dimensional Euclidean space. Proofs of the following theorems can be found in Appendices C–G.

Theorem 34 (Asymptotic optimality of PRM∗) If γPRM > 2 (1 + 1/d)1/d

µ(Xfree ) ζd

1/d
, then

the PRM∗ algorithm is asymptotically optimal.

Theorem 35 (Asymptotic optimality of k-nearest PRM∗) If kPRM > e (1 + 1/d), then the k-nearest implementation of the PRM∗ algorithm is asymptotically optimal.

28

Theorem 36 (Asymptotic optimality of RRG) If γPRM > 2 (1 + 1/d)1/d

µ(Xfree ) ζd

1/d
, then

the RRG algorithm is asymptotically optimal.

Theorem 37 (Asymptotic optimality of k-nearest RRG) If kRRG > e (1 + 1/d), then the k-nearest implementation of the RRG algorithm is asymptotically optimal.

Theorem 38 (Asymptotic optimality of RRT∗) If γRRT∗ > (2 (1+1/d))1/d

µ(Xfree ) ζd

1/d
, then

the RRT∗ algorithm is asymptotically optimal.

Theorem 39 (Asymptotic optimality of k-nearest RRT∗) If kRRT∗ > 2d+1 e (1 + 1/d), then the k-nearest implementation of the RRT∗ algorithm is asymptotically optimal.
The proof of the latter theorem follows from those of Theorems 37 and 38.

4.3 Computational Complexity
The objective of this section is to compare the computational complexity of the algorithms provided in Section 3. First, each algorithm is analyzed in terms of the number of calls to the CollisionFree procedure. Second, the computational complexity of certain primitive procedures such as Nearest and Near (see Section 3.1) are analyzed. Using these results, a thorough analysis of the computational complexity of the all the algorithms is given in terms of the number of simple operations, such as comparisons, additions, multiplications. An analysis of the computational complexity of the query phase, i.e., the complexity of extracting the optimal solution from the graph returned by these algorithms, is also provided.
The following notation for asymptotic computational complexity will be used throughout this section. Let WnALG(P ) be a function of the graph returned by algorithm ALG when ALG is run with inputs P = (Xfree, xinit, Xgoal) and n. Clearly, WnALG(P ) is a random variable. Let f : N → N be an increasing function with limn→∞ f (n) = ∞. The random variable WnALG is said belong to Ω(f (n)), denoted as WnALG ∈ Ω(f (n)), if there exists a problem instance P = (Xfree, xinit, Xgoal) such that lim infn→∞ E[WnALG(P )/f (n)] > 0. Similarly, WnALG is said to belong to O(f (n)) if lim supn→∞ E[WnALG(P )/f (n)] < ∞ for all problem instances P = (Xfree, xinit, Xgoal).
Number of calls to the CollisionFree procedure Let MnALG denote the total number of calls to the CollisionFree procedure by algorithm ALG in iteration n.
First, lower-bounds are established for the PRM and sPRM algorithms.
Lemma 40 (PRM) MnPRM ∈ Ω(n).
Proof Consider the problem instance (Xfree, xinit, Xgoal), where Xfree is composed of two openlydisjoint sets X1 and X2 (see Figure 9). The set X2 is designed to be a hyperrectangle shaped set with one side equal to r/2, where r is the connection radius.
Any r-ball centered at a point in X2 will certainly contain a nonzero measure part of X2. Deﬁne µ¯ as the volume of the smallest region in X2 that can be intersected by an r-ball centered at X2, i.e., µ¯ := infx∈X2 µ(Bx,r ∩ X1). Clearly, µ¯ > 0.
Thus, for any sample Xn that falls into X2, the PRM algorithm will attempt to connect Xn to a certain number of vertices that lies in a subset X1 of X1 such that µ(X1) ≥ µ¯. The expected number of vertices in X1 is at least µ¯ n. Moreover, none of these vertices can be in the same connected component with Xn. Thus, E[MnPRM/n] > µ¯. The result is obtained by taking the limit inferior of both sides.

29

Figure 9: An illustration of Xfree = X1 ∪ X2.

Lemma 41 (sPRM) MnsPRM ∈ Ω(n).

Proof The proof of a stronger result is provided. It is shown that for all problem instances

P = (Xfree, xinit, Xgoal), lim infn→∞ E[MnsPRM/n] > 0, which implies the lemma. Recall from Algorithm 2 that r denotes the connection radius. Let µ¯ denote the volume of the smallest re-

gion that can be formed by intersecting Xfree with an r-ball centered at a point inside Xfree, i.e., µ¯ := infx∈Xfree µ(Bx,r ∩ Xfree). Recall that Xfree is the closure of an open set. Hence, µ¯ > 0.
Clearly, Mn, the number of calls to the CollisionFree procedure in iteration n, is equal to the

number of nodes inside the ball of radius r centered at the last sample point Xn. Moreover, the

volume of the Xfree that lies inside this ball is at least µ¯. Then, the expected value of Mn is lower

bounded by the expected value of a binomial random variable with parameters µ¯/µ(Xfree) and n,

since the underlying point process is binomial.

Thus, E[MnsPRM] ≥

µ¯ µ(Xfree )

n.

Then,

E[Mn/n]

≥

µ¯/Xfree for all n ∈ N. Taking the limit inferior of both sides gives the result.

Clearly, for k-nearest PRM, Mnk-sPRM = k for all n ∈ N with n > k. Similarly, for the RRT, MnRRT = 1 for all n ∈ N.
The next lemma upper-bounds the number of calls to the CollisionFree procedure in the
proposed algorithms.

Lemma 42 (PRM∗, RRG, and RRT∗) MnPRM∗ , MnRRG, MnRRT∗ ∈ O(log n).

Proof First, consider PRM∗. Recall that rn denotes the connection radius of the PRM∗ algorithm. Recall that the rn interior of Xfree, denoted by intrn(Xfree), is deﬁned as the set of all points x, for which the rn-ball centered at x lies entirely inside Xfree. Let A denote the event that the sample Xn drawn at the last iteration falls into the rn interior of Xfree. Then,
E MnPRM∗ = E MnPRM∗ A P(A) + E MnPRM∗ Ac P(Ac).

Let n0 ∈ N be the smallest number such that µ(intrn(Xfree)) > 0. Clearly, such n0 exists, since
limn→∞ rn = 0 and Xfree has non-empty interior. Recall that ζd is the volume of the unit ball in the d-dimensional Euclidean space and that the connection radius of the PRM∗ algorithm is rn = γPRM(log n/n)1/d. Then, for all n ≥ n0

E MnPRM∗

A

= ζd γPRM log n. µ(intrn (Xfree))

On the other hand, given that Xn ∈/ intrn(Xfree), the rn-ball centered at Xn intersects a fragment of Xfree that has volume less than the volume of an rn-ball in the d-dimensional Euclidean space. Then, for all n > n0, E MnPRM∗ Ac ≤ E MnPRM∗ A .

30

Hence, for all n ≥ n0,

MnPRM∗ E log n

≤ ζd γPRM

≤

ζd γPRM .

µ(intrn (Xfree)) µ(intrn0 (Xfree))

Next, consider the RRG. Recall that η is the parameter provided in the Steer procedure (see
Section 3.1). Let D denote the diameter of the set Xfree, i.e., D := supx,x ∈Xfree x − x . Clearly, whenever η ≥ D, V PRM∗ = V RRG = V RRT∗ surely, and the claim holds.
To prove the claim when η < D, let Cn denote the event that for any point x ∈ Xfree the RRG algorithm has a vertex x ∈ VnRRG such that x − x ≤ η. As shown in the proof of Theorem 36 (see Lemma 63), there exists a, b > 0 such that P(Cnc ) ≤ a e−b n. Then,

E MnRRG = E MnRRG Cn P(Cn) + E MnRRG Cnc P(Cnc ),

Clearly, E MnRRG Cnc ≤ n. Hence, the second term of the sum on the right hand side converges to zero as n approaches inﬁnity. On the other hand, given that Cn holds, the new vertex that

will be added to the graph at iteration n, if such a vertex is added at all, will be the same as the

last sample, Xn. To complete the argument, given any set of n points placed inside µ(Xfree), let

Nn denote the number of points that are inside a ball of radius rn that is centered at a point Xn

sampled uniformly at random from µ(Xfree). The expected number of points inside this ball is no

more

than

ζd rnd µ(Xfree )

n.

Hence,

E[MnRRG

| Cn]

<

ζd γPRM µ(Xfree )

log n,

which

implies

the

existence

of

a

constant

φ1 ∈ R≥0 such that lim supn→∞ E[MnRRG/(log n)] ≤ φ1. Finally, since MnRRT∗ = MnRRG holds surely, lim supn→∞ E[MnRRG/(log n)] ≤ φ1 also.

Trivially, Mnk-PRM∗ = Mnk-RRG = Mnk-RRT∗ = k log n for all n with n/ log n > k.

Complexity of the CollisionFree procedure In this section, complexity of the CollisionFree procedure in terms of the number of obstacles in the environment is analyzed, which is a widelystudied problem in the literature (see, e.g., Lin and Manocha (2004) for a survey). The main result is based on Six and Wood (1982), which shows that checking collision with m obstacles can be executed in O(logd m) time using data structures based on spatial trees (see also Edelsbrunner and Maurer, 1981; Hopcroft et al., 1983).

Complexity of the Nearest procedure The nearest neighbor search problem has been widely studied in the literature, since it has many applications in, e.g., computer graphics, database systems, image processing, data mining, pattern recognition, etc. (Samet, 1989b,a). Clearly, a brute-force algorithm that examines every vertex runs in O(n) time and requires O(1) space. However, in many online real-time applications such as robotics, it is highly desirable to reduce the computation time of each iteration under sublinear bounds, e.g., in O(log n) time, especially for anytime algorithms that provide better solutions as the number of iterations increase.
Fortunately, existing algorithms for computing an “approximate” nearest neighbor, if not an exact one, are computationally very eﬃcient. In the sequel, a vertex y is said to be an ε-approximate nearest neighbor of a point x if y − x ≤ (1 + ε) z − x , where z is the true nearest neighbor of x. An approximate nearest neighbor can be computed using balanced-box decomposition (BBD) trees, which achieves O(cd,ε log n) query time using O(d n) space (Arya et al., 1999), where cd,ε ≤ d 1+6d/ε d. This algorithm is computationally optimal in ﬁxed dimensions, since it closely matches a lower bound for algorithms that use a tree structure stored in roughly linear space (Arya et al., 1999). Using approximate nearest neighbor computation in the context of both PRMs and RRTs was discussed very recently in Yershova and LaValle (2007); Plaku and Kavraki (2008).

31

Let G = (V, E) be a graph with V ⊆ X and let x ∈ X . The discussion above implies that the number of simple operations executed by the Nearest(G, x) procedure is Θ(log |V |) in ﬁxed dimensions, if the Nearest procedure is implemented using a tree structure that is stored in linear space.
Complexity of the Near procedure Problems similar to that solved by the Near procedure are also widely-studied in the literature, generally under the name of range search problems, as they have many applications in, for instance, computer graphics and spatial database systems (Samet, 1989a). In the worst case and in ﬁxed dimensions, computing the exact set of vertices that reside in a ball of radius rn centered at a query point x takes O(n1−1/d + m) time using k-d trees (Lee and Wong, 1977), where m is the number of vertices returned by the search (see also Chanzy et al. (2001) for an analysis of the average case).
Similar to the nearest neighbor search, computing approximate solutions to the range search problem is computationally easier. A range search algorithm is said to be ε-approximate if it returns all vertices that reside in the ball of size rn and no vertices outside a ball of radius (1+ε) rn, but may or may not return the vertices that lie outside the former ball and insid√e the latter ball. Computing ε-approximate solutions using BBD-trees requires O(2d log n + d2(3 d/ε)d−1) time when using O(d n) space, in the worst case (Arya and Mount, 2000). Thus, in ﬁxed dimensions, the complexity of this algorithm is O(log n + (1/ε)d−1), which is known to be optimal, closely matching a lower bound (Arya and Mount, 2000). More recently, algorithms that can provide trade-oﬀs between time and space were also proposed (Arya et al., 2005).
Note that the Near procedure can be implemented as an approximate range search while maintaining the asymptotic optimality guarantee. Notice that the expected number of vertices returned by the Near procedure also does not change, except by a constant factor. Hence, the Near procedure can be implemented to run in order log n expected time in the limit and linear space in ﬁxed dimensions.
Time complexity of the processing phase The following results characterize the asymptotic computational complexity of various sampling-based algorithms in terms of the number of simple operations such as comparisons, additions, and multiplications.
Let n denote the total number of iterations (or, alternatively, the number of samples), and m denote the number of obstacles in the environment. Then, by Lemmas 40 and 41, NnPRM, NnsPRM ∈ Ω(n2 logd m). In the k-nearest sPRM and RRT algorithms, Ω(log n) time is spent on ﬁnding the (k-)nearest neighbor(s) and Ω(logd m) time is spent on collision checking at each iteration. Hence, Nnk-sPRM, NnRRT ∈ Ω(n log n + n logd m).
In all the proposed algorithms, O(log n) time is spent on ﬁnding the near neighbors, and log n logd m time is spent on collision checking. Thus, NnALG ∈ O(n log n logd m) for ALG ∈ {PRM∗, k-PRM∗, RRG, k-RRG, RRT∗, k-RRT∗}.
Time complexity of the query phase After algorithm ALG returns the graph GAn LG, the optimal path must be extracted from this graph using, e.g., Dijkstra’s shortest path algorithm (Schrijver, 2003). In this section, the complexity of this operation, called the query phase, is discussed.
The following lemma yields the asymptotic computational complexity of computing shortest paths. Let G = (V, E) be a graph. A length function l : E → R>0 is a function that assigns each edge in E a positive length. Given a vertex v ∈ V , the shortest paths tree for G, l, and v is a graph G = (V, E ), where E ⊆ E such that for any v ∈ V \ {v}, there exists a unique path in G that starts from v and reaches v , moreover, this path is the optimal such path in G.
32

Lemma 43 (Complexity of shortest paths (Schrijver, 2003)) Given a graph G = (V, E), a length function l : E → R>0, and a vertex v ∈ V , the shortest path tree for G, l, and v can be found in time O(|V | log(|V |) + |E|).
It remains to determine the number of vertices and edges in GAn LG = (VnALG, EnALG), for each algorithm ALG.
Trivially, |EnALG| ∈ Ω(n) holds for all the algorithms discussed in this paper, in particular, for ALG ∈ {PRM, k-sPRM, RRT}. For the sPRM algorithm, a stronger bound can be provided: |EnsPRM| ∈ Ω(n2). To prove this claim, consider the problem instance (Xfree, xinit, Xgoal), where Xfree = X = (0, 1)d. Then, the straight path between any two vertices will be collision-free. Thus, the number of edges is exactly equal to the number of calls to the CollisionFree procedure. Then, the result follows from Lemma 41.
For the proposed algorithms, |EnPRM∗|, |EnRRG| ∈ O(n log n). Since the number of edges is always less than or equal to the total number of calls to the CollisionFree procedure, this claim follows directly from Lemma 42. Finally, |Enk-PRM∗|, |Enk-RRG| ∈ O(n log n) and |EnRRT∗|, |Enk-RRT∗| ∈ O(n) all hold trivially.
Space complexity Space complexity of an algorithm ALG is deﬁned as the amount of memory that is used by ALG to compute the graph GAn LG = (VnALG, EnALG). Clearly, in all algorithms discussed in this paper, the space complexity is the size of GAn LG, i.e., |VnALG| + |EnALG|. Since the number of edges is at least as much as the number of vertices in GAn LG for all algorithms discussed in this paper, the space complexity of an algorithm, in this context, is the number edges in the graph that it returns, which was determined in the previous section.
5 Numerical Experiments
This section is devoted to an experimental study of the algorithms considered in the paper. All algorithms were implemented in C and run on a computer with 2.66 GHz processor and 4GB RAM running the Linux operating system. Unless otherwise noted, total variation of a path is its cost.
A ﬁrst set of experiments were run to illustrate the diﬀerent performance of k-nearest PRM and of PRM∗. The k-nearest PRM and the PRM∗ algorithms were run alongside in two dimensional conﬁguration-space and the cost of the best path in both algorithms is plotted versus the number of iterations in Figure 10. The k-nearest PRM does not converge to optimal solutions, unlike PRM∗. The performance of the PRM∗ algorithm is also shown in conﬁguration spaces of dimensions up to ﬁve in Figure 11.
The main bulk of the experiments were aimed at demonstrating the performance of the RRT∗ algorithm, especially in comparison with its “standard” counterpart, i.e., RRT. Three problem instances were considered. In the ﬁrst two, the cost function is the Euclidean path length.
The ﬁrst scenario includes no obstacles. Both algorithms are run in a square environment. The trees maintained by the algorithms are shown in Figure 12 at several stages. The ﬁgure illustrates that, in this case, the RRT algorithm does not improve the feasible solution to converge to an optimum solution. On the other hand, running the RRT∗ algorithm further improves the paths in the tree to lower cost ones. The convergence properties of the two algorithms are also investigated in Monte-Carlo runs. Both algorithms were run for 20,000 iterations 500 times and the cost of the best path in the trees were averaged for each iteration. The results √are shown in Figure 13, which shows that in the limit the RRT algorithm has cost very close to a 2 factor the optimal solution (see LaValle and Kuﬀner (2009) for a similar result in a deterministic setting), whereas the RRT∗ converges to the optimal solution. Moreover, the variance over diﬀerent RRT runs approaches
33

2.5, while that of the RRT∗ approaches zero. Hence, almost all RRT∗ runs have the property of convergence to an optimal solution, as expected.
In the second scenario, both algorithms are run in an environment in presence of obstacles. In Figure 14, the trees maintained by the algorithms are shown after 20,000 iterations. The tree maintained by the RRT∗ algorithm is also shown in Figure 15 in diﬀerent stages. It can be observed that the RRT∗ ﬁrst rapidly explores the state space just like the RRT. Moreover, as the number of samples increase, the RRT∗ improves its tree to include paths with smaller cost and eventually discovers a path in a diﬀerent homotopy class, which reduces the cost of reaching the target considerably. Results of a Monte-Carlo study for this scenario is presented in Figure 16. Both algorithms were run alongside up until 20,000 iterations 500 times and cost of the best path in the trees were averaged for each iteration. The ﬁgures illustrate that all runs of the RRT∗ algorithm converges to the optimum, whereas the RRT algorithm is about 1.5 of the optimal solution on average. The high variance in solutions returned by the RRT algorithm stems from the fact that there are two diﬀerent homotopy classes of paths that reach the goal. If the RRT luckily converges to a path of the homotopy class that contains an optimum solution, then the resulting path is relatively closer to the optimum than it is on average. If, on the other hand, the RRT ﬁrst explores a path of the second homotopy class, which is often the case for this particular scenario, then the solution that RRT converges to is generally around twice the optimum.
Finally, in the third scenario, where no obstacles are present, the cost function is selected to be the line integral of a function, which evaluates to 2 in the high cost region, 1/2 in the low cost region, and 1 everywhere else. The tree maintained by the RRT∗ algorithm is shown after 20,000 iterations in Figure 17. Notice that the tree either avoids the high cost region or crosses it quickly, and vice-versa for the low-cost region. (Incidentally, this behavior corresponds to the well known Snell-Descartes law for refraction of light, see Rowe and Alexander (2000) for a path-planning application.)
To compare the running time, both algorithms were run alongside in an environment with no obstacles for up to one million iterations. Figure 18, shows the ratio of the running time of RRT∗ and that of RRT versus the number of iterations averaged over 50 runs. As expected from the complexity analysis of Section 4.3, this ratio converges to a constant value. A similar ﬁgure is produced for the second scenario and provided in Figure 19.
The RRT∗ algorithm was also run in a 5-dimensional state space. The number of iterations versus the cost of the best path averaged over 100 trials is shown in Figure 20. A comparison with the RRT algorithm is provided in the same ﬁgure. The ratio of the running times of the RRT∗ and the RRT algorithms is provided in Figure 21. The same experiment is carried out for a 10-dimensional conﬁguration space. The results are shown in Figure 22.
6 Conclusion
This paper presented the results of a thorough analysis of sampling-based algorithms for optimal path planning. It is shown that broadly used algorithms from the literature, while probabilistically complete, are not asymptotically optimal, i.e., they will return a solution to the path planning problem with high probability if one exists, but the cost of the solution returned by the algorithm will not converge to the optimal cost as the number of samples increases. In particular, it is proven that the PRM and RRT algorithms are not asymptotically optimal. A simpliﬁed version of PRM is asymptotically optimal, but is computationally expensive. In addition, it is shown that certain heuristic versions of PRM are not only not asymptotically complete, but also not necessarily complete.
34

1.14 1.13

1.12

1.11

1.1

k=5

1.09

Normalized cost

1.08

1.07
1.06
k=7
1.05

1.04

1.03

k=10

1.02 1.01
1 100

1,000

k=13

PRM*

10,000 Number of iterations

100,000

k=15
1,000,000

Figure 10: The cost of the best path in the k-nearest sPRM algorithm, and that in the PRM∗ algorithm are shown versus the number of iterations in simulation examples with no obstacles. The k-nearest sPRM algorithm was run for k = 5, 7, 10, 13, 15, each of which is shown separately in blue, and the PRM∗ algorithm is shown in red. The values are normalized so that the cost of the optimal path is equal to one. The iterations were stopped when the query phase of the algorithms exceeded the memory limit (approximately 4GB).

In order to address these limitations of existing algorithms, a number of new algorithms are introduced, and proven to be asymptotically optimal and computational eﬃcient, with respect to probabilistically complete algorithms in this class. In other words, asymptotic optimality imposes only a constant factor increase in complexity with respect to probabilistic completeness. The ﬁrst algorithm, called PRM∗, is a variant of PRM, with a variable connection radius that scales as log(n)/n, where n is the number of samples. In other words, the average number of connections made at each iteration is proportional to log(n). The second new algorithm, called RRG, incrementally builds a connected roadmap, augmenting the RRT algorithm with connections within a ball scaling as log(n)/n. The third new algorithm, called RRT∗, is a version of RRG that incrementally builds a tree. Experimental evidence that demonstrate the eﬀectiveness of the algorithms proposed and support the theoretical claims were also provided.
A common theme in the paper is that, in order to ensure both asymptotic optimality and computational eﬃciency, connections between samples should be sought within balls of radius scaling as log(n)/n. If these balls shrink faster as n increases, the algorithms are not asymptotically optimal (but may still be probabilistically complete); on the other hand, if these balls shrink slower, the complexity of the algorithms will suﬀer. On average, the proposed scaling laws will result in an average number of connections per iteration that is proportional to log(n). Hence, it is natural to
35

Cost

1.3165 1.3144 1.3094 1.3044 1.2994 1.2944 1.2894

Cost

1.2384 1.2355 1.2305 1.2255 1.2205 1.2155 1.2105

1.2844

1.2055

1.2794

1.2005

102

103

104

105

102

103

104

Number of samples

Number of samples

(a)

(b)

Cost

1.228 1.2267 1.2217 1.2167 1.2117 1.2067 1.2017 1.1967 1.1917 1.1867

Cost

1.2173 1.2146 1.2096 1.2046 1.1996 1.1946 1.1896 1.1846 1.1796

1.1817

1.1746

102

103

104

102

103

104

Number of samples

Number of samples

(c)

(d)

Figure 11: Cost of the best path in the PRM∗ algorithm is shown in up to 2, 3, 4, and 5 dimensional conﬁguration spaces, in Figures (a), (b), (c), and (d), respectively. The initial condition and goal region are on opposite vertices of the unit cube (0, 1)d. The obstacle region is a cube centered at (0.5, 0.5, . . . , 0.5) and has volume 0.5 in all cases.

consider variants of these algorithms that make connections to k log(n) neighbors surely. Indeed, it is shown that these algorithms do share the same asymptotic optimality and computational eﬃciency properties of their counterparts, as long as k is no smaller than a constant kR∗ RG. It is remarkable that this constant only depends on the dimension of the space, and is otherwise independent from the problem instance.
The analysis of the results in the paper relies on techniques used to analyze random geometric graphs. Indeed, the algorithms considered in this paper build graphs that have many characteristics in common with well known classes of random geometric graphs. Interestingly, such geometric graphs exhibit phase transition phenomena, including percolation and connectivity, for thresholds matching those found for probabilistic completeness and asymptotic optimality of sampling-based algorithms. This leads to a natural conjecture that a sampling-based path planning algorithm is probabilistically complete if and only if the underlying random geometric graph percolates, and is asymptotically optimal if and only if the underlying random geometric graph is connected.
36

10

10

10

10

8

8

8

8

6

6

6

6

4

4

4

4

2

2

2

2

0

0

0

0

−2

−2

−2

−2

−4

−4

−4

−4

−6

−6

−6

−6

−8

−8

−8

−8

−10

−10

−10

−10

−10

−8

−6

−4

−2

0

2

4

6

8

10

−10

−8

−6

−4

−2

0

2

4

6

8

10

−10

−8

−6

−4

−2

0

2

4

6

8

10

−10

−8

−6

−4

−2

0

2

4

6

8

10

(a)

(b)

(c)

(d)

10

10

10

10

8

8

8

8

6

6

6

6

4

4

4

4

2

2

2

2

0

0

0

0

−2

−2

−2

−2

−4

−4

−4

−4

−6

−6

−6

−6

−8

−8

−8

−8

−10

−10

−10

−10

−10

−8

−6

−4

−2

0

2

4

6

8

10

−10

−8

−6

−4

−2

0

2

4

6

8

10

−10

−8

−6

−4

−2

0

2

4

6

8

10

−10

−8

−6

−4

−2

0

2

4

6

8

10

(e)

(f )

(g)

(h)

10

10

8

8

6

6

4

4

2

2

0

0

−2

−2

−4

−4

−6

−6

−8

−8

−10

−10

−10

−8

−6

−4

−2

0

2

4

6

8

10 −10

−8

−6

−4

−2

0

2

4

6

8

10

(i)

(j)

Figure 12: A Comparison of the RRT∗ and RRT algorithms on a simulation example with no obstacles. Both algorithms were run with the same sample sequence. Consequently, in this case, the vertices of the trees at a given iteration number are the same for both of the algorithms; only the edges diﬀer. The edges formed by the RRT algorithm are shown in (a)-(d) and (i), whereas those formed by the RRT∗ algorithm are shown in (e)-(h) and (j). The tree snapshots (a), (e) contain 250 vertices, (b), (f) 500 vertices, (c), (g) 2500 vertices, (d), (h) 10,000 vertices and (i), (j) 20,000 vertices. The goal regions are shown in magenta (in upper right). The best paths that reach the target in all the trees are highlighted w37ith red.

Cost

14 13 12 11 10
9 0

2000

4000

6000

8000 10000 12000 14000 16000 18000 20000 Number of iterations

(a)

Variance

3 2.5
2 1.5
1 0.5
0 0

2000

4000

6000

8000 10000 12000 14000 16000 18000 20000 Number of iterations

(b)
Figure 13: The cost of the best paths in the RRT (shown in red) and the RRT∗ (shown in blue) plotted against iterations averaged over 500 trials in (a). The optimal cost is shown in black. The variance of the trials is shown in (b).

The work presented in this paper can be extended in numerous directions. First of all, it would be of interest to establish broader connections between sampling-based path planning algorithms and random geometric graphs, e.g., by proving or disproving the conjecture above, and by possibly improving on current algorithms through a better understanding of the underlying mathematical objects. Similar analysis techniques can also be used to analyze other sampling-based path planning algorithms that were not analyzed in this paper, such as EST. In addition, it is of interest to investigate deterministic sampling-based algorithms, in which samples are generated using deterministic dense sequences of points with, e.g., low dispersion, as opposed to random sequences.
Second, it is of great practical interest to address motion planning problems subject to more complex constraints. For example, motion planning problems for mobile robots should consider the robot’s dynamics, and hence diﬀerential constraints on the feasible trajectories (these are also called kino-dynamic planning problems). In addition, it is of interest to consider optimal planning problems in the presence of temporal/logic constraints on the trajectories, e.g., expressed using formal speciﬁcation languages such as Linear Temporal Logic, or the µ-calculus. Such constraints correspond to, e.g., rules of the road constraints for autonomous ground vehicles, mission speciﬁcations for autonomous robots, and rules of engagement in military applications. Ultimately, incremental sampling-based algorithms with asymptotic optimality properties may provide the ba-
38

10

10

8

8

6

6

4

4

2

2

0

0

−2

−2

−4

−4

−6

−6

−8

−8

−10

−10

−10 −8 −6 −4 −2

0

2

4

6

8

10 −10 −8 −6 −4 −2

0

2

4

6

8

10

(a)

(b)

Figure 14: A Comparison of the RRT (shown in (a)) and RRT∗ (shown in (b)) algorithms on a simulation example with obstacles. Both algorithms were run with the same sample sequence for 20,000 samples. The cost of best path in the RRT and the RRG were 21.02 and 14.51, respectively.

sic elements for the on-line solution of diﬀerential games, as those arising when planning in the presence of dynamic obstacles.
Finally, it is noted that the proposed algorithms may have applications outside of the robotic motion planning domain. In fact, the class of sampling-based algorithm described in this paper can be readily extended to deal with problems described by partial diﬀerential equations, such as the eikonal equation and the Hamilton-Jacobi-Bellman equation.
Acknowledgments
The authors are grateful to Professors M.S. Branicky, G.J. Gordon, and S. LaValle, as well as the anonymous reviewers, for their insightful comments on draft versions of this paper. This research was supported in part by the Michigan/AFRL Collaborative Center on Control Sciences, AFOSR grant #FA 8650-07-2-3744, and by the National Science Foundation, grant CNS-1016213.
References
M. Abramowitz and I. A. Stegun, editors. Handbook of Mathematical Functions. Dover, 1964.
R. Alterovitz, S. Patil, and A. Derbakova. Rapidly-exploring roadmaps: Weighing exploration vs. reginement in optimal motion planning. In IEEE Conference on Robotics and Automation (ICRA), 2011.

39

10

10

10

8

8

8

6

6

6

4

4

4

2

2

2

0

0

0

−2

−2

−2

−4

−4

−4

−6

−6

−6

−8

−8

−8

−10

−10

−10

−10 −8 −6 −4 −2

0

2

4

6

8

10 −10 −8 −6 −4 −2

0

2

4

6

8

10 −10 −8 −6 −4 −2

0

2

4

6

8

10

(a)

(b)

(c)

10

10

10

8

8

8

6

6

6

4

4

4

2

2

2

0

0

0

−2

−2

−2

−4

−4

−4

−6

−6

−6

−8

−8

−8

−10

−10

−10

−10 −8 −6 −4 −2

0

2

4

6

8

10 −10 −8 −6 −4 −2

0

2

4

6

8

10 −10 −8 −6 −4 −2

0

2

4

6

8

10

(d)

(e)

(f )

Figure 15: RRT∗ algorithm shown after 500 (a), 1,500 (b), 2,500 (c), 5,000 (d), 10,000 (e), 15,000 (f) iterations.

S. Arya and D. M. Mount. Approximate range searching. Computational Geometry: Theory and Applications, 17:135–163, 2000.
S. Arya, D. M. Mount, R. Silverman, and A. Y. Wu. An optimal algorithm for approximate nearest neighbor search in ﬁxed dimensions. Journal of the ACM, 45(6):891–923, November 1999.
S. Arya, T. Malamatos, and D. M. Mount. Space-time tradeoﬀs for approximate spherical range counting. In Symposium on Discrete Algorithms, 2005.
P. Balister, B. Bollob´as, A. Sarkar, and M. Walters. Connectivity of random k-nearest neighbour graphs. Advances in Applied Probability, 37:1–24, 2005.
P. Balister, B. Bollob´as, and A. Sarkar. Percolation, connectivity, coverage and colouring of random geometric graphs. In B. Bollob´as, R. Kozma, and D. Mikl´os, editors, Handbook of Large-Scale Random Networks, volume 18 of Bolyai Society Mathematical Studies, chapter 2, pages 117–142. Springer, 2009a.
40

Cost

24 22 20 18 16 14 2000

4000

6000

8000 10000 12000 14000 Number of iterations

16000

18000

20000

(a)

Variance

14 12 10
8 6 4 2 0 2000

4000

6000

8000 10000 12000 14000 Number of iterations

16000

18000

20000

(b)
Figure 16: An environment cluttered with obstacles is considered. The cost of the best paths in the RRT (shown in red) and the RRT∗ (shown in blue) plotted against iterations averaged over 500 trials in (a). The optimal cost is shown in black. The variance of the trials is shown in (b).

P. Balister, B. Bollob´as, A. Sarkar, and M. Walters. A critical constant for the k nearest-neighbour model. Advances in Applied Probability, 41(1):1–12, 2009b.
J. Barraquand and J. C. Latombe. Robot motion planning: A distributed representation approach. International Journal of Robotics Research, 10(6):628–649, 1993.
J. Barraquand, L. E. Kavraki, J. C. Latombe, T. Li, R. Motwani, and P. Raghavan. A random sampling scheme for path planning. International Journal of Robotics Research, 16:759–774, 1997.
D. Berenson, J. Kuﬀner, and H. Choset. An optimization approach to planning for mobile manipulation. In IEEE International Conference on Robotics and Automation, 2008.
D. Berenson, T. Simeon, and S. Srinivasa. Addressing cost-space chasms in manipulation planning. In IEEE Conference on Robotics and Automation (ICRA), 2011.
A. Bhatia and E. Frazzoli. Incremental search methods for reachability analysis of continuous and hybrid systems. In R. Alur and G.J. Pappas, editors, Hybrid Systems: Computation and Control, number 2993 in Lecture Notes in Computer Science, pages 142–156. Springer-Verlag, Philadelphia, PA, March 2004.
B. Bollob´as. Random Graphs. Cambridge University Press, second edition, 2001.
41

10

8

6

4

2

0

−2

−4

−6

−8

−10

−10

−8

−6

−4

−2

0

2

4

6

8

10

Figure 17: RRT∗ algorithm at the end of iteration 20,000 in an environment with no obstacles. The upper yellow region is the high-cost region, whereas the lower yellow region is low-cost.

Running time ratio

40

30

20

10

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Number of iterations (in millions)

Figure 18: A comparison of the running time of the RRT∗ and the RRT algorithms. The ratio of the running time of the RRT∗ over that of the RRT up until each iteration is plotted versus the
number of iterations.

B. Bollob´as and O. M. Riordan. Percolation. Cambridge University Press, 2006. M. S. Branicky, S. M. LaValle, K. Olson, and L. Yang. Quasi-randomized path planning. In IEEE
Conference on Robotics and Automation, 2001. M. S. Branicky, M. M. Curtis, J. A. Levine, and S. B. Morgan. RRTs for nonlinear, discrete, and
hybrid planning and control. In IEEE Conference on Decision and Control, 2003. M. S. Branicky, M. M. Curtis, J. Levine, and S. Morgan. Sampling-based planning, control, and
42

Running time ratio

35

30

25

20

15

10

5

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Number of iterations (in millions)

Figure 19: A comparison of the running time of the RRT∗ and the RRT algorithms in an environment with obstacles. The ratio of the running time of the RRT∗ over that of the RRT up until
each iteration is plotted versus the number of iterations.

veriﬁcation of hybrid systems. IEEE Proc. Control Theory and Applications, 153(5):575–590, Sept. 2006.
R. Brooks and T. Lozano-Perez. A subdivision algorithm in conﬁguration space for ﬁndpath with rotation. In International Joint Conference on Artiﬁcial Intelligence, 1983.
J. Bruce and M.M. Veloso. Real-Time Randomized Path Planning for Robot Navigation, volume 2752 of Lecture Notes in Computer Science, chapter RoboCup 2002: Robot Soccer World Cup VI, pages 288–295. Springer, 2003.
A. Bry and N. Roy. Rapidly-exploring random belief trees for motion planning under uncertainty. In IEEE Conference on Robotics and Automation (ICRA), 2011.
J. Canny. The Complexity of Robot Motion Planning. MIT Press, 1988.
J. Canny and J. H. Reif. New lower bound techniques for robot motion planning problems. In IEEE Symposium on Foundations of Computer Science (FoCS), pages 49–60, Los Angeles, CA, 1987.
P. Chanzy, L. Devroye, and C. Zamora-Cura. Analysis of range search for random k-d trees. Acta Informatica, 37:355–383, 2001.
H. Choset, K.M. Lynch, S. Hutchinson, G. Kantor, W. Burgard, L.E. Kavraki, and S. Thrun. Principles of Robot Motion: Theory, Algorithms, and Implementations. MIT Press, Boston, MA, 2005.
J. Cortes, L. Jailet, and T. Simeon. Molecular disassembly with RRT-like algorithms. In IEEE International Conference on Robotics and Automation (ICRA), 2007.
D. Dolgov, S. Thrun, M. Montemerlo, and J. Diebel. Experimental Robotics, chapter Path Planning for Autonomous Driving in Unknown Environments, pages 55–64. Springer, 2009.
D. P. Dubhashi and A. Panconesi. Concentration of Measure for the Analysis of Randomized Algorithms. Cambridge University Press, 2009.
43

Cost

22

21

20

19

18

17

16

15

14

13

0

10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 100,000

Number of iterations

(a)

25

20

Variance

15

10

5

0

0

10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 100,000

Number of iterations

(b)

Figure 20: The cost of the best paths in the RRT (shown in red) and the RRT∗ (shown in blue) run in a 5 dimensional obstacle-free conﬁguration space plotted against iterations averaged over 100 trials in (a). The optimal cost is shown in black. The variance of the trials is shown in (b).

H. Edelsbrunner and H. A. Maurer. On the intersection of orthogonal objects. Information Processing Letters, 13(4,5):177–181, April 1981.
D Eppstein, MS Paterson, and F F Yao. On nearest-neighbor graphs. Discrete and Computational Geometry, 17:263–282, Jan 1997. URL http://www.springerlink.com/index/ RM3FJ00T9AD4WBX9.pdf.
D. Ferguson and A. Stentz. Anytime RRTs. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2006.
P. W. Finn and L. E. Kavraki. Computational approaches to drug design. Algorithmica, 25:347–371, 1999.
E. Frazzoli, M. A. Dahleh, and E. Feron. Real-time motion planning for agile autonomous vehicles. Journal of Guidance, Control, and Dynamics, 25(1):116–129, 2002.
S. S. Ge and Y.J. Cui. Dynamic motion planning for mobile robots using potential ﬁeld method. Autonomous Robots, 13(3):207–222, 2002.
E. N. Gilbert. Random plane networks. J. Soc. Indust. Appl. Math., 9(4):533–543, 1961.
44

Running time ratio

35

30

25

20

15

10

5

0

0

10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 100,000

Number of iterations

(a)

Figure 21: The ratio of the running time of the RRT and the RRT∗ algorithms is shown versus the number of iterations.

G. Grimmett and D. Stirzaker. Probability and Random Processes. Oxford University Press, Third edition, 2001.
P. Gupta and P. R. Kumar. Critical power for asymptotic connectivity in wireless networks. In W. M. McEneany, G. Yin, and Q. Zhang, editors, Stochastic Analysis, Control, Optimization and Applications: A Volume in Honor of W.H. Fleming, pages 547–566. Birkh¨auser, Boston, MA, 1998.
P. Gupta and P. R. Kumar. The capacity of wireless networks. IEEE Trans. on Information Theory, 46:388–404, 2000.
N. Henze. On the fraction of points with speciﬁed nearest-neighbour interrelations and degree of attraction. Advances in Applied Probability, 19:873–895, 1987.
J.E. Hopcroft, J.T. Schwartz, and M. Sharir. Eﬃcient detection of intersections among spheres. Int. Journal of Robotics Research, 2:77–80, 1983.
D. Hsu, J. C. Latombe, and R. Motwani. Path planning in expansive conﬁguration spaces. In IEEE Conference on Robotics and Automation, 1997.
D. Hsu, J. C. Latombe, and R. Motwani. Path planning in expansive conﬁguration spaces. Int. J. of Computational Geometry and Applications, 9(4&5):495–512, 1999.
D. Hsu, R. Kindel, J. C. Latombe, and S. Rock. Randomized kinodynamic motion planning with moving obstacles. International Journal of Robotics Research, 21(3):233–255, 2002.
D. Hsu, J. C. Latombe, and H. Kurniawati. On the probabilistic foundations of probabilistic roadmap planning. International Journal of Robotics Research, 25:7, 2006.
L. Jaillet, J. Cortes, and T .Simeon. Sampling-based path planning on conﬁguration-space costmaps. IEEE Transactions on Robotics, 26(4):635–646, August 2010.
S. Karaman and E. Frazzoli. Optimal kinodynamic motion planning using incremental samplingbased methods. In IEEE Conf. on Decision and Control, 2010a.
45

Cost

32

30

28

26

24

22

20

18

0

5,000 10,000 15,000 20,000 25,000 30,000 35,000 40,000 45,000 50,000

Number of iterations

(a)

Variance

40

35

30

25

20

15

10

5

0

0

5,000 10,000 15,000 20,000 25,000 30,000 35,000 40,000 45,000 50,000

Number of iterations

(b)

Figure 22: The cost of the best paths in the RRT (shown in red) and the RRT∗ (shown in blue) run in a 10 dimensional conﬁguration space involving obstacles plotted against iterations averaged over 25 trials in (a). The variance of the trials is shown in (b).

S. Karaman and E. Frazzoli. Incremental sampling-based algorithms for optimal motion planning. In Robotics: Science and Systems (RSS), 2010b.
S. Karaman and E. Frazzoli. Incremental sampling-based algorithms for a class of pursuit-evasion games. In Workshop on Algorithmic Foundations of Robotics (WAFR), pages 71–87, 2010c.
S. Karaman, M. Walter, A. Perez, E. Frazzoli, and S. Teller. Anytime motion planning using the RRT∗. In IEEE Conference on Robotics and Automation (ICRA), 2011.
L. Kavraki and J. C. Latombe. Randomized preprocessing of conﬁguration space for fast path planning. In IEEE International Conference on Robotics and Automation, 1994.
L. E. Kavraki, P. Svestka, J. C. Latombe, and M. H. Overmars. Probabilistic roadmaps for path planning in high-dimensional conﬁguration spaces. IEEE Transactions on Robotics and Automation, 12(4):566–580, 1996.
L. E. Kavraki, M. N. Kolountzakis, and J. C. Latombe. Analysis of probabilistic roadmaps for path planning. IEEE Transactions on Roborics and Automation, 14(1):166–171, 1998.

46

O. Khatib. Real-time obstacle avoidance for manipulators and mobile robots. International Journal of Robotics Research, 5(1):90–98, 1986.
Y. Koren and J. Borenstein. Potential ﬁeld methods and their inherent limitations for mobile robot navigation. In IEEE Conference on Robotics and Automation, 1991.
E. Koyuncu, N.K. Ure, and G. Inalhan. Integration of path/manuever planning in complex environments for agile maneuvering UCAVs. Jounal of Intelligent and Robotic Systems, 57(1–4): 143–170, 2010.
J. J. Kuﬀner and S. M. LaValle. RRT-connect: An eﬃcient approach to single-quert path planning. In Proceedings of the IEEE International Conference on Robotics and Automation, 2000.
J. J. Kuﬀner, S. Kagami, K. Nishiwaki, M. Inaba, and H. Inoue. Dynamically-stable motion planning for humanoid robots. Autonomous Robots, 15:105–118, 2002.
Y. Kuwata, J. Teo, G. Fiore, S. Karaman, E. Frazzoli, and J.P. How. Real-time motion planning with applications to autonomous urban driving. IEEE Transactions on Control Systems, 17(5): 1105–1118, 2009.
A. L. Ladd and L. Kavraki. Measure theoretic analysis of probabilistic path planning. IEEE Transactions on Robotics and Automation, 20(2):229–242, 2004.
J. C. Latombe. Robot Motion Planning. Kluwer Academic Publishers, Boston, MA, 1991.
J. C. Latombe. Motion planning: A journey of robots, molecules, digital actors, and other artifacts. International Journal of Robotics Research, 18(11):1119–1128, 1999.
S. M. LaValle. Planning Algorithms. Cambridge University Press, 2006.
S. M. LaValle and J. J. Kuﬀner. Randomized kinodynamic planning. International Journal of Robotics Research, 20(5):378–400, May 2001.
S. M. LaValle and J. J. Kuﬀner. Space ﬁlling trees. Technical Report CMU-RI-TR-09-47, Carnegie Mellon University, The Robotics Institute, 2009.
S. M. LaValle, M. S. Branicky, and S. R. Lindemann. On the relationship between classical grid search and probabilistic roadmaps. International Journal of Robotics Research, 23(7–8):673–692, 2004.
D. T. Lee and C. K. Wong. Worst-case analysis for region and partial region searches in multidimensional binary search trees and quad trees. Acta Informatica, 9:23–29, 1977.
M. Likhachev and D. Ferguson. Planning long dynamically-feasible maneuvers for autonomous vehicles. International Journal of Robotics Research, 28(8):933–945, 2009.
M. Likhachev, G. Gordon, and S. Thrun. Anytime A* with provable bounds on sub-optimality. In Advances in Neural Information Processing Systems, 2004.
M. Likhachev, D. Ferguson, G. Gordon, A. Stentz, and S. Thrun. Anytime search in dynamic graphs. Artiﬁcial intelligence Journal, 172(14):1613–1643, 2008.
M. C. Lin and D. Manocha. Collision and proximity queries. In J.E. Goodman and J. O’Rourke, editors, Handbook of Discrete and Computational Geometry. Chapman and Hall/CRC, second edition, 2004.
47

S. R. Lindemann and S. M. LaValle. Current issues in sampling-based motion planning. In P. Dario and R. Chatila, editors, Eleventh International Symposium on Robotics Research, pages 36–54. Springer, 2005.
Y. Liu and N.I. Badler. Real-time reach planning for animated characters using hardware acceleration. In IEEE International Conference on Computer Animation and Social Characters, pages 86–93, 2003.
T. Lozano-Perez and M. A. Wesley. An algorithm for planning collision-free paths among polyhedral obstacles. Communications of the ACM, 22(10):560–570, 1979.
B. Luders, S. Karaman, E. Frazzoli, and J. P. How. Bounds on tracking error using closed-loop rapidly-exploring random trees. In American Control Conference, 2010.
R. Meester and R. Roy. Continuum Percolation. Cambridge University Press, 1996.
J. R. Munkres. Topology. Prentice Hall, second edition, 2000.
O. Nechushtan, B. Raveh, and D. Halperin. Sampling-diagram automata: a tool for analyzing path quality in tree planners. In D. Hsu, V. Isler, J. C. Latombe, and M.C. Lin, editors, Algorithmic Foundations of Robotics IX, volume 68 of Springer tracts in advanced robotics, pages 285–301. Springer, 2010.
H. Niederreiter. Random Number Generation and Quasi-Monte-Carlo Methods. Society for Industrial and Applied Mathematics, 1992.
M. D. Penrose. Random Geometric Graphs. Oxford University Press, 2003.
E. Plaku and L. E. Kavraki. Quantitative analysis of nearest-neighbors search in high-dimensional sampling-based motion planning. In Workshop on Algorithmic Foundations of Robotics (WAFR), 2008.
E. Plaku, K.E. Bekris, B.Y. Chen, A.M. Ladd, and L.E. Kavraki. Sampling-based roadmap of trees for parallel motion planning. IEEE Transactions on Robotics, 21:597–608, 2005.
S. Prentice and N. Roy. The belief roadmap: Eﬃcient planning in blief space by factoring the covariance. International Journal of Robotics Research, 28(11–12):1448–1465, 2009.
J. Quintanilla, S. Torquato, and R.M. Ziﬀ. Eﬃcient measurement of the percolation threshold for fully penetrable discs. Journal of Physics A, 33(42):L399–L407, 2000.
J. H. Reif. Complexity of the mover’s problem and generalizations. In Proceedings of the IEEE Symposium on Foundations of Computer Science, 1979.
S. I. Resnick. A probability path. Birkh¨auser, 1999.
E. Rimon and D. E. Koditschek. Exact robot navigation using artiﬁcial potential ﬁelds. IEEE Transactions on Robotics and Automation, 8(5):501–518, 1992.
N. C. Rowe and R. S. Alexander. Finding optimal-path maps for path planning across weighted regions. The International Journal of Robotics Research, 19:83–95, 2000.
M. Sahimi. Applications of Percolation Theory. Taylor & Francis, 1994.
48

H. Samet. Design and Analysis of Spatial Data Structures. Addison-Wesley, 1989a.
H. Samet. Applications of Spatial Data Structures: Computer Graphics, Image Processesing and Gis. Addison-Wesley, 1989b.
A. Schrijver. Combinatorial Optimization, volume A. Springer, 2003.
J. T. Schwartz and M. Sharir. On the ‘piano movers’ problem: II. general techniques for computing topological properties of real algebraic manifolds. Advances in Applied Mathematics, 4:298–351, 1983.
A. Shkolnik, M. Levashov, I. R. Manchester, and R. Tedrake. Bounding on rough terrain with the LittleDog robot. Submitted for publication, 2011.
H. Six and D. Wood. Counting and reporting intersections of D-ranges. IEEE Trans. on Computers, pages 46–55, 1982.
D. Stentz. The focussed D* algorithm for real-time replanning. In International Joint Conference on Artiﬁcial Intelligence, 1995.
M. Stilman, J. Schamburek, J. Kuﬀner, and T. Asfour. Manipulation planning among movable obstacles. In IEEE International Conference on Robotics and Automation, 2007.
D. Stoyan, W. S. Kendall, and J. Mecke. Stochastic Geometry and Its Applications. John Wiley & Sons, 1995.
R. Tedrake, I. R. Manchester, M. M. Tobekin, and J. W. Roberts. LQR-trees: Feedback motion planning via sums of squares veriﬁcation. International Journal of Robotics Research (to appear), 2010.
S. Teller, M. R. Walter, M. Antone, A. Correa, R. Davis, L. Fletcher, E. Frazzoli, J. Glass, J.P. How, A. S. Huang, J. Jeon, S. Karaman, B. Luders, N. Roy, and T. Sainath. A voice-commandable robotic forklift working alongside humans in minimally-prepared outdoor environments. In IEEE International Conference on Robotics and Automation, 2010.
C. Urmson and R. Simmons. Approaches for heuristically biasing RRT growth. In Proceedings of the IEEE/RSJ International Conference on Robotics and Systems (IROS), 2003.
A. R. Wade. Explicit laws of large numbers for random nearest-neighbor-type graphs. Advances in Applied Probability, 39:326–342, 2007.
A. R. Wade. Asymptotic theory for the multidimensional random on-line nearest-neighbour graph. Stochastic Processes and their Applications, 119(6):1889–1911, 2009.
N. A. Wedge and M.S. Branicky. On heavy-tailed runtimes and restarts in rapidly-exploring random trees. In Twenty-third AAAI Conference on Artiﬁcial Intelligence, 2008.
F. Xue and P. R. Kumar. The number of neighbors needed for connectivity of wireless networks. Wireless Networks, 10:169–181, 2004.
A. Yershova and S. M. LaValle. Improving motion-planning algorithms by eﬃcient nearest-neighbor searching. IEEE Transactions on Robotics, 23(1):151–157, 2007.
49

A. Yershova and S. M. LaValle. Motion planning in highly constrained spaces. Technical report, University of Illinois at Urbana-Champaign, 2008.
M. Zucker, J. J. Kuﬀner, and M. S. Branicky. Multiple RRTs for rapid replanning in dynamic environments. In IEEE Conference on Robotics and Automation, 2007.
Appendix
A Notation
Let N denote the set of positive integers and R denote the set of reals. Let N0 = N ∪ {0}, and R>0, R≥0 denote the sets of positive and non-negative reals, respectively. A sequence on a set A is a mapping from N to A, denoted as {ai}i∈N, where ai ∈ A is the element that i ∈ N is mapped to. Given a, b ∈ R, closed and open intervals between a and b are denoted by [a, b] and (a, b), respectively. The Euclidean norm is denoted by · . Given a set X ⊂ Rd, the closure of X is denoted by cl(X ). The closed ball of radius r > 0 centered at x ∈ Rd, i.e., , i.e., {y ∈ Rd | y − x ≤ r}, is denoted as Bx,r; Bx,r is also called the r-ball centered at x. Given a set X ⊆ Rd, the Lebesgue measure of X is denoted by µ(X ). The Lebesgue measure of a set is also referred to as its volume. The volume of the unit ball in Rd, is denoted by ζd, i.e., ζd = µ(B0,1). The letter e is used to denote the base of the natural logarithm, also called Euler’s number.
Given a probability space (Ω, F, P), where Ω is a sample space, F ⊆ 2Ω is a σ−algebra, and P is a probability measure, an event A is an element of F. The complement of an event A is denoted by Ac. Given a sequence of events {An}n∈N, the event ∩∞ n=1 ∪∞ i=n Ai is denoted by lim supn→∞ An (also called the event that An occurs inﬁnitely often); the event ∪∞ n=1 ∩∞ i=n Ai is denoted by lim infn→∞ An. A (real) random variable is a measurable function that maps Ω into R. An extended (real) random variable can also take the values ±∞. The expected value of a random variable Y is E[Y ] = Ω Y dP. A sequence of random variables {Yn}n∈N is said to converge surely to a random variable Y if limn→∞ Yn(ω) = Y (ω) for all ω ∈ Ω; the sequence is said to converge almost surely if P({limn→∞ Yn = Y }) = 1. Finally, if ϕ(ω) is a property that is either true or false for a given ω ∈ Ω, the event that denotes the set of all samples ω for which ϕ(ω) holds, i.e., {ω ∈ Ω | ϕ(ω) holds}, is written as {ϕ}, e.g., {ω ∈ Ω | limn→∞ Yn(ω) = Y (ω)} is simply written as {limn→∞ Yn = Y }. The Poisson random variable with parameter λ is denoted by Poisson(λ). The binomial random variable with parameters n and p is denoted by Binomial(n, p).
Let f (n) and g(n) be two functions with domain and range N or R. The function f (n) is said to be O(g(n)), denoted as f (n) ∈ O(g(n)), if there exists two constants M and n0 such that f (n) ≤ M g(n) for all n ≥ n0. The function f (n) is said to be Ω(g(n)), denoted as f (n) ∈ Ω(g(n)), if there exists constants M and n0 such that f (n) ≥ M g(n) for all n ≥ n0. The function f (n) is said to be Θ(g(n)), denoted as f (n) ∈ Θ(g(n)), if f (n) ∈ O(g(n)) and f (n) ∈ Ω(g(n)).
Let X be a subset of Rd. A (directed) graph G = (V, E) on X is composed of a vertex set V and an edge set E, such that V is a ﬁnite subset of X , and E is a subset of V × V . A directed path on G is a sequence (v1, v2, . . . , vn) of vertices such that (vi, vi+1) ∈ E for all 1 ≤ i ≤ n − 1. Given a vertex v ∈ V , the sets {u ∈ V | (u, v) ∈ E} and {u ∈ V | (v, u) ∈ E} are said to be its incoming neighbors and outgoing neighbors, respectively. A (directed) tree is a directed graph, in which each vertex but one has a unique incoming neighbor; the vertex with no incoming neighbor is called the root vertex. Vertices of a tree are often also called nodes.
50

B Proof of Theorem 33 (Non-optimality of RRT)
For simplicity, the theorem will be proven assuming that (i) the environment contains no obstacles, i.e., Xfree = [0, 1]d, a√nd (ii) the parameter η of the steering procedure is set large enough, e.g., η ≥ diam (Xfree) = d. One one hand, considering this case is enough to prove that the RRT algorithm is not asymptotically optimal, as it demonstrates a case for which the RRT algorithm fails to converge to an optimal solution, although the problem instance is clearly robustly optimal. On the other hand, these assumptions are not essential, and the claims extend to the more general case, but the technical details of the proof are considerably more complicated.
The proof can be outlined as follows. Order the vertices in the RRT according to the iteration at which they are added to the tree. The set of vertices that contains the k-th child of the root along with all its descendants in the tree is called the k-th branch of the tree. First, it is shown that a necessary condition for the asymptotic optimality of RRT is that inﬁnitely many branches of the tree contain vertices outside a small ball centered at the initial condition. Then, the RRT algorithm is shown to violate this condition, with probability one.

B.1 A necessary condition

First, we provide a necessary condition for the RRT algorithm to be asymptotically optimal.

Lemma 44 Let 0 < R < infy∈Xgoal y − xinit . The event {limN→∞ YnRRT = c∗} occurs only if the k-th branch of the RRT contains vertices outside the R-ball centered at xinit for inﬁnitely many k.

Proof Let {x1, x2, . . . } denote the set of children to the root vertex in the order they are added to the tree. Let Γ(xk) denote the optimal cost of a path starting from the root vertex, passing through xk, and reaching the goal region. By our assumption that the measure of the set of all points that are on the optimal path is zero (see Assumption 27 and Lemma 28), the probability that Γ(xk) = c∗ is zero for all k ∈ N. Hence,

∞

P k∈N {Γ(xk) = c∗} ≤

P {Γ(xk) = c∗} = 0.

k=1

Let Ak denote the event that at least one vertex in the k-th branch of the tree is outside the ball

of radius R centered at xinit in some iteration of the RRT algorithm. Consider the case when the

event {lim supk→∞ Ak} does not occur and the events {Γ(xk) > c∗} occur for all k ∈ N. Then, Ak

occurs for only ﬁnitely many k. Let K denote the largest number such that AK occurs. Then, the

cost of the best path in the tree is at least sup{Γ(xk) | k ∈ {1, 2, . . . , K}}, which is strictly larger

than c∗, since {Γ(xk) > c∗} for all ﬁnite k. Thus, limn→∞ YnRRT > c∗ must hold. That is, we have

argued that

c
lim sup Ak ∩
k→∞

{Γ(xk) > c∗} ⊆
k∈N

lim
n→∞

YnRRT

>

c∗

.

Taking the complement of both sides and using monotonicity of probability measures,

P

lim
n→∞

YnRRT

=

c∗

≤ P lim sup Ak ∪
k→∞
≤ P lim sup Ak + P
k→∞

k∈N{Γ(xk) = c∗} , k∈N{Γ(xk) = c∗} ,

where the last inequality follows from the union bound. The lemma follows from the fact that the last term in the right hand side is equal to zero as shown above.

51

B.2 Length of the ﬁrst path in a branch
The following result provides a useful characterization of the RRT structure.
Lemma 45 Let U = {X1, X2, . . . , Xn} be a set of independently sampled and uniformly distributed points in the d-dimensional unit cube, [0, 1]d. Let Xn+1 be a point that is sampled independently from all the other points according to the uniform distribution on [0, 1]d. Then, the probability that among all points in U the point Xi is the one that is closest to Xn+1 is 1/n, for all i ∈ {1, 2, . . . , n}. Moreover, the expected distance from Xn+1 to its nearest neighbor in U is n−1/d.
Proof Since the probability distribution is uniform, the probability that Xn+1 is closest to Xi is the same for all i ∈ {1, 2, . . . , n}, which implies that this probability is equal to 1/n. The expected distance to the closest point in U is an application of the order statistics of the uniform distribution.

An immediate consequence of this result is that each vertex of the RRT has unbounded degree,
almost surely, as the number of samples approaches inﬁnity.
One can also deﬁne a notion of inﬁnite paths in the RRT, as follows. Let Λ be the set of inﬁnite sequences of natural numbers α = (α1, α2, . . .). For any i ∈ N, let πi : Σ → Ni, (α1, α2, . . . , αi, . . .) → (α1, α2, . . . , αi), be a function returning the preﬁx of length i of an inﬁnite sequence in Λ. The lexicographic ordering of Λ is such that, given α, β ∈ Σ, α ≤ β if and only if there exists j ∈ N such that αi = βi for all i ∈ N, i ≤ j − 1, and αj ≤ βj. This is a total ordering of Λ, since N is a totally ordered set. Given α ∈ Λ and i ∈ N, let Lπi(α) be the sum of the distances from the root vertex xinit to its α1-th child, from this vertex to its α2-th child, etc., for a total of i terms. Because of Lemma 45, this construction is well deﬁned, almost surely, for a suﬃciently large number of samples. For
any inﬁnite sequence α ∈ Λ, let Lα = limi→+∞ Lπi(α); the limit exists since Lπi(α) is non-decreasing in i.
Consider inﬁnite strings of the form k = (k, 1, 1, . . .), k ∈ N, and introduce the shorthand Lk := L(k,1,1,...). The following lemma shows that, for any k ∈ N, Lk has ﬁnite expectation, which immediately implies that Lk takes only ﬁnite values with probability one. The lemma also provides a couple of other useful properties of Lk, which will be used later on.

Lemma 46 The expected value E[Lk] is non-negative and ﬁnite, and monotonically non-increasing, in the sense that E[Lk+1] ≤ E[Lk], for any k ∈ N. Moreover, limk→∞ E[Lk] = 0.

Proof Under the simplifying assumptions that there are no obstacles in the unit cube and η is large enough, the vertex set VnRRT of the graph maintained by the RRT algorithm is precisely the ﬁrst n samples and each new sample is connected to its nearest neighbor in VnRRT.
Deﬁne Zi as a random variable describing the contribution to L1 realized at iteration i; in other words, Zi is the distance of the i-th sample to its nearest neighbor among the ﬁrst i − 1 samples if the i-th sample is on the path used in computing L1, and zero otherwise. Then, using Lemma 45,

∞

∞

∞

E[L1] = E

Zi = E[Zi] = i−1/d i−1 = Zeta(1 + 1/d),

i=1

i=1

i=1

where the second equality follows from the monotone convergence theorem and Zeta is the Riemann zeta function. Since Zeta(y) is ﬁnite for any y > 1, E[L1] is a ﬁnite number for all d ∈ N.

52

Let Nk be the iteration at which the ﬁrst sample contributing to Lk is generated. Then, an argument similar to the one given above yields

∞

Nk

E[Lk+1] =

i−(1+1/d) = E[L1] − i−(1+1/d).

i=Nk +1

i=1

Then, clearly, E[Lk+1] < E[Lk] for all k ∈ N. Moreover, since Nk ≥ k, it is the case that limk→∞ E[Lk] = 0.

B.3 Length of the longest path in a branch
Given k ∈ N, and the sequence k = (k, 1, 1, . . .), the quantity supα≥k Lα is an upper bound on the length of any path in the k-th branch of the RRT, or in any of the following branches. The next result bounds the probability that this quantity is very large.

Lemma 47 For any > 0,

P sup Lα >
α≥k

≤ E[Lk] .

First, we state and prove the following intermediate result.

Lemma 48 E[Lα] ≤ E[Lk], for all α ≥ k.
Proof The proof is by induction. Since α ≥ k, then π1(α) ≥ k, and Lemma 46 implies that E[L(π1(α),1,1,...)] ≤ E[Lk]. Moreover, it is also the case that, for any i ∈ N (and some abuse of notation), E[L(πi+1(α),1,1,...)] ≤ E[L(πi(α),1,1,...)], by a similar argument considering a tree rooted at the last vertex reached by the ﬁnite path πi(α). Since (πi+1(α), 1, 1, . . .) ≥ (πi(α), 1, 1, . . .) ≥ (k, 1, 1, . . .), the result follows.

Proof of Lemma 47 Deﬁne the random variable α¯ := inf{α ≥ k | Lα > }, and set α¯ := k if Lα ≤ for all α ≥ k. Note that α¯ ≥ k holds surely. Hence, by Lemma 48, E[Lα¯] ≤ E[Lk]. Let I be the indicator random variable for the event S := {supα≥k Lα > }. Then,
E[Lk] ≥ E[Lα¯] = E[Lα¯I ] + E[Lα¯(1 − I )] ≥ P(S ),
where the last inequality follows from the fact that Lα¯ is at least whenever the event S occurs.

A useful corollary of Lemmas 46 and 47 is the following.

Corollary 49 For any > 0, limk→∞ P({supα≥k Lα > }) = 0.

B.4 Violation of the necessary condition
Recall from Lemma 44 that a necessary condition for asymptotic optimality is that the k-th branch of the RRT contains vertices outside the R-ball centered at xinit for inﬁnitely many k, where 0 < R < infy∈Xgoal y − xinit . Clearly, the latter event can occur only if longest path in the k-th branch of the RRT is longer than R for inﬁnitely many k. That is,

P

lim
n→∞

YnRRT

=

c∗

≤ P lim sup supα≥k Lα > R
k→∞

.

53

The event on the right hand side is monotonic in the sense that {supα>k+1 Lα > R} ⊇ {supα≥k Lα > R} for all k ∈ N. Hence, limk→∞{supα≥k Lα > R} exists. In particular, P(lim supk→∞{supα≥k Lα > R}) = P(limk→∞{supα≥k Lα > R}) = limk→∞ P({supα≥k Lα > R}), where the last equality follows from the continuity of probability measures. Since limk→∞ P supα≥k Lα > R = 0 for all R > 0 by Corollary 49, P({limn→∞ YnRRT = c∗}) = 0.

C Proof of Theorem 34 (Asymptotic optimality of PRM∗)
An outline of the proof is given below, before the details are provided.

C.1 Outline of the proof
Let σ∗ denote a robustly optimal path. By deﬁnition, σ∗ has weak δ-clearance. First, deﬁne a
sequence {δn}n∈N such that δn > 0 for all n ∈ N and δn approaches zero as n approaches inﬁnity. Construct a sequence {σn}n∈N of paths such that σn has strong δn-clearance for all n ∈ N and σn converges to σ∗ as n approaches inﬁnity.
Second, deﬁne a sequence {qn}n∈N. For all n ∈ N, construct a set Bn = {Bn,1, Bn,2, . . . , Bn,Mn} of overlapping balls, each with radius qn, that collectively “cover” the path σn. See Figures 23 and 24. Let xm ∈ Bn,m and xm+1 ∈ Bn,m+1 be any two points from two consecutive balls in Bn. Construct Bn such that (i) xm and xm+1 have distance no more than the connection radius r(n) and (ii) the straight path connecting xm and xm+1 lies entirely within the obstacle free space. These requirements can be satisﬁed by setting δn and qn to certain constant fractions of r(n).
Let An denote the event that each ball in Bn contains at least one vertex of the graph returned by the PRM∗ algorithm, when the algorithm is run with n samples. Third, show that An occurs for all large n, with probability one. Clearly, in this case, the PRM∗ algorithm will connect the
vertices in consecutive balls with an edge, and any path formed in this way will be collision-free.
Finally, show that any sequence of paths generated in this way converges to the optimal path σ∗. Using the robustness of σ∗, show that the cost of the best path in the graph returned by the PRM∗ algorithm converges to c(σ∗) almost surely.

C.2 Construction of the sequence {σn}n∈N of paths
The following lemma establishes a connection between the notions of strong and weak δ-clearance.

Lemma 50 Let σ∗ be a path be a path that has strong δ-clearance. Let {δn}n∈N be a sequence of
real numbers such that limn→∞ δn = 0 and 0 ≤ δn ≤ δ for all n ∈ N. Then, there exists a sequence {σn}n∈N of paths such that limn→∞ σn = σ∗ and σn has strong δn-clearance for all n ∈ N.

Proof First, deﬁne a sequence {Xn}n∈N of subsets of Xfree such that Xn is the closure of the δn-interior of Xfree, i.e.,
Xn := cl(intδn (Xfree))

for all n ∈ N. Note that, by deﬁnition, (i) Xn are closed subsets of Xfree, and (ii) any point Xn has

distance at least δn to any point in the obstacle set Xobs.

Then, construct the sequence {σn}n∈N of paths, where σn ∈ ΣXn, as follows. Let ψ : [0, 1] → Σfree denote the homotopy with ψ(0) = σ∗; the existence of ψ is guaranteed by weak δ-clearance of
σ∗. Deﬁne

αn

:=

max {α
α∈[0,1]

|

ψ(α)

∈

ΣXn }

and

σn := ψ(αn).

54

Since ΣXn is closed, the maximum in the deﬁnition of αn is attained. Moreover, since ψ(1) has
strong δ-clearance and δn ≤ δ, σn ∈ ΣXn, which implies the strong δn-clearance of σn. Clearly, n∈N Xn = Xfree, since limn→∞ δn = 0. Also, by weak δ-clearance of σ∗, for any α ∈
(0, 1], there exists some δα ∈ (0, δ] such that ψ(α) has strong δα-clearance. Then, limn→∞ αn = 0, which implies limn→∞ σn = σ∗.

Recall that the connection radius of the PRM∗ algorithm was deﬁned as

rn = γPRM

log n

1/d
> 2(1 + 1/d)1/d

n

µ(Xfree) 1/d ζd

log n 1/d n

(see Algorithm 4 and the deﬁnition of the Near procedure in Section 3.1). Let θ1 be a small positive constant; the precise value of θ1 will be provided shortly in the proof of Lemma 52. Deﬁne

δn := min

δ,

1 2

+ +

θ1 θ1

rn

,

for all n ∈ N.

By deﬁnition, 0 ≤ δn ≤ δ holds. Moreover, limn→∞ δn = 0, since limn→∞ rn = 0. Then, by Lemma 50, there exists a sequence {σn}n∈N of paths such that limn→∞ σn = σ∗ and σn has strong
δn-clearance for all n ∈ N.

C.3 Construction of the sequence {Bn}n∈N of sets of balls
First, a construction of a ﬁnite set of balls that collectively “cover” a path σn is provided. The construction is illustrated in Figure 23.

Deﬁnition 51 (Covering balls) Given a path σn : [0, 1] → X , and the real numbers qn, ln ∈ R>0, the set CoveringBalls(σn, qn, ln) is deﬁned as a set {Bn,1, Bn,2, . . . , Bn,Mn} of Mn balls of radius qn such that Bn,m is centered at σ(τm), and

• the center of Bn,1 is σ(0), i.e., τ1 = 0,

• the centers of two consecutive balls are exactly ln apart, i.e., τm := min{τ ∈ [τm−1, 1] | σ(τ )− σ(τm−1) ≥ ln} for all m ∈ {2, 3, . . . , Mn},

• and M − 1 is the largest number of balls that can be generated in this manner while the center of the last ball, Bn,Mn is σ(1), i.e., τMn = 1.

For each n ∈ N, deﬁne

qn

:=

1

δn . + θ1

Construct the set Bn = {Bn,1, Bn,2, . . . , Bn,Mn} of balls as Bn := CoveringBalls(σn, qn, θ1qn) using Deﬁnition 51 (see Figure 23). By construction, each ball in Bn has radius qn and the centers

of consecutive balls in Bn are θ1qn apart (see Figure 24 for an illustration of covering balls with

this set of parameters). The balls in Bn collectively cover the path σn.

C.4 The probability that each ball in Bn contains at least one vertex

Recall that GPnRM∗ = (VnPRM∗, EnPRM∗) denotes the graph returned by the PRM∗ algorithm, when

the algorithm is run with n samples. Let An,m denote the event that the ball Bn,m contains at

least one vertex of the graph generated by the PRM∗ algorithm, i.e., An,m = Bn,m ∩ VnPRM∗ = ∅ .

Let An denote the event that all balls in Bn contain at least one vertex of the PRM∗ graph, i.e.,

An =

Mn m=1

An,m.

55

... ...
Figure 23: An illustration of the CoveringBalls construction. A set of balls that collectively cover the trajectory σn is shown. All balls have the same radius, qn. The spacing between the centers of two consecutive balls is ln.

Figure 24: An illustration of the covering balls for PRM∗ algorithm. The δn-ball is guaranteed to be inside the obstacle-free space. The connection radius rn is also shown as the radius of the connection ball centered at a vertex x ∈ Bn,m. The vertex x is connected to all other vertices that lie within the connection ball.

Lemma 52 If γPRM > 2 (1 + 1/d)1/d

µ(Xfree ) ζd

1/d
, then there exists a constant θ1 > 0 such that

the event that every ball in Bn contains at least one vertex of the PRM∗ graph occurs for all large

enough n with probability one, i.e.,

P

lim inf
n→∞

An

= 1.

Proof The proof is based on a Borel-Cantelli argument which can be summarized as follows. Recall

that Acn denotes the complement of An. First, the sum

∞ n=1

P(Acn)

is

shown

to

be

bounded.

By

the Borel-Cantelli lemma (Grimmett and Stirzaker, 2001), this implies that the probability that

An holds inﬁnitely often as n approaches inﬁnity is zero. Hence, the probability that An holds

inﬁnitely often is one. In the rest of the proof, an upper bound on P(An) is computed, and this

upper bound is shown to be summable.

First, compute a bound on the number of balls in Bn as follows. Let sn denote the length of

σn, i.e., sn := TV(σn). Recall that the balls in Bn were constructed such that the centers of two

consecutive balls in Bn have distance θ1 qn. The segment of σn that starts at the center of Bn,m

and ends at the center of Bn,m+1 has length at least θ1qn, except for the last segment, which has

56

length less than or equal to θ1qn. Let n0 ∈ N be the number such that δn < δ for all n ≥ n0. Then, for all n ≥ n0,

card (Bn) = Mn

≤

sn = (1 + θ1)sn = (2 + θ1) sn

θ1qn

θ1δn

θ1 rn

= (2 + θ1) sn

n 1/d .

θ1 γPRM log n

Second, compute the volume of a single ball in Bi as follows. Recall that µ(·) denotes the usual Lebesgue measure, and ζd denotes the volume of a unit ball in the d-dimensional Euclidean space. For all n ≥ n0,

µ(Bn,m) = ζd qnd = ζd

δn 1 + θ1

d
= ζd

rn 2 + θ1

d
= ζd

γPRM d log n

2 + θ1

n

For all n ≥ I, the probability that a single ball, say Bn,1, does not contain a vertex of the graph generated by the PRM∗ algorithm, when the algorithm is run with n samples, is

P Acn,1 = =

1 − µ(Bn,1) n µ(Xfree)

1 − ζd

γPRM d log n n

µ(Xfree) 2 + θ1

n

Using the inequality (1 − 1/f (n))r ≤ e−r/f(n), the right-hand side can be bounded as

P(A ) ≤ e = n . n,1

−

ζd µ(Xfree )

γPRM 2+θ1

d
log n

−

ζd µ(Xfree

)

γPRM 2+θ1

d

Hence,

P (Acn) = P

Mn m=1

Acn,m

Mn

≤

P Acn,m = Mn P(Acn,1)

m=1

≤ (2 + θ1)sn θ1 γPRM

n log n

i 1/d

−

ζd µ(Xfree

)

γPRM 2+θ1

d

=

(2 + θ1)sn θ1 γPRM

1 (log n)d

−
n

ζd µ(Xfree )

γPRM 2+θ1

d

−

1 d

where the ﬁrst inequality follows from the union bound.

Finally,

∞ n=1

P(Acn)

<

∞

holds,

if

ζd µ(Xfree )

γPRM 2+θ1

d

−

1 d

>

1,

which

can

be

satisﬁed

for

any

γP RM > 2(1 + 1/d)1/d

µ(Xfree ) ζd

1/d
by appropriately choosing θ1.

Then, by the Borel-Cantelli

lemma (Grimmett and Stirzaker, 2001), P(lim supn→∞ Acn) = 0, which implies P(lim infn→∞ An) =

1.

C.5 Connecting the vertices in subsequent balls in Bn
Let Zn := {x1, x2, . . . , xMn} be any set of points such that xm ∈ Bn,m for each m ∈ {1, 2, . . . , Mn}. The following lemma states that for all n ∈ N and all m ∈ {1, 2, . . . , Mn − 1}, the distance between xm and xm+1 is less than the connection radius, rn, which implies that the PRM∗ algorithm will attempt to connect the two points xm and xm+1 if they are in the vertex set of the PRM∗ algorithm.

57

Lemma 53 If xn,m ∈ Bn,m and xn,m+1 ∈ Bn,m+1, then xn,m+1 − xn,m ≤ rn, for all n ∈ N and all m ∈ {1, 2, . . . , Mi − 1}.

Proof

Recall

that each ball

in

Bn

has

radius

qn

=

δn (1+θ1

)

.

Given any two points xm

∈ Bn,m

and

xm+1 ∈ Bn,m+1, all of the following hold: (i) xm has distance qn to the center of Bn,m, (ii) xm+1

has distance qn to the center of Bn,m+1, and (iii) centers of Bn,m and Bn,m+1 have distance θ1 qn

to each other. Then,

xn,m+1 − xn,m

≤

(2

+

θ1) qn

=

2 1

+ +

θ1 θ1

δn

≤

rn,

where the ﬁrst inequality is obtained by an application of the triangle inequality and the last

inequality

follows

from

the

deﬁnition

of

δn

=

min{δ,

1+θ1 2+θ1

rn}.

By Lemma 53, conclude that the PRM∗ algorithm will attempt to connect any two vertices in consecutive balls in Bn. The next lemma shows that any such connection attempt will, in fact, be successful. That is, the path connecting xn,m and xn,m+1 is collision-free for all m ∈ {1, 2, . . . , Mn}.

Lemma 54 For all n ∈ N and all m ∈ {1, 2, . . . , Mn}, if xm ∈ Bn,m and xm+1 ∈ Bn,m+1, then the line segment connecting xn,m and xn,m+1 lies in the obstacle-free space, i.e.,

α xn,m + (1 − α) xn,m+1 ∈ Xfree,

for all α ∈ [0, 1].

Proof Recall that σn has strong δn-delta clearance and that the radius qn of each ball in Bn was

deﬁned as qn =

δn 1+θ1

,

where

θ1

>

0

is

a

constant.

Hence, any point along the trajectory σn

has

distance at least (1 + θ1) qn to any point in the obstacle set. Let ym and ym+1 denote the centers of

the balls Bn,m and Bn,m+1, respectively. Since ym = σ(τm) and ym+1 = σ(τm+1) for some τm and

τm+1, ym and ym+1 also have distance (1 + θ1)qn to any point in the obstacle set.

Clearly, xm − ym ≤ qn. Moreover, the following inequality holds:

xm+1−ym ≤ (xm−ym+1)+(ym+1−ym) ≤ xm+1−ym+1 + ym+1−ym ≤ qn+θ1 qn = (1+θ1) qn.

where the second inequality follows from the triangle inequality and the third inequality follows from the construction of balls in Bn.
For any convex combination xα := α xm + (1 − α) xm+1, where α ∈ [0, 1], the distance between xα and ym can be bounded as follows:

α xm + (1 + α) xm+1 − ym

= α (xm − ym) + (1 + α) (xm+1 − ym) = α xm − ym + (1 + α) xm+1 − ym = α qn + (1 + α) (1 + qn) ≤ (1 + θ1) qn,

where the second equality follows from the linearity of the norm. Hence, any point along the line segment connecting xm and xm+1 has distance at most (1 + θ1) qn to ym. Since, ym has distance at least (1 + θ1)qn to any point in the obstacle set, the line segment connecting xm and xm+1 is collision-free.

58

C.6 Convergence to the optimal path

Let Pn denote the set of all paths in the graph GPnRM∗ = (VnPRM∗, EnPRM∗). Let σn be the path

that is closest to σn in terms of the bounded variation norm among all those paths in Pn, i.e.,

σn := graph

mGiPnnRσM∈∗P,nheσnc−e

σn the

. Note that the sequence {σn}n∈N is a random sequence of paths, set Pn of paths is random. The following lemma states that the

since the bounded

variation distance between σn and σn approaches to zero, with probability one.

Lemma 55 The random variable σn − σn BV converges to zero almost surely, i.e., P limn→∞ σn − σn BV = 0 = 1.

Proof The proof of this lemma is based on a Borel-Cantelli argument. It is shown that n∈N P( σn− σn BV > ) is ﬁnite for any > 0, which implies that σn − σn converges to zero almost surely by the Borel-Cantelli lemma (Grimmett and Stirzaker, 2001). This proof uses a Poissonization argument in one of the intermediate steps. That is, a particular result is shown to hold in the Poisson process described in Lemma 11. Subsequently, the result is de-Poissonized, i.e., shown to hold also for the original process.
Fix some > 0. Let α, β ∈ (0, 1) be two constants, both independent of n. Recall that qn is the radius of each ball in the set Bn of balls covering the path σn. Let In,m denote the indicator variable for the event that the ball Bn,m has no point that is within a distance β qn from the center of Bn,m. For a more precise deﬁnition, let β Bn,m denote the ball that is centered at the center of Bn,m and has radius β rn. Then,
In,m := 1, if (β Bn,m) ∩ V PRM∗ = ∅, 0, otherwise.

Let Kn denote the number of balls in Bn that do not contain a vertex that is within a β qn distance

to the center of that particular ball, i.e., Kn :=

Mn m=1

In,m.

Consider the event that In,m holds for at most an α fraction of the balls in Bn, i.e., {Kn ≤ α Mn}.

This event is important for the following reason. Recall that the vertices in subsequent balls in Bn are connected by edges in GPnRM∗ by Lemmas 53 and 54. If only at most an α fraction of the balls

do not have a vertex that is less than a distance of β rn from their centers (hence, a (1 − α) fraction

have at least one vertex within a distance of β rn from their cente√rs), i.e., {Kn ≤ α Mn√} holds, then the bounded variation diﬀerence between σn and σn is at most ( 2 α + β(1 − α))L ≤ 2 (α + β)L,

where L is a ﬁnite bound on the length of all paths in {σn}n∈N, i.e., L := supn∈N TV(σn). That is,

√ {Kn ≤ α Mn} ⊆ σn − σn BV ≤ 2 (α + β) L

Taking the complement of both sides and using the monotonicity of probability measures, √
P σn − σn BV > 2 (α + β) L ≤ P ({Kn ≥ α Mn}) .

In the rest of the proof, it is shown that the right hand side of the inequality above is summable
for all small α, β > 0, which implies that P ({ σn − σn > }) is summable for all small > 0. For this purpose, the process that provides independent uniform samples from Xfree is approx-
imated by an equivalent Poisson process described in Section 2.2. A more precise deﬁnition is
given as follows. Let {X1, X2, . . . , Xn} denote the binomial point process corresponding to the SampleFree procedure. Let ν < 1 be a constant independent of n. Recall that Poisson(ν n)

59

denotes the Poisson random variable with intensity ν n (hence, mean value ν n). Then, the process Pν n := {X1, X2, . . . , XPoisson(ν n)} is a Poisson process restricted to µ(Xfree) with intensity ν n/µ(Xfree) (see Lemma 11). Thus, the expected number of points of this Poisson process is ν n.
Clearly, the set of points generated by one process is a subset of the those generated by the other. However, since ν < 1, in most trials the Poisson point process Pν n is a subset of the binomial point process.
Deﬁne the random variable Kn denote the number of balls of that fail to have one sample within a distance β rn to their centers, when the underlying point process is Pν n (instead of the independent uniform samples provided by the SampleFree procedure). In other words, Kn is the random variable that is deﬁned similar to Kn, except that the former is deﬁned with respect to the points of Pν n whereas the latter is deﬁned with respect to the n samples returned by SampleFree procedure.
Since {Kn > α Mn} is a decreasing event, i.e., the probability that it occurs increases if Pνn includes fewer samples, the following bound holds (see, e.g., Penrose, 2003)

P {Kn ≥ α Mn} ≤ P {Kn ≥ α Mn} + P({Poisson(ν n) ≥ n}).

Since a Poisson random variable has exponentially-decaying tails, the second term on the right hand side can be bounded as
P({Poisson(ν n) ≥ n}) ≤ e−cn,

where c > 0 is a constant. The ﬁrst term on the right hand side can be computed directly as follows. First, for all small
β, the balls of radius β rn are all disjoint (see Figure 25). Denote this set of balls by Bn,m =
{Bn,1, Bn,2, . . . , Bn,Mn}. More precisely, Bn,m is the ball of radius β qn centered at the center of Bn,m. Second, observe that the event {Kn > α Mn} is equivalent to the event that at least an α
fraction of all the balls in Bn include at least one point of the process Pν n. Since, the point process Pν n is Poisson and the balls in Bn are disjoint for all small enough β, the probability that a single ball in Bn does not contain a sample is pn := exp(−ζd (βqn)d ν n/µ(Xfree)) ≤ exp(−c β ν log n) for some constant c. Third, by the independence property of the Poisson point process, the number of balls in Bn that do not include a point of the point process Pν n is a binomial random variable with parameters Mn and pn. Then, for all large n,

P Kn ≥ α Mn ≤ P ({Binomial(Mn, pn) ≥ αMn}) ≤ exp(−Mn pn).

Combining the two inequalities above, the following bound is obtained for the original sampling

process

P ({Kn ≥ α Mn}) ≤ e−c n + e−Mn pn .

Summing up both sides,
∞
P ({Kn ≥ α n}) < ∞.
n=1
This argument holds for all α, β, ν > 0. Hence, for all > 0,

∞
P
n=1

σn − σn BV >

< ∞.

Then, by the Borel-Cantelli lemma, P ({limn→∞ σn − σn BV = 0}) = 1.

60

Figure 25: The set Bn,m of non-intersection balls is illustrated.

Finally, the following lemma states that the cost of the minimum cost path in the graph returned by the PRM∗ algorithm converges to the optimal cost c∗ with probability one. Recall that YnPRM∗ denotes the cost of the minimum-cost path in the graph returned by the PRM∗ algorithm, when
the algorithm is run with n samples.

Lemma 56 Under the assumptions of Theorem 34, the cost of the minimum-cost path present in the graph returned by the PRM∗ algorithm converges to the optimal cost c∗ as the number of samples
approaches inﬁnity, with probability one, i.e.,

P

lim
n→∞

YnPRM∗

=

c∗

= 1.

Proof Recall that σ∗ denotes the optimal path, and that limn→∞ σn = σ∗ holds surely. By Lemma 55, limn→∞ σn − σn BV = 0 holds with probability one. Thus, by repeated application of the triangle inequality, limn→∞ σn − σ∗ BV = 0, i.e.,

P

lim
n→∞

σn − σ∗

BV = 0

= 1.

Then, by the robustness of the optimal path σ∗, it follows that

P

lim
n→∞

c(σn)

=

c∗

= 1.

That is the costs of the paths {σn}n∈N converges to the optimal cost almost surely, as the number of samples approaches inﬁnity.

D Proof of Theorem 35 (Asymptotic Optimality of k-nearest PRM∗)
The proof of this theorem is similar to that of Theorem 34. For the reader’s convenience, a complete proof is provided at the expense of repeating some of the arguments.

61

D.1 Outline of the proof
Let σ∗ be a robust optimal path with weak δ-clearance. First, deﬁne the sequence {σn}n∈N of paths as in the proof of Theorem 34.
Second, deﬁne a sequence {qn}n∈N and tile σn with a set Bn = {Bn,1, Bn,2, . . . , Bn,M } of overlapping balls of radius qn. See Figures 23 and 26. Let xm ∈ Bn,m and xm+1 ∈ Bn,m+1 be any two points from subsequent balls in Bn. Construct Bn such that the straight path connecting xm and xm+1 lies entirely inside the obstacle free space. Also, construct a set Bn of balls such that (i) Bn,m and Bn,m are centered at the same point and (ii) Bn,m contains Bn,m, and Bn,m+1, for all m ∈ {1, 2, . . . , Mn − 1}.
Let An denote the event that each ball in Bn contains at least one vertex, and An denote the event that each ball in Bn contains at most k(n) vertices of the graph returned by the k-nearest PRM∗ algorithm. Third, show that An and An occur together for all large n, with probability one. Clearly, this implies that the PRM∗ algorithm will connect vertices in subsequent ball in Bn with an edge, and any path formed by connecting such vertices will be collision-free.
Finally, show that any sequence of paths formed in this way converges to σ∗. Using the robustness of σ∗, show that the best path in the graph returned by the k-nearest PRM∗ algorithm converges to c(σ∗) almost surely.

D.2 Construction of the sequence {σn}n∈N of paths
Let θ1, θ2 ∈ R>0 be two constants, the precise values of which will be provided shortly. Deﬁne

δn := min

δ, (1 + θ1)

(1 + 1/d + θ2) µ(Xfree) 1/d ζd

log n n

1/d

.

Since limn→∞ δn = 0 and 0 ≤ δn ≤ δ for all n ∈ N, by Lemma 50, there exists a sequence {σn}n∈N of paths such that limn→∞ σn = σ∗ and σn is strongly δn-clear for all n ∈ N.

D.3 Construction of the sequence {Bn}n∈N of sets of balls

Deﬁne

qn

:=

1

δn . + θ1

For each n ∈ N, use Deﬁnition 51 to construct a set Bn = {Bn,1, Bn,2, . . . , Bn,Mn} of overlapping balls that collectively cover σn as Bn := CoveringBalls(σn, qn, θ1qn) (see Figures 23 and 26 for an illustration).

D.4 The probability that each ball in Bn contains at least one vertex

Recall that GnkPRM∗ = (VnkPRM∗, EnkPRM∗) denotes the graph returned by the k-nearest PRM∗

algorithm, when the algorithm is run with n samples. Let An,m denote the event that the ball Bn,m

contains at least one vertex from VnkPRM∗, i.e., An,m = Bn,m ∩ VnkPRM∗ = ∅ . Let An denote the

event that all balls in Bn,m contains at least one vertex of GknPRM∗, i.e., An =

Mn m=1

An,m.

Recall that Acn denotes the complement of the event An, µ(·) denotes the Lebesgue measure,

and ζd is the volume of the unit ball in the d-dimensional Euclidean space. Let sn denote the length

of σn.

62

Figure 26: An illustration of the covering balls for the k-nearest PRM∗ algorithm. The δn ball is guaranteed to contain the balls Bn,m and Bn,m+1.

Lemma 57 For all θ1, θ2 > 0,

In particular,

P(Acn)

≤

sn θ1

ζd

1/d

1

.

θ1 (1 + 1/d + θ2) µ(Xfree)

(log n)1/d n1+θ2

∞ n=1

P(Acn)

<

∞

for

all

θ1, θ2

>

0.

Proof Let n0 ∈ N be a number for which δn < δ for all n > n0. A bound on the number of balls in Bn can computed as follows. For all n > n0,

Mn

=

|Bn| ≤

sn θ1 qn

=

sn θ1

ζd

1/d

(1 + 1/d + θ2)µ(Xfree)

n 1/d .
log n

The volume of each ball Bn can be computed as

µ(Bn,m)

=

ζd(qn)d

=

(1

+

1/d

+

θ2)

log µ(Xfree) n

n .

The probability that the ball Bn,m does not contain a vertex of the k-nearest PRM∗ algorithm can be bounded as

P(Acn,m) =

1 − µ(Bn,m) µ(Xfree)

n
=

log n 1 − (1 + 1/d + θ2) n

n
≤ n−(1+1/d+θ2).

Finally, the probability that at least one of the balls in Bn contains no vertex of the k-nearest PRM∗ can be bounded as

Clearly,

P(An) = P
≤ sn θ1
= sn θ1

Mn
m=1 An,m

Mn
≤ P(An,m) = Mn P(An,1)
m=1

ζd

1/d

n

1/d
n−(1+1/d+θ2)

(1 + 1/d + θ2) µ(Xfree)

log n

ζd

1/d

1

.

(1 + 1/d + θ2) µ(Xfree)

(log n)1/d n1+θ2

∞ n=1

P(Acn)

<

∞

for

all

θ1,

θ2

>

0.

63

D.5 Construction of the sequence {Bn}n∈N of sets of balls
Construct a set Bn = {Bn,1, Bn,2, . . . , Bn,Mn} of balls as Bn := CoveringBalls(σn, δn, θ1qn) so that each ball in Bn has radius δn and the spacing between two balls is θ1qn (see Figure 26).
Clearly, the centers of balls in Bn coincide with the centers of the balls in Bn, i.e., the center of Bn,m is the same as the center of Bn,m for all m ∈ {1, 2, . . . , Mn} and all n ∈ N. However, the balls in Bn have a larger radius than those in Bn.

D.6 The probability that each ball in Bn contains at most k(n) vertices
Recall that the k-nearest PRM algorithm connects each vertex in the graph with its k(n) nearest vertices when the algorithm is run with n samples, where k(n) = kPRM log n. Let An denote the event that all balls in Bn contain at most k(n) vertices of GknPRM∗.
Recall that Anc denotes the complement of the event An.

Lemma 58 If kPRM > e (1 + 1/d), then there exists some θ1, θ2 > 0 such that

P(Anc)

≤

sn θ1

ζd (1 + 1/d + θ2)µ(Xfree)

1/d

1

(log n)1/d n−(1+θ1)d(1+1/d+θ2) .

In particular,

∞ n=1

P(Anc)

<

∞

for

some

θ1, θ2

>

0.

Proof Let n0 ∈ N be a number for which δn < δ for all n > n0. As shown in the proof of Lemma 57, the number of balls in Bn satisﬁes

Mn

=

|Bn|

≤

sn θ1qn

=

sn θ1

ζd

1/d

(1 + 1/d + θ2)µ(Xfree)

n 1/d .
log n

For all n > n0, the volume of Bn,m can be computed as

µ(Bn,m)

=

ζd

(δn)d

=

(1

+

θ1)d

(1

+

1/d

+

θ2)

µ(Xfree)

log n .
n

Let In,m,i denote the indicator random variable of the event that sample i falls into ball Bn,m. The expected value of In,m,i can be computed as

E[In,m,i]

=

µ(Bn,m) µ(Xfree)

=

(1

+

θ1)d

(1

+

1/d

+

θ2)

log n .
n

Let Nn,m denote the number of vertices that fall inside the ball Bn,m, i.e., Nn,m =
Then,
n
E[Nn,m] = E[In,m,i] = n E[In,m,1] = (1 + θ1)d(1 + 1/d + θ2) log n.

n i=1

In,m,i.

i=1

Since {In,m,i}ni=1 are independent identically distributed random variables, large deviations of their sum, Mn,m, can be bounded by the following Chernoﬀ bound (Dubhashi and Panconesi, 2009):

P Nn,m > (1 + ) E[Nn,m]

e

E[Nn,m ]

≤

,

(1 + )(1+ )

for all > 0. In particular, for = e − 1,

P Nn,m > e E[Nn,m]

≤ e−E[Nn,m] = e−(1+θ1)d(1+1/d+θ2) log n = n−(1+θ1)d(1+1/d+θ2).

64

Since k(n) > e (1+1/d) log n, there exists some θ1, θ2 > 0 independent of n such that e E[Nn,k] = e (1 + θ1) (1 + 1/d + θ2) log n ≤ k(n). Then, for the same values of θ1 and θ2,

P Nn,m > k(n)

≤ P Nn,m > e E[Nn,m]

≤ n−(1+θ1)d(1+1/d+θ2).

Finally, consider the probability of the event that at least one ball in Bn contains more than k(n) nodes. Using the union bound together with the inequality above

P

Mn m=1

Nn,m > k(n)

Hence,

Mn

≤

P Nn,m > k(n)

m=1

= Mn P {Nn,1 > k(n)}

P(Anc) = P

Mn m=1

Nn,m > k(n)

≤ sn θ1

ζd (1 + 1/d + θ2)µ(Xfree)

1/d

1

(log n)1/d n−(1+θ1)d(1+1/d+θ2) .

Clearly,

∞ n=1

P(Anc)

<

∞

for

the

same

values

of

θ1

and

θ2.

D.7 Connecting the vertices in the subsequent balls in Bn
First, note the following lemma.
Lemma 59 If kPRM > e (1 + 1/d)1/d, then there exists θ1, θ2 > 0 such that the event that each ball in Bn contains at least one vertex and each ball in Bn contains at most k(n) vertices occurs for all large n, with probability one, i.e.,

P

lim inf
n→∞

(An

∩

An)

= 1.

Proof Consider the event Acn ∪ Anc, which is the complement of An ∩ An. Using the union bound, P Acn ∪ Anc ≤ P(Acn) + P(Anc).

Summing both sides,

∞

∞

∞

P(Acn ∪ ANc ) ≤ P(Acn) + P(Anc) < ∞,

n=1

n=1

n=1

where the last inequality follows from Lemmas 57 and 58. Then, by the Borel-Cantelli lemma, P (lim supn→∞(Acn ∪ Anc)) = P (lim supn→∞(An ∩ An)c) = 0, which implies P (lim infn→∞(An ∩ An)) = 1.

Note that for each m ∈ {1, 2, . . . , Mn − 1}, both Bn,m and Bn,m+1 lies entirely inside the ball Bn,m (see Figure 26). Hence, whenever the balls Bn,m and Bn,m+1 contain at least one vertex each, and Bn,m contains at most k(n) vertices, the k-nearest PRM∗ algorithm attempts to connect all vertices in Bn,m and Bn,m+1 with one another.
The following lemma guarantees that connecting any two points from two consecutive balls in
Bn results in a collision-free trajectory. The proof of the lemma is essentially the same as that of Lemma 54.

Lemma 60 For all n ∈ N and all m ∈ {1, 2, . . . , Mn}, if xm ∈ Bn,m and xm+1 ∈ Bn,m+1, then the line segment connecting xm and xm+1 lies in the obstacle-free space, i.e.,

α xm + (1 − α) xm+1 ∈ Xfree,

for all α ∈ [0, 1].

65

D.8 Convergence to the optimal path
The proof of the following lemma is similar to that of Lemma 55, and is omitted here. Let Pn denote the set of all paths in the graph returned by k-PRM∗ algorithm at the end of n
iterations. Let σn be the path that is closest to σn in terms of the bounded variation norm among all those paths in Pn, i.e., σn := minσ ∈Pn σ − σn .
Lemma 61 The random variable σn − σn BV converges to zero almost surely, i.e.,
P limn→∞ σn − σn BV = 0 = 1.
A corollary of the lemma above is that limn→∞ σn = σ∗ with probability one. Then, the result follows by the robustness of the optimal solution (see the proof of Lemma 56 for details).

E Proof of Theorem 36 (Asymptotic optimality of RRG)
E.1 Outline of the proof
The proof of this theorem is similar to that of Theorem 34. The main diﬀerence is the deﬁnition of Cn that denotes the event that the RRG algorithm has suﬃciently explored the obstacle free space. More precisely, Cn is the event that for any point x in the obstacle free space, the graph maintained by the RRG algorithm algorithm includes a vertex that can be connected to x.
Construct the sequence {σn}n∈N of paths and the sequence {Bn}n∈N of balls as in the proof of Theorem 34. Let An denote the event that each ball in Bn contains a vertex of the graph maintained by the RRG by the end of iteration n. Compute n by conditioning on the event that Ci holds for all i ∈ { θ3 n , . . . , n}, where 0 < θ3 < 1 is a constant. Show that the probability that Ci fails to occur for any such i is small enough to guarantee that An occurs for all large n with probability one. Complete the proof as in the proof of Theorem 34.

E.2 Deﬁnitions of {σn}n∈N and {Bn}n∈N
Let θ1 > 0 be a constant. Deﬁne δn, σn, qn, and Bn as in the proof of Theorem 34.

E.3 Probability that each ball in Bn contains at least one vertex

Let An,m be the event that the ball Bn,m contains at least one vertex of the RRG at the end of n

iterations. Let An be the event that all balls in Bn contain at least one vertex of the RRG at the

end of iteration n, i.e., An =

Mn m=1

An,m,

where

Mn

is

the

number

of

balls

in

Bn.

Recall

that

γRRG

is the constant used in deﬁning the connection radius of the RRG algorithm (see Algorithm 5).

Lemma 62 If γRRG > 2(1 + 1/d)1/d

µ(Xfree ) ζd

1/d
then there exists θ1 > 0 such that An occurs for

all large n with probability one, i.e.,

P (lim infn→∞ An) = 1.
The proof of this lemma requires two intermediate results, which are provided next. Recall that η is the parameter used in the Steer procedure (see the deﬁnition of Steer procedure
in Section 3.1). Let Cn denote the event that for any point x ∈ Xfree, the graph returned by the RRG algorithm includes a vertex v such that x − v ≤ η and the line segment joining v and x is collision-free. The following lemma establishes an bound on the probability that this event fails to occur at iteration n.

66

Lemma 63 There exists constants a, b ∈ R>0 such that P (Cnc ) ≤ a e−b n for all n ∈ N.

Proof Partition Xfree into ﬁnitely many convex sets such that each partition is bounded by a ball
a radius η. Such a ﬁnite partition exists by the boundedness of Xfree. Denote this partition by
X1, X2, . . . , XM . Since the probability of failure decays to zero with an exponential rate, for any m ∈ {1, 2, . . . , M }, the probability that Xm fails to contain a vertex of the RRG decays to zero with an exponential rate, i.e.,

P x ∈ VnRRG ∩ Xm ≤ am e−bm n

The probability that at least one partition fails to contain one vertex of the RRG also decays to zero with an exponential rate. That is, there exists a, b ∈ R>0 such that

M

M

P

M m=1

x ∈ VnRRG ∩ Xm

≤

P x ∈ VnRRG ∩ Xm ≤

am e−bmn ≤ a e−b n,

m=1

m=1

where the ﬁrst inequality follows from the union bound.

Let 0 < θ3 < 1 be a constant independent of n. Consider the event that Ci occurs for all i that

is greater than θ3 n, i.e.,

n i= θ3 n

Ci.

The following lemma analyzes the probability of the event

that

n i= θ3 n

Ci fails to occur.

Lemma 64 For any θ3 ∈ (0, 1),
∞
P
n=1

n

c

Ci

i= θ3n

< ∞.

Proof The following inequalities hold:

∞
P
n=1

n

c

i= θ3 n Ci

∞
=P
n=1

n i= θ3 n

Cic

∞n

∞n

≤

P(Cic) ≤

a e−b i,

n=1 i= θ3 i

n=1 i= θ3 n

where the last inequality follows from Lemma 63. The right-hand side is ﬁnite for all a, b > 0.

Proof of Lemma 62 It is shown that

∞ n=1

P

(Acn)

<

∞,

which,

by

the

Borel-Cantelli

Lemma

(Grim-

mett and Stirzaker, 2001), implies that Acn occurs inﬁnitely often with probability zero, i.e.,

P(lim supn→∞ Acn) = 0, which in turn implies P(lim infn→∞ An) = 1.

Let n0 ∈ N be a number for which δn < δ for all n > n0. First, for all n > n0, the number of

balls in Bn can be bounded by (see the proof of Lemma 52 for details)

Mn

=

|Bn|

≤

(2 + θ1) sn θ1 γRRG

n 1/d .
log n

Second, for all n > n0, the volume of each ball in Bn can be calculated as (see the proof of

Lemma 52)

µ(Bn,m) = ζd

γPRM

d log n ,

2 + θ1 n

where ζd is the volume of the unit ball in the d-dimensional Euclidean space.

67

Third, conditioning on the event

n i= θ3 n

Ci,

each

new

sample

will

be

added

to

the

graph

maintained by the RRG algorithm as a new vertex between iterations i = θ3 n and i = n. Thus,

P Acn,m

n
i= θ3 n Ci

≤

1 − µ(Bn,m) n− θ3 n ≤ 1 − µ(Bn,m) (1−θ3) n

µ(Xfree)

µ(Xfree)

≤ 1 − ζd

γRRG d log n (1−θ3) n

µ(Xfree) 2 + θ1

n

≤ e ≤ n , −

(1−θ3) ζd µ(Xfree )

γRRG 2+θ1

d
log n

−

(1−θ3) ζd µ(Xfree )

γRRG 2+θ1

d

where the fourth inequality follows from (1 − 1/f (n))g(n) ≤ eg(n)/f(n). Fourth,

P Acn

n
i= θ3 n Ci

≤P

Mn m=1

Acn,m

n
i= θ3 n Ci

Mn

≤

P Acn,m

m=1

n
i= θ3 n Ci

= Mn P Acn,1

n
i= θ3 n Ci

≤ (2 + θ1) sn

n

n . 1/d

−

(1−θ3) ζd µ(Xfree )

γRRG d 2+θ1

θ1 γRRG log n

Hence,

∞
P Acn
n=1

n
i= θ3 n Ci < ∞,

whenever

(1−θ3) ζd µ(Xfree )

γRRG 2+θ1

d
− 1/d > 1, i.e., γRRG > (2 + θ1)(1 + 1/d)1/d

µ(Xfree ) (1−θ3) ζd

1/d
, which is

satisﬁed by appropriately choosing the constants θ1 and θ3, since γRRG > 2 (1+1/d)1/d

µ(Xfree ) ζd

1/d
.

Finally,

P Acn

n
i= θ3 n Ci

= P Acn ∩ ∩ni= θ3 n Ci

P

n i= θ3 n

Ci

≥ P(Acn ∩ (∩ni= θ3 n Ci)) = 1 − P(An ∪ (∩ni= θ3 n Ci)c)

≥ 1 − P(An) − P (∩ni= θ3 n Ci)c

= P(Acn) − P (∩ni= θ3 n Ci)c .

Taking the inﬁnite sum of both sides yields

∞

∞

P(Acn) ≤ P Acn

n=1

n=1

n
i= θ3 n Cn

∞
+P
n=1

n

c

i= θ3 n Cn

.

The ﬁrst term on the right hand side is shown to be ﬁnite above. The second term is ﬁnite by

Lemma 64. Hence,

∞ n=1

P(An)

<

∞.

Then, by the Borel Cantelli lemma, Acn occurs inﬁnitely

often with probability zero, which implies that its complement An occurs for all large n, with

probability one.

68

E.4 Convergence to the optimal path
The proof of the following lemma is similar to that of Lemma 55, and is omitted here. Let Pn denote the set of all paths in the graph returned by RRG algorithm at the end of n
iterations. Let σn be the path that is closest to σn in terms of the bounded variation norm among all those paths in Pn, i.e., σn := minσ ∈Pn σ − σn .
Lemma 65 The random variable σn − σn BV converges to zero almost surely, i.e.,
P limn→∞ σn − σn BV = 0 = 1.
A corollary of the lemma above is that limn→∞ σn = σ∗ with probability one. Then, the result follows by the robustness of the optimal solution (see the proof of Lemma 56 for details).

F Proof of Theorem 37 (asymptotic optimality of k-nearest RRG)

F.1 Outline of the proof
The proof of this theorem is a combination of that of Theorem 35 and 36. Deﬁne the sequences {σn}n∈N, {Bn}n∈N, and {Bn}n∈N as in the proof of Theorem 35. Deﬁne
the event Cn as in the proof of Theorem 36. Let An denote the event that each ball in Bn contains at least one vertex, and An denote the event that each ball in Bn contains at most k(n) vertices of the graph maintained by the RRG algorithm, by the end of iteration n. Compute An and An by conditioning on the event that Ci holds for all i = θ3 n to n. Show that this is enough to guarantee that An and An hold together for all large n, with probability one.

F.2 Deﬁnitions of {σn}n∈N, {Bn}n∈N, and {Bn}n∈N
Let θ1, θ2 > 0 be two constants. Deﬁne δn, σn, qn, Bn, and Bn as in the proof of Theorem 35.

F.3 The probability that each ball in Bn contains at least one vertex

Let An,m denote the event that the ball Bn,m contains at least one vertex of the graph maintained

by the k-nearest RRG algorithm by the end of iteration n. Let An denote the event that all balls

in Bn,m contain at least one vertex of the same graph, i.e., An =

Mn m=1

An,m.

Let sn

denote the

length of σn, i.e., T V (σn). Recall η is the parameter in the Steer procedure. Let Cn denote the

event that for any point x ∈ Xfree, the k-nearest RRG algorithm includes a vertex v such that

x − v ≤ η.

Lemma 66 For any θ1, θ2 > 0 and any θ3 ∈ (0, 1),

P Acn

n
i= θ3 n Ci

≤ sn

ζd

1/d

1 .

θ1 θ1 (1 + 1/d + θ2) µ(Xfree)

(log n)1/d n(1−θ3)(1+1/d+θ2)−1/d

In particular,

∞ n=1

P(Acn

|

n i= θ3 n

Ci) < ∞ for any θ1, θ2 > 0 and some θ3 ∈ (0, 1).

Proof Let n0 ∈ N be a number for which δn < δ for all n > n0. Then, for all n > n0,

Mn

=

|Bn| ≤

sn θ1 qn

=

sn θ1

ζd

1/d

(1 + 1/d + θ2)µ(Xfree)

n 1/d .
log n

69

The volume of each ball Bn can be computed as

µ(Bn,m)

=

ζd(qn)d

=

(1

+

1/d

+

θ2)

log µ(Xfree) n

n .

Given

n i=

θ3 n

Ci, the probability that the ball Bn,m does not contain a vertex of the k-nearest

PRM∗ algorithm can be bounded as

P Acn,m

n
i= θ3 n Ci

=

1 − µ(Bn,m) µ(Xfree)

(1−θ3) n
=

log n 1 − (1 + 1/d + θ2) n

(1−θ3) n
≤ n−(1−θ3) (1+1/d+θ2).

Finally, the probability that at least one of the balls in Bn contains no vertex of the k-nearest PRM∗ can be bounded as

P(An) = P ≤ sn θ1

Mn
m=1 An,m

Mn

≤

P(An,m) = Mn P(An,1)

m=1

ζd

1/d

1 .

(1 + 1/d + θ2) µ(Xfree)

(log n)1/d n(1−θ3) (1+1/d+θ2)−1/d

Clearly, for all θ1, θ2 > 0, there exists some θ3 ∈ (0, 1) such that

∞ n=1

P(Acn)

<

∞.

F.4 The probability that each ball in Bn contains at most k(n) vertices
Let An denote the event that all balls in Bn contain at most k(n) vertices of the graph maintained by the RRG algorithm, by end of iteration n.

Lemma 67 If kPRM > e (1 + 1/d), then there exists θ1, θ2, θ3 > 0 such that

P Anc

n
i= θ3 n Ci

≤ sn θ1

ζd (1 + 1/d + θ2)µ(Xfree)

1/d

1

(log n)1/d n−(1−θ3)(1+θ1)d(1+1/d+θ2) .

In particular,

∞ n=1

P(Acn

|

n i= θ3 n

Ci) < ∞ for some θ1, θ2 > 0 and some θ3 > 0.

Proof Let n0 ∈ N be a number for which λn < δ for all n > n0. Then, the number of balls in Bn and the volume of each ball can be computed as

Mn

=

|Bn|

≤

sn θ1qn

=

sn θ1

ζd

1/d

(1 + 1/d + θ2)µ(Xfree)

n 1/d .
log n

µ(Bn,m)

=

ζd

(λn)d

=

(1

+

θ1)d

(1

+

1/d

+

θ2)

µ(Xfree)

log n

n .

Let In,m,i denote the indicator random variable of the event that sample i falls into ball Bn,m. The expected value of In,m,i can be computed as

E[In,m,i]

=

µ(Bn,m) µ(Xfree)

=

(1

+

θ1)d

(1

+

1/d

+

θ2)

log n .
n

Let Nn,m denote the number of vertices that fall inside the ball Bn,m between iterations θ3 n and

n, i.e., Nn,m =

n i= θ3 n

In,m,i.

Then,

n

E[Nn,m] =

E[In,m,i] = (1 − θ3) n E[In,m,1] = (1 − θ3) (1 + θ1)d (1 + 1/d + θ2) log n.

i= θ3 n

70

Since {In,m,i}ni=1 are independent identically distributed random variables, large deviations of their sum, Mn,m, can be bounded by the following Chernoﬀ bound (Dubhashi and Panconesi, 2009):

P Nn,m > (1 + ) E[Nn,m]

e

E[Nn,m ]

≤

,

(1 + )(1+ )

for all > 0. In particular, for = e − 1,

P Nn,m > e E[Nn,m]

≤ e−E[Nn,m] = n−(1−θ3) (1+θ1)d (1+1/d+θ2).

Since k(n) > e (1 + 1/d) log n, there exists some θ1, θ2 > 0 and θ3 ∈ (0, 1), independent of n, such that e E[Nn,k] = e (1 − θ3) (1 + θ1) (1 + 1/d + θ2) log n ≤ k(n). Then, for the same values of θ1 and θ2,

P Nn,m > k(n)

≤ P Nn,m > e E[Nn,m]

≤ n−(1−θ3) (1+θ1)d (1+1/d+θ2).

Finally, consider the probability of the event that at least one ball in Bn contains more than k(n) nodes. Using the union bound together with the inequality above

Mn

P

m=1 Nn,m > k(n)

Hence,

Mn

≤

P Nn,m > k(n)

m=1

= Mn P {Nn,1 > k(n)}

P Anc Clearly,

n
i= θ3 n Ci

=P ≤ sn
θ1

Mn
m=1 Nn,m > k(n)

ζd (1 + 1/d + θ2)µ(Xfree)

1/d

1

(log n)1/d n−(1−θ3) (1+θ1)d(1+1/d+θ2) .

∞
n=1 P

Anc | ∩ni= θ3 n

Ci

< ∞ for the same values of θ1, θ2, and θ3.

.

F.5 Connecting the vertices in subsequent balls in Bn
Lemma 68 If kPRM > e (1 + 1/d)1/d, then there exists θ1, θ2 > 0 such that the event that each ball in Bn contains at least one vertex and each ball in Bn contains at most k(n) vertices occurs for all large n, with probability one, i.e.,

P

lim inf
n→∞

(An

∩

An)

= 1.

First note the following lemma.

Lemma 69 For any θ3 ∈ (0, 1),

∞
P
n=1

n

c

i= θ3n Cn

< ∞.

Proof Since the RRG algorithm and the k-nearest RRG algorithm have the same vertex sets, i.e., VnRRG = VnkRRG surely for all n ∈ N, the lemma follows from Lemma 64.

71

Proof of Lemma 68 Note that

P (Acn ∪ Anc)

n
i= θ3 n Ci

= P Acn ∩ ∩ni= θ3 n Ci

P

n i= θ3 n

Ci

≥ P (Acn ∪ Anc) ∩ ∩ni= θ3 n Ci ≥ P(Acn ∪ Anc) − P (∩ni= θ3 n Ci)c ,

where the last inequality follows from the union bound. Rearranging and using the union bound,

P(Acn ∪ Anc) ≤ P Acn ∩ni= θ3 n Ci + P Acn ∩ni= θ3 n Ci + P ∩ni= θ3 n Ci c .

Summing both sides,

∞

∞

P(Acn∪Anc) ≤

P Acn

n=1

n=1

∞

n
i= θ3 n Ci

+P
n=1

Anc

∞

n

i= θ3 n Ci

+P
n=1

n

c

i= θ3 n Ci

,

where the right hand side is ﬁnite by Lemmas 66, 67, and 69, by picking θ3 close to one. Hence,

∞ n=1

P(Acn

∪

Anc)

<

∞.

Then, by the Borel-Cantelli lemma, P(lim supn→∞(Acn ∪ Anc)) = 0, or

equivalently P(lim infn→∞(An ∩ An)) = 1.

F.6 Convergence to the optimal path
The proof of the following two lemmas are essentially the same as that of Lemma 55, and is omitted here. Let Pn denote the set of all paths in the graph returned by k-RRG algorithm at the end of n iterations. Let σn be the path that is closest to σn in terms of the bounded variation norm among all those paths in Pn, i.e., σn := minσ ∈Pn σ − σn .
Lemma 70 The random variable σn − σn BV converges to zero almost surely, i.e.,
P limn→∞ σn − σn BV = 0 = 1.
A corollary of the lemma above is that limn→∞ σn = σ∗ with probability one. Then, the result follows by the robustness of the optimal solution (see the proof of Lemma 56 for details).

G Proof of Theorem 38 (Asymptotic optimality of RRT∗)
For simplicity, the proof will assume the steering parameter η to be large enough, i.e., η ≥ diam(X ), although the results hold for any η > 0.

G.1 Marked point process
Consider the following marked point process. Let {X1, X2, . . . , Xn} be a independent uniformly distributed points drawn from Xfree and let {Y1, Y2, . . . , Yn} be independent uniform random variables with support [0, 1]. Each point Xi is associated with a mark Yi that describes the order of Xi in the process. More precisely, a point Xi is assumed to be drawn after another point Xi if Yi < Yi. We will also assume that the point process includes the point xinit with mark Y = 0.
Consider the graph formed by adding an edge (Xi , Xi), whenever (i) Yi < Yi and (ii) Xi − Xi ≤ rn both hold. Notice that, formed in this way, Gn includes no directed cycles. Denote this

72

graph by Gn = (Vn, En). Also, consider a subgraph Gn of Gn formed as follows. Let c(Xi) denote the cost of best path starting from xinit and reaching Xi. In Gn, each vertex Xi has a single parent Xi with the smallest cost c(Xi). Since the graph is built incrementally, the cost of the best path reaching Xi will be the same as the one reaching Xi in both Gn and Gn. Clearly, Gn is equivalent to the graph returned by the RRT∗ algorithm at the end of n iterations, if the steering parameter
η is large enough.
Let Yn and the Yn denote the costs of the best paths starting from xinit and reaching the goal region in Gn and Gn, respectively. Then, lim supn→∞ Yn = lim supn→∞ Yn surely. In the rest of the proof, it is shown that P({lim supn→∞ Yn}) = 1, which implies that P({lim supn→∞ Yn}) = 1, which in turn implies the result.

G.2 Deﬁnitions of {σn}n∈N and {Bn}n∈N
Let σ∗ denote an optimal path. Deﬁne

δn := min{δ, 4 rn},

where rn is the connection radius of the RRT∗ algorithm. Let {σn}n∈N be the sequence paths, the existence of which is guaranteed by Lemma 50.

For each n ∈ N, construct a sequence {Bn}n∈N of balls that cover σn as Bn = {Bn,1, Bn,2, . . . , Bn,Mn} :=

CoveringBalls(σn, rn, 2 rn) (see Deﬁnition 51), where rn is the connection radius of the RRT∗ al-

gorithm, i.e., rn = γRRT∗

log n n

1/d
. Clearly, the balls in Bn are openly disjoint, since the spacing

between any two consecutive balls is 2 rn.

G.3 Connecting the vertices in subsequent balls in Bn

For all m ∈ {1, 2, . . . , Mn}, let An,m denote the event that there exists two vertices Xi, Xi ∈ VnRRT∗

such that Xi ∈ Bn,m, Xi ∈ Bn,m+1 and Yi ≤ Yi, where Yi and Yi are the marks associated with

points Xi and Xi , respectively. Notice that, in this case, Xi and Xi will be connected with an edge

in Gn. Let An denote the event that An,m holds for all m ∈ {1, 2, . . . , M }, i.e., An =

M m=1

An,m

.

Lemma 71 If γRRT∗ > 4

µ(Xfree ) ζd

1/d
, then An occurs for all large n, with probability one, i.e.,

P

lim inf
n→∞

An

= 1.

Proof The proof of this result is based on a Poissonization argument. Let Poisson(λ) be a Poisson
random variable with parameter λ = θ n, where θ ∈ (0, 1) is a constant independent of n. Consider
the point process that consists of exactly Poisson(θ n) points, i.e., {X1, X2, . . . , XPoisson(θ n)}. This point process is a Poisson point process with intensity θ n /µ(Xfree) by Lemma 11.
Let A˜n,m denote the event that there exists two vertices Xi and Xi in the vertex set of the RRT∗ algorithm such that Xi and Xi are connected with an edge in G˜n, where G˜n is the graph returned by the RRT∗ when the algorithm is run for Poisson(θ n) many iterations, i.e., Poisson(θ n)
samples are drawn from Xfree. Clearly, P(Acn,m) = P(A˜cn,m | {Poisson(θ n) = n}). Moreover,

P(Acn,m) ≤ P(A˜cn,m) + P({Poisson(θ n) > n}).

since P(Acn,m) is non-increasing with n (see, e.g., Penrose, 2003). Since θ < 1, P({Poisson(θ n) > n}) ≤ e−a n, where a > 0 is a constant independent of n.

73

To compute P(Acn,m), a number of deﬁnitions are provided. Let Nn,m denote the number of

vertices that lie in the interior of Bn,m.

Clearly, E[Nn,m] =

ζd γRd RT∗ µ(Xfree )

log n, for all m ∈ {1, 2, . . . , Mn}.

For

notational

simplicity,

deﬁne

α

:=

. ζd γRd RT∗
µ(Xfree )

Let

∈ (0, 1) be a constant independent of n. Deﬁne

the event

Cn,m, := {Nn,m ≥ (1 − ) E[Nn,m]} = {Nn,m ≥ (1 − ) α log n}

Since Nn,m, is binomially distributed, its large deviations from its mean can be bounded as follows (Penrose, 2003),

P Cnc,m, = P({Nn,m, ≤ (1 − ) E[Nn,m]}) ≤ e−α H( ) log n = n−αH( ),

where H( ) = + (1 − ) log(1 − ). Notice that H( ) is a continuous function of with H(0) = 0 and H(1) = 1. Hence, H( ) can be made arbitrary close to one by taking close to one.
Then,

P(A˜cn,m) = P(A˜cn,m | Cn,m, ∩ Cn,m+1, ) P(Cn,m, ∩ Cn,m+1, ) +P(A˜cn,m | (Cn,m, ∩ Cn,m+1, )c) P((Cn,m, ∩ Cn,m+1, )c)
≤ P(A˜cn,m | Cn,m, ∩ Cn,m+1, ) P(Cn,m, ∩ Cn,m+1, ) + P(Cnc,m, ) + P(Cnc,m+1, ),

where the last inequality follows from the union bound. First, using the spatial independence of the underlying point process,

P (Cn,m, ∩ Cn,m+1, ) = P (Cn,m, ) P (Cn,m+1, ) ≤ n−2 α H( ).

Second, observe that P(Acn,m | Nn,m = k, Nn,m+1 = k ) is a non-increasing function of both k and k , since the probability of the event A˜n,m can not increase with the increasing number of points in both balls, Bn,m and Bn,m+1. Then,
P(A˜cn,m | Cn,m, ∩ Cn,m+1, ) = P(A˜cn,m | {Nn,m ≥ (1 − ) α log Nn,m, Nn,m+1 ≥ (1 − ) α log Nn,m+1}) ≤ P(A˜cn,m | {Nn,m = (1 − ) α log Nn,m, Nn,m+1 = (1 − ) α log Nn,m+1})

The term on the right hand side is one minus the probability that the maximum of α log n

number of uniform samples drawn from [0, 1] is smaller than the minimum of α log n number of

samples again drawn from [0, 1], where all the samples are drawn independently. This probability

can be calculated as follows. From the order statistics of uniform distribution, the minimum

of α log n points sampled independently and uniformly from [0, 1] has the following probability

distribution function:

(1 − x)α log n−1

fmin(x)

=

Beta(1, α

, log(n))

where Beta(·, ·) is the Beta function (also called the Euler integral) (Abramowitz and Stegun, 1964). The maximum of the same number of independent uniformly distributed random variables with support [0, 1] has the following cumulative distribution function:

Fmax(x) = xα log n

74

Then,

P(A˜cn,m | Cn,m, ∩ Cn,m+1, ) ≤

1
Fmax(x) fmin(x) dx
0

Gamma((1 − ) α log n) Gamma((1 − ) log n) =
2 Gamma(2(1 − ) α log(n))

((1 − ) α log n)! ((1 − ) α log n)! ≤
2 (2 (1 − ) α log n)!

((1 − ) α log n)! =
2(2(1 − ) α log n)(2(1 − ) α log n − 1) · · · 1

≤

1

= n− log(2) (1− ) α ,

2(1− ) α log n

where Gamma(·) is the gamma function (Abramowitz and Stegun, 1964). Then,

P(A˜cn,m) ≤ n−α 2 H( )+log(2) (1− ) + 2 n−α H( ).
Since 2 H( ) + log(2) (1 − ) and H( ) are both continuous and increasing in the interval (0.5, 1), the former is equal to 2 − log(4) > 0.5 and the latter is equal to 1 as approaches one from below, there exists some ¯ ∈ (0.5, 1) such that both 2 H(¯) + log(2) (1 − ¯) > 0.5 and H(¯) > 0.5. Thus,
P(A˜cn,m) ≤ n−α/2 + 2 n−α/2 = 3 n−α/2.
Hence,
P(Acn,m) ≤ P(A˜cn,m) + P(Poisson(θ n) > n) ≤ 3 n−α/2 + e−a n

Recall that An denotes the event that An,m holds for all m ∈ {1, 2, . . . , Mn}. Then,

P(Acn) = P

Mn

c

m=1 An,m

=P

Mn m=1

Acn,m

Mn

≤

P Acn,m

m=1

= Mn P(Acn,1),

where the last inequality follows from the union bound. The number of balls in Bn can be bounded

as

n 1/d

|Bn| = Mn ≤ β log n

,

where β is a constant. Combining this with the inequality above,

P(Acn) ≤ β

n 1/d log n

3 n−α/2 + e−a n ,

which is summable for α > 2 (1 + 1/d). Thus, by the Borel-Cantelli lemma, the probability that Acn occurs inﬁnitely often is zero, i.e., P(lim supn→∞ Acn) = 0, which implies that An occurs for all large n with probability one, i.e., P(lim infn→∞ An) = 1.

75

G.4 Convergence to the optimal path
The proof of the following lemma is similar to that of Lemma 55, and is omitted here. Let Pn denote the set of all paths in the graph returned by RRT∗ algorithm at the end of n
iterations. Let σn be the path that is closest to σn in terms of the bounded variation norm among all those paths in Pn, i.e., σn := minσ ∈Pn σ − σn . Lemma 72 The random variable σn − σn BV converges to zero almost surely, i.e.,
P limn→∞ σn − σn BV = 0 = 1. A corollary of the lemma above is that limn→∞ σn = σ∗ with probability one. Then, the result follows by the robustness of the optimal solution (see the proof of Lemma 56 for details).
76

