IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Typesetting math: 100%

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Journals & Magazines > IEEE Transactions on Image Pr... > Volume: 29
Towards Unsupervised Deep Image Enhancement With Generative Adversarial Network
Publisher: IEEE
Cite This
PDF
  << Results   
Zhangkai Ni ; Wenhan Yang ; Shiqi Wang ; Lin Ma ; Sam Kwong
All Authors
View Document
5
Paper
Citations
1459
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Related Work
    III.
    Proposed Unsupervised GAN for Image Enhancement
    IV.
    Experimental Results
    V.
    Analysis and Discussions

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
Media
More Like This

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: Improving the aesthetic quality of images is challenging and eager for the public. To address this problem, most existing algorithms are based on supervised learning meth... View more
Metadata
Abstract:
Improving the aesthetic quality of images is challenging and eager for the public. To address this problem, most existing algorithms are based on supervised learning methods to learn an automatic photo enhancer for paired data, which consists of low-quality photos and corresponding expert-retouched versions. However, the style and characteristics of photos retouched by experts may not meet the needs or preferences of general users. In this paper, we present an unsupervised image enhancement generative adversarial network (UEGAN), which learns the corresponding image-to-image mapping from a set of images with desired characteristics in an unsupervised manner, rather than learning on a large number of paired images. The proposed model is based on single deep GAN which embeds the modulation and attention mechanisms to capture richer global and local features. Based on the proposed model, we introduce two losses to deal with the unsupervised image enhancement: (1) fidelity loss, which is defined as a ℓ2 regularization in the feature domain of a pre-trained VGG network to ensure the content between the enhanced image and the input image is the same, and (2) quality loss that is formulated as a relativistic hinge adversarial loss to endow the input image the desired characteristics. Both quantitative and qualitative results show that the proposed model effectively improves the aesthetic quality of images. Our code is available at: https://github.com/eezkni/UEGAN.
Published in: IEEE Transactions on Image Processing ( Volume: 29 )
Page(s): 9140 - 9151
Date of Publication: 22 September 2020
ISSN Information:
PubMed ID: 32960763
INSPEC Accession Number: 19978798
DOI: 10.1109/TIP.2020.3023615
Publisher: IEEE
Funding Agency:
Contents
SECTION I.
Introduction

With the rapid development of mobile Internet, smart electronic devices, and social networks, it is becoming more and more popular to record and upload the wonderful lives of people through social media and online sharing communities. However, due to the high cost of high-quality hardware devices and the lack of professional photography skills, the aesthetic quality of photos taken by the general public is often unsatisfactory. Professional image-editing is expensive, and it is hard to provide such services in an automated manner as aesthetic feelings and preferences are usually a personal issue. Therefore, the automatic image enhancement techniques providing the user-oriented image beautification are preferred.

Compared with high-quality images, low-quality images usually suffer from multiple degradations in visual quality, such as poor colors, low contrast, and intensive noises et al . Therefore, the image enhancement process needs to address this degradation with a series of enhancement operations, such as contrast enhancement, color correction, and details adjustment et al . The earliest conventional image enhancement approaches mainly focused on contrast enhancement of low-quality image [1] – [2] [3] . The most common histogram adjustment transfers the luminance histogram of a low-quality image to a given distribution (may be provided by other reference images) to stretch the contrast of the low-quality image. According to the transformation scope, this kind of method can be further classified into two categories: global histogram equalization (GHE) [2] , [4] and local histogram equalization (LHE) [3] , [5] . The former uses a single histogram transformation function to adjust all pixels of the entire image. It may lead to improper enhancement results in some local regions, such as under-exposure, over-exposure, color distortion, et al . To address this issue, the LHE derives the content adaptive transform functions based on the statistical information in local region and applies these transforms locally. However, the LHE is computationally complex and not always powerful because the extracted transformation depends on the dominating information in the local region. Therefore, they are also easy to generate visually unsatisfactory texture details, dull or over-saturated color.

For the past few years, deep convolutional neural networks (CNN) have made significant progress in low-level vision tasks [6] – [7] [8] . In order to improve the modeling capacity and adaptivity, deep learning-based models are built to introduce the excellent expressive power of deep networks to facilitate automatic image enhancement with the knowledge of big data. Ignatov et al. [7] designed an end-to-end deep learning network that improves photos from mobile devices to the quality of digital single-lens reflex (DSLR) photos. Ren et al. [9] present a hybrid loss to optimize the framework from three aspects ( i.e., color, texture, and content) to produce more visually pleasing results. Inspired by bilateral grid processing, Gharbi et al. [6] made real-time image enhancement possible, which dynamically generates the image transformation based on local and global information. To deal with low-light image enhancement, Wang et al. [10] established a large-scale under-exposed image dataset and learned an image-to-illumination mapping based on the Retinex model to enhance extremely low-light images.

However, these methods follow the route of fully supervised learning relying on large-scale datasets with paired low / high -quality images. First, paired data is usually expensive, and sometimes it takes a lot of effort and resources to build the dataset by professional photographers. Second, the judgment of image quality is usually closely related to the personality, aesthetics, taste, and experience of a person. “There are a thousand Hamlets in a thousand people’s eyes.” In other words, everyone has his/her different attitude towards the quality of the photography. To demonstrate this, a typical low-quality photo in MIT-Adobe FiveK Dataset [11] and its corresponding five high-quality versions retouched by five different experts in photo beautification, are shown in Fig. 1 , respectively. It can be observed that the images processed by one expert are very different from the image retouched by another expert. Consequently, it is impractical to create a large-scale dataset with paired low and high-quality images to meets the preference of everyone. On the contrary, a more feasible way is to express the personal preferences of a user by providing a set of image collections that he/she loves. Therefore, an urgent demand is needed to build an enhancement model to learn the enhancement mapping from the low-quality dataset to a high-quality one even without the specific paired images. In this way, we can get rid of the burden of creating one-to-one paired data and rely only on the target dataset with the desired characteristics preferred by someone.
Fig. 1. - An illustration of different expert-retouched versions of a low-quality photo in MIT-Adobe FiveK dataset [11]. (a) is the low-quality photo and (b) to (f) are different high-quality counterparts retouched by different experts. The obvious perceptual differences exist among different high-quality versions.
Fig. 1.

An illustration of different expert-retouched versions of a low-quality photo in MIT-Adobe FiveK dataset [11] . (a) is the low-quality photo and (b) to (f) are different high-quality counterparts retouched by different experts. The obvious perceptual differences exist among different high-quality versions.

Show All

Benefit from the development of generative adversarial learning [12] – [13] [14] and reinforcement learning (RL) [15] , some works make attempts to handle the image enhancement tasks only with the help of unpaired data. The milestone work of transferring image style between unpaired data is CycleGAN [12] . It employs two generators and two discriminators and uses cycle consistency loss to achieve visually impressive results. Chen et al. [13] proposes to construct a bi-directional GAN with three improvements to transfer low-quality images into corresponding high-quality ones, and the experimental results show that this model is significantly better than CycleGAN. Hu et al. [15] design the first RL-based framework to train an effective photo post-processing model. Jiang et al. [16] carry out the first study on the task of low-light enhancement with an unsupervised framework. The method applies a self-regularized attention generator and dual discriminators to guide the generator globally and locally.

Rather than utilizing a cyclic generative adversarial network (GAN) to learn bi-directional mappings between the low-quality photos and high-quality ones, we build a unidirectional GAN to address the image aesthetic quality enhancement task, called the unsupervised image enhancement GAN (UEGAN). Inspired by the properties as mentioned above, our network consists of a joint global and local generator and a multi-scale discriminator with effective constraints. 1) The generator consists of an encoder and decoder with a global attention module and a modulation module embedded, which adjusts the features at different scales locally and globally. The multi-scale discriminator also inspects the results at different levels of granularity and guides the generator to produce better results to obtain global consistency and finer details. 2) To keep the content invariance, a fidelity loss is introduced to regularize the consistency between the input content and resulting content. 3) The global features extracted from the entire image reveal high-level information such as lighting conditions and color distributions. To capture these properties, the global attention module is designed to adjust pixels according to the information of a local neighborhood to meet both local adaptivity and global consistency. 4) For preventing over-enhancement, an identity loss is introduced to constrain the consistency between the enhanced result of the input high-quality image and the input one. This benefits controlling the enhancement procedure to be more quality-free and thus prevents over-enhancement. The main contributions of this work are summarized as follows:

    We design a single GAN framework that gets rid of the needs of paired training data for image aesthetic quality enhancement. To the best of our knowledge, this is the first trial to employ a unidirectional GAN framework to apply unsupervised learning to enhance the aesthetic quality of images (instead of low-light image enhancement).

    We propose a global attention module and a modulation module to construct the joint global and local generator to capture global features and adaptively adjust the local features. Together with the proposed multi-scale discriminator to inspect the quality of the generated results at different scales, well-enhanced results in perception and aesthetics are produced with both global consistency and finer details.

    We propose to jointly use quality loss , fidelity loss , and identity loss to train our model to make it towards extracting quality-free features and controlling the enhancement procedure to be more robust to the quality change. Thus, our method can obtain more reasonable results and prevent over-enhancement. Extensive experimental results on various datasets demonstrate the superiority of the proposed model quantificationally and qualitatively.

The remaining of this paper is organized as follows. In Section II , the related work is succinctly described. In Section III , the proposed unsupervised image aesthetic quality enhancement model is presented in detail. In Section IV , extensive experimental results of the proposed are reported. In Section V , the ablation studies and analysis are presented. Finally, Section VI draws the conclusion.
SECTION II.
Related Work
A. Traditional Image Enhancement

Extensive research has been conducted over the past few decades to improve the quality of photos. Most existing conventional image enhancement algorithms aim to stretch contrast and improve sharpness. The following three types of approaches are the most representative: histogram adjustment , unsharp masking , and Retinex-based approaches. These approaches are succinctly described as follows.
1) Histogram Adjustment:

Based on the basic idea of mapping the luminance histogram to a specific distribution, many methods estimate the mapping function based on the statistical information of the entire image [2] , [4] , [17] , while the details usually tend to be over-enhanced due to the dominance of some high-frequency information. Instead of estimating a single mapping function for the entire image, other approaches dynamically adjust the histogram based on local statistical information [3] , [5] , [18] . However, higher computational complexity limits the applicability of this method.
2) Unsharp Masking:

Unsharp masking (UM) aims to improve image sharpness [19] . The framework of the UM approach can be summarized into the following two phases: First, the input image is decomposed into a base layer and a detail layer by applying a low or high pass filter. Second, all pixels in the detail layer are scaled by a single global weighting factor, or different pixels are adaptively scaled by pixel-wise weighting factors, and then added back to the base layer to obtain an enhanced version. Various works have been proposed to improve the performance of UM from two aspects: 1) design a more reasonable layer decomposition method to decouple different frequency bands [20] , [21] ; and 2) propose a better estimation algorithm for the adjustment scaling factor [19] , [22] .
3) Retinex-Based Approaches:

Many researchers are working on Retinex-based image enhancement due to clear physical meaning. The basic assumption of the Retinex model is that the observed photo can be decomposed into reflection and illumination [23] . The enhanced image depends on the decomposed layer, i.e. , illumination and reflectance layers. Therefore, the Retinex-based model is usually approached as an illumination estimation problem [24] – [25] [26] . However, such approaches might generate unnatural results due to the ambiguity and difficulty in accurately estimating the illumination and reflection map.
B. Learning-Based Image Enhancement
1) Supervised Learning Approaches:

Given the explosive growth of CNN, image enhancement models based on learning methods have emerged in large numbers with impressive results. Yan et al. [8] took the first step in exploring the use of CNN for photo editing. Ignatov et al. [7] build a large-scale DSLR Photo Enhancement Dataset ( i.e. , DPED), which consists of 6K photos captured simultaneously by a DSLR camera and three smartphones, respectively. With the paired data, it is easy to learn a mapping function between the low-quality photos captured by smartphones and the high-quality photos captured by the professional DSLR camera. Ren et al. [9] proposed a hybrid framework to address the low-light enhancement problem by jointly considering the content and structure. However, the promising performance of these models is inseparable from the premise of a large number of pairs of degraded images and corresponding high-quality counterparts.
2) Unsupervised Learning Approaches:

Different from super-resolution, deraining, and denoising, the high-quality images are usually already present, and their low-quality versions can be easily generated by degrading them. In most cases, the image enhancement requires generating high-quality counterparts from low-quality images if need paired low-/high-quality during the training phase. High-quality photos are usually obtained by experts using professional photo editing programs ( i.e. , Adobe Photoshop and Lightroom) to retouch low-quality photos. This is expensive, time-consuming, and the editing style might depend heavily on the expert rather than the real users. In order to get rid of paired training data, a few works attempted to address the image enhancement issue with unsupervised learning. Inspired by the well-known CycleGAN [12] , Chen et al. [13] designed a dual GAN model to learn a bi-directional mapping between the source domain and target domain. Specifically, the learned transformation from the source domain to the target domain is first used to generate the high-quality image, and then the inverse mapping from the target domain to the source domain is learned to translate the generated high-quality image back to the source domain. The cycle consistency loss is constrained to enforce the closeness between the input low-quality photos and those generated by the reverse translation. The cycle consistency works well if both bi-directional generators provide an ideal mapping between the two domains. However, the instability of GAN increases training difficulty and risk to local minima when the cycle consistency is applied.
SECTION III.
Proposed Unsupervised GAN for Image Enhancement
A. Motivations and Objectives

We observe that professional photographer usually follows these instincts when performing image editing:

    Combination of global transformation and local adjustment. The content and intrinsic semantic information should be kept the same between the low-quality and retouched versions. The expert might first perform a global transformation based on the overall lighting conditions ( e.g. , well-exposure or under/over-exposure) and tone ( e.g. , cool or warm colors) in the scenes. The local corrections then make finer adjustments based on the joint consideration of both global information and local content.

    Over-enhancement prevention. The trade-off between fidelity and quality is crucial. Over-enhancement donates the visual effects caused by excessively enhancing the properties of images related to the aesthetic feeling, such as very warm colors, high contrast, and over-exposure, etc . However, this can also make the results to deviate from fidelity and produce unnatural results. That is, a good automatic photo enhancer should be aware of over-enhancement while producing good visual effects.

Base on the observations mentioned above, we are dedicated to learning an image-to-image mapping function F to generate the high-quality counterpart x g of a given low-quality photo x l , which can be modeled as follows,
x g = F ( x l ) . (1)
View Source Right-click on figure for MathML and additional features. \begin{equation*} x_{g} = \mathscr {F}(x_{l}).\tag{1}\end{equation*} One critical issue in image enhancement tasks is how to define quality as well as high quality. Any user can easily provide a collection of images expressing their personal preferences without explicitly stating the quality he/she loves. Therefore, rather than defining F as various clearly defined rules, it is better to formulate it as a process of transforming low-quality image distribution under the guidance of the desired high-quality image distribution. This promotes us to learn a user-oriented photo enhancer based on unpaired data in an unsupervised manner. Based on this consideration, we make efforts in utilizing the set-level supervision of GAN to achieve our goals through adversarial learning.

B. Network Architecture
1) Joint Global and Local Generator:

The generator plays a crucial role in our proposed UEGAN as it directly affects the quality of the final generated photos. The expert might first perform a global transform based on the overall lighting conditions or tone in the scenes. Therefore, the global features act as an image prior to guiding the generation and adjusting the local features. Based on this observation, we first propose a global attention module (GAM) to exploit the global attention of local features. Each channel of feature maps is extracted from the local neighborhood by the convolution layer. The focus of global attention is the ‘holistic’ understanding of each channel. In order to model the global attention of the intermediate features z ∈ R C × H × W , our proposed method can be summarized as the following three steps as shown in Fig. 3 : 1) extracting global statistics information f m pool ( ⋅ ) of each channel via Eqn. (2) ; 2) digging the inter-channel relationship ρ using the extracted g mean via the multi-layer perceptron f FC ( ⋅ ) in Eqn. (3) ; 3) fusing global and local features via Eqn. (4) .
g mean = ρ = z ^ = f m pool ( z ) , f FC ( g mean ) , Conv ( C ( E ( ρ ) , z ) ) , (2) (3) (4)
View Source Right-click on figure for MathML and additional features. \begin{align*} g_{\text {mean}}=&f_{\text {pool}}^{\text {m}}(z), \tag{2}\\ \rho=&f_{\text {FC}}(g_{\text {mean}}), \tag{3}\\ \hat {z}=&\text {Conv}(C(E(\rho), z)),\tag{4}\end{align*} where f m pool ( ⋅ ) means the average pooling operation, f FC ( ⋅ ) is two fully-connected layers, E ( ⋅ ) represents expanding the spatial dimension of ρ to that of z , C ( ⋅ ) is the concatenation operation, and C o n v ( ⋅ ) is a convolution layer.

Fig. 2. - The framework of the proposed UEGAN for image enhancement. The blue, red, and black lines indicate the low-quality images data flow, high-quality images data flow, and features data flow, respectively. The generator only inputs low-quality or high-quality images at a time.
Fig. 2.

The framework of the proposed UEGAN for image enhancement. The blue, red, and black lines indicate the low-quality images data flow, high-quality images data flow, and features data flow, respectively. The generator only inputs low-quality or high-quality images at a time.

Show All
Fig. 3. - The structure of the global attention module, where $E$ and $C$ denote the expanding and concatenation operations, respectively.
Fig. 3.

The structure of the global attention module, where E and C denote the expanding and concatenation operations, respectively.

Show All

Fig. 2 shows the proposed modulation module (MM) in the joint global and local generator. In particular, we use skip connections between encoder and decoder at different scales locally and globally to prevent the information loss caused by resolution change. Unlike traditional U-Net [27] , the features of the encoder are concatenated to those of the symmetric decoder at each stage ( i.e. , four stages in our model). Our proposed modulation module learns to generate two branches of features and then merge them together with the multiplication operation. In our model, to further reuse the features, the learned modulation layer multiples the features of the first stage of the encoder and those of the penultimate layer by element-wise multiplication. Learning global features and feature modulation can effectively enhance the visual effect of the resulting image. The global features can also guide to penalize some low-quality features that might lead to visual artifacts or poorly reconstructed details. Complex image processing can be approximated by a set of simple local smoothing curves [28] , the proposed joint global and local generator G is more capable than traditional U-Net for learning complex mappings from low-quality images to high-quality ones.
2) Multi-Scale Discriminator:

In order to distinguish between real high-quality image and generated “pseudo” high-quality image, the discriminator requires a large receptive field to capture the global characteristics. This directly leads to the need for deeper networks or larger convolution kernels. The last layer of the discriminator usually captures the information from a larger region of the image and can guide the generator to produce the image with better global consistency. However, the intermediate layer of the discriminator with a smaller receptive field can force the generator to pay more attention to finer details. Based on this observation, as shown in Fig. 2 , we propose a multi-scale discriminator D that uses multi-scale features to guide the generator to produce images with both global consistency and finer details.
C. Loss Function
1) Quality Loss:

We use quality loss to adapt the distribution of enhanced results to that of high-quality images. The quality loss guides the generator to produce more visually pleasing results. In the previous GAN frameworks, the discriminator aims at distinguishing between real samples and the generated ones. However, we observe that simply applying the discriminator D to separate generated images and real high-quality images is not enough to obtain a good generator that transfers low-quality images into high-quality ones. The reason might be lies in that the quality ambiguity between low/high-quality images, some images in the low-quality image set are better than those in the high-quality image set. To address this issue, we also train the discriminator to distinguish between real low-quality images and real high-quality images as shown in Fig. 2 .

Specifically, our proposed discriminator is based on the recently proposed relativistic discriminator structure [29] , which not only assesses the probability that the real data ( i.e. , real high-quality image) is more authentic than the fake data ( i.e. , generated high-quality image or real low-quality image), but also guides the generator to produce high-quality images more realistic than real high-quality images. In addition, we employ an improved form of the relativistic discriminator, Relativistic average HingeGAN (RaHingeGAN) [29] , [30] as follows:
L D = L G qua = E x l ∼ P l [ max ( 0 , 1 + ( D ( x l ) − E x h ∼ P h D ( x h ) ) ) ] + E x h ∼ P h [ max ( 0 , 1 − ( D ( x h ) − E x l ∼ P l D ( x l ) ) ) ] + E x g ∼ P g [ max ( 0 , 1 + ( D ( x g ) − E x h ∼ P h D ( x h ) ) ) ] + E x h ∼ P h [ max ( 0 , 1 − ( D ( x h ) − E x g ∼ P g D ( x g ) ) ) ] , E x h ∼ P h [ max ( 0 , 1 + ( D ( x h ) − E x g ∼ P g D ( x g ) ) ) ] + E x g ∼ P g [ max ( 0 , 1 − ( D ( x g ) − E x h ∼ P h D ( x h ) ) ) ] , (5) (6)
View Source Right-click on figure for MathML and additional features. \begin{align*} \mathcal {L}^{D}_{}=&\mathbb {E}_{x_{l}\sim P_{l}}\left [{\text {max}(0, 1 + (D(x_{l}) - E_{x_{h}\sim P_{h}}D(x_{h})))}\right]\qquad \\&+ \mathbb {E}_{x_{h}\sim P_{h}}\left [{\text {max}(0, 1 - (D(x_{h})-E_{x_{l}\sim P_{l}}D(x_{l})))}\right]\qquad \\&+ \mathbb {E}_{x_{g}\sim P_{g}}\left [{\text {max}(0, 1 + (D(x_{g}) - E_{x_{h}\sim P_{h}}D(x_{h})))}\right] \qquad \\&+ \mathbb {E}_{x_{h}\sim P_{h}}\left [{\text {max}(0, 1\! -\! (D(x_{h})-E_{x_{g}\sim P_{g}}D(x_{g})))}\right], \qquad \tag{5}\\ \mathcal {L}^{G}_{\text {qua}}=&\mathbb {E}_{x_{h}\sim P_{h}}\left [{\text {max}(0, 1 + (D(x_{h}) - E_{x_{g}\sim P_{g}}D(x_{g})))}\right] \qquad \\&+ \mathbb {E}_{x_{g}\sim P_{g}}\left [{\text {max}(0, 1 \!-\! (D(x_{g})-E_{x_{h}\sim P_{h}}D(x_{h})))}\right],\tag{6}\end{align*} where x l , x h , and x g denote the real low-quality image, real high-quality image, and generated high-quality image, respectively.

2) Fidelity Loss:

Since we train our model for image enhancement in an unsupervised manner, the quality loss itself might not ensure that the generated image has similar content to that of the input low-quality image. The simplest way is to measure the distance between the input and output images in the pixel domain. However, we cannot employ this strategy because the generated high-quality image is typically different from the input low-quality image in the pixel domain due to contrast stretching and color rendering. Therefore, we use fidelity loss to constrain the training of the generator, so as to achieve the purpose of generated high-quality images and inputting low-quality images with similar content. The fidelity loss is defined as the ℓ 2 norm between the feature maps of the input low-quality image and those of the generated high-quality images extracted by the pre-trained VGG network [31] as follows:
L fid = ∑ J j = 1 { E x l ∼ P l [ ∥ ϕ j ( x l ) − ϕ j ( G ( x l ) ) ∥ 2 ] } , (7)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \mathcal {L}_{\text {fid}} = \sum \nolimits _{j=1}^{J}\{\mathbb {E}_{x_{l}\sim P_{l}}\left [{\left \|{\phi _{j}(x_{l}) - \phi _{j}(G(x_{l}))}\right \|_{2}}\right] \},\tag{7}\end{equation*} where ϕ j ( ⋅ ) indicates the process of extracting the feature maps obtained by the j t h layer of the VGG network and J is the total number of layers used. Specifically, the R e l u _ 1 _ 1 , R e l u _ 2 _ 1 , R e l u _ 3 _ 1 , R e l u _ 4 _ 1 , and R e l u _ 5 _ 1 layers of VGG-19 network are adopted in this work.

3) Identity Loss:

The identity loss is defined as ℓ 1 distance between the input high-quality image and the corresponding output of the generator G as follows:
L idt = E x h ∼ P h [ ∥ x h − G ( x h ) ∥ 1 ] . (8)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \mathcal {L}_{\text {idt}} = \mathbb {E}_{x_{h}\sim P_{h}}\left [{\left \|{x_{h} - G(x_{h})}\right \|_{1}}\right].\tag{8}\end{equation*} The identity loss is calculated based on high-quality input images. Therefore, if the color distribution and contrast of the input image meet the characteristics of the high-quality image set, the identity loss intends to encourage preservation of the color distributions and contrast between the input and output. It ensures that the generator should make almost no changes to the image in content, contrast, and color during the image enhancement process. As a result, the identity loss makes it possible to simultaneously maintain the content, color rendering, and contrast of the input high-quality image.

3) Total Loss:

By jointly considering quality loss , fidelity loss , and identity loss , our final loss is defined as the weighted sum of these losses which as follows:
L total = λ qua L G qua + λ fid L fid + λ idt L idt , (9)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \mathcal {L}_{\text {total}} =\lambda _{\text {qua}}\mathcal {L}^{G}_{\text {qua}} + \lambda _{\text {fid}}\mathcal {L}_{\text {fid}} + \lambda _{\text {idt}}\mathcal {L}_{\text {idt}},\tag{9}\end{equation*} where λ qua , λ fid , and λ idt are weighting parameters to balance the relative importance of L G qua , L fid and L idt .

SECTION IV.
Experimental Results
A. Dataset
1) MIT-Adobe FiveK Dataset:

This dataset was constructed by Bychkovsky et al. [11] for the image enhancement task, where high-quality images are generated by experts retouching. It consists of 5000 raw photos and 25,000 retouched photos generated from those raw photos by five experienced photographers. Therefore, this dataset includes five subsets, each with 5,000 raw and corresponding retouched photo pairs. Following the works in [13] , [15] , we select the retouched photos generated by photographer C as the target photos ( i.e. , ground truth) since the user rates this subset best. In order to generate unpaired training data, the subset is randomly divided into three partitions: 1) the first partition has 2,250 raw photos as low-quality input; 2) the second partition consists of retouched version of another 2,250 raw photos and served as the desired high-quality photos; 3) the last partition is the remaining 500 raw photos used for validation (100 images) and testing (400 images). These three parts have no overlaps with each other.
2) Flickr Dataset:

In addition to training on the photographer results of the MIT-Adobe FiveK Dataset, we also collected a high-quality image collection from Flickr for unpaired training. These images are crawled from the Flickr images tagged with “High Dynamic Range” to ensure relatively consistent quality and then manually selected by the authors. Finally, we select 2,000 images as the desired high-quality labels.
B. Implementation Details

We built our network in Pytorch and train it for 150 epochs on an NVidia GeForce RTX 2080 Ti GPU with a mini-batch size of 10. The entire network is optimized from scratch using Adam optimizer [32] with a learning rate of 0.0001. The leaning rate is fixed at the first 75 epochs and then linearly decays to zero in the next 75 epochs. For the MIT-Adobe FiveK Dataset, we use Lightroom to decode the images into the png format and resize the long side of the images to 512 resolution. For data augmentation, we randomly cropped 256 × 256 patches from images.

In the MIT-Adobe FiveK Dataset, we set the hyper-parameters λ qua , λ fid , and λ idt as 0.05, 1, and 0.1, respectively as empirically these values provide the best performance in quantitative and qualitative performance. When coming to the Flickr Dataset, the hyper-parameters λ qua , λ fid , and λ idt are also empirically set as 0.05, 1 and 0.1.
C. Evaluation Metrics

The most commonly-used full-reference image quality assessment metrics ( i.e. , PSNR and SSIM) focus only on signal fidelity but may not accurately reflect aesthetic and perceptual quality. Although the evaluation of aesthetic quality is challenging, we still have the tool to measure the enhancement quality to an extent with the quantitative evaluation. To this end, the NIMA [33] score is used to quantify the aesthetic quality. The NIMA is an effective CNN-based image aesthetic quality assessment method trained on the large-scale aesthetic dataset AVA [34] . It is predicts the distribution of human opinion scores rather than the mean opinion scores ( i.e , MOS). Therefore, we use PSNR, SSIM, and NIMA to compare our proposed method with the state-of-the-art methods at the pixel level, structural level, and aesthetics level, where the first two metrics are performed in terms of the similarity between the enhanced results and the corresponding expert-retouched ( i.e. , ground truth). In general, higher PSNR, SSIM and NIMA values correspond to reasonably better results.
D. Quantitative Comparison

Most previous methods for automatic photo quality enhancement are based on supervised learning that requires paired data [6] , [7] , [9] – [10] [11] . Recently, a series of works based on GANs or reinforcement learning (RL) attempted to use only unpaired data to solve this tasks. We compared our proposed method with CycleGAN [12] , and three unpaired photo enhancement methods: Deep Photo Enhancer (DPE) [13] , EnlightenGAN [16] and Exposure [15] . CycleGAN, DPE, and EnlightenGAN are GAN-based methods and Exposure is an RL and filter-based method.

Table I lists the quantitative comparison results of various models on MIT-Adobe FiveK dataset [11] . In this table, the best performance of each evaluation metric ( i.e. , PSNR, SSIM, and NIMA) is boldfaced in black. Please note that the program codes of all models under comparison are downloaded from the link provided by the corresponding authors. Specifically, we used the codes provided by the corresponding authors to retrain the CycleGAN and EnlightenGAN on MIT-Adobe FiveK dataset. We test the Exposure and DPE using the models pre-trained on MIT-Adobe FiveK dataset provided by the corresponding authors, because it achieved better performance than our retrained model. Besides, the Flickr dataset we collected has no ground truth, thus we can only perform qualitative experiments on it. From Table I , one can observe that our proposed UEGAN achieves the best performance in terms of PSNR, SSIM, and NIMA compared with other state-of-the-art image quality enhancement methods trained with unpaired data.
TABLE I Quantitative Comparison Between Our Proposed Method and State-of-the-Art Methods on MIT-Adobe FiveK Dataset [11]
Table I- Quantitative Comparison Between Our Proposed Method and State-of-the-Art Methods on MIT-Adobe FiveK Dataset [11]

From the experimental results listed in Table I , the following conclusions can be drawn. 1) Our proposed UEGAN, DPE, and Exposure are ranked in the top three in the quantitative comparison and are superior to inputs on all three evaluation metrics. Specifically, our proposed UEGAN has consistently achieved the best performance. 2) Compared with the input, CycleGAN has been obtained worse performance in SSIM and NIMA, which is mainly due to the existence of blocking artifacts in the generated results. 3) Similar to CycleGAN, EnlightenGAN even performed worse on all three evaluation metrics than the input, which may be caused by significant changes in contrast.
E. Qualitative Comparison

Besides the superiority in quantitative evaluation, our proposed UEGAN method is also superior to other enhancement methods in qualitative comparison. As shown in Fig. 4 – 7 , four representative test images were selected from MIT-Adobe FiveK dataset for conducting visual comparisons. One can observe that the input images are diverse and challenging, including: 1) Fig. 4 (a) is an outdoor scene with normal lighting condition; 2) Fig. 5 (a) is a landscape image with under-exposed lake surface and buildings; 3) Fig. 6 (a) is a sky scene with a tiny airplane; 4) Fig. 7 (a) is a globally under-exposed outdoor scene with little portrait details. Compared to their respective expert retouched versions shown in Fig. 4 (d) – Fig. 7 (d) , all input images have significantly worse visual experiences. Additional results are provided in the supplementary material.
Fig. 4. - Visual quality comparison with state-of-the-art methods (i.e., CycleGAN, DPE, EnlightenGAN, and Exposure) on a test image from the MIT-Adobe FiveK [11] dataset.
Fig. 4.

Visual quality comparison with state-of-the-art methods ( i.e. , CycleGAN, DPE, EnlightenGAN, and Exposure) on a test image from the MIT-Adobe FiveK [11] dataset.

Show All
Fig. 5. - Visual quality comparison with state-of-the-art methods (i.e., CycleGAN, DPE, EnlightenGAN, and Exposure) on a test image from the MIT-Adobe FiveK [11] dataset.
Fig. 5.

Visual quality comparison with state-of-the-art methods ( i.e. , CycleGAN, DPE, EnlightenGAN, and Exposure) on a test image from the MIT-Adobe FiveK [11] dataset.

Show All
Fig. 6. - Visual quality comparison with state-of-the-art methods (i.e., CycleGAN, DPE, EnlightenGAN, and Exposure) on a test image from the MIT-Adobe FiveK [11] dataset.
Fig. 6.

Visual quality comparison with state-of-the-art methods ( i.e. , CycleGAN, DPE, EnlightenGAN, and Exposure) on a test image from the MIT-Adobe FiveK [11] dataset.

Show All
Fig. 7. - Visual quality comparison with state-of-the-art methods (i.e., CycleGAN, DPE, EnlightenGAN, and Exposure) on a test image from the MIT-Adobe FiveK [11] dataset.
Fig. 7.

Visual quality comparison with state-of-the-art methods ( i.e. , CycleGAN, DPE, EnlightenGAN, and Exposure) on a test image from the MIT-Adobe FiveK [11] dataset.

Show All

As shown, we obtain some interesting insights. First, the proposed UEGAN trained on our collected Flickr dataset shows the best visual quality among all methods as it generates vivid colors and clear textures. Besides, the results of our proposed UEGAN trained on MIT-Adobe FiveK dataset are satisfactory in enhancing the input image. Second, CycleGAN is less effective in generating vivid colors and also leads to blocking artifacts, which degrade the image quality. In contrast, our method generates visually pleasing results with clear details and sharp structures. Third, Exposure is a filter-based method, which tends to produce over-saturated results and falsely remove textures. However, the results of our proposed UEGAN look natural with good color rendition. Fourth, EnlightenGAN significantly changes the contrast but makes the resulting images dull. On the contrary, our proposed UEGAN can generate satisfactory contrast and natural appearance with the appropriate saturation. Last, DPE produces competitive results compared with ours in structure and contrast enhancement, while it may generate unrealistically looking results. In short, our proposed UEGAN generates natural and pleasing results with satisfactory contrast, vibrant colors and clear details, which is superior to the state-of-the-art methods compared and comparable to the corresponding expert-retouched results.
F. User Study

Our ultimate goal is to learn the implicit characteristics of the target domain to generate high-quality images with similar properties. To measure the subjective quality, we have performed a user study with 28 participants and 40 image sets ( e.g. , each image set contains 1 test image and the corresponding six generated versions) using pairwise comparisons on six methods (including two versions of our method). The participants are asked to choose his/her favorite result from the displayed pair and the generated images are presented randomly to avoid subjective bias. The corresponding pairwise comparison results are shown in Table II , where each figure indicates the number of times the method in that row outperforms the method in that column. It can be seen that, in all cases, the results of DPE and our proposed UEGAN are preferred much more frequently than the results of other models ( i.e. , CycleGAN, Exposure, and EnlightenGAN). Among all the comparison methods, the preferred percentages of the proposed UEGAN trained on MIT-Adobe FiveK Dataset [8] over CycleGAN, Exposure, EnlightenGAN, and DPE are respectively 93.39%, 77.41%, 86.34%, and 64.20%, and the preferred percentages of our UEGAN trained on our collect Flickr dataset compared with CycleGAN, Exposure, EnlightenGAN, and DPE are 97.14%, 87.41%, 91.78%, and 80.90%, respectively. It can be seen that the proposed model is selected more frequently than the compared models, which means that the proposed UEGAN can produce more visually pleasing results than all state-of-the-art models in the comparison.
TABLE II The Pairwise Comparison Preference Matrix in User Study. EG Denotes EnlightenGAN
Table II- The Pairwise Comparison Preference Matrix in User Study. EG Denotes EnlightenGAN

To measure the overall quality, we again randomly selected 100 test images and the corresponding 100 generated results for each model. Each time, six enhanced versions of a test image are present randomly to the participants and asked them to select their favorite one. Finally, 2800 subjective votes are obtained in total and the results are shown in Fig. 8 . The results show that the enhanced results obtained by our proposed UEGAN are preferred more frequently than those by other methods in the comparison. This further reveals that the proposed UEGAN is superior to all state-of-the-art models in improving the aesthetic quality of the photos.
Fig. 8. - User preference results of different aesthetic quality enhancement algorithms.
Fig. 8.

User preference results of different aesthetic quality enhancement algorithms.

Show All

SECTION V.
Analysis and Discussions
A. Ablation Studies
1) Loss Analysis:

In this section, we study the effect of quality loss , fidelity loss , and identity loss quantitatively and qualitatively. Table III shows the PSNR, SSIM, and NIMA results achieved by using L G qua + L fid and L G qua + L fid + L idt . We can observe that using only L G qua + L fid loss achieves better perfomance than the state-of-the-art DPE [13] , EnlightenGAN [16] and Exposure [15] , but adding identity loss could further improve quantization performance ( i.e. , PSNR, SSIM, and NIMA). Fig. 9 shows two visual comparisons between the results of our proposed UEGAN trained with L G qua + L fid and L G qua + L fid + L idt , respectively. It can be observed that the two results generated by our model are more visually pleasing than the input in Fig. 9 (a) . However, compared with the ground truth in Fig. 9 (d) , adding identity loss can suppress over-enhancement to some extent to produce more realistic colors and contrast, as shown in Fig. 9 (b) and (c) .
TABLE III Average PSNR, SSIM, and NIMA Results of Enhanced Results on MIT-Adobe FiveK Dataset [11]
Table III- Average PSNR, SSIM, and NIMA Results of Enhanced Results on MIT-Adobe FiveK Dataset [11]
Fig. 9. - Visual quality comparison results of our proposed UEGAN trained with different loss. (a) Inputs. (b) $\mathcal {L}^{G}_{\text {qua}} + \mathcal {L}_{\text {fid}}$ . (c) $\mathcal {L}^{G}_{\text {qua}} + \mathcal {L}_{\text {fid}} + \mathcal {L}_{\text {idt}}$ . (d) Expert-retouched (i.e., Ground Truth).
Fig. 9.

Visual quality comparison results of our proposed UEGAN trained with different loss. (a) Inputs. (b) L G qua + L fid . (c) L G qua + L fid + L idt . (d) Expert-retouched ( i.e. , Ground Truth).

Show All

Fig. 10 shows the results generated by our proposed UEGAN by fixing the weighting parameters of L fid and L idt at 1.0 and 0.1, respectively, and increasing that of L G qua from 0.05 to 0.4, respectively. We can observe that if we increase the weight of the L G qua , the contrast becomes higher and the colors will be more vivid, but the result tends to be over-enhanced and thus loses fidelity. Therefore, we jointly consider fidelity loss, quality loss, and identity loss to improve the visual effect as much as possible while keeping the content the same and avoiding over-enhancement.
Fig. 10. - Visual quality comparison results of fidelity loss vs. quality loss. (a) Inputs. (b) - (d) are results obtained by fixing the weighting parameters of $\mathcal {L}_{\text {fid}}$ and $\mathcal {L}_{\text {idt}}$ at 1.0 and 0.1, respectively, and setting $\mathcal {L}^{G}_{\text {qua}}$ to 0.05, 0.2, and 0.4, respectively.
Fig. 10.

Visual quality comparison results of fidelity loss vs. quality loss. (a) Inputs. (b) - (d) are results obtained by fixing the weighting parameters of L fid and L idt at 1.0 and 0.1, respectively, and setting L G qua to 0.05, 0.2, and 0.4, respectively.

Show All

2) Architecture Analysis:

In this section, we investigate the effect of each individual component ( i.e. , global attention module (GAM) and modulation module (MM)) in our proposed UEGAN described in Section III-B . We conduct ablation studies by comparing the proposed UEGAN with the following UEGAN variants: 1) GAM + U-Net: removing the MM and concatenating the features of the first stage of the encoder to those of the penultimate layer; 2) GAM + MM-P: we apply the MM at the pixel level. That is, the generator learns a modulation layer that multiplies the input image with the features of the last layer; 3) UEGAN w/o GAM: removing the GAM from the proposed generator. 4) UEGAN w/o GAM and MM: removing both the GAM and MM from the generator. The quantitative comparison results of all the different architectures are shown in Table IV . It can be observed that, compared with the traditional U-Net ( i.e. , GAM+U-Net), our proposed UEGAN achieves the best improvements. Using MM at the feature level can significantly improve the performance than that at the pixel level ( i.e. , GAM+MM-P). Both GAM or MM lead to better PSNR, SSIM, and NIMA, and combining them can further improve the quantitative performance to achieve the best.
TABLE IV Comparison of Average PSNR, SSIM, and NIMA Performance of Different Network Architectures on MIT-Adobe FiveK Dataset [11]
Table IV- Comparison of Average PSNR, SSIM, and NIMA Performance of Different Network Architectures on MIT-Adobe FiveK Dataset [11]

B. Limitations

The proposed method is completely unsupervised and inevitably has limitations. A typical artifact that is present on the resulting image is color deviation. For example, the color of the ground of the second image in the first row of Fig. 11 is different from that of the input and ground truth. Even though they might produce more pleasing results sometimes coincidentally, this kind of adjustment changes the content and makes the results look unreal. In addition, as shown by the blue box in the second row of Fig. 11 , our method cannot remove noise from the generated results. However, this kind of noise is common in under-exposed images.
Fig. 11. - Failure cases generated by our method compared with the ground truth. (a) Inputs. (b) Our results. (c) Ground truth.
Fig. 11.

Failure cases generated by our method compared with the ground truth. (a) Inputs. (b) Our results. (c) Ground truth.

Show All

SECTION VI.
Conclusion

In this paper, we present an unsupervised deep generative adversarial network model developed for image enhancement, call the Unsupervised image Enhancement GAN (UEGAN). The proposed model is able to learn the corresponding image-to-image mapping from a set of images provided by public users with desired characteristics in an unsupervised manner, which makes it possible to learn a user-oriented automatic photo enhancer. We embed the global attention module (GAM) and modulation module (MM) into the generator to capture global features and adjust the features adaptively. In addition, we combine fidelity loss, quality loss, and identity loss with the proposed network to improve the visual quality of the enhanced results. The quantitative and qualitative experimental results show that our proposed method UEGAN is superior to the four state-of-the-art methods.

Authors
Figures
References
Citations
Keywords
Metrics
Media
   Back to Results   
More Like This
Image Enhancement and Feature Extraction Based on Low-Resolution Satellite Data

IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing

Published: 2015
Hyperspectral Stimulated Raman Scattering Image Enhancement, Denoising and Segmentation via a Deep Neural-Net

2021 Photonics North (PN)

Published: 2021
Show More
References
1. T. Arici, S. Dikbas and Y. Altunbasak, "A histogram modification framework and its application for image contrast enhancement", IEEE Trans. Image Process. , vol. 18, no. 9, pp. 1921-1935, Sep. 2009.
Show in Context View Article Full Text: PDF (6476) Google Scholar
2. G. Thomas, D. Flores-Tapia and S. Pistorius, "Histogram specification: A fast and flexible method to process digital images", IEEE Trans. Instrum. Meas. , vol. 60, no. 5, pp. 1565-1578, May 2011.
Show in Context View Article Full Text: PDF (2522) Google Scholar
3. C. Lee, C. Lee and C.-S. Kim, "Contrast enhancement based on layered difference representation of 2D histograms", IEEE Trans. Image Process. , vol. 22, no. 12, pp. 5372-5384, Dec. 2013.
Show in Context View Article Full Text: PDF (3454) Google Scholar
4. D. Coltuc, P. Bolon and J.-M. Chassery, "Exact histogram specification", IEEE Trans. Image Process. , vol. 15, no. 5, pp. 1143-1152, May 2006.
Show in Context View Article Full Text: PDF (3792) Google Scholar
5. M. Abdullah-Al-Wadud, M. H. Kabir, M. A. A. Dewan and O. Chae, "A dynamic histogram equalization for image contrast enhancement", IEEE Trans. Consum. Electron. , vol. 53, no. 2, pp. 593-600, May 2007.
Show in Context View Article Full Text: PDF (2070) Google Scholar
6. M. Gharbi, J. Chen, J. T. Barron, S. W. Hasinoff and F. Durand, "Deep bilateral learning for real-time image enhancement", ACM Trans. Graph. , vol. 36, no. 4, pp. 118, 2017.
Show in Context CrossRef Google Scholar
7. A. Ignatov, N. Kobyshev, R. Timofte and K. Vanhoey, "DSLR-quality photos on mobile devices with deep convolutional networks", Proc. IEEE Int. Conf. Comput. Vis. , pp. 3277-3285, Oct. 2017.
Show in Context View Article Full Text: PDF (6959) Google Scholar
8. Z. Yan, H. Zhang, B. Wang, S. Paris and Y. Yu, "Automatic photo adjustment using deep neural networks", ACM Trans. Graph. , vol. 35, no. 2, pp. 11, 2016.
Show in Context CrossRef Google Scholar
9. W. Ren et al., "Low-light image enhancement via a deep hybrid network", IEEE Trans. Image Process. , vol. 28, no. 9, pp. 4364-4375, Sep. 2019.
Show in Context View Article Full Text: PDF (6052) Google Scholar
10. R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng and J. Jia, "Underexposed photo enhancement using deep illumination estimation", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , pp. 6849-6857, Jun. 2019.
Show in Context View Article Full Text: PDF (3284) Google Scholar
11. V. Bychkovsky, S. Paris, E. Chan and F. Durand, "Learning photographic global tonal adjustment with a database of input/output image pairs", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , pp. 97-104, Jun. 2011.
Show in Context Google Scholar
12. J.-Y. Zhu, T. Park, P. Isola and A. A. Efros, "Unpaired image-to-image translation using cycle-consistent adversarial networks", Proc. IEEE Int. Conf. Comput. Vis. , pp. 2223-2232, Oct. 2017.
Show in Context View Article Full Text: PDF (4554) Google Scholar
13. Y.-S. Chen, Y.-C. Wang, M.-H. Kao and Y.-Y. Chuang, "Deep photo enhancer: Unpaired learning for image enhancement from photographs with GANs", Proc. IEEE Conf. Comput. Vis. And Pattern Recognit. , pp. 6306-6314, Jun. 2018.
Show in Context View Article Full Text: PDF (2682) Google Scholar
14. C. H. Lin, C.-C. Chang, Y.-S. Chen, D.-C. Juan, W. Wei and H.-T. Chen, "COCO-GAN: Generation by parts via conditional coordinating", Proc. IEEE Int. Conf. Comput. Vis. , pp. 4512-4521, Oct. 2019.
Show in Context View Article Full Text: PDF (8698) Google Scholar
15. Y. Hu, H. He, C. Xu, B. Wang and S. Lin, "Exposure: A white-box photo post-processing framework", ACM Trans. Graph. , vol. 37, no. 2, pp. 26, 2018.
Show in Context CrossRef Google Scholar
16. Y. Jiang et al., "EnlightenGAN: Deep light enhancement without paired supervision" in arXiv:1906.06972, 2019, [online] Available: http://arxiv.org/abs/1906.06972.
Show in Context Google Scholar
17. H. Ibrahim and N. P. Kong, "Brightness preserving dynamic histogram equalization for image contrast enhancement", IEEE Trans. Consum. Electron. , vol. 53, no. 4, pp. 1752-1758, Nov. 2007.
Show in Context View Article Full Text: PDF (1797) Google Scholar
18. J. A. Stark, "Adaptive image contrast enhancement using generalizations of histogram equalization", IEEE Trans. Image Process. , vol. 9, no. 5, pp. 889-896, May 2000.
Show in Context View Article Full Text: PDF (1397) Google Scholar
19. W. Ye and K.-K. Ma, "Blurriness-guided unsharp masking", IEEE Trans. Image Process. , vol. 27, no. 9, pp. 4465-4477, Sep. 2018.
Show in Context View Article Full Text: PDF (2434) Google Scholar
20. S. K. Mitra, H. Li, I.-S. Lin and T.-H. Yu, "A new class of nonlinear filters for image enhancement", Proc. IEEE Int. Conf. Acoust. Speech Signal Process. , pp. 2525-2528, Apr. 1991.
Show in Context View Article Full Text: PDF (753) Google Scholar
21. K. He, J. Sun and X. Tang, "Guided image filtering", IEEE Trans. Pattern Anal. Mach. Intell. , vol. 35, no. 6, pp. 1397-1409, Jun. 2013.
Show in Context View Article Full Text: PDF (8315) Google Scholar
22. A. Polesel, G. Ramponi and V. J. Mathews, "Image enhancement via adaptive unsharp masking", IEEE Trans. Image Process. , vol. 9, no. 3, pp. 505-510, Mar. 2000.
Show in Context View Article Full Text: PDF (580) Google Scholar
23. M. Li, J. Liu, W. Yang, X. Sun and Z. Guo, "Structure-revealing low-light image enhancement via robust retinex model", IEEE Trans. Image Process. , vol. 27, no. 6, pp. 2828-2841, Jun. 2018.
Show in Context View Article Full Text: PDF (8528) Google Scholar
24. S. Wang, J. Zheng, H.-M. Hu and B. Li, "Naturalness preserved enhancement algorithm for non-uniform illumination images", IEEE Trans. Image Process. , vol. 22, no. 9, pp. 3538-3548, Sep. 2013.
Show in Context View Article Full Text: PDF (1847) Google Scholar
25. X. Guo, Y. Li and H. Ling, "LIME: Low-light image enhancement via illumination map estimation", IEEE Trans. Image Process. , vol. 26, no. 2, pp. 982-993, Feb. 2017.
Show in Context View Article Full Text: PDF (9438) Google Scholar
26. Z. Ying, G. Li, Y. Ren, R. Wang and W. Wang, "A new low-light image enhancement algorithm using camera response model", Proc. IEEE Int. Conf. Comput. Vis. Workshops , pp. 3015-3022, Oct. 2017.
Show in Context View Article Full Text: PDF (2609) Google Scholar
27. O. Ronneberger, P. Fischer and T. Brox, "U-Net: Convolutional networks for biomedical image segmentation", Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent , pp. 234-241, 2015.
Show in Context Google Scholar
28. J. Chen, A. Adams, N. Wadhwa and S. W. Hasinoff, "Bilateral guided upsampling", ACM Trans. Graph. , vol. 35, no. 6, pp. 1-8, Nov. 2016.
Show in Context CrossRef Google Scholar
29. A. Jolicoeur-Martineau, "The relativistic discriminator: A key element missing from standard GAN" in arXiv:1807.00734, 2018, [online] Available: http://arxiv.org/abs/1807.00734.
Show in Context Google Scholar
30. H. Zhang, I. Goodfellow, D. Metaxas and A. Odena, "Self-attention generative adversarial networks", Proc. Int. Conf. Mach. Learn. , pp. 7354-7363, 2019.
Show in Context Google Scholar
31. K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition" in arXiv:1409.1556, 2014, [online] Available: http://arxiv.org/abs/1409.1556.
Show in Context Google Scholar
32. D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization" in arXiv:1412.6980, 2014, [online] Available: http://arxiv.org/abs/1412.6980.
Show in Context Google Scholar
33. H. Talebi and P. Milanfar, "NIMA: Neural image assessment", IEEE Trans. Image Process. , vol. 27, no. 8, pp. 3998-4011, Aug. 2018.
Show in Context View Article Full Text: PDF (6150) Google Scholar
34. N. Murray, L. Marchesotti and F. Perronnin, "AVA: A large-scale database for aesthetic visual analysis", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , pp. 2408-2415, Jun. 2012.
Show in Context View Article Full Text: PDF (1112) Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
