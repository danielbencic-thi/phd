AN INTRODUCTION TO OPTIMIZATION

WILEY SERIES IN DISCRETE MATHEMATICS AND OPTIMIZATION
A complete list of titles in this series appears at the end of this volume.

AN INTRODUCTION TO OPTIMIZATION
Fourth Edition Edwin K. P. Chong
Colorado State University
Stanislaw H. 2ak
Purdue University
®WILEY
A JOHN WILEY & SONS, INC., PUBLICATION

Copyright © 2013 by John Wiley & Sons, Inc. All rights reserved

Published by John Wiley & Sons, Inc., Hoboken, New Jersey

Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permission.

Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing this book, they make no representations or warranties with respect to the accuracy or completeness of the contents of this book and specifically disclaim any implied warranties of merchantability or fitness for a particular purpose. No warranty may be created or extended by sales representatives or written sales materials. The advice and strategies contained herein may not be suitable for your situation. You should consult with a professional where appropriate. Neither the publisher nor author shall be liable for any loss of profit or any other commercial damages, including but not limited to special, incidental, consequential, or other damages.

For general information on our other products and services or for technical support, please contact our Customer Care Department within the United States at (800) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002.

Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic formats. For more information about Wiley products, visit our web site at www.wiley.com.

Library of Congress Cataloging-in-Publication Data

Chong, Edwin Kah Pin.

An introduction to optimization / Edwin K. P. Chong, Colorado State University, Stanislaw H. Zak,

Purdue University. — Fourth edition,

pages cm

Summary: "The purpose of the book is to give the reader a working knowledge of optimization theory

and methods" — Provided by publisher.

Includes bibliographical references and index.

ISBN 978-1-118-27901-4 (hardback)

1. Mathematical optimization. I. Zak, Stanislaw H. II. Title.

QA402.5.C476 2012

519.6—dc23

2012031772

Printed in the United States of America.

10 9 8 7 6 5 4 3 2 1

To my wife, Yat-Yee, and to my parents, Paul
and Julienne Chong. Edwin K. P. Chong
To JMJ; my wife, Mary Ann; and my parents, Janina and
Konstanty Zak. Stanislaw H. Zak

CONTENTS

Preface

xiii

PART I MATHEMATICAL REVIEW

1 Methods of Proof and Some Notation

3

1.1 Methods of Proof

3

1.2 Notation

5

Exercises

6

2 Vector Spaces and Matrices

7

2.1 Vector and Matrix

7

2.2 Rank of a Matrix

13

2.3 Linear Equations

17

2.4 Inner Products and Norms

19

Exercises

22

3 Transformations

25

3.1 Linear Transformations

25

viii

CONTENTS

3.2 Eigenvalues and Eigenvectors

26

3.3 Orthogonal Projections

29

3.4 Quadratic Forms

31

3.5 Matrix Norms

35

Exercises

40

Concepts from Geometry

45

4.1 Line Segments

45

4.2 Hyperplanes and Linear Varieties

46

4.3 Convex Sets

48

4.4 Neighborhoods

50

4.5 Polytopes and Polyhedra

52

Exercises

53

Elements of Calculus

55

5.1 Sequences and Limits

55

5.2 Differentiability

62

5.3 The Derivative Matrix

63

5.4 Differentiation Rules

67

5.5 Level Sets and Gradients

68

5.6 Taylor Series

72

Exercises

77

PART II UNCONSTRAINED OPTIMIZATION

Basics of Set-Constrained and Unconstrained Optimization

81

6.1 Introduction

81

6.2 Conditions for Local Minimizers

83

Exercises

93

One-Dimensional Search Methods

103

7.1 Introduction

103

7.2 Golden Section Search

104

7.3 Fibonacci Method

108

7.4 Bisection Method

116

7.5 Newton's Method

116

7.6 Secant Method

120

7.7 Bracketing

123

CONTENTS

IX

7.8 Line Search in Multidimensional Optimization

124

Exercises

126

8 Gradient Methods

131

8.1 Introduction

131

8.2 The Method of Steepest Descent

133

8.3 Analysis of Gradient Methods

141

Exercises

153

9 Newton's Method

161

9.1 Introduction

161

9.2 Analysis of Newton's Method

164

9.3 Levenberg-Marquardt Modification

168

9.4 Newton's Method for Nonlinear Least Squares

168

Exercises

171

10 Conjugate Direction Methods

175

10.1 Introduction

175

10.2 The Conjugate Direction Algorithm

177

10.3 The Conjugate Gradient Algorithm

182

10.4 The Conjugate Gradient Algorithm for Nonquadratic

Problems

186

Exercises

189

11 Quasi-Newton Methods

193

11.1 Introduction

193

11.2 Approximating the Inverse Hessian

194

11.3 The Rank One Correction Formula

197

11.4 The DFP Algorithm

202

11.5 The BFGS Algorithm

207

Exercises

211

12 Solving Linear Equations

217

12.1 Least-Squares Analysis

217

12.2 The Recursive Least-Squares Algorithm

227

12.3 Solution to a Linear Equation with Minimum Norm

231

12.4 Kaczmarz's Algorithm

232

12.5 Solving Linear Equations in General

236

X

CONTENTS

Exercises

244

13 Unconstrained Optimization and Neural Networks

253

13.1 Introduction

253

13.2 Single-Neuron Training

256

13.3 The Backpropagation Algorithm

258

Exercises

270

14 Global Search Algorithms

273

14.1 Introduction

273

14.2 The Nelder-Mead Simplex Algorithm

274

14.3 Simulated Annealing

278

14.4 Particle Swarm Optimization

282

14.5 Genetic Algorithms

285

Exercises

298

PART III LINEAR PROGRAMMING

15 Introduction to Linear Programming

305

15.1 Brief History of Linear Programming

305

15.2 Simple Examples of Linear Programs

307

15.3 Two-Dimensional Linear Programs

314

15.4 Convex Polyhedra and Linear Programming

316

15.5 Standard Form Linear Programs

318

15.6 Basic Solutions

324

15.7 Properties of Basic Solutions

327

15.8 Geometric View of Linear Programs

330

Exercises

335

16 Simplex Method

339

16.1 Solving Linear Equations Using Row Operations

339

16.2 The Canonical Augmented Matrix

346

16.3 Updating the Augmented Matrix

349

16.4 The Simplex Algorithm

350

16.5 Matrix Form of the Simplex Method

357

16.6 Two-Phase Simplex Method

361

16.7 Revised Simplex Method

364

Exercises

369

CONTENTS

XI

17 Duality

379

17.1 Dual Linear Programs

379

17.2 Properties of Dual Problems

387

Exercises

394

18 Nonsimplex Methods

403

18.1 Introduction

403

18.2 Khachiyan's Method

405

18.3 Affine Scaling Method

408

18.4 Karmarkar's Method

413

Exercises

426

19 Integer Linear Programming

429

19.1 Introduction

429

19.2 Unimodular Matrices

430

19.3 The Gomory Cutting-Plane Method

437

Exercises

447

PART IV NONLINEAR CONSTRAINED OPTIMIZATION

20 Problems with Equality Constraints

453

20.1 Introduction

453

20.2 Problem Formulation

455

20.3 Tangent and Normal Spaces

456

20.4 Lagrange Condition

463

20.5 Second-Order Conditions

472

20.6 Minimizing Quadratics Subject to Linear Constraints

476

Exercises

481

21 Problems with Inequality Constraints

487

21.1 Karush-Kuhn-Tucker Condition

487

21.2 Second-Order Conditions

496

Exercises

501

22 Convex Optimization Problems

509

22.1 Introduction

509

22.2 Convex Functions

512

22.3 Convex Optimization Problems

521

xii

CONTENTS

22.4 Semidefinite Programming

527

Exercises

540

23 Algorithms for Constrained Optimization

549

23.1 Introduction

549

23.2 Projections

549

23.3 Projected Gradient Methods with Linear Constraints

553

23.4 Lagrangian Algorithms

557

23.5 Penalty Methods

564

Exercises

571

24 Multiobjective Optimization

577

24.1 Introduction

577

24.2 Pareto Solutions

578

24.3 Computing the Pareto Front

581

24.4 From Multiobjective to Single-Objective Optimization

585

24.5 Uncertain Linear Programming Problems

588

Exercises

596

References

599

Index

609

PREFACE
Optimization is central to any problem involving decision making, whether in engineering or in economics. The task of decision making entails choosing among various alternatives. This choice is governed by our desire to make the "best" decision. The measure of goodness of the alternatives is described by an objective function or performance index. Optimization theory and methods deal with selecting the best alternative in the sense of the given objective function.
The area of optimization has received enormous attention in recent years, primarily because of the rapid progress in computer technology, including the development and availability of user-friendly software, high-speed and parallel processors, and artificial neural networks. A clear example of this phenomenon is the wide accessibility of optimization software tools such as the Optimization Toolbox of MATLAB1and the many other commercial software packages.
There are currently several excellent graduate textbooks on optimization theory and methods (e.g., [3], [39], [43], [51], [87], [88], [104], [129]), as well as undergraduate textbooks on the subject with an emphasis on engineering design (e.g., [1] and [109]). However, there is a need for an introductory
1MATLAB is a registered trademark of The MathWorks, Inc.
xiii

XIV

PREFACE

textbook on optimization theory and methods at a senior undergraduate or beginning graduate level. The present text was written with this goal in mind. The material is an outgrowth of our lecture notes for a one-semester course in optimization methods for seniors and beginning graduate students at Purdue University, West Lafayette, Indiana. In our presentation, we assume a working knowledge of basic linear algebra and multivariable calculus. For the reader's convenience, a part of this book (Part I) is devoted to a review of the required mathematical background material. Numerous figures throughout the text complement the written presentation of the material. We also include a variety of exercises at the end of each chapter. A solutions manual with complete solutions to the exercises is available from the publisher to instructors who adopt this text. Some of the exercises require using MATLAB. The student edition of MATLAB is sufficient for all of the MATLAB exercises included in the text. The MATLAB source listings for the MATLAB exercises are also included in the solutions manual.
The purpose of the book is to give the reader a working knowledge of optimization theory and methods. To accomplish this goal, we include many examples that illustrate the theory and algorithms discussed in the text. However, it is not our intention to provide a cookbook of the most recent numerical techniques for optimization; rather, our goal is to equip the reader with sufficient background for further study of advanced topics in optimization.
The field of optimization is still a very active research area. In recent years, various new approaches to optimization have been proposed. In this text, we have tried to reflect at least some of the flavor of recent activity in the area. For example, we include a discussion of randomized search methods—these include particle swarm optimization and genetic algorithms, topics of increasing importance in the study of complex adaptive systems. There has also been a recent surge of applications of optimization methods to a variety of new problems. An example of this is the use of descent algorithms for the training of feedforward neural networks. An entire chapter in the book is devoted to this topic. The area of neural networks is an active area of ongoing research, and many books have been devoted to this subject. The topic of neural network training fits perfectly into the framework of unconstrained optimization methods. Therefore, the chapter on feedforward neural networks not only provides an example of application of unconstrained optimization methods but also gives the reader an accessible introduction to what is currently a topic of wide interest.
The material in this book is organized into four parts. Part I contains a review of some basic definitions, notations, and relations from linear algebra, geometry, and calculus that we use frequently throughout the book. In Part II we consider unconstrained optimization problems. We first discuss some theoretical foundations of set-constrained and unconstrained optimization, including necessary and sufficient conditions for minimizers and maximizers. This is followed by a treatment of various iterative optimization algorithms, including line search methods, together with their properties. A discussion of global

PREFACE

XV

search algorithms is included in this part. We also analyze the least-squares optimization problem and the associated recursive least-squares algorithm. Parts III and IV are devoted to constrained optimization. Part III deals with linear programming problems, which form an important class of constrained optimization problems. We give examples and analyze properties of linear programs, and then discuss the simplex method for solving linear programs. We also provide a brief treatment of dual linear programming problems. We then describe some nonsimplex algorithms for solving linear programs: Khachiyan's method, the affine scaling method, and Karmarkar's method. We wrap up Part III by discussing integer linear programming problems. In Part IV we treat nonlinear constrained optimization. Here, as in Part II, we first present some theoretical foundations of nonlinear constrained optimization problems, including convex optimization problems. We then discuss different algorithms for solving constrained optimization problems. We also treat multiobjective optimization.
Although we have made every effort to ensure an error-free text, we suspect that some errors remain undetected. For this purpose, we provide online updated errata that can be found at the Web site for the book, accessible via
http://www.wiley.com/mathematics
We are grateful to several people for their help during the course of writing this book. In particular, we thank Dennis Goodman of Lawrence Livermore Laboratories for his comments on early versions of Part II and for making available to us his lecture notes on nonlinear optimization. We thank Moshe Kam of Drexel University for pointing out some useful references on nonsimplex methods. We are grateful to Ed Silverman and Russell Quong for their valuable remarks on Part I of the first edition. We also thank the students of ECE 580 at Purdue University and ECE 520 and MATH 520 at Colorado State University for their many helpful comments and suggestions. In particular, we are grateful to Christopher Taylor for his diligent proofreading of early manuscripts of this book. This fourth edition incorporates many valuable suggestions of users of the first, second, and third editions, to whom we are grateful.
E. K. P. CHONG AND S. H. ZAK
Fort Collins, Colorado, and West Lafayette, Indiana

PART I
MATHEMATICAL REVIEW

CHAPTER 1

METHODS OF PROOF AND SOME NOTATION

1.1 Methods of Proof

Consider two statements, "A" and "B," which could be either true or false. For example, let "A" be the statement "John is an engineering student," and let "B" be the statement "John is taking a course on optimization." We can combine these statements to form other statements, such as "A and B" or "A or B." In our example, "A and B" means "John is an engineering student, and he is taking a course on optimization." We can also form statements such as "not A," "not B," "not (A and B)," and so on. For example, "not A" means "John is not an engineering student." The truth or falsity of the combined statements depend on the truth or falsity of the original statements, "A" and "B." This relationship is expressed by means of truth tables; see Tables 1.1 and 1.2.
From the tables, it is easy to see that the statement "not (A and B)" is equivalent to "(not A) or (not B)" (see Exercise 1.3). This is called DeMorgan's law.
In proving statements, it is convenient to express a combined statement by a conditional, such as "A implies B," which we denote "A=>B." The conditional

An Introduction to Optimization, Fourth Edition.

3

By E. K. P. Chong and S. H. Zak. Copyright © 2013 John Wiley & Sons, Inc.

4

METHODS OF PROOF AND SOME NOTATION

Table 1.1 Truth Table for "A and B" and "A or B"

A B A and B A or B

F F

F

F

F T

F

T

T F

F

T

T T

T

T

Table 1.2 Truth Table for "not A"
A not A F Y~ T F

Table 1.3 Truth Table for Conditionals and Biconditionals

A B A ^ B A <=B A < ^ B

FF T

T

T

FT T

F

F

TF F

T

F

TT T

T

T

"A=>B" is simply the combined statement "(not A) or B" and is often also read "A only if B," or "if A then B," or "A is sufficient for B," or "B is necessary for A."
We can combine two conditional statements to form a biconditional statement of the form "A<i=>B," which simply means "(A=*-B) and (B=>A)." The statement "ΑΦ^Β" reads "A if and only if B," or "A is equivalent to B," or "A is necessary and sufficient for B." Truth tables for conditional and biconditional statements are given in Table 1.3.
It is easy to verify, using the truth table, that the statement "A=>B" is equivalent to the statement "(not B)=>(not A)." The latter is called the contrapositive of the former. If we take the contrapositive to DeMorgan's law, we obtain the assertion that "not (A or B)" is equivalent to "(not A) and (not B)."
Most statements we deal with have the form "A=>B." To prove such a statement, we may use one of the following three different techniques:
1. The direct method

NOTATION

5

2. Proof by contraposition
3. Proof by contradiction or reductio ad absurdum
In the case of the direct method, we start with "A," then deduce a chain of various consequences to end with "B."
A useful method for proving statements is proof by contraposition, based on the equivalence of the statements "A=>B" and "(not B)=>(not A)." We start with "not B," then deduce various consequences to end with "not A" as a conclusion.
Another method of proof that we use is proof by contradiction, based on the equivalence of the statements "A=>B" and "not (A and (not B))." Here we begin with "A and (not B)" and derive a contradiction.
Occasionally, we use the principle of induction to prove statements. This principle may be stated as follows. Assume that a given property of positive integers satisfies the following conditions:
■ The number 1 possesses this property.
■ If the number n possesses this property, then the number n + 1 possesses it too.
The principle of induction states that under these assumptions any positive integer possesses the property.
The principle of induction is easily understood using the following intuitive argument. If the number 1 possesses the given property, then the second condition implies that the number 2 possesses the property. But, then again, the second condition implies that the number 3 possesses this property, and so on. The principle of induction is a formal statement of this intuitive reasoning.
For a detailed treatment of different methods of proof, see [130].

1.2 Notation
Throughout, we use the following notation. If X is a set, then we write x € X to mean that x is an element of X. When an object x is not an element of a set X, we write x $. X. We also use the "curly bracket notation" for sets, writing down the first few elements of a set followed by three dots. For example, {xi,X2,^3,. · ·} is the set containing the elements χ\,Χ2,χζ, and so on. Alternatively, we can explicitly display the law of formation. For example, {x : x £ R, x > 5} reads "the set of all x such that x is real and x is greater than 5." The colon following x reads "such that." An alternative, notation for the same set is {x £ M : x > 5}.
If X and Y are sets, then we write X C Y to mean that every element of X is also an element of Y. In this case, we say that X is a subset of Y. If X and Y are sets, then we denote by X \ Y ("X minus Y") the set of all points in X that are not in Y. Note that X \ Y is a subset of X. The

6

METHODS OF PROOF AND SOME NOTATION

notation / : X —■> Y means " / is a function from the set X into the set V." The symbol := denotes arithmetic assignment. Thus, a statement of the form x := y means "x becomes y." The symbol = means "equals by definition."
Throughout the text, we mark the end of theorems, lemmas, propositions, and corollaries using the symbol □. We mark the end of proofs, definitions, and examples by | .
We use the IEEE style when citing reference items. For example, [77] represents reference number 77 in the list of references at the end of the book.

EXERCISES
1.1 Construct the truth table for the statement "(not B)=>(not A)," and use it to show that this statement is equivalent to the statement "A=^B."
1.2 Construct the truth table for the statement "not (A and (not B))," and use it to show that this statement is equivalent to the statement "A=>B."
1.3 Prove DeMorgan's law by constructing the appropriate truth tables.
1.4 Prove that for any statements A and B, we have "A <^ (A and B) or (A and (not B))." This is useful because it allows us to prove a statement A by proving the two separate cases "(A and B)" and "(A and (not B))." For example, to prove that \x\ > x for any x G M, we separately prove the cases "|x| > x and x > 0" and "|x| > x and x < 0." Proving the two cases turns out to be easier than proving the statement \x\ > x directly (see Section 2.4 and Exercise 2.7).
1.5 (This exercise is adopted from [22, pp. 80-81]) Suppose that you are shown four cards, laid out in a row. Each card has a letter on one side and a number on the other. On the visible side of the cards are printed the symbols
S 83 A
Determine which cards you should turn over to decide if the following rule is true or false: "If there is a vowel on one side of the card, then there is an even number on the other side."

CHAPTER 2
VECTOR SPACES AND MATRICES
2.1 Vector and Matrix We define a column n-vector to be an array of n numbers, denoted
ai
a — . a<2 The number α^ is called the zth component of the vector a. Denote by R the set of real numbers and by Rn the set of column n-vectors with real components. We call Rn an n-dimensional real vector space. We commonly denote elements of Rn by lowercase bold letters (e.g., x). The components of x £ Rn are denoted # i , . . . , xn.
We define a row n-vector as [αι,α2,.. ·,αη]·
An Introduction to Optimization, Fourth Edition. By E. K. P. Chong and S. H. Zak. Copyright © 2013 John Wiley & Sons, Inc.

8

VECTOR SPACES AND MATRICES

The transpose of a given column vector a is a row vector with corresponding elements, denoted aT. For example, if

02
a

then

aT = [αι,α2ii, - · · i ^n\

Equivalently, we may write a = [αχ, α 2 , . . . , α η ] τ . Throughout the text we adopt the convention that the term vector (without the qualifier row or col-

umn) refers to a column vector. Two vectors a = [ai, a 2 , . . . , an]T and b = [b\, 62, · · · ? M T a r e eQual if
ai — bi, i = 1,2,... ,n.

The sum of the vectors a and 6, denoted a + 6, is the vector

a + b= [ai H-6i,a2 + 6 2 , . . . , a n + 6 n ] T .

The operation of addition of vectors has the following properties:

1. The operation is commutative:

a + b = b + a.

2. The operation is associative: (a + b)-\-c = a + (b + c).

3. There is a zero vector such that

0 = [0,0,...,0]T a + Q = 0-\-a = a.

The vector
[Oi - & ι , α 2 - &2?· . · , Α η - &n]
is called the difference between a and b and is denoted a — b. The vector 0 - b is denoted —6. Note that

b + (a — b) — a,

— (a — b) = b — a.

VECTOR AND MATRIX

9

The vector b — a is the unique solution of the vector equation a + x = b.
Indeed, suppose that x = [xi, x2,..., xn]T is a solution to a + x = b. Then, a\+x\ =h, a2 + X2 = fo,

an -\- xn — on,

and thus

x = b — a.

We define an operation of multiplication of a vector a G Mn by a real scalar aGRas
αα = [ααι, αα2,. · ·, α;αη] .

This operation has the following properties:

1. The operation is distributive: for any real scalars a and /?,

a(a + 6) = aa + α&, (a + β)α — aa + /3a.

2. The operation is associative: α(βα) = (α/3)α.

3. The scalar 1 satisfies

la = a.

4. Any scalar a satisfies

a0 = 0.

5. The scalar 0 satisfies

0a = 0.

6. The scalar —1 satisfies

(—l)a = —a.

Note that aa = 0 if and only if a = 0 or a = 0. To see this, observe that aa = 0 is equivalent to ααι = aa2 = · · · = ααη = 0. If a = 0 or a = 0, then aa = 0. If a ^ 0, then at least one of its components α^ φ 0. For this component, αα^ = 0, and hence we must have a = 0. Similar arguments can be applied to the case when a / 0 .

10

VECTOR SPACES AND MATRICES

A set of vectors {αχ,... ,ak} is said to be linearly independent if the equal-
ity a\a\ + a2a2 + l· akak = 0
implies that all coefficients a*, i = 1 , . . . , fc, are equal to zero. A set of the vectors {αχ,..., ak} is linearly dependent if it is not linearly independent.
Note that the set composed of the single vector 0 is linearly dependent, for if a φ 0, then aO — 0. In fact, any set of vectors containing the vector 0 is linearly dependent.
A set composed of a single nonzero vector a φ 0 is linearly independent since aa = 0 implies that a = 0.
A vector a is said to be a linear combination of vectors αχ, a 2 , . . . , ak if there are scalars α χ , . . . , α^ such that

a = OL\a\ + α 2 α 2 Η + QfcOfc.

Proposition 2.1 A set of vectors { α ι , α 2 , . . . ,ak} is linearly dependent if

and only if one of the vectors from the set is a linear combination of the

remaining vectors.

□

Proof. =>: If {αι, a 2 , . . . , a^} is linearly dependent, then

OLICLI + a2a2 H l· α^α^ = 0,

where at least one of the scalars α; Φ 0, whence

OL\

di =

αι

OL2

OLk

a2 — · · ·

α^.

<=: Suppose that

αχ = α2α2 + α 3 α 3 Η h α^α^,

then

( - l ) a i + a2a2 H l· akak = 0.

Because the first scalar is nonzero, the set of vectors { α ι , α 2 , . . . ,α/c} is lin-

early dependent. The same argument holds if α^, i = 2,...,/c, is a linear

combination of the remaining vectors.

I

A subset V of Rn is called a subspace of Rn if V is closed under the operations of vector addition and scalar multiplication. That is, if a and b are vectors in V, then the vectors a + b and aa are also in V for every scalar a.
Every subspace contains the zero vector 0, for if a is an element of the subspace, so is (—I)a = — a. Hence, a — a — 0 also belongs to the subspace.
Let α ι , α 2 , . . . ,α^ be arbitrary vectors in W1. The set of all their linear combinations is called the span of αχ, α 2 , . . . , ak and is denoted

span[ai,a2,...,a/e] = < ^ α ^ : a x , . . . ,ak £ R > .

VECTOR AND MATRIX

11

Given a vector a, the subspace span [a] is composed of the vectors a o , where a is an arbitrary real number (a G R). Also observe that if a is a linear combination of αι, α 2 , . . . , α/~, then

s p a n [ a i , a 2 , . . . , α^,α] — s p a n [ a i , a 2 , . . . , α&].
The span of any set of vectors is a subspace. Given a subspace V, any set of linearly independent vectors
{oi, C&2,. · ·, a>k} C V such that V = span[ai, a 2 , . . . , a/-] is referred to as a basis of the subspace V. All bases of a subspace V contain the same number of vectors. This number is called the dimension of V, denoted dim V.

Proposition 2.2 If {ai, a 2 , . . . , a / J zs a fraszs 0/ V, t/ien an?/ vector aofV can be represented uniquely as

a = OL\CL\ + a 2 a 2 H h α^α^,

where a^ G R, z = 1, 2 , . . . , k.

□

Proof To prove the uniqueness of the representation of a in terms of the basis vectors, assume that

a = OL\a\ + α2α2 + · · · + ο^α/c
and α = βια,ι + ß 2 a 2 H h Α α ^ .
We now show that ai — βι, i = 1 , . . . , k. We have

α ι α ι + a 2 a 2 H h α^α^ = / ^ « l H- β2α2 Η h /3fcafc
or (ai - )3i)ai + (a2 - Α ) α 2 + · · · + (afc ~ ßk)ak = 0.
Because the set {a* : z = 1,2,..., A:} is linearly independent, OL\ — β\ = a2 — /?2 = · · · = a/e — ßk — 0, which implies that a* = /?*, 2 = 1 , . . . , fc. I

Suppose that we are given a basis {αχ, α 2 , . . . , α^} of V and a vector a G V such that
α = αχθι + a2a2 H h α^α^.
The coefficients a*, i = 1 , . . . , /c, are called the coordinates of a with respect to the basis {ai, a 2 , . . . , α^}.
The natural basis for Rn is the set of vectors

"1"

Ό"

0

1

0

0

, e2 =

0

0

_0.

.0.

12

VECTOR SPACES AND MATRICES

The reason for calling these vectors the natural basis is that

Xl

X2

x

X\e\ + £ 2 ^ 2 + · · · + Xn^n

We can similarly define complex vector spaces. For this, let C denote the set of complex numbers and C n the set of column n-vectors with complex components. As the reader can easily verify, the set Cn has properties similar to those of Rn, where scalars can take complex values.
A matrix is a rectangular array of numbers, commonly denoted by uppercase bold letters (e.g., A). A matrix with m rows and n columns is called an m x n matrix, and we write

an «12

a\n

Ö21 Ö22

d2n

Gml Om2
The real number α^ located in the ith row and jth column is called the (i, j ) t h entry. We can think of A in terms of its n columns, each of which is a column vector in Rm. Alternatively, we can think of A in terms of its m rows, each of which is a row n-vector.
Consider the ra x n matrix A above. The transpose of matrix A, denoted A , is the n x m matrix

an a2i
A1 =
Ö12 «22

0>ml «7712

that is, the columns of A are thGei nrow0s,2ηof A , and vice versa. Let the symbol MmXn denote the set oimxn matrices whose entries are real
numbers. We treat column vectors in IRn as elements of R n x l . Similarly, we treat row n-vectors as elements of R l x n . Accordingly, vector transposition is simply a special case of matrix transposition, and we will no longer distinguish between the two. Note that there is a slight inconsistency in the notation of row vectors when identified as 1 x n matrices: We separate the components of the row vector with commas, whereas in matrix notation we do not generally use commas. However, the use of commas in separating elements in a row helps to clarify their separation. We use use such commas even in separating matrices arranged in a horizontal row.

2.2 Rank of a Matrix Consider the m x n matrix
an au A =

RANK OF A MATRIX

13

«In
&2η

Let us denote the kth column of A by α^:

Q>ik

Q>k

Q>mk
The maximal number of linearly independent columns of A is called the rank of the matrix A, denoted rank A. Note that rank A is the dimension of span[ai,... ,an].
Proposition 2.3 The rank of a matrix A is invariant under the following operations:
1. Multiplication of the columns of A by nonzero scalars.

2. Interchange of the columns.

3. Addition to a given column a linear combination of other columns. D

Proof.

1. Let bk = a^a/e, where ctk φ 0, k = 1 , . . . , n, and let B = [61? f>2) · · ·, bn]. Obviously, span[ai,a2,...,on] = span[öi,62,... ,6n],

and thus

rank A = rankf?.

2. The number of linearly independent vectors does not depend on their order.

3. Let

b\ = a\ + C2a2 + l· cnan, b2 = a2,

Vn — Ö-n·

14

VECTOR SPACES AND MATRICES

So, for any OJI, . . . , a n ,

αι6ι + 0:262 H l· o;n6n = a\a\ + (c*2 + ^1^2)^2 H l· ( a n + aicn)an,

and hence

span[6i,62,··· ,&n] C s p a n [ a i , a 2 , . . . , a n ] .

On the other hand,

αι = 61 - c2b2
«2 = 62,

cn6n,

ttn — On.

Hence,

s p a n [ a i , a 2 , . . . , a n ] C span[6i, 62,. · · ,&n]·

Therefore, rank A = ranki?.

|

A matrix A is said to be square if the number of its rows is equal to the number of its columns (i.e., it is n x n). Associated with each square matrix A is a scalar called the determinant of the matrix A, denoted d e t A or \A\. The determinant of a square matrix is a function of its columns and has the following properties:
1. The determinant of the matrix A = [αι, a2,..., an] is a linear function of each column; that is,

det[ai,...,Ofc_i,aafc + ßak , a f c + i , . . . , a n ] = adet[ai,. ..,afc_i,afc ,afc+i,... ,an] + /?det[ai,... ,ak-\,ak , α ^ + ι , . . . , a n ]
for each a, /? G R, a^1}, a^2) € Rn. 2. If for some k we have α^ = α^+χ, then
d e t A = d e t [ a i , . . . , α ^ , α ^ + ι , . . . ,αη] = d e t [ a i , . . . ,a,k,ak, , On] = 0.

3. Let

In = [ e i , e 2 , . . . , e n ]

1 0 ... 0 0 1 ... 0
0 0··· 1

RANK OF A MATRIX

15

where {βχ,..., en} is the natural basis for Rn. Then d e t / n = 1.

Note that if a — β = 0 in property 1, then
d e t [ a i , . . . ,afc_i,0,afc+i,.. . , o n ] = 0.
Thus, if one of the columns is 0, then the determinant is equal to zero. The determinant does not change its value if we add to a column another
column multiplied by a scalar. This follows from properties 1 and 2 as shown below:
d e t [ a i , . . . ,afc_i,a/fc + actj, Ofc+i,... , α ? , . . . , an] = det[ai,... ,afc_i,afc,afc+i, · · · , a?·,... ,an] + adet[oi,..., afc_i, α^,α^+ι,.. . ,α?,... ,an] = det[ai,... ,an].
However, the determinant changes its sign if we interchange columns. To show this property, note that
det[oi,... ,afc_i,afc,afc+i,... ,an] = det[ai,.. . ,afc + afc+i,afc+i,... ,on] = det[ai,. ..,ak + ak+i,ak+i - (ak + a f c + i ) , . . . , a n ] = det[ai,. . .,afc + α&+ι, - a * , . . . , a n ] = -det[ai,. . ,afc + a/c+i,afc,... ,an] = - ( d e t [ a i , . . . ,<ifc,afc,... ,on] + d e t [ a i , . . . , a f c + i , a f c , . . . ,an]) = - d e t [ a i , . . . , afc+i, ak,..., a„].
A pth-order minor of an m x n matrix A, with p < min{m, n}, is the determinant of a p x p matrix obtained from A by deleting m — p rows and n — p columns. (The notation min{m, n) represents the smaller of m and n.)
We can use minors to investigate the rank of a matrix. In particular, we have the following proposition.
P r o p o s i t i o n 2.4 If an m x n (m > n) matrix A has a nonzero nth-order minor, then the columns of A are linearly independent; that is, rank A = n.
D
Proof. Suppose that A has a nonzero nth-order minor. Without loss of generality, we assume that the nth-order minor corresponding to the first n rows of A is nonzero. Let #;, i = 1 , . . . , n, be scalars such that
χιαι + x2a2 H l· xno>n = 0.

16

VECTOR SPACES AND MATRICES

The vector equality above is equivalent to the following set of m equations:

CLiiXi + a\2X2 + 021^1 + «22^2 +

+ «2n^n

an\X\ + an2X2 H l· annxn = 0

ümlXl + Am2^2 + h on

0.

For i = 1 , . . . , n, let

a>u
di

Then, χχαι + · · · -l· xnön = 0.

The nth-order minor is det[di, ά 2 , . . . , αη], assumed to be nonzero. From

the properties of determinants it follows that the columns άι, α<ι,..., an are

linearly independent. Therefore, all X{ = 0, i = 1 , . . . ,n. Hence, the columns

Oi, a 2 , . . . , an are linearly independent.

I

From the above it follows that if there is a nonzero minor, then the columns associated with this nonzero minor are linearly independent.
If a matrix A has an rth-order minor | M | with the properties (i) \M\ φ 0 and (ii) any minor of A that is formed by adding a row and a column of A to M is zero, then
rank A = r.
Thus, the rank of a matrix is equal to the highest order of its nonzero minor(s). A nonsingular (or invertible) matrix is a square matrix whose determinant
is nonzero. Suppose that A is an n x n square matrix. Then, A is nonsingular if and only if there is another n x n matrix B such that

AB = BA = J n ,

where In denotes the n x n identity matrix:

1 0 0 1

[0 0 · · · 1J We call the matrix B above the inverse matrix of A, and write B — A~ .

LINEAR EQUATIONS

17

2.3 Linear Equations

Suppose that we are given m equations in n unknowns of the form

CLnXi + ai2#2 H

h CLln^n = &1,

a2lXl + a22#2 H l· «2n^n = h .

We can represent the set of equations above as a vector equation

where

X1O1 + X2«2 + · ' * + X n « n = 6,

aij

a2j

aj =

,

h
b2 6 =

&mj

J>m

Associated with this system of equations is the matrix

A = [αι,θ2,.. · ,αη], and an augmented matrix

[A, 6] = [ α ι , α 2 , . . . , α η , & ] . We can also represent the system of equations above as

Ax — 6,

where rxi

x —

Theorem 2.1 The system of equations Ax — b has a solution if and only if
rank A = rank [A, b]. D
Proof =>: Suppose that the system Ax = b has a solution. Therefore, b is a linear combination of the columns of A; that is, there exist # 1 , . . . , xn such

18

VECTOR SPACES AND MATRICES

that xia,i+X2a>2-\ VXnO'n — b. It follows that b belongs to s p a n [ a i , . . . , an] and hence

rank A — dim s p a n f a i , . . . , an] = dim s p a n [ a i , . . . , an, b] = rank [A, 6].

<=: Suppose that rank A = rank [A, 6] = r. Thus, we have r linearly

independent columns of A. Without loss of generality, let o i , a<i,..., ar be

these columns. Therefore, a\, a 2 , . . . , ar are also linearly independent columns

of the matrix [A, b]. Because rank[A, b] = r, the remaining columns of [A, 6]

can be expressed as linear combinations of o i , ei2,..., ar. In particular, b can

be expressed as a linear combination of these columns. Hence, there exist

# 1 , . . . ,xn such that x\a\ 4- #2^2 + l· xn^n — b.

I

Theorem 2.2 Consider the equation Ax — b, where A G ]Rmxn and

rank A = m. A solution to Ax — b can be obtained by assigning arbitrary

values for n — m variables and solving for the remaining ones.

Ώ

Proof. We have rank A = m, and therefore we can find m linearly independent columns of A. Without loss of generality, let αι, a 2 , . . . , a m be such columns. Rewrite the equation Ax = b as

X\d\ + # 2 ^ 2 H

h XmO>m = b — X m + i a m + i — ·

%nQ"n·

Assign to x m + i , £m+2> · · ·, #n arbitrary values, say

%m+l ~ Üm+li %m+2 == ^ m + 2 i · · · 5 *^n ~ Urn

and let

B = [oi,a2,...,am]GRmxm.

Note that det B φ 0. We can represent the system of equations above as

Xl X2
— [b — ( i m + i a m + i — · · · — dnan].

The matrix B is invertible, and therefore we can solve for [x\,X2, · · · ,^m]T· Specifically,
X2
B [b — dm_|_iam-j-i — · · · — d n a n ] .

INNER PRODUCTS AND NORMS

19

2.4 Inner Products and Norms

The absolute value of a real number a, denoted |a|, is defined as

ία if α > 0 | α | ~ [ - α ifa<0.

The following formulas hold:

1. |α| = | - α | .

2. -\a\ <a< \a\.

3. \a + b\ < |a| + |6|.

4. | | a | - | 6 | | < | o - 6 | < | a | + |6|. 5. \ab\ = \a\\b\.

6. \a\ < c and \b\ < d imply that \a + b\ <c + d.

7. The inequality \a\ < b is equivalent to — b < a < b (i.e., a < b and —a < b). The same holds if we replace every occurrence of "<" by "<."

8. The inequality \a\ > b is equivalent to a > b or —a > b. The same holds if we replace every occurrence of ">" by ">."

For x, y G Rn, we define the Euclidean inner product by

n
(x,y) = ^XiVi
2=1

= xTy.

The inner product is a real-valued function ( · , · ) : M.n x Rn —> R having the following properties:

1. Positivity: (a?, x) > 0, (x, x) = 0 if and only if x = 0.

2. Symmetry: {x,y) = (y,x).

3. Additivity: (x + y,z) = (x, z) + (y, z).

4. Homogeneity: (rx,y) = r(x,y) for every r G R.

The properties of additivity and homogeneity in the second vector also hold; that is,

(x,y + z) = (x,y) + (x,z), (x, ry) = r{x,y) for every r G R.

20

VECTOR SPACES AND MATRICES

The above can be shown using properties 2 to 4. Indeed,
(sc,l/ + z) = (y + z,x) = (y,x) + (z,x) = (x,y) + (x,z)
and (x, ry) = (ri/, a?) = r(y, a?) = r(x, y).
It is possible to define other real-valued functions on E n x Rn that satisfy properties 1 to 4 above (see Exercise 2.8). Many results involving the Euclidean inner product also hold for these other forms of inner products.
The vectors x and y are said to be orthogonal if (as, y) = 0. The Euclidean norm of a vector x is defined as
||x|| = y/{x,x) = VxTx.
Theorem 2.3 Cauchy-Schwarz Inequality. For any two vectors x and y in M71, the Cauchy-Schwarz inequality

\(x,y)\<\\x\\\\y\\

holds. Furthermore, equality holds if and only if x = ay for some a G i □

Proof First assume that x and y are unit vectors; that is, ||x|| = \\y\\ = 1. Then,

0 < ll^-t/H2 =

(x-y,x-y)

= \\x\\2-2(x,y) + \\yf

= 2-2(x,y)

or
fay) < i,
with equality holding if and only if x = y. Next, assuming that neither x nor y is zero (for the inequality obviously
holds if one of them is zero), we replace x and y by the unit vectors #/||χ|| and 2//||ΐ/||. Then, apply property 4 to get

<*,»>< IMIIIi/ll.

Now replace x by — x and again apply property 4 to get

-(x,y) < \\x\\\\y\\·

The last two inequalities imply the absolute value inequality. Equality holds

if and only if a;/||x|| = ±2//||y||; that is, x = ay for some a G R.

I

INNER PRODUCTS AND NORMS

21

The Euclidean norm of a vector ||x|| has the following properties: 1. Positivity: ||x|| > 0, ||&|| = 0 if and only if x = 0. 2. Homogeneity: ||ra?|| = |r|||ic||, r G R. 3. Triangle inequality: ||ic + 2/|| < ||x|| -f \\y\\. The triangle inequality can be proved using the Cauchy-Schwarz inequality, as follows. We have
\\x + y\\2 = \\x\\2 + 2{x,y) + \\y\\2.

By the Cauchy-Schwarz inequality,

\\x + y\\2<\\x\\2 + 2\\x\\\\y\\ + \\y\\2 = (\\x\\ + \\y\\)2,

and therefore

H* + 2/11 < 11*11+ llvl|.

Note that if x and y are orthogonal: (x,y) = 0, then

||* + i/||2 = IMI2 + ||y||2,

which is the Pythagorean theorem for Rn. The Euclidean norm is an example of a general vector norm, which is any
function satisfying the three properties of positivity, homogeneity, and triangle inequality. Other examples of vector norms on Rn include the 1-norm, defined by ||cc||i = \x\\ + · · · + |xn|, and the oo-norm, defined by || (where the notation max* represents the largest over all the possible index values of i). The Euclidean norm is often referred to as the 2-norm, and denoted ||aj||2· The norms above are special cases of the p-norm, given by

11*11 = ί(Ι^ιΙρ + ··· + Ι^Ιρ)1/ρ i f i < p < o o

p 1 m a x { | # i | , . . . , \xn\}

iip = oc.

We can use norms to define the notion of a continuous function, as follows. A function / : Rn —» Rm is continuous at x if for all ε > 0, there exists δ > 0 such that \\y - x\\ < δ ^ \\f(y) ~ / ( * ) | | < ε. If the function / is continuous at every point in Rn, we say that it is continuous on Rn. Note that / = [/i,..., fm]T is continuous if and only if each component /^, i = 1 , . . . , m, is continuous.
For the complex vector space Cn, we define an inner product (x,y) to be ΣΓ=ι xiVii where the bar denotes complex conjugation. The inner product on Cn is a complex-valued function having the following properties:
1. (cc, a;) > 0, (sc, x) = 0 if and only if x = 0.

22 VECTOR SPACES AND MATRICES
2. (x,y) = (y,x). 3. (x + y,z) = (x,z) + (y,z). 4. (rx,y) = r(x,y), where r G C. Prom properties 1 to 4, we can deduce other properties, such as
(x, ny + r2z) = ή {x, y) + f2(x, *), where τ*ι,Γ2 G C. For C n , the vector norm can similarly be defined by ||x||2 = (x,x). For more information, consult Gel'fand [47].

EXERCISES

2.1 Let A G R m x n and rank A = m. Show that m < n.
2.2 Prove that the system Ax — 6, A G M m x n , has a unique solution if and only if rank A = rank [A, b] = n.
2.3 (Adapted from [38].) We know that if k > n + 1, then the vectors α ι , α 2 , . . . , α & G Rn are linearly dependent; that is, there exist scalars a i , . . . ,c*fc such that at least one α^ φ 0 and Σ ί = ι α*α* = ^- Show that if k > n + 2, then there exist scalars α ι , . . . , α& such that at least one α* ^ 0, Σ * = 1 α»ο» = 0, and Σί=ι ai = °· #ζη£: Introduce the vectors ä; — [1, α ^ ] Τ G Mn+1, z = 1 , . . . ,fc,and use the fact that any n + 2 vectors in IRn+1 are linearly dependent.
2.4 Consider a n m x m matrix Ai" that has block form

T» /r

-M- m—k,k -Lm—k

I Mk,k Ok,m-kj

where Mk,k is /c x /c, Mm-k,k is (m — k) x k, Im-k is the (m — k) x (m — k) identity matrix, and Ok,m-k is the k x (m — k) zero matrix.

a. Show that

|detM| - |detMfcjfc|.

This result is relevant to the proof of Proposition 19.1.

b . Under certain assumptions, the following stronger result holds:

d e t M = det(-Mfc,fc) Identify cases where this is true, and show that it is false in general.

EXERCISES

23

2.5 It is well known that for any a, 6, c, d £ C, det a 6 ad — be.

Suppose now that A, B, C, and D are real or complex square matrices of the same size. Give a sufficient condition under which

det

A C

B D

AD - BC.

An interesting discussion on determinants of block matrices is provided in [121].

2.6 Consider the following system of linear equations:

Xl + X2 + 2^3 + X4 = 1 x\ — 2x2 — XA — —2.
Use Theorem 2.1 to check if the system has a solution. Then, use the method of Theorem 2.2 to find a general solution to the system.

2.7 Prove the seven properties of the absolute value of a real number.
2.8 Consider the function (·, -)2 : M2 x R2 -> R, defined by (x, y)2 = 2x\yi + 3^22/1 + 3#i2/2 + 5x22/2, where x = [xi,X2]T and y = [yi,y2]T> Show that (·, ·)2 satisfies conditions 1 to 4 for inner products. Note: This is a special case of Exercise 3.21.
2.9 Show that for any two vectors x,y G Rn, |||x|| — ||y||| < \\x — y\\. Hint: Write x = (x — y) + y, and use the triangle inequality. Do the same for y.

2.10 Use Exercise 2.9 to show that the norm || · || is a uniformly continuous function] that is, for all ε > 0, there exists δ > 0 such that if \\x — y\\ < (5, then |||*||-Ill/Ill < ε ·

CHAPTER 3

TRANSFORMATIONS

3.1 Linear Transformations

A function C : Rn —» Rm is called a linear transformation if: 1. C(ax) = aC(x) for every x G Rn and a G R. 2. £ ( x + y) = C(x) + £(y) for every x, y G Rn. If we fix the bases for Rn and Rm, then the linear transformation C can be
represented by a matrix. Specifically, there exists A G R m x n such that the following representation holds. Suppose that x G Rn is a given vector, and x' is the representation of x with respect to the given basis for Rn. If y = £ ( x ) , and y' is the representation of y with respect to the given basis for Rm, then

y' = Ax'.

We call A the matrix representation of C with respect to the given bases for Rn and Rm. In the special case where we assume the natural bases for Rn and Rm, the matrix representation A satisfies

C(x) = Ax.

An Introduction to Optimization, Fourth Edition.

25

By E. K. P. Chong and S. H. Zak. Copyright © 2013 John Wiley & Sons, Inc.

26

TRANSFORMATIONS

Let {ei, e 2 , . . . , en} and {e^, e 2 , . . . , e^} be two bases for Rn. Define the matrix
T = [ei, e 2 , . . . , e^]_1 [ei, e2,.. ·, en].
We call T the transformation matrix from {ei, β 2 , . . . , e n } to {e^, e 2 , . . . , e'n}. It is clear that
[ei,e2,...,en] = [e'^e^,... ,e^]T;
that is, the ith column of T is the vector of coordinates of e^ with respect to the basis { e ' ^ e ^ , . . . ,e'n}.
Fix a vector in IRn. Let x be the column of the coordinates of the vector with respect to { e i , e 2 , . . . , e n } and x' the coordinates of the same vector with respect to {e^, e 2 , . . . , e'n}. Then, we can show that x' = Tx (see Exercise 3.1).
Consider a linear transformation
C : Rn -+ Mn,
and let A be its representation with respect to { e i , e 2 , . . . , e n } and B its representation with respect to {e^, e 2 , . . . , e'n}. Let y = Ax and y' = Bx'. Therefore, y' = Ty = TAx - Bx' = BTx, and hence TA = BT, or A = TlBT.
Two n x n matrices A and B are similar if there exists a nonsingular matrix T such that A = T~lBT. In conclusion, similar matrices correspond to the same linear transformation with respect to different bases.

3.2 Eigenvalues and Eigenvectors
Let A be an n x n real square matrix. A scalar λ (possibly complex) and a nonzero vector v satisfying the equation Av = Xv are said to be, respectively, an eigenvalue and an eigenvector of A. For λ to be an eigenvalue it is necessary and sufficient for the matrix XI — A to be singular; that is, det[AJ — A] = 0 , where I is the n x n identity matrix. This leads to an nth-order polynomial equation
det[AJ - A] = Xn + α η _ ι λ η _ 1 + · · · + axX + a0 = 0.
We call the polynomial det[A7 — A] the characteristic polynomial of the matrix A, and the equation above the characteristic equation. According to the fundamental theorem of algebra, the characteristic equation must have n (possibly nondistinct) roots that are the eigenvalues of A. The following theorem states that if A has n distinct eigenvalues, then it also has n linearly independent eigenvectors.
Theorem 3.1 Suppose that the characteristic equation det[AJ — A] = 0 has n distinct roots λχ, λ2,..., λ η . Then, there exist n linearly independent vectors V\, V2,..., vn such that
Ανι = XiVi, i = 1,2, . . . , n .
D

EIGENVALUES AND EIGENVECTORS

27

Proof. Because det[\il — A] = 0, i = l , . . . , n , there exist nonzero Vi, i = 1 , . . . , n, such that Avi = A ^ , i — 1 , . . . , n. We now prove the linear independence of {vi, v 2 , . . . , vn}. To do this, let c i , . . . , cn be scalars such that Σ™=ι CiVi — 0. We show that cz- = 0, i = 1 , . . . , n.
Consider the matrix
Z = (X2I - A)(X3I -A)··· (XnI - A).
We first show that c\ = 0. Note that
Zvn = ( λ 2 Ι - A)(X3I - A) · · · ( A n _ i l - Λ ) ( λ η Ι - A ) u n = ( λ 2 Ι - Α)(λ37 - A) · · · ( λ η - ΐ / - Α)(ληΤ7η - At>„) = 0
since Xnvn — A v n = 0. Repeating the argument above, we get
Zvk = 0, fc = 2 , 3 , . . . , n .
But
Z v i = ( λ 2 Ι - A)(X3I - A) · · · (An_il - Α ) ( λ η / - Α)νχ
= ( λ 2 / - Α)(λ37 - A) · · · (Än_!t;i - Ανι)(λη - λχ)

= ( λ 2 / - Α)(Χ3Ι - A)v! · · · ( λ η - ι - λι)(λη - λι) = (λ2 - λι)(λ3 - λι) · · · (λη_ι - λι)(λη - λι)ι>ι.

Using the equation above, we see that

(n
i=l

\

n

/

i=l

= C\ZV\

= Ci(A2 - λ ι ) ( λ 3 - λ ι ) · · · ( λ η " λ ι ) « ι = 0.

Because the λζ are distinct, it must follow that C\ = 0.

Using similar arguments, we can show that all c2- must vanish, and therefore

the set of eigenvectors {vi, i>2,..., vn} is linearly independent.

I

Consider a basis formed by a linearly independent set of eigenvectors {vi, v 2 , . . . , vn}. With respect to this basis, the matrix A is diagonal [i.e., if dij is the (i, j ) t h element of A, then α^ = 0 for all i φ j]. Indeed, let

T= [ v i , v 2 , . . . , v n ]

28

TRANSFORMATIONS

Then,

TAT-1=TA[vuv2,...,vn]

= T[Av1,Av2,...,Avn]

= Τ[λινι,λ2ν2,·..,ληυη]

λι

θ"

= TT-i

λ2

0

λη

Ιλι

λ2

L°

Xn

because TT1 = I.

A matrix A is symmetric if A = A T .

Theorem 3.2 J4ZZ eigenvalues of a real symmetric matrix are real.

Q

Proof Let

where x ^ 0. Taking the inner product of Ax with a? yields

(Αχ,χ) = (Χχ,χ) = X(x,x).

On the other hand,

(Aa?,ir} = (a;, A a;) = (χ,Αχ) = (χ,Χχ) — X{x,x).
The above follows from the definition of the inner product on C n . We note that (a?, x) is real and (x,x) > 0. Hence,

X(x,x) = X(x,x)

and (λ-λ)(ίυ,χ) =0.

Because (x, x) > 0,

λ = λ.

Thus, λ is real.

I

Theorem 3.3 Any real symmetric n x n matrix has a set of n eigenvectors

that are mutually orthogonal.

Q

ORTHOGONAL PROJECTIONS

29

Proof. We prove the result for the case when the n eigenvalues are distinct. For a general proof, see [62, p. 104].
Suppose that Av\ = Ai^i, Av2 = X2v2, where λι φ \2. Then,
(Avi,v2) = ( λ ι ν ι , ν 2 ) = λ ι ( ν ι , ν 2 ) . Because A = A ,
(Av1,v2) = (vuATv2) = (vuAv2) = \2{vi,v2).
Therefore,
Al(Vl,V2> =A2(V1,V2>.
Because Ai φ \2, it follows that
(vuv2) = 0.

If A is symmetric, then a set of its eigenvectors forms an orthogonal basis for Rn. If the basis {v\, v2,..., vn} is normalized so that each element has norm of unity, then defining the matrix

we have and hence

T = [Vi,U2,...,«n],

TTT = I

/TIT

rrt—1

A matrix whose transpose is its inverse is said to be an orthogonal matrix.

3.3 Orthogonal Projections
Recall that a subspace V of Rn is a subset that is closed under the operations of vector addition and scalar multiplication. In other words, V is a subspace of Rn if xi,x2 e V => α χ ι + βχ2 e V for all α,/3 G R. Furthermore, the dimension of a subspace V is equal to the maximum number of linearly independent vectors in V. If V is a subspace of Rn, then the orthogonal complement of V, denoted V-1", consists of all vectors that are orthogonal to every vector in V. Thus,
V± = {x: vTx = 0 for all v G V}.
The orthogonal complement of V is also a subspace (see Exercise 3.7). Together, V and V1- span Rn in the sense that every vector x G Rn can be represented uniquely as
X = X\ + X 2 ,

30

TRANSFORMATIONS

where X\ G V and x<i G V-1. We call the representation above the orthogonal
decomposition of x (with respect to V). We say that X\ and x<i are orthogonal projections of x onto the subspaces V and V1-, respectively. We write Rn = V Θ V1- and say that Rn is a direct sum of V and V-1. We say that a linear transformation P is an orthogonal projector onto V if for all x G Rn, we have P x G V and a; - Px G V x .
In the subsequent discussion we use the following notation. Let A G R m x n .
Let the range, or image, of A be denoted

11(A) = {Ax:xe R n } ,

and the nullspace, or kernel, of A be denoted

ΛΓ(Α) = {x G Rn : A x = 0}.

Note that 11(A) and N(A) are subspaces (see Exercise 3.9).

T h e o r e m 3.4 Let A be a given matrix. Then, IZ(A)1- = λί(Ατ) and

λί(Α)±=1Ι(ΑΤ).

D

Proof. Suppose that x G 11(A)1-. Then, yT(ATx) = (Ay)Tx = 0 for all y, so that A T x = 0. Hence, x G Λ/*(ΑΤ). This implies that 11(A)1- c Λ/*(ΑΤ).
If now x G Λ/*(Α ), then (Ay)Tx = yT(ATx) = 0 for all y, so that x G π ( Α ) \ and consequently, λί(Ατ) C ^ ( A ) - 1 . Thus, H(A)^ = λί(Ατ).
The equation Λί(Α)1- = 1Z(A ) follows from what we have proved above and the fact that for any subspace V, we have (V-1)1- = V (see Exercise 3.11).
■
Theorem 3.4 allows us to establish the following necessary and sufficient
condition for orthogonal projectors. For this, note that if P is an orthog-
onal projector onto V, then Px = x for all x G V, and Ti(P) = V (see
Exercise 3.14).

T h e o r e m 3.5 A matrix P is an orthogonal projector [onto the subspace V =

n{P)] if and only if P2 = P = PT.

D

Proof. =>: Suppose that P is an orthogonal projector onto V = Έ,(Ρ). Then, 11(1 -P)C H(P)-1. But, H(P)1- = λί(Ρτ) by Theorem 3.4. Therefore,

11(1 - P) C Af(P'). Hence, PT(I - P)y = 0 for all y, which implies that

P (I — P) = O, where O is the matrix with all entries equal to zero; i.e.,

the zero matrix. Therefore, PT = PTP, and thus P = PT = P2.

<=: Suppose that P2 = P = PT. For any x, we have (Py)T(I - P)x = yTPT(I - P)x - yTP(I - P)x = 0 for all y. Thus, (I - P)x G ft(P)\

which means that P is an orthogonal projector.

I

QUADRATIC FORMS

31

3.4 Quadratic Forms

A quadratic form f : Rn

is a function f(x) = xTQx,

where Q is an n x n real matrix. There is no loss of generality in assuming Q to be symmetric: Q = QT. For if the matrix Q is not symmetric, we can always replace it with the symmetric matrix

QO = QO=\(Q

+ QT)·

Note that

xTQx = xTQ0x = xT ( -Q + -QT J x.

A quadratic form xTQx, Q = QT, is said to be positive definite ifxTQx > 0 for all nonzero vectors x. It is positive semidefinite if xTQx > 0 for all x. Similarly, we define the quadratic form to be negative definite, or negative semidefinite, if xTQx < 0 for all nonzero vectors x, or xTQx < 0 for all x, respectively.
Recall that the minors of a matrix Q are the determinants of the matrices obtained by successively removing rows and columns from Q. The principal minors are det Q itself and the determinants of matrices obtained by successively removing an ith row and an ith column. That is, the principal minors are

Qiiii Qt2ii
det

Qiit2 Qt2l2

Qiiip Ql2ip
, 1 < i\ < - - - < ip < n, p = 1,2,..., n.

Qipii Qipl2

■ *Λ.

The leading principal minors are det Q and the minors obtained by successively removing the last row and the last column. That is, the leading principal minors are

Δι =gn,

Δ 2 = det 911 912
921 922

Qll <7l2 913
det Q21 Q22 Q23
Q31 Q32 933

An = detQ.

We now prove Sylvester's criterion, which allows us to determine if a quadratic form xTQx is positive definite using only the leading principal minors of Q.

32

TRANSFORMATIONS

Theorem 3.6 Sylvester's Criterion. A quadratic form xTQx, Q = QJ, is positive definite if and only if the leading principal minors of Q are positive.
D
Proof. The key to the proof of Sylvester's criterion is the fact that a quadratic form whose leading principal minors are nonzero can be expressed in some basis as a sum of squares

where x<i are the coordinates of the vector x in the new basis, Δο = 1, and Δ ι , . . . , Δ η are the leading principal minors of Q.
To this end, consider a quadratic form f(x) = xTQx, where Q = QT. Let {ei, β2,.. ·, e n } be the natural basis for Rn, and let
X = X\e\ + #2^2 + · * ' + Χη&η
be a given vector in Rn. Let {vi, t>2> · · · ? vn} be another basis for Rn. Then, the vector x is represented in the new basis as i , where

X — [vi,V2,...,Vn]i = Vx. Accordingly, the quadratic form can be written as
xTQx = xTVTQVx = xTQx,

where

Qn

Qln

Q = VTQV

Qnl
Note that q^ = (vi,Qvj). Our goal is to determine conditions on the new basis {vi, I>2J . . . , vn} such that q^ = 0 for i φ j .
We seek the new basis in the form
vi = a n e i , v2 = α2ιβι +α22β2,

vn = θίηχβι + α η 2 β 2 Η l· a n n e „ . Observe that for j = 1 , . . . , i — 1, if
(vuQej) = 0 , then
(vuQvj) =0.

QUADRATIC FORMS

33

Our goal t h e n is to determine the coefficients an, α ^ , . . . , ^ , i = 1 , . . . , n, such that the vector

Vi = ane\ + 0 ^ 2 H l· α ^ satisfies the i relations

{vuQej) {euQvi) In this case, we get

= 0, = 1.
an

j = l , . . . , z - 1, 0

0

ttn

For each i = l , . . . , n , the i relations above determine the coefficients &ii,...,Ciii in a unique way. Indeed, upon substituting the expression for Vi into t h e equations above, we obtain t h e set of equations

otnqii + ai2qi2 H h 0:2291* = 0,

α * ι φ - ι 1 + ^ 2 9 i - i 2 H h Oiuqi-i» = 0, Oil 9a + <*<2φ2 H h a ^ i = 1.

T h e set of equations above can be expressed in matrix form as

9ll 912 * * * Qli OL%\

~°1

921 922 · · · qi% OL%2

0!

=

qn q%i — - q%\ &ii

1

If t h e leading principal minors of t h e m a t r i x Q do not vanish, t h e n t h e coefficients aij can be obtained using Cramer's rule. In particular,

Hence,

9n
an = -— det 92-11 qn

9i 2-1 0

:

0

92-ii-i 0 92 2-1 1

' 1 Δι Δι
Δ2 Q

Δη-l
Δη J

34

TRANSFORMATIONS

In the new basis, the quadratic form can be expressed as a sum of squares
xTQx = xTQx Δ ι -2 , ΔΛι___Χζ$ + · + —.—χί.
We now show that a necessary and sufficient condition for the quadratic form to be positive definite is Δ* > 0, i = 1 , . . . , n.
Sufficiency is clear, for if Δ^ > 0, i = 1 , . . . , n, then by the previous argument there is a basis such that
xTQx = xTQx > 0
for any x φ 0 (or, equivalently, any x φ 0). To prove necessity, we first show that for i = 1 , . . . , n, we have Δ* φ 0. To
see this, suppose that Δ& = 0 for some k. Note that Δ& = det Qk,

Qn

Qik

Qk =

Qki

Qkk

Then, there exists a vector v e Mfc, v φ 0, such that vTQk = 0. Now let x e W1 be given by x = [υτ, 0 T ] T . Then,

xTQx = vTQkv = 0.

But x φ 0, which contradicts the fact that the quadratic form / is positive definite. Therefore, if xTQx > 0, then Ai φ 0, i — 1 , . . . , n. Then, using our previous argument, we may write

xTQx = x Qx = ^~xl~2 + —^Ί

+ —Δ —η xi

where x = [υχ,.. .,νη]χ. Hence, if the quadratic form is positive definite,

then all leading principal minors must be positive.

I

Note that if Q is not symmetric, Sylvester's criterion cannot be used to check positive definiteness of the quadratic form x1Qx. To see this, consider

an example where

Q Γ l °1
-4 1

The leading principal minors of Q are Δι = 1 > 0 and Δ2 = det Q = 1 > 0. However, if x = [1,1]T, then xTQx = — 2 < 0, and hence the associated

quadratic form is not positive definite. Note that

x1 Qx = x1 1 0 -4 1

1 0

-4 1 +

x

1 -2

= x

-2

X —X
1

L^QX.

MATRIX NORMS

35

The leading principal minors of Q0 are Δ ι = 1 > 0 and Δ 2 = det Q0 = — 3 < 0, as expected.
A necessary condition for a real quadratic form to be positive semidefinite is that the leading principal minors be nonnegative. However, this is not a sufficient condition (see Exercise 3.16). In fact, a real quadratic form is positive semidefinite if and only if all principal minors are nonnegative (for a proof of this fact, see [44, p. 307]).
A symmetric matrix Q is said to be positive definite if the quadratic form xTQx is positive definite. If Q is positive definite, we write Q > 0. Similarly, we define a symmetric matrix Q to be positive semidefinite (Q > 0), negative definite (Q < 0), and negative semidefinite (Q < 0) if the corresponding quadratic forms have the respective properties. The symmetric matrix Q is indefinite if it is neither positive semidefinite nor negative semidefinite. Note that the matrix Q is positive definite (semidefinite) if and only if the matrix —Q is negative definite (semidefinite).
Sylvester's criterion provides a way of checking the definiteness of a quadratic form, or equivalently, a symmetric matrix. An alternative method involves checking the eigenvalues of Q, as stated below.

Theorem 3.7 A symmetric matrix Q is positive definite (or positive semidef-

inite) if and only if all eigenvalues of Q are positive (or nonnegative).

□

Proof For any x, let y = T~lx = TTx, where T is an orthogonal ma-

trix whose columns are eigenvectors of Q. Then, xTQx = yTT QTy =

ΣΓ=ι ^iVi- I^0111 this, the result follows.

I

Through diagonalization, we can show that a symmetric positive semidefinite matrix Q has a positive semidefinite (symmetric) square root Q1'2 satisfying Q1/2Q1//2 = Q. For this, we use T as above and define

A.'2

o l

Q1/2 - T

o

\T \

which is easily verified to have the desired properties. Note that the quadratic form xTQx can be expressed as ||Q1/^2a?||2.
In summary, we have presented two tests for definiteness of quadratic forms and symmetric matrices. We point out again that nonnegativity of leading principal minors is a necessary but not a sufficient condition for positive semidefinit eness.

3.5 Matrix Norms
The norm of a matrix may be chosen in a variety of ways. Because the set of matrices R m X n can be viewed as the real vector space Mmn, matrix norms

36

TRANSFORMATIONS

should be no different from regular vector norms. Therefore, we define the norm of a matrix A, denoted ||A||, to be any function || · || that satisfies the following conditions:
1. ||A|| > 0 if A φ O, and | | 0 | | = 0, where O is a matrix with all entries equal to zero.
2. \\cA\\ = |c|||A||, for any c G R.
3. ||Λ + Β | | < | | Α | | + ||Β||.
An example of a matrix norm is the Frobenius norm, defined as

Ki=l 3=1
where A e R m x n . Note that the Frobenius norm is equivalent to the Euclidean norm on Rmn.
For our purposes, we consider only matrix norms that satisfy the following additional condition:
4. ||AB|| < ||Α||||Β||.
It turns out that the Frobenius norm satisfies condition 4 as well. In many problems, both matrices and vectors appear simultaneously.
Therefore, it is convenient to construct the norm of a matrix in such a way that it will be related to vector norms. To this end we consider a special class of matrix norms, called induced norms. Let || · ||(n) and || · ||(m) be vector norms on Rn and Rm, respectively. We say that the matrix norm is induced by, or is compatible with, the given vector norms if for any matrix A e R m x n and any vector x G l n , the following inequality is satisfied:
||Ax||(m) < ||A||||x||(n).
We can define an induced matrix norm as
max II A x I(m)5
IK») = 1
that is, || A|| is the maximum of the norms of the vectors Ax where the vector x runs over the set of all vectors with unit norm. When there is no ambiguity, we omit the subscripts (m) and (n) from || · ||(m) and || · ||(n).
Because of the continuity of a vector norm (see Exercise 2.10), for each matrix A the maximum
max \\Ax\\
is attainable; that is, a vector x0 exists such that ||xo|| = 1 and || Acc0|| = ||-A||. This fact follows from the theorem of Weierstrass (see Theorem 4.2).

MATRIX NORMS

37

The induced norm satisfies conditions 1 to 4 and the compatibility condition, as we prove below.

Proof of Condition 1. Let Αφ O. Then, a vector x, ||x|| = 1, can be found

such that Ax φ 0, and thus ||Ax|| Φ 0. Hence, ||A|| = max^i^x ||Aaj|| φ 0.

If, on the other hand, A — O, then ||A|| = max||x||=i ||Οχ|| = 0 .

I

Proof of Condition 2. By definition, ||cA|| = max||a.||=i ||cAx||. Ob-

viously, ||cAaj|| = |c|||Acc||, and therefore \\cA\\ = m a x ^ i ^ i |c|||Ax|| =

|c|max||x||=i \\Ax\\ = |c|||A||.

■

Proof of Compatibility Condition. Let y φ 0 be any vector. Then, x =

y/\\y\\ satisfies the condition ||x|| = 1. Consequently, \\Ay\\ = ||Α(||?/||χ)|| =

||y||||Aa;|| < ||y||||A||.

■

Proof of Condition 3. For the matrix A + B, we can find a vector XQ such that \\A + B | | = ||(A + B)a50|| and ||z0|| = 1. Then, we have

\\A + B\\ = \\(A + B)x0\\ = \\Ax0 + Bx0\\ <\\Ax0\\ + \\Bxo\\ <||A||||*0|| + ||B||||*o|| = ||A|| + ||B||,

which shows that condition 3 holds.

I

Proof of Condition 4- For the matrix AB, we can find a vector xo such that Hzoll = 1 and ||ΑΒχ0|| = \\AB\\. Then, we have
||AB|| = \\ABx0\\ = \\A(Bx0)\\ <\\A\\\\Bx0\\ <||Α||||Β||||*ο|| = ||A||||B||,

which shows that condition 4 holds.

I

Theorem 3.8 Let

ΙΝΙ=ίΣΐ^Π = ν^>·
The matrix norm induced by this vector norm is \\A\\ = >/ÄT,

38

TRANSFORMATIONS

where λι is the largest eigenvalue of the matrix A A.

G

Proof. We have

||Ax||2 = (Ax, Ax) = (x, AT Ax).

The matrix A A is symmetric and positive semidefinite. Let λι > λ2 > • · · > λη > 0 be its eigenvalues and Xi, X2, ·. ·, xn the orthonormal set of the eigenvectors corresponding to these eigenvalues. Now, we take an arbitrary vector x with ||x|| = 1 and represent it as a linear combination of Xi, i = l,...,n:
X = C\X\ + C2X2 + l· CnXn.

Note that

(x,x)=c\ + cl + --- + c2n = 1.

Furthermore,

\\Ax\\2 = (x,ATAx) — (c\X\ H l· cnxn, ciAiXi H = \ic\ H h A n 4
<Ai(c? + -.. + 4 )

h cnAnxn)

For the eigenvector aJi of A T A corresponding to the eigenvalue λι, we have

||Axi||2 = (®i, ΑΎ Αχλ) = (χι,λχΧι) = λι,

and hence

max IIAxII — ν λ ι · ll*ll=i

This completes the proof.

I

Using arguments similar to the above, we can deduce the following important inequalities.
Rayleigh's Inequalities. If an n x n matrix P is real symmetric positive definite, then
Xmin(P)\\x\\2 < XTPX < A m a x ( P ) | | x | | 2 ,
where Amin(P) denotes the smallest eigenvalue of P , and Amax(P) denotes the largest eigenvalue of P .

Example 3.1 Consider the matrix

A= 2 1 1 2

MATRIX NORMS

39

and let the norm in R2 be given by

Then,

11*11 = V*2?+ A-

ATA

5 4

4 5

and det[A/2 - AT A] = λ2 - 10A + 9 = (λ - 1)(λ - 9). Thus, ||A|| = \/9 = 3. The eigenvector of A A corresponding to λι = 9 is

xi V2

Note that \\Axi\\ = \\A\\. Indeed,

I" *1 1 i 2 [ill

lil 72 ||AiCi|| =

ll

2j

i II i3l

7i L3j

= 3.

Because A = AT in this example, we also have ||A|| = maxi<i<n |λ$(Α)|,

where λ ι ( Α ) , . . . , λη(Α) are the eigenvalues of A (possibly repeated).

I

Warning: In general, maxi<2<n |^i(-^-)| Φ 11-^11 · Instead, we have ||A|| >
maxi<i<n |λί(Α)|, as illustrated in the following example (see also Exercise 5.2). Example 3.2 Let
A= 0 1 0 0

then ATA = 0 0 0 1

and

d e t [ A I 2 - A T A ] = det

λ 0

0 = λ(λ-1). λ-1

Note that 0 is the only eigenvalue of A. Thus, for i = 1,2, ||A|| = 1 >

\Xi(A)\=0.

I

40

TRANSFORMATIONS

For a more complete but still basic treatment of topics in linear algebra as discussed in this and the preceding chapter, see [47], [66], [95], [126]. For a treatment of matrices, we refer the reader to [44], [62]. Numerical aspects of matrix computations are discussed in [41], [53].

EXERCISES
3.1 Fix a vector in Rn. Let x be the column of the coordinates of the vector with respect to the basis {ei, β 2 , . . . , e n } and x' the coordinates of the same vector with respect to the basis {e^, e 2 , . . . , e'n}. Show that x' = Tx, where T is the transformation matrix from {ei, β 2 , . . . , e n } to {e[,e2l..., e'n}.
3.2 For each of the following cases, find the transformation matrix T from {ei,e2,e3} to {ei,e72,e£}:
a. e[ = ei + 3e2 - 4e3, e2 = 2ei - e2 + 5e3, e 3 = 4ei + 5e2 + 3e3.
b . ei = e[ + e'2 + 3e'3, e2 = 2e[ - e2 + 4e3, e 3 = Se^ + 5e3.
3.3 Consider two bases of R3, {ei,e2,e3} and {e[,e2,e3}, where e\ = 2e[ + e2 — e3, e2 = 2ei — e2 H- 2β3, and e3 = 3ei + e'3. Suppose that a linear transformation has a matrix representation in {ei, e2, e3} of the form
2-10 0 1 -1 00 1
Find the matrix representation of this linear transformation in the basis
\ e l > e2> e 3 J ·
3.4 Consider two bases of R4, {ei,e2,e3,e4} and {e[,e2,e,3,e,4}, where e[ = e i , e2 = e\+e2, e'3 = e i + e 2 + e3, and e'4 = e\ -he2 + e 3 + e4. Suppose that a linear transformation has a matrix representation in {ei, e2, e3, e±) of the form
20 10 -3 2 0 1 0 1 -1 2 1 0 0 3
Find the matrix representation of this linear transformation in the basis
l e l ? e2> e3> e 4 J ·

EXERCISES

41

3.5 Consider a linear transformation given by the matrix
-10 0 0 110 0 2 521 -1 1 0 3
Find a basis for R4 with respect to which the matrix representation for the linear transformation above is diagonal.
3.6 Let λ ι , . . . , λη be the eigenvalues of the matrix A G R n x n . Show that the eigenvalues of the matrix In — A are 1 — λ χ , . . . , 1 — λη.
3.7 Let V be a subspace. Show that V1- is also a subspace.
3.8 Find the nullspace of
^4-2 0" 2 1 -1 2 -3 1

3.9 Let A G R m x n be a matrix. Show that 11(A) is a subspace of Rm and λί(Α) is a subspace of Rn.
3.10 Prove that if A and B are two matrices with m rows, and λί(Ατ) C λί(Βτ), then 11(B) c 11(A). Hint: Use the fact that for any matrix M with m rows, we have dim7£(M") + dim Af(MT) = m [this is one of the fundamental theorems of linear algebra (see [126, p. 75])].
3.11 Let V be a subspace. Show that (V^1- = V. Hint: Use Exercise 3.10.
3.12 Let V and W be subspaces. Show that if V C W, then W± CV±.
3.13 Let V be a subspace of Rn. Show that there exist matrices V and U such that V = 1Z(V) = Af(U).
3.14 Let P be an orthogonal projector onto a subspace V. Show that
a. Px = x for all x G V.
b . 1Z(P) = V.

42

TRANSFORMATIONS

3.15 Is the quadratic form

1 -fi x 1 1x
positive definite, positive semidefinite, negative definite, negative semidefinite, or indefinite?

3.16 Let

2 22 A= 2 2 2
2 2 0

Show that although all leading principal minors of A are nonnegative, A is not positive semidefinite.

3.17 Consider the matrix Q

"o 1 f
10 1 1 10

a. Is this matrix positive definite, negative definite, or indefinite?
b . Is this matrix positive definite, negative definite, or indefinite on the subspace M = {x : x\ + X2 + X3 = 0} ?

3.18 For each of the following quadratic forms, determine if it is positive definite, negative definite, positive semidefinite, negative semidefinite, or indefinite.

a. f(xi,X2,X3) =%2

b.

f(xi,X2,X3)=Xi'\'2xl-XiXs

C. f(xi,X2,X3) = X1+X3 + ΊΧ\Χ2 + 2χιΧ3 + 2X2^3

3.19 Find a transformation that brings the following quadratic form into the diagonal form:
f{x\,X2,xz) = kx\ +x\ + 9^3 - 4xi#2 - 6x2#3 + 12xix3.
Hint: Read the proof of Theorem 3.6.

EXERCISES

43

3.20 Consider the quadratic form

f(xi,X2,%3) = x\ + x\ + $xl + 2ξχιχ2 - 2χιχ3 + 4χ2^3·
Find the values of the parameter £ for which this quadratic form is positive definite.

3.21 Consider the function (-,-)Q : Rn x Rn -► R, defined by (x,y)Q = &TQy, where x,y G Rn and Q G R n x n is a symmetric positive definite matrix. Show that (·, -)Q satisfies conditions 1 to 4 for inner products (see Section 2.4).

3.22 Consider the vector norm || · ||οο on Rn given by |j21?|]oo — max^ \xi\, where x — [a?i,..., xn]T. Define the norm || - ||oo o n ^ m similarly. Show that the matrix norm induced by these vector norms is given by
n
\\A\loo =maxY]|aifc|,
k=l
where a^· is the (i,j)th element of A G R m x n .

3.23 Consider the vector norm || · ||i on Rn given by ||ic||i = ΣΓ=ι \χί\·> where x = [ x i , . . . , x n ] T . Define the norm || · ||i on Rm similarly. Show that the matrix norm induced by these vector norms is given by

||A||i =

771
max^2\aik\,
i=l

where a^· is the (i, j)th element of A G W1

CHAPTER 4

CONCEPTS FROM GEOMETRY

4.1 Line Segments

In the following analysis we concern ourselves only with Rn. The elements of this space are the n-component vectors x = [xi, X2,..., £η]Τ·
The line segment between two points x and y in Rn is the set of points on the straight line joining points x and y (see Figure 4.1). Note that if z lies on the line segment between x and y, then

z-y = a(x-y),

where a is a real number from the interval [0,1]. The equation above can be rewritten as z = ax + (1 — a)y. Hence, the line segment between x and y can be represented as

{ax + (l-a)y\ae

[0,1]}.

An Introduction to Optimization, Fourth Edition.

45

By E. K. P. Chong and S. H. Zak. Copyright © 2013 John Wiley & Sons, Inc.

46

CONCEPTS FROM GEOMETRY

Figure 4.1 Line segment.

4.2 Hyperplanes and Linear Varieties

Let ui,U2,...,un,v G R, where at least one of the ui is nonzero. The set of all points x = [x\, X2, · . . , xn}T that satisfy the linear equation

UiXi + U2X2 H

h ^n^n = V

is called a hyperplane of the space Rn. We may describe the hyperplane by

{ x e R n : uTx = v},

where

U= [wi,U2,...,Wn]T.

A hyperplane is not necessarily a subspace of Rn since, in general, it does

not contain the origin. For n = 2, the equation of the hyperplane has the

form u\Xi + U2X2 = v, which is the equation of a straight line. Thus, straight lines are hyperplanes in R2. In R3 (three-dimensional space), hyperplanes are

ordinary planes. By translating a hyperplane so that it contains the origin of Rn, it becomes a subspace of Rn (see Figure 4.2). Because the dimension of

this subspace is n — 1, we say that the hyperplane has dimension n — 1. The hyperplane H = {x : U\X\ H h unxn = v} divides Rn into two half-
spaces. One of these half-spaces consists of the points satisfying the inequality

U\Xi + U2X2 H + unxn > v, denoted

H+ = {x e Rn : uTx > v},

where, as before,

U=

[ui,U2,...,Un]T.

The other half-space consists of the points satisfying the inequality UiXi + U2X2 H h unxn < v, denoted

H- = {x e Rn : uTx < v}.

The half-space H+ is called the positive half-space, and the half-space if_ is called the negative half-space.

HYPERPLANES AND LINEAR VARIETIES

47

Figure 4.2 Translation of a hyperplane.
Let a = [ai, a2,..., an]T be an arbitrary point of the hyperplane H. Thus, uTa — v = 0. We can write
uTx — v = uTx — v — (uTa — v) = uT(x — a)
= wi(»i - «l) + u2(x2 - a2) H l· un(xn - an) — 0.
The numbers (xi — α^), i = 1 , . . . , n, are the components of the vector x — a. Therefore, the hyperplane H consists of the points x for which (u, x — a) = 0. In other words, the hyperplane H consists of the points x for which the vectors u and x — a are orthogonal (see Figure 4.3). We call the vector u the normal to the hyperplane H. The set H+ consists of those points x for which (u, x — a) > 0, and H- consists of those points x for which (u,x — a) < 0.
A linear variety is a set of the form
{x e Rn : Ax = 6}
for some matrix A G R m x n and vector b G Mm. If dimAf(A) = r, we say that the linear variety has dimension r. A linear variety is a subspace if and only if b = 0. If A = O, the linear variety is Rn. If the dimension of the linear variety is less than n, then it is the intersection of a finite number of hyperplanes.

48

CONCEPTS FROM GEOMETRY

Figure 4.3 The hyperplane H = {x G Rn : uT(x - a) = 0}.
4.3 Convex Sets Recall that the line segment between two points u,v G Rn is the set {w G Rn : w = au + (1 — a)v,a G [0,1]}. A point w = au + (1 — a)v (where a G [0,1]) is called a convex combination of the points n and v.
A set Θ C W1 is convex if for all iz, t; G Θ, the line segment between u and v is in Θ. Figure 4.4 gives examples of convex sets, whereas Figure 4.5 gives examples of sets that are not convex. Note that Θ is convex if and only if au + (1 — a)v G Θ for all u, v G Θ and a G (0,1).
Examples of convex sets include the following: ■ The empty set ■ A set consisting of a single point ■ A line or a line segment
Θ Figure 4.4 Convex sets.

CONVEX SETS

49

Figure 4.5 Sets that are not convex.
A subspace A hyperplane A linear variety A half-space

Theorem 4.1 Convex subsets ofW1 have the following properties: a. If Θ is a convex set and ß is a real number, then the set

βθ = {χ:χ = βν,ν£θ}

is also convex. b. If ©i and 02 are convex sets, then the set

θ ι + Θ2 = {x : x = Vi +1>2, Vi e θ ι , v2 G 62}

is also convex.

c. The intersection of any collection of convex sets is convex (see Figure 4.6

for an illustration of this result for two sets).

□

Proof. a. Let βν\,βν<ι G /?©, where Vi,V2 G Θ. Because Θ is convex, we have OLV\ + (1 — a)v2 G Θ for any a G (0,1). Hence,
αβυι + (1 - a)ßv2 = ß{av1 + (1 - α)ν2) G /?θ, and thus βθ is convex.

50

CONCEPTS FROM GEOMETRY

Figure 4.6 Intersection of two convex sets.
b . Let V\,v2 G ©i + 02- Then, v± = v[ + v'{, and v2 = v2 -f v2, where vi,V2 € θ ι , and v",v2 G Θ2. Because θ ι and ©2 are convex, for all a €(0,1),
051 = av[ 4- (1 — OL)V2 G θ ι
and x2 = av" + (1 - α)«2 £ θ 2 .
By definition of ©i + ©2, X\ + ^2 G ©i + ©2- Now,
a v i + (1 - a;)v2 = «(vi + v'/) + (1 - OL)(V'2 -f «2) = 051 +CC2 G ©1 + ©2-
Hence, ©i + ©2 is convex.
c. Let C be a collection of convex sets. Let #1,052 G f l e e c ® (where n © e c ® represents the intersection of all elements in C). Then, 05i, 052 G © for each Θ e C. Because each © G C is convex, α χ ι + (1 — α)θ52 G © for all a G (0,1) and each © G C. Thus, αχχ + (1 — OJ)O52 G f l e e c ®* I
A point 05 in a convex set © is said to be an extreme point of © if there are no two distinct points u and v in © such that 05 = au + (1 — a)v for some a G (0,1). For example, in Figure 4.4, any point on the boundary of the disk is an extreme point, the vertex (corner) of the set on the right is an extreme point, and the endpoint of the half-line is also an extreme point.
4.4 Neighborhoods
A neighborhood of a point 05 G Rn is the set
{yeRn:||y-*||<e},
where ε is some positive number. The neighborhood is also called a ball with radius ε and center 05.
In the plane R2, a neighborhood of x = [#i, x2]T consists of all the points inside a disk centered at 05. In R3, a neighborhood of 05 = [x\,X2, #3]T consists of all the points inside a sphere centered at 05 (see Figure 4.7).

NEIGHBORHOODS

51

disk

sphere

Figure 4.7 Examples of neighborhoods of a point in R2 and R3.

A point x G S is said to be an interior point of the set S if the set S contains some neighborhood of x; that is, if all points within some neighborhood of x are also in S (see Figure 4.8). The set of all the interior points of S is called the interior of S.
A point x is said to be a boundary point of the set S if every neighborhood of x contains a point in S and a point not in S (see Figure 4.8). Note that a boundary point of S may or may not be an element of S. The set of all boundary points of S is called the boundary of S.
A set S is said to be open if it contains a neighborhood of each of its points; that is, if each of its points is an interior point, or equivalently, if S contains no boundary points.
A set S is said to be closed if it contains its boundary (see Figure 4.9). We can show that a set is closed if and only if its complement is open.
A set that is contained in a ball of finite radius is said to be bounded. A set is compact if it is both closed and bounded. Compact sets are important in optimization problems for the following reason.
Theorem 4.2 Theorem of Weierstrass. Let f : Ω —► R be a continuous function, where Ω C Rn is a compact set Then, there exists a point XQ G Ω

Figure 4.8 x is an interior point; y is a boundary point.

52

CONCEPTS FROM GEOMETRY

A*2 3 +
2 I TsV!
1 +

Si = {[x1,X2]T:1<Xi<2,1<X2<2} S-j is open
S2={[X1,X2]T-3^X1^4,1<X2^2} S2 is closed

0

H
2

1
3

1
4

1
5

^ *1

Figure 4.9 Open and closed sets.

such that f(xo) < f(x) for allx G Ω. In other words, f achieves its minimum

on Ω.

ü

Proof. See [112, p. 89] or [2, p. 154].

I

4.5 Polytopes and Polyhedra
Let Θ be a convex set, and suppose that y is a boundary point of Θ. A hyperplane passing through y is called a hyperplane of support (or supporting hyperplane) of the set Θ if the entire set Θ lies completely in one of the two half-spaces into which this hyperplane divides the space Rn.
Recall that by Theorem 4.1, the intersection of any number of convex sets is convex. In what follows we are concerned with the intersection of a finite number of half-spaces. Because every half-space H+ or H- is convex in Rn, the intersection of any number of half-spaces is a convex set.
A set that can be expressed as the intersection of a finite number of halfspaces is called a convex polytope (see Figure 4.10).
A nonempty bounded polytope is called a polyhedron (see Figure 4.11). For every convex polyhedron Θ C Mn, there exists a nonnegative integer k < n such that Θ is contained in a linear variety of dimension k, but is not

Figure 4.10 Polytopes.

EXERCISES

53

Figure 4.11 One-dimensional polyhedron.
entirely contained in any (k — 1)-dimensional linear variety of Rn. Furthermore, there exists only one fc-dimensional linear variety containing Θ, called the carrier of the polyhedron Θ, and k is called the dimension of Θ. For example, a zero-dimensional polyhedron is a point of Rn, and its carrier is itself. A one-dimensional polyhedron is a segment, and its carrier is the straight line on which it lies. The boundary of any fc-dimensional polyhedron, k > 0, consists of a finite number of (A: — 1)-dimensional polyhedra. For example, the boundary of a one-dimensional polyhedron consists of two points that are the endpoints of the segment.
The (k—l)-dimensional polyhedra forming the boundary of a fc-dimensional polyhedron are called the faces of the polyhedron. Each of these faces has, in turn, (k — 2)-dimensional faces. We also consider each of these (k — 2)dimensional faces to be faces of the original fc-dimensional polyhedron. Thus, every fc-dimensional polyhedron has faces of dimensions k — l,fc — 2 , . . . , 1 , 0 . A zero-dimensional face of a polyhedron is called a vertex, and a one-dimensional face is called an edge.
EXERCISES
4.1 Show that a set S C Rn is a linear variety if and only if for all x,y G S and a G R, we have ax + (1 — a)y G S.
4.2 Show that the set {x G Rn : ||cc|| < r } is convex, where r > 0 is a given real number and ||x|| = VxTx is the Euclidean norm of x G Rn.
4.3 Show that for any matrix A G MrnXn and vector b G Mm, the set (linear variety) {x G Rn : Ax = b} is convex.
4.4 Show that the set {x G Rn : x > 0} is convex (where x > 0 means that every component of x is nonnegative).

CHAPTER 5

ELEMENTS OF CALCULUS

5.1 Sequences and Limits

A sequence of real numbers is a function whose domain is the set of natural numbers 1,2,..., fc,... and whose range is contained in R. Thus, a sequence of real numbers can be viewed as a set of numbers {x\, #2, . . . , £ & , . . . } , which is often also denoted as {xk} (or sometimes as {xk}kLi, to indicate explicitly the range of values that k can take).
A sequence {xk} is increasing if x\ < X2 < · · · < Xk · · *; that is, Xk < Xk+i for all k. If Xk < ^fc+i, then we say that the sequence is nondecreasing. Similarly, we can define decreasing and nonincreasing sequences. Nonincreasing or nondecreasing sequences are called monotone sequences.
A number x* G R is called the limit of the sequence {xk} if for any positive ε there is a number K (which may depend on e) such that for all k > K, \xk — x* | < ε; that is, Xk lies between x* — ε and x* + ε for all k > K. In this case we write
x* = lim Xk
fc—»oo
or
Xk —► x * .

j4n Introduction to Optimization, Fourth Edition.

55

By E. K. P. Chong and S. H. Zak. Copyright © 2013 John Wiley & Sons, Inc.

56

ELEMENTS OF CALCULUS

A sequence that has a limit is called a convergent sequence. The notion of a sequence can be extended to sequences with elements in
Rn. Specifically, a sequence in Rn is a function whose domain is the set of natural numbers 1,2,...,A:,... and whose range is contained in Rn. We use the notation {χ(χ\χ(2\ . . . } or {x^} for sequences in Rn. For limits of sequences in Rn, we need to replace absolute values with vector norms. In other words, x* is the limit of {x^} if for any positive ε there is a number K (which may depend on e) such that for all k > K, \\x^ —x*\\ < ε. As before, if a sequence {x^} is convergent, we write x* = lim^oo x^ or x^ —> x*.

Theorem 5.1 A convergent sequence has only one limit.

Q

Proof. We prove this result by contradiction. Suppose that a sequence {x^} has two different limits, say X\ and x2. Then, we have ||xi — a^21| > 0. Let

e = -||a;i-a:2||.

From the definition of a limit, there exist K\ and K2 such that for k > K\ we have \x^ — x\\\ < ε, and for k > K2 we have \\χ^ — x2\\ < ε. Let K = max{K1,K2}. Then, if k > K, we have \\x^ -X\\\ <e and \\x^ -x2\\ < e. Adding \\x^ - x i | | < ε and \\x^ - x2\\ < ε yields
\\x^ - Xl\\ + \\x^ - x2\\ <2ε.

Applying the triangle inequality gives

|| — asi 4- »21| = \\x{k) ~ xi ~ x(k) + 0521|

=

\\(xW-Xl)-(xW-X2)\\

< \\χ^ - Xl\\ + \\x<<k) - x2\\.

Therefore,

|| -£Ci 4-X2II = \\xi -«2II < 2 ε .

However, this contradicts the assumption that ||xi — »2II — 2ε, which com-

pletes the proof.

I

A sequence {x^} in Rn is bounded if there exists a number B > 0 such that ||a;(fc) || < B for all k = 1,2,....

Theorem 5.2 Every convergent sequence is bounded.

G

Proof. Let {x^} be a convergent sequence with limit x*. Choose ε = 1.
Then, by definition of the limit, there exists a natural number K such that
for all k> K, \\x^k) - a:*!! < 1 .

SEQUENCES AND LIMITS

57

By the result of Exercise 2.9, we get

\\x{k)\\ - ||**H < \\x{k) - s * | | < 1 for all k > K.

Therefore, Letting

\\x{k)\\ < ||a5*|| + l for all jfe > K. B = max {llas^H, ||*<2>||,..., ||*<*>||, | | * · | | + l } ,

we have

B> \\x{k)\\ for all/c,

which means that the sequence {x^} is bounded.

I

For a sequence {xk} in R, a number B is called an upper bound if Xk < B

for all k = 1,2, In this case, we say that {xk} is bounded above. Similarly,

B is called a lower bound if x^ > B for all fc = 1,2,

In this case, we

say that {α:^} is bounded below. Clearly, a sequence is bounded if it is both

bounded above and bounded below.

Any sequence {xk} in R that has an upper bound has a least upper bound

(also called the supremum), which is the smallest number B that is an upper

bound of {xk}- Similarly, any sequence {xk} in R that has a lower bound has

a greatest lower bound (also called the infimum). If B is the least upper bound

of the sequence {x^}, then Xk < B for all fc, and for any ε > 0, there exists

a number K such that XK > B — ε. An analogous statement applies to the

greatest lower bound: If B is the greatest lower bound of {x^}? then Xk > B

for all fc, and for any ε > 0, there exists a number K such that XK < B + ε.

Theorem 5.3 Every monotone bounded sequence in R is convergent

□

Proof. We prove the theorem for nondecreasing sequences. The proof for nonincreasing sequences is analogous.
Let {xk} be a bounded nondecreasing sequence in R and x* the least upper bound. Fix a number ε > 0. Then, there exists a number K such that XK > x* — ε. Because {xk} is nondecreasing, for any k > K,

Xk > XK > x* — ε.

Also, because x* is an upper bound of {xk}, we have

Xk < X* < X* + £·

Therefore, for any k > K,

\xk - x*\ < e,

which means that #& —* x*.

I

58

ELEMENTS OF CALCULUS

Suppose that we are given a sequence {x^} and an increasing sequence of natural numbers {ra^}. The sequence

{a5<mfc>} = { * < m i \ x < m a \ . . . }

is called a subsequence of the sequence {x^}. A subsequence of a given sequence can thus be obtained by neglecting some elements of the given sequence.

Theorem 5.4 Consider a convergent sequence {x^} with limit x*. Then,

any subsequence of {x^} also converges to x*.

□

Proof. Let {x^™^} be a subsequence of {x^}, where {rrik} is an increasing sequence of natural numbers. Observe that rrik > k for all k = 1,2, To show this, first note that m\>\ because πΐ\ is a natural number. Next, we proceed by induction by assuming that mk > k. Then, we have rafc+i > rrik > fc, which implies that rrik+i > k + 1. Therefore, we have shown that rrik > k for all k = 1,2,....
Let ε > 0 be given. Then, by definition of the limit, there exists K such that ||x(fc)— x*|| < ε for any k > K. Because rrik > A:, we also have ||x^mfc^— x*|| < ε for any k > K. This means that

lim x(mfc) =x*.
k-+oc

It turns out that any bounded sequence contains a convergent subsequence. This result is called the Bolzano-Weierstrass theorem (see [2, p. 70]).
Consider a function / : W1 —> Rm and a point Xo € l n . Suppose that there exists / * such that for any convergent sequence {x^} with limit Xo, we have
lim /(«<*>) = / * .
k—+oo
Then, we use the notation lim f(x)
X—>Xo
to represent the limit /*. It turns out that / is continuous at Xo if and only if for any convergent
sequence {x^} with limit x0> we have

lim /(*<*>) = / ( lim x&A = f(x0)

fc—>oo

yfc—>oo

J

(see [2, p. 137]). Therefore, using the notation introduced above, the function / is continuous at XQ if and only if

lim f(x) = f(x0).
X—*XQ

SEQUENCES AND LIMITS

59

We end this section with some results involving sequences and limits of matrices. These results are useful in the analysis of algorithms (e.g., Newton's algorithm in Chapter 9).
We say that a sequence {A^} o f m x n matrices converges to the m x n matrix A if
lim | | A - Ak\\ = 0 .
/c—»oo

L e m m a 5.1 Let A e

l. Then, Hindoo A = O if and only if the eigen-

values of A satisfy \\i(A)\ < 1, i — 1 , . . . , n.

D

Proof. To prove this theorem, we use the Jordan form (see, e.g., [47]). Specifically, it is well known that any square matrix is similar to the Jordan form: There exists a nonsingular T such that

TAT1 = diag [ J m i ( λ ι ) , . . . , J m s (λι), J n i (λ2), ...,Jtu (λς)] = J ,

where Jr(A) is the r x r matrix:

"λ 1

0

λ Jr(A) -

0
The λ ι , . . . , Xq above are distinct eigenvalues of A, the multiplicity of λι is mi H + ms, and so on.
We may rewrite the above as A = T~XJT. To complete the proof, observe that

(jr(\))k

xk Jfe-lJ λ * - ι
\k

where
Furthermore, Hence,

0

Xk

k\ i\(k-i)\
Ak = T~lJkT.

lim Ak = T l ( lim Jk ) T = O

/c—»oo

k—+oo

60

ELEMENTS OF CALCULUS

if and only if |λ;| < 1, i — 1 , . . . , n.

I

Lemma 5.2 The series of n x n matrices

In + A + A 2 + · · · + Ak + · · ·

converges if and only if Hm^oo A = O. In this case the sum of the series

equals (In — A ) - 1 .

D

Proof. The necessity of the condition is obvious. To prove the sufficiency, suppose that lim^oo A = O. By Lemma 5.1 we
deduce that |λ;(Α)| < 1, i = 1 , . . . , n. This implies that det(i"n — A) φ 0, and hence ( J n — A ) - 1 exists. Consider now the following relation:
(In + A + A2 + · · · + Ak)(In - A) = In - A*+1.
Postmultiplying the equation above by (Jn — A ) - 1 yields
In + A + A2 + . . . + Ak = ( I n - A ) " 1 - A f c + 1 ( / n - A ) " 1 .

Hence,

k
lim Γ Α ' ^ / . - Α ) - 1 ,
J=0

because lim^oo A + 1 = O. Thus,

oo
ΣΑΐ = (Ιη-Α)-\
3=0

which completes the proof.

A matrix-valued function A : Rr —► R n x n is continuous at a point £0 G
if lim | | Α ( £ ) - Α ( ξ ο ) | | = 0 .

Lemma 5.3 Let A : Rr —> R n X n be an n x n matrix-valued function that

is continuous at ξ0. J / A ( £ 0 ) _ 1 exists, then A ( £ ) _ 1 exists for ξ sufficiently

close to £0 and A ( · ) - 1 is continuous at £0.

□

Proof. We follow [114]. We first prove the existence of A ( £ ) _ 1 for all ξ sufficiently close to £o· We have

•A(€) = Mio) - Mio) + Mi) = ^(€o)(Jn - *(€)),

where

Κ{ζ) =

Α{ζ0)-ΗΑ{ζο)-Α{ζ)).

Thus, and

SEQUENCES AND LIMITS

61

ΙΙ^(€)ΙΙ<Ι|Α(€ο)-ΊΐΙΙ^«ο)-Α(€)||
lim \\Κ(ξ)\\ = 0.

Because A is continuous at £0> f°r a u £ close enough to £0, we have

where Θ € (0,1). Then,

||*(€)||<0<ι

and (In-K«-))-1

exists. But then Α ( ξ ) - 1 = (Α(ξ0)(Ιη - A · « ) ) ) " 1 = (/„ -

Κ(ζ))-ιΑ{ξο)-\

which means that A ( £ ) - 1 exists for £ sufficiently close to £0. To prove the continuity of A ( · ) - 1 note that

ΙΙΑ(ξο)-1 - Α(ξ)-ι\\ = WAV-)"1 - Aß,)"1»

= ||((7η-ΑΓ(ί))-1-Ιη)Α(€0)-1||.

However, since ||ϋΓ(ξ)|| < 1, it follows from Lemma 5.2 that

( i n - i f ( ξ ) ) - 1 - / „ = κ(ξ) + κ2(ζ) + · · · = «■({)(/„ + Ä - ( 0 + · · · ) ·

Hence,

| | ( I n - A - « ) ) " 1 - 1 „ | | < ||Jr(i)||(l + ||Ä-(€)|| + IIÄC(Oll2 + · · ·)
l|Ä-(i)ll
I-||Ä-(OH'

when \\Κ(ζ)\\ < 1. Therefore,

HA«)-1 - Αίίο)-1!! < rqj^Hiill^o)-1!!·

Because lim ||ÄT(€)||=0,
ΙΙ*-*οΙΗο
we obtain
lim JA^-A^^O,
lls- «oil->Ό
which completes the proof.

62

ELEMENTS OF CALCULUS

5.2 Differentiability

Differential calculus is based on the idea of approximating an arbitrary function by an affine function. A function A : Rn —► Rm is affine if there exists a linear function £ : Rn —► R m and a vector y G R m such that

A{x) = C(x) + y
for every x G Rn. Consider a function / : Rn -► Rm and a point x0 G Rn. We wish to find an affine function A that approximates / near the point XoFirst, it is natural to impose the condition

A{x0) = /(a?0).
Because *A(sc) = £ ( x ) + y, we obtain 1/ = f(xo) — £(#0)· By the linearity of £,
£(sc) + 1 / = £(x) - £(aj0) H- f(xo) = £ ( « - «0) + /(«o)· Hence, we may write

A(x) = C(x - x0) + /(»o)·

Next, we require that A(x) approaches f(x) faster than x approaches Xo; that is,
iim l l / ( » ) - ^ ) l l = 0 ,
x—».χο,ίΕ^Ω \\x ~ Xo\\
The conditions above on A ensure that A approximates / near Xo in the sense that the error in the approximation at a given point is "small" compared with the distance of the point from x$.
In summary, a function / : Ω —► Rm, Ω C Rn, is said to be differentiable at Xo £ Ω if there is an affine function that approximates / near XQ; that is, there exists a linear function £ : Rn —> Rm such that

l i m \\f(x)-(C(x-x0)

+ f(x0))\\ = 0

x—>χο,χ£Ω

\\X — Xo\\

The linear function £ above is determined uniquely by / and XQ and is called the derivative of / at x$. The function / is said to be differentiable on Ω if / is differentiable at every point of its domain Ω.
In R, an affine function has the form ax + 6, with a, b G R. Hence, a realvalued function f{x) of a real variable x that is differentiable at Xo can be approximated near x0 by a function

A(x) = ax -\-b.

Because f(xo) — A{x§) = axo + 6, we obtain

A{x) = ax-\-b = a(x - x0) + /(ffo)·

THE DERIVATIVE MATRIX

63

0 '

x0

Figure 5.1 Illustration of the notion of the derivative.

The linear part of A(x), denoted earlier by C(x), is in this case just ax. The

norm of a real number is its absolute value, so by the definition of differentia-

bility we have

l i m \f(x)-(a(x-xo) + f(xo))\ = 0

X—>XQ

\x -xo\

which is equivalent to

IimMlM=e.
x—>χο x — Xo

The number a is commonly denoted f'(xo) and is called the derivative of /

at xo· The affine function A is therefore given by

A(x) = f(x0) + f'(xo)(x ~ xo)This affine function is tangent to / at XQ (see Figure 5.1).

5.3 The Derivative Matrix

Any linear transformation from Rn to Rm, and in particular the derivative C of / : Rn —► Rm, can be represented by an m x n matrix. To find the matrix representation L of the derivative £ of a differentiate function / : Rn —► Rm, we use the natural basis { e i , . . . , en} for Rn. Consider the vectors

Xj — XQ + t€j, j = 1, . . . , Π.

By the definition of the derivative, we have

H m fjxA-jtLej + fjxo)) = 0

t^o

t

64

ELEMENTS OF CALCULUS

f o r j 1 , . . . , n. This means that

f{Xj) lim

f(x0)

Le^

t

for j = 1 , . . . , n. But Lej is the jth. column of the matrix L. On the other hand, the vector Xj differs from XQ only in the jth coordinate, and in that coordinate the difference is just the number t. Therefore, the left side of the preceding equation is the partial derivative

3/
dxj (®o).

Because vector limits are computed by taking the limit of each coordinate function, it follows that if

fi(x)

/(*)

5

fm(x)_

then

[Mj(^o)'

ddxfj_(®o)\;

_
—

5

ίΐ^(*ο)_

and the matrix L has the form

af_
dxi

*

o

)

dx„ (*o)

[fe(«o) ··· f£(*o)
m^Ό) ··· !fc(*o)

The matrix L is called the Jacobian matrix, or derivative matrix, of / at Xo> and is denoted Df(xo). For convenience, we often refer to Df(x0) simply as the derivative of / at XQ. We summarize the foregoing discussion in the following theorem.

Theorem 5.5 If a function f : Rn —► Rm is differentiable at Xo, then the derivative of f at XQ is determined uniquely and is represented by the m x n derivative matrix Df(xo). The best affine approximation to f near XQ is then given by
A{x) = f(xo) + Df(x0)(x - x 0 ) ,

in the sense that

f(x) = A(x) + r(x)

THE DERIVATIVE MATRIX

65

and Ηηΐφ-^ο ||r(x)||/||x — Xo|| — 0· The columns of the derivative matrix Df(xo) are vector partial derivatives. The vector

is a tangent vector at XQ to the curve f obtained by varying only the jth

coordinate ofx.

D

If / : Rn

is differentiate, then the function V / defined by

£(«)'

V/(x)

Df(x)T

&(*>.
is called the gradient of / . The gradient is a function from Rn to Rn, and can be pictured as a vector field, by drawing the arrow representing V/(cc) so that its tail starts at x.
Given / : Rn —> R, if V / is differentiate, we say that / is twice differentiate, and we write the derivative of V / as

Γ d2l a2/
D2f = dx\dx2

d2f dx2dx\
d2f
dxj

d2f dxndxi
d2f
dxndx2

d2f

d2f

. ■

^

_ dx\dxn dx2dxn

^

~ 1 dxidxj

renrpsents

t a kinff t h e

n a rtial

derivat

to Xj first, then with respect to X{.) The matrix D2f(x) is called the Hessian

matrix of / at x, and is often also denoted F(x). A function / : Ω —* Rm, Ω C Rn, is said to be continuously differentiate

on Ω if it is differentiate (on Ω), and Df : Ω —> R m x n is continuous; that

is, the components of / have continuous partial derivatives. In this case, we

write / G C1. If the components of / have continuous partial derivatives of order p, then we write f eCp.

Note that the Hessian matrix of a function / : Rn —► R at x is symmetric

if / is twice continuously differentiate at x. This is a well-known result

from calculus called ClairauVs theorem or Schwarz's theorem. However, if the

second partial derivatives of / are not continuous, then there is no guarantee

that the Hessian is symmetric, as shown in the following well-known example.

Example 5.1 Consider the function

{xlx2{x2l-xl)/{x\JtX2) f{x) = \0

iftf^O nx = 0.

66

ELEMENTS OF CALCULUS

Let us compute its Hessian at the point 0 = [0,0]T. We have

F =

d2f dx\ d2f dx\X2

d2f dx2X\
a2/ dx\ J

We now proceed with computing the components of the Hessian and evaluating them at the point [0,0]T one by one. We start with

where

&£ = _d_ (df\ dx\ dx\ \dx\J '

df_(x)
dx\
\χ2{χ\-χ\ 10

+ ±χ\χ22)/(χ\ + χΙ)2

ήχφθ

if x = 0.

Note that

Hence,

Also,

^([0)χ2Γ) = -χ,

Hence, the mixed partial is

a2/ (0) = - 1 .

We next compute

where

df , v dx-2{Xl'X2)

&l = _d_(df_
dx\ dx2 \dx2J '

Note that

0

if x = 0.

g W ) = o.

DIFFERENTIATION RULES

67

Hence, Also,

§<·>-
|£([*i,0]T)=*i.

Hence, the mixed partial is

d2f (0) = 1.

Therefore, the Hessian evaluated at the point 0 is

which is not symmetric.

0 -1 F(0) = 1 0

5.4 Differentiation Rules
We now introduce the chain rule for differentiating the composition g(f(t)), of a function / : R -► Rn and a function g : Rn -* R.
Theorem 5.6 Lei # : V —► R &e differentiable on an open set D c R n , and Ze£ / : (a, 6) —> P 6e differentiable on (a, 6). Tften, £/ie composite function h : (a, 6) —> R given ft?/ ft(t) = g(f(t)) is differentiable on (a, 6), and

Λ'(ί) = Dg(f(t))Df(t) = V 5 ( / ( i ) ) T

Proof. By definition,

v y s—ί

S — t

s-+t

if the limit exists. By Theorem 5.5 we write

9(f(s)) - g(f(t)) = Dg(f(t))(f(s)

where lims_>t r(s)/(s — t) = 0. Therefore,

S -t
- /(*)) + r(s),

s-t

s-t

s-t

68

ELEMENTS OF CALCULUS

Letting s —»t yields

h\t) = YimDg{f{t))f{s)-f{t)

s—>t

S —t

+^ - =
S — t

Dg(f(t))Df(t).

I

Next, we present the product rule. Let / : Rn -► R m and g : Rn -> R m be two differentiable functions. Define the function h : Rn —► R by h(x) = f(x)Tg(x). Then, /i is also differentiable and

Dh(x) =

f(x)TDg(x)+g(x)TDf(x).

We end this section with a list of some useful formulas from multivariable calculus. In each case, we compute the derivative with respect to x. Let A G R m x n be a given matrix and y G Rm a given vector. Then,

D(yT Ax) = yT A D(xTAx) = xT(A + A T ) if m = n.

It follows from the first formula above that if y G Rn, then

D(yTx)=yT.

It follows from the second formula above that if Q is a symmetric matrix,
then D(xTQx) = 2xTQ.

In particular,

D(xTx) = 2xT.

5.5 Level Sets and Gradients
The level set of a function / : Rn —> R at level c is the set of points
S = {x: f(x) = c}.
For / : R2 —» R, we are usually interested in 5 when it is a curve. For / : R3 —> R, the sets S most often considered are surfaces. Example 5.2 Consider the following real-valued function on R2:
f{x) = 100(x2 - x\f + (1 - * i ) 2 , x = [xuX2]T·
The function above is called Rosenbrock's function. A plot of the function / is shown in Figure 5.2. The level sets of / at levels 0.7, 7, 70, 200, and 700 are depicted in Figure 5.3. These level sets have a particular shape resembling

LEVEL SETS AND GRADIENTS

69

-1 - 2
Figure 5.2 Graph of Rosenbrock's function.
-2 -1.5 -1 -0.5 Figure 5.3 Level sets of Rosenbrock's (banana) function.

70

ELEMENTS OF CALCULUS

f(x1}x2)=c

Figure 5.4 Orthogonality of the gradient to the level set.

bananas. For this reason, Rosenbrock's function is also called the banana

function.

I

To say that a point XQ is on the level set S at level c means that f(xo) = c. Now suppose that there is a curve 7 lying in S and parameterized by a continuously differentiable function g : R —► Rn. Suppose also that g(to) = XQ and Dg(to) = υ φ 0, so that v is a tangent vector to 7 at Xo (see Figure 5.4). Applying the chain rule to the function h(t) = f(g(i)) at to gives

ti(t0) = Df{g{t0))Dg{t0) But since 7 lies on 5, we have

= Df(x0)v.

h(t) = f(g(t)) = c;

that is, h is constant. Thus, h'(to) — 0 and Df(x0)v = Wf(x0)Tv = 0.
Hence, we have proved, assuming / continuously differentiable, the following theorem (see Figure 5.4).

Theorem 5.7 The vector Vf(xo) is orthogonal to the tangent vector to an

arbitrary smooth curve passing through XQ on the level set determined by

f(x) = f(x0).

□

Af(Xl.X2)

LEVEL SETS AND GRADIENTS

71

Figure 5.5 Illustration of a path of steepest ascent.
It is natural to say that V/(iCo) is orthogonal or normal to the level set S corresponding to XQ, and it is also natural to take as the tangent plane (or line) to S at xo the set of all points x satisfying
Vf(x0)T(x - xo) = 0 if V / ( x 0 ) Φ 0.
As we shall see later, Vf(xo) is the direction of maximum rate of increase of / at Xo. Because Vf(xo) is orthogonal to the level set through XQ determined by f(x) = f(xo), we deduce the following fact: The direction of maximum rate of increase of a real-valued differentiable function at a point is orthogonal to the level set of the function through that point.
Figure 5.5 illustrates the discussion above for the case / : R2 —> R. The curve on the shaded surface in Figure 5.5 running from bottom to top has the property that its projection onto the (x 1,^2)-plane is always orthogonal to the level curves and is called a path of steepest ascent because it always heads in the direction of maximum rate of increase for / .
The graph of / : Rn -+ R is the set {[xT,f(x)]T : x e R n } c R n + 1 . The notion of the gradient of a function has an alternative useful interpretation

72

ELEMENTS OF CALCULUS

in terms of the tangent hyperplane to its graph. To proceed, let XQ G Rn and ZQ = f(xo)- The point [a?o~,zo]T € R n + 1 is a point on the graph of / . If / is differentiable at £, then the graph admits a nonvertical tangent hyperplane at ξ = [χζ, ZQ)T . The hyperplane through ξ is the set of all points [ x i , . . . , xn, z]T G R n + 1 satisfying the equation
ui(xi - xoi) H h un(xn - x0n) + v(z - ZQ) = 0,
where the vector [u\,... ,un,v]T G R n + 1 is normal to the hyperplane. Assuming that this hyperplane is nonvertical (that is, v ^ O ) , let

Thus, we can rewrite the hyperplane equation above as

z = di(xi - x0i) + . . . + dn(xn - xon) + ZQ.
We can think of the right side of the above equation as a function z : Rn —> R. Observe that for the hyperplane to be tangent to the graph of / , the functions / and z must have the same partial derivatives at the point XQ. Hence, if / is differentiable at a?o, its tangent hyperplane can be written in terms of its gradient, as given by the equation

z - z0 = Df(x0)(x - x0) = (x-

x0)TVf(x0).

5.6 Taylor Series

The basis for many numerical methods and models for optimization is Taylor's formula, which is given by Taylor's theorem.
Theorem 5.8 Taylor's Theorem. Assume that a function f : R —► R is m times continuously differentiable (i.e., f G Cm) on an interval [a, 6]. Denote h = b — a. Then,

/(*>) = /(«) + £/(1)(«) + | / ( 2 ) ( a ) + ■ · ■ + J^TJy^^(a)

+ Rm,

(called Taylor's formula) where / W is the ith derivative of f, and

umfi

n\m—l

urn

* " = (m-l)! /(TO)(Q + °h) = ^ ! / ( m ) ( a + Θ'Η)>

withe,e'e (0,1).

□

Proof. We have

i C = f(b) - f(a) - £/<*>(„) - */<»>(<,)

^L/(m-i)(„).

TAYLOR SERIES 73

Denote by g<m(x) an auxiliary function obtained from i?m by replacing a by x. Hence,
9m(x) = f(b) - /(*) - b-^f^(x) - ^ ^ / ( 2 > ( Z )

(m-1)! ;

[ h

Differentiating gm(x) yields

g£Hx) = -f{1)(x) + / ( 1 ) ( x ) - ^1/! ( 2 ) ( x )

(6-x)m-2 + Hm L> ( m _ i)i J
__{b-xT^ ( m ) ~ (m-1)! 7 l j '

/(3)(x) + ■ W ( m _ 1}, / W

Observe that gm(6) = 0 and (?m(a) = i?m. Applying the mean-value theorem yields

flro(ft) - f l m ( o )
b— a

=_ 9(£l>)(α + ΘΙι),

where Θ £ (0,1). The equation above is equivalent to

Rm h

(b-a- ΘΚ)m—l f{m\a
(m-1)!

+ eh) = - hm-(\ml-9- )1)!m—l- / ( m ) ( a + 0ft).

Hence,

flm

=

/

tro(1
(m -

"1?)!r

1

/

(

m

)

(

o

+ gft).

To derive the formula

i?m = iL/W(a + ^ ) !

see, e.g., [81] or [83].

I

An important property of Taylor's theorem arises from the form of the re-
mainder i2m. To discuss this property further, we introduce the order symbols, O and o.
Let g be a real-valued function defined in some neighborhood of 0 G Mn, with g(x) φ 0 if x φ 0. Let / : Ω -+ Rm be defined in a domain Ω C W1 that
includes 0. Then, we write

74

ELEMENTS OF CALCULUS

1. f(x) — 0(g(x)) to mean that the quotient ||/(a:)||/|^(ic)| is bounded near 0; that is, there exist numbers K > 0 and δ > 0 such that if \\x\\ < J, xen,then\\f(x)\\/\g(x)\<K.
2. f(x) = o(g(x)) to mean that

lim M M 0.
χ-+0,χ€Ω \g(&)\

The symbol 0(g(x)) [read "big-oh of g{x)"] is used to represent a function that is bounded by a scaled version of g in a neighborhood of 0. Examples of such a function are:

x = O(x).

x3 2x2 + 3a:4 =

0(x2).

■ cosx = O(l).

■ sin a; = 0(x).

On the other hand, o(g(x)) [read "little-oh of g(x)"] represents a function that goes to zero "faster" than g(x) in the sense that 1ίπιχ_>ο ll0(fi,(2C))ll/l^(iC)l — 0· Examples of such functions are:
■ x2 = o(x).

x3

2x2 + 3x4

o(x).

■ X3 — o(x2).
m x = o(l).
Note that if f(x) = o(g(x)), then f(x) = 0(g(x)) (but the converse is not necessarily true). Also, if f(x) = 0(||cc||p), then f(x) — ο(\\χ\\ρ~ε) for any ε>0.
Suppose that / G Cm. Recall that the remainder term in Taylor's theorem has the form
Rm = —urnrf(m)(a + eh), ml
where Θ £ (0,1). Substituting this into Taylor's formula, we get

f(b) = f(a)+±fW(a)+^fW(a)+. · .+^^/<">-ΐ)(«)+^/(«0(α+βΑ).

TAYLOR SERIES

75

By the continuity of /<m), we have / ( m ) ( a + 0ft) -> / ( m ) ( a ) as ft ^ 0; that is, /(™)(a + 6>ft) = /<m)(a) + o(l). Therefore,

um

um

-mr!/ ( m ) ( a + 0ft) - ^mr!/ ( m ) ( a ) + o(ftm),

since ftmo(l) = o(ftm). We may then write Taylor's formula as

/(&) - /(β) + £ / ( 1 ) ( α ) + | - / ( 2 ) ( α ) + · · · + ^ , / ( m ) ( a ) + o(hm)-
If, in addition, we assume that / G C m + 1 , we may replace the term o(ftm) above by 0 ( f t m + 1 ) . To see this, we first write Taylor's formula with i?m+i:

f(b) = /(a) + £/(1)(") + |-/(2)(a) + · · · + ^/(m)(a) + V i >

where

Um+l

with 6»' G (0,1). Because /("»+1) is bounded on [a, 6] (by Theorem 4.2),

i?m+1=0(fcm+1).

Therefore, if / G C m + 1 , we may write Taylor's formula as

h

h2

f(b) = f(a) + £ / ( 1 ) ( a ) + ^/2\a)

hm
+ ■■■ + ^f(m)(a)

+ 0(/*™+1).

We now turn to the Taylor series expansion of a real-valued function / : Rn —> R about the point xo G Rn. Suppose that / G C2. Let x and a?0 be points in Rn, and let z(a) = sc0 + <*(# ~~ x o ) / | | # — #o||· Define φ : R —► R by

0(α) = / ( ζ ( α ) ) = f(x0 + a(aj - xo)/\\x ~ »oil). Using the chain rule, we obtain

<A'(a) = g ( a )

= Z?/(z(a))ZM«) =

Df(z(a))^~Xo)

\\χ-χο\\

=

(x-x0)TDf(z(a))T/\\x-x0\\

76

ELEMENTS OF CALCULUS

and

d
da

v£)

{a)

;(x-x0)TD2f(z(a))T(x-x0) ;|x-»ol
(x - x0)' D2f(z(a))(x - x0), i|a5-aJo||

where we recall that

Γ d2f

D2f =

~dx\ d2f dx\dx2

d2f dx2dx\
d2f ~dx\

a2/
dxndxi d2f
dxndx2

Observe that

d2f

d2f

d2f

_dx\dxn dx2dxn

dxt

/{χ)=φ(\\χ-χο\\) - Λ(η\ ι I'35 " x*W*'(cΛ 1!
Hence,

I

I

*

-■*oll2 2!

0<"kf(,(O(\\)

_+L

O(||x-xo||2)·

f(x) = f(x0) + γ|£>/(χο)(® - xo)

+ - ( » - x0)TD2f(x0)(x

- x0) + o(||a? - x0||2).

If we assume that / G C3, we may use the formula for the remainder term A3 to conclude that

f(x) = f(x0) + —Df(x0)(x - xo)

+

1 2[(* ~ *o)

D2f(x0)(x

- x0) + 0 ( | | * - *ο|Γ).

We end with a statement of the mean value theorem, which is closely related to Taylor's theorem.

EXERCISES

77

Theorem 5.9 If a function f : Rn —► Rm is differentiable on an open set Ω C Rn, then for any pair of points x,y G Ω, there exists a matrix M such

that

f(x)-f(y)

= M(x-y).

D

The mean value theorem follows from Taylor's theorem (for the case where m = 1) applied to each component of / . It is easy to see that M is a matrix whose rows are the rows of Df evaluated at points that lie on the line segment joining x and y (these points may differ from row to row).
For further reading in calculus, consult [13], [81], [83], [115], [120], [134]. A basic treatment of real analysis can be found in [2], [112], whereas a more advanced treatment is provided in [89], [111]. For stimulating reading on the "big-oh" notation, see [77, pp. 104-108].

EXERCISES

5.1 Show that a sufficient condition for limfc_>oo Ak = O is \\A\\ < 1. 5.2 Show that for any matrix A G R n X n ,

Hint: Use Exercise 5.1.

||A|| > max |λ,(Α)|.
1<2<η

5.3 Consider the function fix) =

(aTx)(bTx),

where a, 6, and x are n-dimensional vectors. a. Find Vf(x). b . Find the Hessian F(x).

5.4 Define the functions / : R2 -> R and g : R -> R2 by f(x) = xf/6 + x | / 4 , g(t) = [3t + 5,2i - 6]T. Let F : R -► R be given by F(t) = f(g(t)). Evaluate ^■(t) using the chain rule.
5.5 Consider f{x) = XiX2/2, g(s,t) = [4s+3t,2s+t]T. Evaluate -j^f(g(s,t)) and §if(g(s,t)) using the chain rule.
5.6 Let x(t) = [el + t3,t2,t + 1]T, t G R, and f(x) = x\x2xl + x\X2 + #3, x = [^i,x2,^3]T G R3. Find -^f(x(t)) in terms of t.

78

ELEMENTS OF CALCULUS

5.7 Suppose that f(x) = o(g(x)). Show that for any given ε > 0, there exists δ > 0 such that if \\x\\ < 5, then ||/(#)|| < ε|^(χ)|.
5.8 Use Exercise 5.7 to show that if functions / : Rn -> R and g : Rn -► R satisfy f(x) = —g(x) + o(g(x)) and g{x) > 0 for all x φ 0, then for all £C ^ 0 sufficiently small, we have f(x) < 0.

5.9 Let

fl(xi,X2) =Xi -x\, /2(xi,x2) = 2xix2-
Sketch the level sets associated with f\{x\,X2) = 12 and / 2 ( # i , x 2 ) = 16 on the same diagram. Indicate on the diagram the values of x = [xi,#2]T for which f(x) = [ / i ( x i , x 2 ) , / 2 ( x i , x 2 ) ] T = [12,16]T.

5.10 Write down the Taylor series expansion of the following functions about the given points Xo. Neglect terms of order three or higher.

a. f(x) = Xle-X* +x2 + l,x0 = [1,0]T. b . f(x) =xj + 2x\x\ + x\, x0 = [1,1]T. c. f(x) = e*1"*2 + eXl+:E2 +x1+x2 + l,x0

= [1,0]T.

PART II
UNCONSTRAINED OPTIMIZATION

CHAPTER 6

BASICS OF SET-CONSTRAINED AND UNCONSTRAINED OPTIMIZATION

6.1 Introduction

In this chapter we consider the optimization problem
minimize f(x)
subject to x G Ω.
The function / : Rn —► R that we wish to minimize is a real-valued function called the objective function or cost function. The vector x is an n-vector of independent variables: x = [xi, #2, · · ·, #n]T £ Rn· The variables X i , . . . , xn are often referred to as decision variables. The set Ω is a subset of Rn called the constraint set or feasible set.
The optimization problem above can be viewed as a decision problem that involves finding the "best" vector x of the decision variables over all possible vectors in Ω. By the "best" vector we mean the one that results in the-smallest value of the objective function. This vector is called the minimizer of / over Ω. It is possible that there may be many minimizers. In this case, finding any of the minimizers will suffice.

An Introduction to Optimization, Fourth Edition.

81

By E. K. P. Chong and S. H. Zak. Copyright © 2013 John Wiley & Sons, Inc.

82

BASICS OF SET-CONSTRAINED AND UNCONSTRAINED OPTIMIZATION

There are also optimization problems that require maximization of the objective function, in which case we seek maximizers. Minimizers and maximizers are also called extremizers. Maximization problems, however, can be represented equivalently in the minimization form above because maximizing / is equivalent to minimizing —/. Therefore, we can confine our attention to minimization problems without loss of generality.
The problem above is a general form of a constrained optimization problem, because the decision variables are constrained to be in the constraint set Ω. If Ω = Rn, then we refer to the problem as an unconstrained optimization problem. In this chapter we discuss basic properties of the general optimization problem above, which includes the unconstrained case. In the remaining chapters of this part, we deal with iterative algorithms for solving unconstrained optimization problems.
The constraint "x G Ω" is called a set constraint Often, the constraint set Ω takes the form Ω = {x : h(x) = 0, g(x) < 0}, where h and g are given functions. We refer to such constraints as functional constraints. The remainder of this chapter deals with general set constraints, including the special case where Ω = Rn. The case where Ω = Rn is called the unconstrained case. In Parts III and IV we consider constrained optimization problems with functional constraints.
In considering the general optimization problem above, we distinguish between two kinds of minimizers, as specified by the following definitions.

Definition 6.1 Suppose that / : Rn —► R is a real-valued function defined on some set Ω C Rn. A point x* G Ω is a local minimizer of / over Ω if there

exists ε > 0 such that f(x) > f(x*) for all x G Ω \ {x*} and \\x — x*\\ < ε.

A point sc* G Ω is a global minimizer of / over Ω if f(x) > f(x*) for all

icefi\{ai*}.

■

If in the definitions above we replace ">" with ">," then we have a strict local minimizer and a strict global minimizer, respectively. In Figure 6.1, we illustrate the definitions for n = 1.
If x* is a global minimizer of / over Ω, we write f(x*) = π ύ η ^ Ω / ( # ) and x* = argminxGQ f(x). If the minimization is unconstrained, we simply write x* = argminjp f(x) or x* = arg min/(cc). In other words, given a real-valued function / , the notation arg min f(x) denotes the argument that minimizes the function / (a point in the domain of / ) , assuming that such a point is unique (if there is more than one such point, we pick one arbitrarily). For example, if / : R —> R is given by f(x) = (x + l ) 2 + 3, then a r g m i n / ( x ) = —1. If we write a r g m i n ^ ^ , then we treat ux G Ω" to be a constraint for the minimization. For example, for the function / above, argmina.>0 f(x) = 0.
Strictly speaking, an optimization problem is solved only when a global minimizer is found. However, global minimizers are, in general, difficult to find. Therefore, in practice, we often have to be satisfied with finding local minimizers.

