IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Processing math: 92%

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

        Cart 
        Create Account
        Personal Sign In 

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Journals & Magazines > IEEE Access > Volume: 7
Fast Methods for Eikonal Equations: An Experimental Survey
Publisher: IEEE
Cite This
PDF
J. V. Gómez ; D. Álvarez ; S. Garrido ; L. Moreno
All Authors
View Document
20
Paper
Citations
982
Full
Text Views
Open Access
Comment(s)

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Problem Formulation
    III.
    Fast Marching Methods
    IV.
    Fast Sweeping Methods
    V.
    Other Fast Methods

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
More Like This
Footnotes

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Three main contributions of this paper: 1) Based on previous publications, a common formulation and notation are given for all the algorithms. 2) A survey of the research... View more
Abstract: Fast methods are very popular algorithms to compute time-of-arrival maps (distance maps measured in time units) solving the Eikonal equation. Since fast marching was prop... View more
Metadata
Abstract:
Fast methods are very popular algorithms to compute time-of-arrival maps (distance maps measured in time units) solving the Eikonal equation. Since fast marching was proposed in 1995, it has been applied to many different applications, such as robotics, medical computer vision, fluid simulation, and so on. From then on, many alternatives to the original method have been proposed with two main objectives: reducing its computational time and improving its accuracy. In this paper, we collect the main single-threaded approaches, which improve the computational time of the standard fast marching method and study them within a common mathematical framework. Then, they are evaluated using isotropic environments, which are representative of their possible applications. The studied methods are the fast marching method with the binary heap, the fast marching method with Fibonacci heap, the simplified fast marching method, the untidy fast marching method, the fast iterative method, the group marching method, the fast sweeping method, the locking sweeping method, and the double dynamic queue method.
Published in: IEEE Access ( Volume: 7 )
Page(s): 39005 - 39029
Date of Publication: 22 March 2019
Electronic ISSN: 2169-3536
INSPEC Accession Number: 18576061
DOI: 10.1109/ACCESS.2019.2906782
Publisher: IEEE
Funding Agency:
Three main contributions of this paper: 1) Based on previous publications, a common formulation and notation are given for all the algorithms. 2) A survey of the research... View more
Hide Full Abstract
Contents
SECTION I.
Introduction

The Fast Marching Method (FMM) has been extensively applied since it was first proposed in 1995 [1] as a solution to isotropic control problems using first-order semi-Langragian discretizations on Cartesian grids. The first approach was introduced by Tsitsiklis [1] , but the most popular solution was given a few months later by Sethian [2] using first-order upwind finite differences in the context of isotropic front propagation. The differences and similarities between both works can be found in [3] . The Fast Sweeping Method (FSM) is a more modern iterative algorithm which uses Gauss-Seidel iterations with alternating sweeping ordering to also solve a discretized Eikonal equation on a rectangular grid [4] . As long as the same first-order upwind discretization is used in both methods, the computed solution with the Fast Sweeping Method is exactly the same as the one given by the Fast Marching Method.

These two different methods, commonly named as Fast Methods [5] , [6] , were originally suggested to simulate a wavefront propagation through a regular discretization of the space. However, many different approaches have been proposed, extending these methods to other discretizations and formulations. For a more detailed history of Fast Methods, we refer the interested readers to [7] .

One of the reasons for the popularity of the Fast Methods is that they can be applied in many different fields, such us: path planning in robotics [8] – [9] [10] [11] [12] ; image segmentation [13] – [14] [15] , shape and surface recognition and segmentation [16] , [17] , volumetric data representation [18] or quantification of lesions in contrast-enhanced tomography [19] in computer vision; traveltime computation in geophysical applications such as tomography [20] – [21] [22] or seismology [23] , [24] . Despite the vast amount of literature on Fast Methods, there is a lack of in-depth comparison and benchmarking among the proposed methods.

In this paper, nine sequential (mono-thread), isotropic, grid-based Fast Methods are detailed in the following sections: Fast Marching Method (FMM), Fibonacci-Heap FMM (FMMFib), Simplified FMM (SFMM), Untidy FMM (UFMM), Group Marching Method (GMM), Fast Iterative Method (FIM), Fast Sweeping Method (FSM), Locking Sweeping Method (LSM) and Double Dynamic Queue Method (DDQM). All these algorithms provide exactly the same solution except for UFMM and FIM, which have bounded errors. However, the question of which one is the best for which applications is still open because of the lack of comparison. For example, [5] compares only FMM and FSM in spite of the fact that GMM and UFMM had already been published. Survey [25] mentions most of the algorithms but only compares FMM and SFMM. A more recent work compares FMM, FSM and FIM in 2D [26] . However, FIM was parallelized and implemented in CUDA providing a biased comparison. Fig. 1 schematically shows the comparisons between algorithms carried out in the literature. As an example, UFMM has barely been compared to its counterparts whereas FMM and FSM are compared in many papers despite the fact that it is well known when each of them performs better: FSM is faster in simple environments with constant speed. In addition, results from one work cannot be directly extrapolated to other works since the performance of these methods highly depends on their implementation.
FIGURE 1.

Comparisons among algorithms. Colors refer to different works: orange [27] , gray [5] , yellow [28] , green [26] , black [29] , and blue [25] .

Show All

Statement of Contributions: Three main contributions are included in this paper: 1) Based on previous publications, a common formulation and notation are given for all the algorithms, presented in Section II . This way, it is possible to easily understand their working principles and mathematical formulation. 2) A survey of the research on designing {n} -dimensional sequential Fast Methods is explained along Sections III , IV and V . 3) Extensive and systematic comparison of the mentioned methods is carried out and explained in Section VI , followed by a discussion in Section VII . The experiments are designed to take into account their possible applications and the results previously reported.

Other Algorithms not Included in the Survey

There are some variations of these approaches which focus on improving some of the characteristics of the solution given by the fast methods. For example, in order to enhance the computation time, parallel approaches have been proposed of both the Fast Marching [30] and Fast Sweeping [31] methods. In [32] the Heat Method, used for computing the geodesic distances in near-linear time, was introduced. Although it outperforms the presented methods in terms of computation time, it only works with constant speed functions, so it does not solve the problems analyzed in the experimental section. Additionally, it is obvious that the accuracy of the computed solution for these methods depends on the chosen grid size, however, higher order approaches [33] , [34] are able to improve the accuracy using the same grid at the cost of more computation time. For applications in which a high accuracy solution of the Eikonal equation at the source point is needed, the factored Eikonal equation leads to much more accurate solutions by analytically handling the source singularity [35] , [36] .

Different two-scale methods are proposed in [7] : Fast Marching-Sweeping Method (FMSM), Heap Cell Method (HCM), and Fast Heap Cell Method (FHCM). They combine the FMM and FSM in order obtain the best features of both algorithms, dividing the grid into two different levels and performing marching on a coarser scale and then sweeping on a finer scale. However, these methods have not been included in this analysis for different reasons: 1) the performance of HCM and FHCM depends on the discretization of the coarse grid, where the optimal parameter depends on the speed profile. Furthermore, FHCM includes additional error. 2) the FMSM error is not mathematically bounded. Thus, the comparison with other Fast Methods becomes more complex. 3) they suppose that the speed is almost constant on domains of arbitrary size [7] , although this is not a restriction for the actual speed function, it is a strong assumption for some of the designed experiments.

Additionally, the single-pass methods suggested in [37] have not been included in this survey because, as the authors conclude, it is not always possible to know in advance which method, among those presented, should be used. This is an important drawback for practical applications such as robotics.
SECTION II.
Problem Formulation

Fast Methods are designed to solve nonlinear boundary value problems. 1 That is, given a domain \Omega and a function F:\Omega \rightarrow \mathbb {R}_{+} which represents the local speed of the motion, drive a system from a starting set \mathcal X_{\mathrm {s}} \subset \Omega to a goal set \mathcal X_{\mathrm {g}} \subset \delta \Omega through the fastest possible path. The Eikonal equation computes the minimum time-of-arrival function T(\mathbf {x}) as follows: \begin{align*} |\nabla T(\mathbf {x})|F(\mathbf {x})=&1, \quad \Omega \subset \mathbb {R}^{N} \\ T(\mathbf {x})=&0,\quad \mathbf {x} ~\text {in}~ \mathcal X_{\mathrm {s}} \tag{1}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} |\nabla T(\mathbf {x})|F(\mathbf {x})=&1, \quad \Omega \subset \mathbb {R}^{N} \\ T(\mathbf {x})=&0,\quad \mathbf {x} ~\text {in}~ \mathcal X_{\mathrm {s}} \tag{1}\end{align*}

Once solved, T(\mathbf {x}) represents a distances (time-of-arrival) field containing the time it takes to go from any point \mathbf {x} to the closest point in \mathcal X_{\mathrm {s}} following the speed on F(\mathbf {x}) .

We assume, without loss of generality, that the domain is a unit hypercube of N dimensions: \Omega = [{0,1}]^{N} . The domain is represented with a rectangular Cartesian grid \mathcal X \subset \mathbb {R}^{N} , containing the discretizations of the functions F(\mathbf {x}) and T(\mathbf {x}) , \mathcal F and \mathcal T respectively. We refer to grid points \mathbf {x}_{\mathrm {ij}}= (x_{i}, y_{i}), \mathbf {x}_{\mathrm {ij}}\in \mathcal X as the point \mathbf {x}= (x,y) in the space corresponding to a cell (i,j) of the grid (for the 2D case). For simplicity of notation, we will denote T_{\mathrm {ij}}= T(\mathbf {x}_{\mathrm {ij}}) \approx T(\mathbf {x}), T_{\mathrm {ij}}\in \mathcal T , that is, T_{\mathrm {ij}} represents an approximation to the real value of the function T(\mathbf {x}) . Analogously, F_{\mathrm {ij}}= F(\mathbf {x}_{\mathrm {ij}}) \approx F(\mathbf {x}), F_{\mathrm {ij}}\in \mathcal F . We also denote the set of von Neumann (4-connectivity in 2D) neighbors of grid point \mathbf {x}_{\mathrm {ij}} by \mathcal {N}(\mathbf {x}_{\mathrm {ij}}) . For a general grid of N dimensions, we will refer to cells by their index (or key) i as \mathbf {x}_{\mathrm {i}} , since a flat representation is more efficient for such a data structure.
A. N-Dimensional Discrete Eikonal Equation

In this section the most common first-order discretization of the Eikonal equation is detailed. It is first derived in 2D for better understanding and then an {n} -dimensional approach is explained. The most common first-order discretization is given in [39] , which uses an upwind-difference scheme to approximate partial derivatives of T(\mathbf {x}) ( D_{\mathrm {ij}}^{\pm x} represents the one-sided partial difference operator in direction \pm x ): \begin{align*}&\qquad T_{x}(\mathbf {x}) \approx D_{\mathrm {ij}} ^{\pm x}T=\frac {T_{i\pm 1,j}- T_{\mathrm {ij}}}{\pm \Delta _{\mathrm {x}} } \\&\qquad T_{y}(\mathbf {x}) \approx D_{\mathrm {ij}} ^{\pm y}T=\frac {T_{i,j\pm 1}- T_{\mathrm {ij}}}{\pm \Delta _{\mathrm {y}} }\tag{2}\\&\left \{{ \begin{array}{c} \max (D_{\mathrm {ij}}^{-x}T,0)^{2}+\min (D_{\mathrm {ij}}^{+x}T,0)^{2}+\\ \max (D_{\mathrm {ij}}^{-y}T,0)^{2}+\min (D_{\mathrm {ij}}^{+y}T,0)^{2} \end{array}}\right \} = \frac {1}{ F_{\mathrm {ij}}^{2}} \tag{3}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}&\qquad T_{x}(\mathbf {x}) \approx D_{\mathrm {ij}} ^{\pm x}T=\frac {T_{i\pm 1,j}- T_{\mathrm {ij}}}{\pm \Delta _{\mathrm {x}} } \\&\qquad T_{y}(\mathbf {x}) \approx D_{\mathrm {ij}} ^{\pm y}T=\frac {T_{i,j\pm 1}- T_{\mathrm {ij}}}{\pm \Delta _{\mathrm {y}} }\tag{2}\\&\left \{{ \begin{array}{c} \max (D_{\mathrm {ij}}^{-x}T,0)^{2}+\min (D_{\mathrm {ij}}^{+x}T,0)^{2}+\\ \max (D_{\mathrm {ij}}^{-y}T,0)^{2}+\min (D_{\mathrm {ij}}^{+y}T,0)^{2} \end{array}}\right \} = \frac {1}{ F_{\mathrm {ij}}^{2}} \tag{3}\end{align*} A simpler but less accurate solution to (3) is proposed in [40] : \begin{equation*} \left \{{ \begin{array}{c} \max (D_{\mathrm {ij}}^{-x}T,- D_{\mathrm {ij}}^{+x}T,0)^{2}+\\ \max (D_{\mathrm {ij}}^{-y}T,- D_{\mathrm {ij}}^{+y}T,0)^{2} \end{array}}\right \} = \frac {1}{ F_{\mathrm {ij}}^{2}} \tag{4}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \left \{{ \begin{array}{c} \max (D_{\mathrm {ij}}^{-x}T,- D_{\mathrm {ij}}^{+x}T,0)^{2}+\\ \max (D_{\mathrm {ij}}^{-y}T,- D_{\mathrm {ij}}^{+y}T,0)^{2} \end{array}}\right \} = \frac {1}{ F_{\mathrm {ij}}^{2}} \tag{4}\end{equation*} and \Delta x and \Delta y are the grid spacing in the x and y directions. Substituting (3) in (4) and letting \begin{align*} T=&T_{i,j} \\[-3pt] T_{\mathrm {x}}=&\min (T_{i-1,j},T_{i+1,j}) \\[-3pt] T_{\mathrm {y}}=&\min (T_{i,j-1},T_{i,j+1})\tag{5}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} T=&T_{i,j} \\[-3pt] T_{\mathrm {x}}=&\min (T_{i-1,j},T_{i+1,j}) \\[-3pt] T_{\mathrm {y}}=&\min (T_{i,j-1},T_{i,j+1})\tag{5}\end{align*} we can rewrite the Eikonal Equation, for a discrete 2D space as: \begin{equation*} \max \left ({\frac {T- T_{\mathrm {x}}}{ \Delta _{\mathrm {x}}},0}\right)^{2}+\max \left ({\frac {T- T_{\mathrm {y}}}{ \Delta _{\mathrm {y}}},0}\right)^{2}=\frac {1}{ F_{\mathrm {ij}}^{2}} \tag{6}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \max \left ({\frac {T- T_{\mathrm {x}}}{ \Delta _{\mathrm {x}}},0}\right)^{2}+\max \left ({\frac {T- T_{\mathrm {y}}}{ \Delta _{\mathrm {y}}},0}\right)^{2}=\frac {1}{ F_{\mathrm {ij}}^{2}} \tag{6}\end{equation*} Since we are assuming that the speed of the front is positive ( F>0 ), T must be greater than T_{\mathrm {x}} and T_{\mathrm {y}} whenever the front wave has not already passed over the coordinates (i,j) . Therefore, (6) can be simplified as: \begin{equation*} \left ({\frac {T- T_{\mathrm {x}}}{\Delta x}}\right)^{2}+\left ({\frac {T- T_{\mathrm {y}}}{\Delta y}}\right)^{2}=\frac {1}{ F_{\mathrm {ij}}^{2}} \tag{7}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \left ({\frac {T- T_{\mathrm {x}}}{\Delta x}}\right)^{2}+\left ({\frac {T- T_{\mathrm {y}}}{\Delta y}}\right)^{2}=\frac {1}{ F_{\mathrm {ij}}^{2}} \tag{7}\end{equation*}

Equation (7) is a regular quadratic equation of the form aT^{2}+bT+c=0 , where: \begin{align*} a=&\Delta _{\mathrm {x}}^{2}+ \Delta _{\mathrm {y}}^{2} \\[-3pt] b=&-2(\Delta _{\mathrm {y}}^{2} T_{\mathrm {x}}+ \Delta _{\mathrm {x}}^{2} T_{\mathrm {y}}) \\[-3pt] c=&\Delta _{\mathrm {y}}^{2} T_{\mathrm {x}}^{2}+ \Delta _{\mathrm {x}}^{2} T_{\mathrm {y}}^{2}-\frac { \Delta _{\mathrm {x}}^{2} \Delta _{\mathrm {y}}^{2}}{ F_{\mathrm {ij}}^{2}}\tag{8}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} a=&\Delta _{\mathrm {x}}^{2}+ \Delta _{\mathrm {y}}^{2} \\[-3pt] b=&-2(\Delta _{\mathrm {y}}^{2} T_{\mathrm {x}}+ \Delta _{\mathrm {x}}^{2} T_{\mathrm {y}}) \\[-3pt] c=&\Delta _{\mathrm {y}}^{2} T_{\mathrm {x}}^{2}+ \Delta _{\mathrm {x}}^{2} T_{\mathrm {y}}^{2}-\frac { \Delta _{\mathrm {x}}^{2} \Delta _{\mathrm {y}}^{2}}{ F_{\mathrm {ij}}^{2}}\tag{8}\end{align*}

In order to simplify the notation for the {n} -dimensional case, we assume that the grid is composed of hypercubic cells, that is, \Delta _{\mathrm {x}}= \Delta _{\mathrm {y}}= \Delta _{\mathrm {z}}= {\dots }= h . Let us denote T_{\mathrm {d}} as the generalization of T_{\mathrm {x}} or T_{\mathrm {y}} for dimension d , up to N dimensions. We also denote by F the propagation speed for the point with coordinates (i,j,k, {\dots }) . Operating and simplifying terms, the discretization of the Eikonal is a quadratic equation with parameters: \begin{align*} a=&N \\ b=&-2\sum \limits _{d=1}^{N} T_{\mathrm {d}} \\ c=&\left({\sum \limits _{d=1}^{N} T_{\mathrm {d}}^{2}}\right) - \frac {h^{2}}{ F^{2}} \tag{9}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} a=&N \\ b=&-2\sum \limits _{d=1}^{N} T_{\mathrm {d}} \\ c=&\left({\sum \limits _{d=1}^{N} T_{\mathrm {d}}^{2}}\right) - \frac {h^{2}}{ F^{2}} \tag{9}\end{align*}
B. Solving the {N} -D Discrete Eikonal Equation

A solution to equation (7) is not straightforward, since there may be more unknowns than equations if the dimension is greater than 1. However, the entropy condition formulated in [41] for moving fronts yields a unique viscosity solution of the equation (7) , which forces the wavefront to follow causality [40] . In other words, in order to reach a point with a higher time of arrival, it should have first traveled through neighbors of such a point with smaller values. The opposite would imply a jump in time continuity and the solutions would be erroneous.

The proposed Eikonal solution (9) does not guarantee the causality of the resulting map, since F and h can have arbitrary values. Therefore, before accepting a solution as valid its causality has to be checked. For instance, in 2D, the Eikonal is solved as: \begin{equation*} T = \frac { T_{\mathrm {x}}+Ty}{2}+\frac {1}{2}\sqrt {\frac {2h^{2}}{ F^{2}}-\left ({T_{\mathrm {x}}- T_{\mathrm {y}}}\right)^{2}} \tag{10}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} T = \frac { T_{\mathrm {x}}+Ty}{2}+\frac {1}{2}\sqrt {\frac {2h^{2}}{ F^{2}}-\left ({T_{\mathrm {x}}- T_{\mathrm {y}}}\right)^{2}} \tag{10}\end{equation*} called the two-sided update, as both parents T_{\mathrm {x}} and T_{\mathrm {y}} are taken into account. The solution is only accepted if T \geq \max \left ({T_{\mathrm {x}}, T_{\mathrm {y}}}\right) . The upwind condition [7] shows that: \begin{equation*} T \geq \max \left ({T_{\mathrm {x}}, T_{\mathrm {y}}}\right) \Longleftrightarrow | T_{\mathrm {x}}- T_{\mathrm {y}}| \leq \frac {h}{ F} \tag{11}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} T \geq \max \left ({T_{\mathrm {x}}, T_{\mathrm {y}}}\right) \Longleftrightarrow | T_{\mathrm {x}}- T_{\mathrm {y}}| \leq \frac {h}{ F} \tag{11}\end{equation*}

If this condition fails, the one-sided update is applied instead: \begin{equation*} T = \min \left ({T_{\mathrm {x}}, T_{\mathrm {y}}}\right)+\frac {h}{ F} \tag{12}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} T = \min \left ({T_{\mathrm {x}}, T_{\mathrm {y}}}\right)+\frac {h}{ F} \tag{12}\end{equation*} This is a top-down approach, in which the parents are iteratively discarded until a causal solution is found. However, generalizing (11) is complex, hence, we choose to use a bottom-up approach: (12) is solved and parents are iteratively included until the time of the next parent is higher than the current solution: T_{k} > T . This procedure is detailed in Algorithms 1 and 2 . The MinTDim() function returns the minimum time of the neighbors in a given dimension (left and right for dim = 1 , bottom and top for dim=2 , etc.). This approach has been found to be more robust when used in three or more dimensions with a negligible impact on the computational performance.
Algorithm 1 Solve Eikonal Equation

procedure SolveEikonal( \mathbf {x}_{\mathrm {i}}, \mathcal T, \mathcal F )

a \leftarrow N

for dim = 1:N do

min_{T} \leftarrow MinTDimdim

if min_{T} \neq \infty ~\textbf {and}~min_{T} < T_{\mathrm {i}} then

T_{\texttt {values}}{\texttt {.push}}(min_{T})

else

a \leftarrow a-1

end if

end for

if a = 0 then \triangleright FSM can cause this situation.

return \infty

end if

\mathcal T _{\texttt {values}} \leftarrow Sort { T}_{\texttt {values}}

for dim = 1:a do

\widetilde {T}_{\mathrm {i}} \leftarrow SolveNDims { \mathbf {x}_{\mathrm {i}}, dim, T_{\texttt {values}}, \mathcal F }

if dim = a or \widetilde {T}_{\mathrm {i}}< \mathcal T _{\texttt {values},dim+1} then

break

end if

end for

return \widetilde {T}_{\mathrm {i}}

end procedure
Algorithm 2 Solve Eikonal for N Dimensions

procedure SolveNDims( \mathbf {x}_{\mathrm {i}}, dim, T_{\texttt {values}}, \mathcal F )

if dim = 1 then

return T_{\texttt {values},1}+\frac {h}{ F_{\mathrm {i}}}

end if

sumT \leftarrow \sum \limits _{i=1}^{dim} T_{\texttt {values},i}

sumT^{2} \leftarrow \sum \limits _{i=1}^{dim} T^{2}_{\texttt {values},i}

a \leftarrow dim

b \leftarrow -2sumT

c \leftarrow sumT^{2} - \frac {h^{2}}{ F_{\mathrm {i}}}

q \leftarrow b^{2}-4ac

if q < 0 then \triangleright Noncausal solution

return \infty

else

return \frac {-b+sqrt(q)}{2a}

end if

end procedure

The next sections introduce the different algorithms, compared in this survey, used to solve the Eikonal equation.
SECTION III.
Fast Marching Methods

The Fast Marching Method (FMM) [2] is the most common Eikonal solver. It can be classified as a label-setting, Dijkstra-like algorithm [42] . It uses a first-order upwind finite difference scheme, which is described in detail in Section II , to simulate an isotropic front propagation computing the solution following Bellman’s optimality principle [43] : \begin{equation*} T_{\mathrm {i}}= \min _{ \mathbf {x}_{\mathrm {i}}\in \mathcal {N}(\mathbf {x}_{\mathrm {i}})}(c_{ij}+ T_{\mathrm {j}}) \tag{13}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} T_{\mathrm {i}}= \min _{ \mathbf {x}_{\mathrm {i}}\in \mathcal {N}(\mathbf {x}_{\mathrm {i}})}(c_{ij}+ T_{\mathrm {j}}) \tag{13}\end{equation*} In other words, a node \mathbf {x}_{\mathrm {i}} is connected to the parent \mathbf {x}_{\mathrm {j}} in its neighborhood \mathcal {N}(\mathbf {x}_{\mathrm {i}}) which minimizes (or maximizes) the value of the function (in this case T_{\mathrm {i}} ) composed by the value of T_{\mathrm {j}} plus the addition of the cost of traveling from \mathbf {x}_{\mathrm {j}} to \mathbf {x}_{\mathrm {i}} , represented as c_{ij} . This discretization takes into account the spatial representation (i.e., a rectangular grid in two dimensions) and the values of all the causal upwind neighbors. This is the main difference with Dijsktra’s algorithm, since Dijkstra is designed to work on graphs, assuming discrete traveling, and the value of a node \mathbf {x}_{\mathrm {i}} only depends on one parent \mathbf {x}_{\mathrm {j}} .

The algorithm labels the cells in three different sets: 1) \texttt {Frozen} : those cells whose value has already been computed and will not change during new iterations, 2) \texttt {Unknown} : cells with no value assigned, to be evaluated, and 3) \texttt {Narrow} band (or just \texttt {Narrow} ): the frontier between \texttt {Frozen} and \texttt {Unknown} containing those cells with a value assigned that may still improve. These sets are mutually exclusive, that is, a cell cannot belong to more than one of them at the same time. The implementation of the \texttt {Narrow} set is a critical aspect of FMM, so a more detailed discussion will be carried out in Section III-A .

The procedure to compute FMM is detailed in Algorithm 3 . Initially, all points 2 in the grid belong to the \texttt {Unknown} set and have an infinite arrival time. The initial points (wave sources) are assigned a value of 0 and inserted in \texttt {Frozen} (lines 2–7). Then, the main FMM loop starts by choosing the element with minimum arrival time from \texttt {Narrow} (line 10) and all its non- \texttt {Frozen} neighbors are evaluated: for each of them the Eikonal is solved and the new arrival time value is kept if it improves the existing one. In case the evaluated cell is in \texttt {Unknown} , it is transferred to \texttt {Narrow} (lines 11–18). Finally, the previously chosen point from \texttt {Narrow} is transferred to \texttt {Frozen} (lines 21 and 22) and a new iteration starts until the \texttt {Narrow} set is empty. The arrival times map \mathcal T is returned as the result of the procedure.
SECTION Algorithm 3
Fast Marching Method

procedure FMM( \mathcal X, \mathcal T, \mathcal F, \mathcal X_{\mathrm {s}} )

\texttt {Unknown} \leftarrow \mathcal X, \texttt {Narrow} \leftarrow \emptyset, \texttt {Frozen} \leftarrow \emptyset

T_{\mathrm {i}}\leftarrow \infty ~\forall \mathbf {x}_{\mathrm {i}} \in \mathcal X

for \mathbf {x}_{\mathrm {i}}\in \mathcal X_{\mathrm {s}} do

T_{\mathrm {i}}\leftarrow 0

\texttt {Unknown} \leftarrow \texttt {Unknown} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {i}}\}

end for

while \texttt {Narrow} \neq \emptyset do

\mathbf {x}_{\mathrm {min}} \leftarrow \arg \min _{ \mathbf {x}_{\mathrm {i}}\in \texttt {Narrow} }{\{ T_{\mathrm {i}}\}}~\triangleright ~\texttt {Narrow} top operation.

for \mathbf {x}_{\mathrm {i}}\in (\mathcal {N}(\mathbf {x}_{\mathrm {min}}) \cap \mathcal X \backslash \texttt {Frozen}) do \triangleright For all neighbors not in Frozen.

\widetilde {T}_{\mathrm {i}} \leftarrow SolveEikonal { \mathbf {X}_{\mathrm {I}}, \mathcal T, \mathcal F }

if \widetilde {T}_{\mathrm {i}}< T_{\mathrm {i}} then

T_{\mathrm {i}}\leftarrow \widetilde {T}_{\mathrm {i}}~\triangleright ~\texttt {Narrow} increase operation if \mathbf {x}_{\mathrm {i}}\in \texttt {Narrow} .

end if

if \mathbf {x}_{\mathrm {i}}\in \texttt {Unknown} then \triangleright ~\texttt {Narrow} push operation.

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Unknown} \leftarrow \texttt {Unknown} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

end if

end for

\texttt {Narrow} \leftarrow \texttt {Narrow} \backslash \{ \mathbf {x}_{\mathrm {min}} \}~\triangleright ~\texttt {Narrow} pop operation: add to Frozen.

\texttt {Frozen} \leftarrow \texttt {Frozen} \cup \{ \mathbf {x}_{\mathrm {min}} \}

end while

return \mathcal T

end procedure

A. Binary and Fibonacci Heaps

FMM requires the implementation of the \texttt {Narrow} set to have four different operations: 1) Push: to insert a new element to the set, 2) Increase: to reorder an element, already existing in the set, whose value has been improved, 3) Top: to retrieve the element with minimum value, and 4) Pop: to remove the element with minimum value. As stated before, this is the most critical aspect of the implementation of FMM. The most efficient way to implement \texttt {Narrow} is by using a min-heap data structure. A heap is an ordered tree in which every parent is ordered with respect to its children. In a min-heap, the minimum value is at the root of the tree and the children have higher values. This is satisfied for any parent node of the tree.

Among all the existing heaps, FMM is usually implemented with a binary heap [44] . However, the Fibonacci Heap [45] has a better amortized time for Increase and Push operations, but it has additional computational overhead with respect to other heaps. For relatively small grids, where the narrow band is composed of few elements and the performance is still far from its asymptotic behavior, the binary heap performs better. Table 1 summarizes the time complexities for these heaps. 3 Note that n is the number of cells in the map, as the worst case is to have all the cells in the heap.
TABLE 1 Summary of Amortized Time Complexities for Common Heaps Used in FMM ( n is the Number of Elements in the Heap)

Each cell is pushed and popped at most once in the heap. For each loop, the top of \texttt {Narrow} is accessed ( \mathcal {O}(1) ), the Eikonal is solved for at most 2^{N} neighbors ( \mathcal {O}(1) for a given N ), these cells are pushed or increased ( \mathcal {O}(\log {n}) in the worst case), and finally the top cell is popped ( \mathcal {O}(\log {n}) ). Therefore each loop is at most \mathcal {O}(\log {n}) . Since this loop is executed at most n times, the total FMM complexity is \mathcal {O}(n\log {n}) , where n represents the total number of cells of the grid, which is the worst case scenario. Furthermore, as pointed out in [7] , the method has a bad cache locality, since adjacent cells on the FMM heap have no spatial relationship and this problem becomes worse as the number of dimensions increases.
B. Simplified Fast Marching Method

The Simplified Fast Marching Method (SFMM) [25] is a relatively unknown variation of the standard FMM which in some cases has an impressive performance. SFMM, detailed in Algorithm 4 , is a reduced version of FMM where \texttt {Narrow} , implemented as a simple priority queue which can contain different instances of the same cell with different values. Additionally, it can happen that the same cell belongs to \texttt {Narrow} and \texttt {Frozen} at the same time. The simplification occurs since no Increase operation is required. Every time a cell has an updated value, it is pushed to the queue. Once it is popped and inserted in \texttt {Frozen} , the remaining instances in the queue are simply ignored.
Algorithm 4 Simplified Fast Marching Method

procedure SFMM( \mathcal X, \mathcal T, \mathcal F, \mathcal X_{\mathrm {s}} )

Initialization as FMM (in Algorithm 3)

while \texttt {Narrow} \neq \emptyset do

\mathbf {x}_{\mathrm {min}} \leftarrow \arg \min _{ \mathbf {x}_{\mathrm {i}}\in \texttt {Narrow} }{\{ T_{\mathrm {i}}\}}~\triangleright ~\texttt {Narrow} top operation.

if \mathbf {x}_{\mathrm {min}} \in \texttt {Frozen} then

\texttt {Narrow} \leftarrow \texttt {Narrow} \backslash \{ \mathbf {x}_{\mathrm {min}} \}

else

for \mathbf {x}_{\mathrm {i}}\in (\mathcal {N}(\mathbf {x}_{\mathrm {min}}) \cap \mathcal X \backslash \texttt {Frozen}) do \triangleright All neighbors not in Frozen.

\widetilde {T}_{\mathrm {i}} \leftarrow SolveEikonal { \mathbf {x}_{\mathrm {i}}, \mathcal T, \mathcal F }

if \widetilde {T}_{\mathrm {i}}< T_{\mathrm {i}} then \triangleright Update arrival time.

T_{\mathrm {i}}\leftarrow \widetilde {T}_{\mathrm {i}}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {i}}\}~\triangleright ~\texttt {Narrow} push operation.

end if

if \mathbf {x}_{\mathrm {i}}\in \texttt {Unknown} then

\texttt {Unknown} \leftarrow \texttt {Unknown} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

end if

end for

\texttt {Narrow} \leftarrow \texttt {Narrow} \backslash \{ \mathbf {x}_{\mathrm {min}} \}~\triangleright ~\texttt {Narrow} pop operation.

\texttt {Frozen} \leftarrow \texttt {Frozen} \cup \{ \mathbf {x}_{\mathrm {min}} \}

end if

end while

return \mathcal T

end procedure

The advantage of this method is that all the increase operations are replaced by push operations. Although both have the same computational complexity, the constant for push is much lower (an increase requires removal and Push operations). Note that the overall computational complexity is maintained, \mathcal {O}(n\log {n}) .
C. Untidy Fast Marching Method

The Untidy Fast Marching Method (UFMM) [29] , [46] follows exactly the same procedure as FMM. However, a special heap structure, which reduces the computational complexity of the method to \mathcal {O}(n) , is used: the untidy priority queue.

This untidy priority queue is closer to a look-up table than to a tree. It assumes that the values of \mathcal F are bounded, hence the values of \mathcal T are also bounded. The untidy queue, depicted in Fig. 2 , is a circular array which divides the maximum range of \mathcal T into a set of k consecutive buckets. Each bucket contains an unordered list of cells with similar values of T_{\mathrm {i}} . The low and high threshold values of each bucket evolve with the iterations of the algorithm, trying to maintain a uniform distribution of the elements in \texttt {Narrow} among the buckets.
FIGURE 2.

Untidy priority queue representation. Top: first iteration, the four neighbors of the initial point are pushed. Middle: the first bucket becomes empty, so the circular array advances one position. Cell c2 is first evaluated because it was the first pushed into the bucket. Bottom: after a few iterations, an entire loop on the queue is about to be completed.

Show All

Since the index of the corresponding bucket can be analytically computed, Push is \mathcal {O}(1) , as well as Top and Pop. Besides, as the number of buckets is smaller than the number of cells, the Increase operation is, in average, \mathcal {O}(1) . Therefore, the total complexity of UFMM is \mathcal {O}(n) . However, since elements within a bucket are not sorted (a FIFO strategy is applied in each bucket), errors are introduced into the final result. Nevertheless, it has been shown that the accumulated additional error is bounded by \mathcal {O}(h) , with h being the cell size, which is the same order of magnitude as in the original FMM.
SECTION IV.
Fast Sweeping Methods

The Fast Sweeping Method (FSM) [4] , [47] is an iterative algorithm which computes the time-of-arrival map by successively sweeping (traversing) the whole grid following a specific order. FSM performs Gauss-Seidel iterations in alternating directions. These directions are chosen so that all the possible characteristic curves of the solution to the Eikonal are divided into the possible quadrants (or octants in 3D) of the environment. For instance, a bi-dimensional grid has four possible Gauss-Seidel iterations (the combinations of traversing x and y dimensions forwards and backwards): North-East, North-West, South-East and South-West, as shown in Fig. 3 .
FIGURE 3.

FSM sweep directions in 2D represented with arrows. The darkest cell is the initial point and the shaded cells are those analyzed by the current sweep (time improved or maintained).

Show All

The FSM is a simple algorithm: it performs sweeps until no value is improved. In each sweep, the Eikonal equation is solved for every cell. However, to generalize this algorithm to N dimensions is complex and, up to our knowledge, there are only 2D and 3D solutions. In this survey we introduce a novel {n} -dimensional version, which is detailed in Algorithm 5 . We will denote the sweeping directions as a binary array \texttt {SweepDirs} with elements 1 or −1, with 1 (−1) meaning forwards (backwards) traversal in that dimension. This array is initialized to 1 (North-East in the 2D case or North-East-Top in 3D) and the grid is initialized as in FMM (lines 2–5). The main loop updates \texttt {SweepDirs} and then a sweep is performed in the new direction (lines 9–10).
SECTION Algorithm 5
Fast Sweeping Method

procedure FSM( \mathcal X, \mathcal T, \mathcal F, \mathcal X_{\mathrm {s}} )

Initialization.

\texttt {SweepDirs} \leftarrow [1, {\dots },1]~\triangleright Initialize sweeping directions.

T_{\mathrm {i}}\leftarrow \infty ~\forall \mathbf {x}_{\mathrm {i}} \in \mathcal X

for \mathbf {x}_{\mathrm {i}}\in \mathcal X_{\mathrm {s}} do

T_{\mathrm {i}}\leftarrow 0

end for

\texttt {stop} \leftarrow \texttt {False}

while \texttt {stop} \neq \texttt {True} do

\texttt {SweepDirs} \leftarrow getSweepDirs { \mathcal X, \texttt {SweepDirs}}

\texttt {stop} \leftarrow Sweep { \mathcal X, \mathcal T, \mathcal F, \texttt { SweepDirs}, N}

end while

return \mathcal T

end procedure

The GetSweepDirs() procedure (see Algorithm 6 ) is in charge of generating the appropriate Gauss-Seidel iteration directions. If a 3D \texttt {SweepDirs} = [{1,1,1}] vector is given, the following sequence will be generated: \begin{align*}&1: [-1,-1,-1]\quad 5: [-1,-1,1] \\&2: [{1,-1,-1}]\quad 6: [{1,-1,1}] \\&3: [-1,1,-1]\quad 7: [-1,1,1] \\&4. [{1,1,-1}]\quad 8: [{1,1,1}]\tag{14}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}&1: [-1,-1,-1]\quad 5: [-1,-1,1] \\&2: [{1,-1,-1}]\quad 6: [{1,-1,1}] \\&3: [-1,1,-1]\quad 7: [-1,1,1] \\&4. [{1,1,-1}]\quad 8: [{1,1,1}]\tag{14}\end{align*} Note that the literature describes at least three different sequences for the sweep pattern and shows that the optimal sequence depends on the environment [24] , [47] . The sequence used in this work has been chosen to be calculated efficiently in an {n} -dimensional version. Besides, it is equally valid as the same directions are visited when the sweeps are done.
SECTION Algorithm 6
Sweep Directions Algorithm

procedure getSweepDirs( \mathcal X, \texttt {SweepDirs} )

for i = 1:N do

\texttt {SweepDirs}_{\mathrm {i}} \leftarrow \texttt {SweepDirs}_{\mathrm {i}} + 2

if \texttt {SweepDirs}_{\mathrm {i}} \leq 1 then

break \triangleright Finish For loop.

else

\texttt {SweepDirs}_{\mathrm {i}} \leftarrow -1

end if

end for

return \texttt {SweepDirs}

end procedure

Finally, the Sweep() procedure (see Algorithm 7 ) recursively generates the Gauss-Seidel iterations following the traversal directions specified by the corresponding value of \texttt {SweepDirs} (line 4). Each recursive level traverses the whole corresponding dimension. Note that the extent of dimension n is denoted by \mathcal X _{\mathrm {n}} . Once the most inner loop is reached, the corresponding cell is evaluated and its value updated if necessary (lines 8–12).
SECTION Algorithm 7
Recursive Sweeping Algorithm

procedure Sweep( \mathcal X, \mathcal T, \mathcal F, \texttt {SweepDirs}, n )

\texttt {stop} \leftarrow \texttt {True}

if n > 1 then

for i \in \mathcal X _{\mathrm {n}} following \texttt {SweepDirs}_{\mathrm {n}} do

\texttt {stop} \leftarrow Sweep { \mathcal X, \mathcal T, \mathcal F, \texttt { SweepDirs}, n-1}

end for

else

for i \in \mathcal X _{1} following \texttt {SweepDirs}_{1} do

\widetilde {T}_{\mathrm {i}} \leftarrow SolveEikonal { \mathbf {x}_{\mathrm {i}}, \mathcal T, \mathcal F }~\triangleright \mathbf {x}_{\mathrm {i}} is the corresponding cell.

if \widetilde {T}_{\mathrm {i}}< T_{\mathrm {i}} then

T_{\mathrm {i}}\leftarrow \widetilde {T}_{\mathrm {i}}

\texttt {stop} \leftarrow \texttt {False}

end if

end for

end if

return \texttt {stop}

end procedure

The FSM carries out as many grid traversals as necessary until the value T_{\mathrm {i}} for every cell has converged. Since no ordering is of the data is used, the evaluation of each cell is \mathcal {O}(1) . As there are n cells and t traversals, the total computational complexity of FSM is \mathcal {O}(nt) .

However, the complexity constants depend greatly on the speed function F(\mathbf {x}) . For instance, in the case of an 2D empty map with constant speed of propagation, four sweeps are enough to cover the entire map, therefore the complexity is O(4n) , which is the minimum possible constant (assuming the start point is not in a corner of the map). On the other hand, the more complex the speed function or the environment are, the more sweeps the algorithm will need to converge to the final solution, increasing the complexity of the method.

Note that, as long as the same first-order upwind discretization is used, the \mathcal T returned by FSM is exactly the same as all the FMM-like algorithms (except UFMM).
A. Locking Sweeping Methods

The Locking Sweeping Method (LSM) [27] is a natural improvement over FSM. The FSM might spend time recomputing T_{\mathrm {i}} of a cell even if none of its neighbors has changed their value since the last sweep, which means that the computed value T_{\mathrm {i}} will be the same as in the last sweep. In order to avoid this, LSM labels each cell as locked or unlocked , and only the ones with the latter label are evaluated in each sweep.

The LSM procedure is detailed in Algorithm 8 . During the initialization, all the cells are labeled as \texttt {Frozen} (the meaning of locked and \texttt {Frozen} is the same). Then, the starting cells \mathcal X_{\mathrm {s}} are assigned a 0 value and all their neighbors are labeled as \texttt {Narrow} (the meaning of unlocked and \texttt {Narrow} is the same). Then, the wave propagation is computed performing as many grid traversals as necessary until no cell improves its time-of-arrival value. As in FSM, the GetSweepDirs() procedure is in charge of generating the appropriate Gauss-Seidel iteration directions.
Algorithm 8 Locking Sweeping Method

procedure LSM( \mathcal X, \mathcal T, \mathcal F, \mathcal X_{\mathrm {s}} )

Initialization.

\texttt {Frozen} \leftarrow \mathcal X, \texttt {Narrow} \leftarrow \emptyset

T_{\mathrm {i}}\leftarrow \infty ~\forall \mathbf {x}_{\mathrm {i}} \in \mathcal X

\texttt {SweepDirs} \leftarrow [1, {\dots },1]~\triangleright Initialize sweeping directions.

for \mathbf {x}_{\mathrm {i}}\in \mathcal X_{\mathrm {s}} do

T_{\mathrm {i}}\leftarrow 0

for \mathbf {x}_{\mathrm {j}}\in \mathcal {N}(\mathbf {x}_{\mathrm {i}}) do \triangleright Unlock neighbors of starting cells.

\texttt {Frozen} \leftarrow \texttt {Frozen} \backslash \{ \mathbf {x}_{\mathrm {j}}\}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {j}}\}

end for

end for

\texttt {stop} \leftarrow \texttt {False}

while \texttt {stop} \neq \texttt {True} do

\texttt {SweepDirs} \leftarrow getSweepDirs { \mathcal X, \texttt {SweepDirs}}

\texttt {stop} \leftarrow LockSweep { \mathcal X, \mathcal T, \mathcal F, \texttt { SweepDirs}, N}

end while

return \mathcal T

end procedure

For every iteration, the recursive locking sweeping algorithm, detailed in Algorithm 9 , is performed. Essentially, it is the same procedure as in FSM. However, there are two main differences: 1) the Eikonal equation is computed only for those cells labeled as \texttt {Narrow} , otherwise they are skipped (see line 9), and 2) after every evaluation, if the time-of-arrival value ( T_{\mathrm {i}} ) of cell \mathbf {x}_{\mathrm {i}} is improved, all neighbors of cell \mathbf {x}_{\mathrm {i}} which have a higher value than T_{\mathrm {i}} are labeled as \texttt {Narrow} so that they are evaluated in the next iteration (lines 14–19).
Algorithm 9 Recursive Locking Sweeping Algorithm

procedure LockSweep( \mathcal X, \mathcal T, \mathcal F, \texttt {SweepDirs}, n )

\texttt {stop} \leftarrow \texttt {True}

if n > 1 then

for i \in \mathcal X _{\mathrm {n}} following \texttt {SweepDirs}_{\mathrm {n}} do

\texttt {stop} \leftarrow LockSweep { \mathcal X, \mathcal T, \mathcal F, \texttt { SweepDirs}, n-1}

end for

else

for i \in \mathcal X _{1} following \texttt {SweepDirs}_{1} do

if \mathbf {x}_{\mathrm {i}}\in \texttt {Narrow} then

\widetilde {T}_{\mathrm {i}} \leftarrow SolveEikonal { \mathbf {x}_{\mathrm {i}}, \mathcal T, \mathcal F }~\triangleright \mathbf {x}_{\mathrm {i}} is the corresponding cell.

if \widetilde {T}_{\mathrm {i}}< T_{\mathrm {i}} then

T_{\mathrm {i}}\leftarrow \widetilde {T}_{\mathrm {i}}

\texttt {stop} \leftarrow \texttt {False}

for \mathbf {x}_{\mathrm {j}}\in \mathcal {N}(\mathbf {x}_{\mathrm {i}}) do

if T_{\mathrm {i}}< T_{\mathrm {j}} then \triangleright Add improvable neighbors to Narrow.

\texttt {Frozen} \leftarrow \texttt {Frozen} \backslash \{ \mathbf {x}_{\mathrm {j}}\}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {j}}\}

end if

end for

end if

\texttt {Narrow} \leftarrow \texttt {Narrow} \backslash \{ \mathbf {x}_{\mathrm {i}}\}~\triangleright Add \mathbf {x}_{\mathrm {i}} to Frozen.

\texttt {Frozen} \leftarrow \texttt {Frozen} \cup \{ \mathbf {x}_{\mathrm {i}}\}

end if

end for

end if

return \texttt {stop}

end procedure

Note that the asymptotic computational complexity of FSM is kept as \mathcal {O}(n) and the number of required sweeps is also maintained. However in practice, it turns out that most of the cells are locked during a sweep, therefore, the time saved during the computation is important.
SECTION V.
Other Fast Methods

This section includes different algorithms which cannot be categorized as Fast Marching-like or Fast Sweeping methods.
A. Group Marching Method

The Group Marching Method (GMM) [48] is an FMM-based Eikonal solver which solves for a group of grid points in \texttt {Narrow} at once, instead of sorting them in a heap structure.

Consider a front propagating, at a given time, the \texttt {Narrow} band will be composed by the set of cells belonging to the wavefront. GMM selects a group G out of \texttt {Narrow} composed by the global minimum and the local minima in \texttt {Narrow} . Then, every neighboring cell to G is evaluated and added to \texttt {Narrow} . These points in G have to be chosen carefully so that causality is not violated, since GMM does not sort the \texttt {Narrow} set. In order to select those values, GMM follows: \begin{equation*} G = \{ \mathbf {x}_{\mathrm {i}}\in \texttt {Narrow}: T_{\mathrm {i}}\leq \min (T_{ \texttt {Narrow} }) + \delta _\tau \} \tag{15}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} G = \{ \mathbf {x}_{\mathrm {i}}\in \texttt {Narrow}: T_{\mathrm {i}}\leq \min (T_{ \texttt {Narrow} }) + \delta _\tau \} \tag{15}\end{equation*} where \begin{equation*} \delta _\tau = \frac {1}{\max (\mathcal F)} \tag{16}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \delta _\tau = \frac {1}{\max (\mathcal F)} \tag{16}\end{equation*}

Although in [48] , in which the GMM was presented, \delta _\tau = \frac {h}{\max (\mathcal F)\sqrt {N}} was used, we have chosen (16) as detailed in [28] , since the results for the original \delta _\tau are much worse than FMM in most cases, reaching one order of magnitude of difference. If the time difference between two adjacent cells is larger than \delta _\tau , their values will barely affect each other since the wavefront propagation direction is more perpendicular than parallel to the line segment formed by both cells. However, the downwind points (those to be evaluated in future iterations) can be affected by both adjacent cells. Therefore, points in G are evaluated twice to avoid instabilities.

GMM is detailed in Algorithm 10 . Its initialization is done in the same way as in FMM. Then, a reverse traversal through the selected points is performed, computing and updating their value (lines 20–24). Next, in lines 28–40 a forward traversal is carried out. The operations used are the same as in the reverse traversal but updating the \texttt {Narrow} and \texttt {Frozen} sets in the same way as it is done in FMM. Then, the main loop updates the threshold T_{\mathrm {m}} every iteration. The simple design of the method allows a straightforward implementation of a generalized {n} -dimensional solution.
Algorithm 10 Group Marching Method

procedure GMM( \mathcal X, \mathcal T, \mathcal F, \mathcal X_{\mathrm {s}} )

\texttt {Unknown} \leftarrow \mathcal X, \texttt {Narrow} \leftarrow \emptyset, \texttt {Frozen} \leftarrow \emptyset

T_{\mathrm {i}}\leftarrow \infty ~\forall \mathbf {x}_{\mathrm {i}} \in \mathcal X

\delta _\tau \leftarrow \frac {1}{\max (\mathcal F)}

for \mathbf {x}_{\mathrm {i}}\in \mathcal X_{\mathrm {s}} do

T_{\mathrm {i}}\leftarrow 0

\texttt {Unknown} \leftarrow \texttt {Unknown} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Frozen} \leftarrow \texttt {Frozen} \cup \{ \mathbf {x}_{\mathrm {i}}\}

for \mathbf {x}_{\mathrm {j}}\in \mathcal {N}(\mathbf {x}_{\mathrm {i}}) do \triangleright Adding neighbors of starting points to Narrow.

T_{\mathrm {i}} \leftarrow SolveEikonal { \mathbf {X}_{\mathrm {J}}, \mathcal T, \mathcal F }

if T_{\mathrm {i}}< T_{\mathrm {m}} then

T_{\mathrm {m}} \leftarrow T_{\mathrm {i}}

end if

\texttt {Unknown} \leftarrow \texttt {Unknown} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {i}}\}

end for

end for

while \texttt {Narrow} \neq \emptyset do

T_{\mathrm {m}} \leftarrow T_{\mathrm {m}} + \delta _\tau

for \mathbf {x}_{\mathrm {i}}\in (\texttt {Narrow} \leq T_{\mathrm {m}})~\texttt {REVERSE} do \triangleright Reverse traversal.

for \mathbf {x}_{\mathrm {j}}\in (\mathcal {N}(\mathbf {x}_{\mathrm {i}})\cap \mathcal X \backslash \texttt {Frozen}) do

\widetilde {T}_{\mathrm {i}} \leftarrow SolveEikonal { \mathbf {x}_{\mathrm {j}}, \mathcal T, \mathcal F }

if \widetilde {T}_{\mathrm {i}}< T_{\mathrm {i}} then

T_{\mathrm {i}}\leftarrow \widetilde {T}_{\mathrm {i}}

end if

end for

end for

for \mathbf {x}_{\mathrm {i}}\in (\texttt {Narrow} \leq T_{\mathrm {m}})~\texttt {FORWARD} do \triangleright Forward tranversal.

for \mathbf {x}_{\mathrm {j}}\in (\mathcal {N}(\mathbf {x}_{\mathrm {i}})\cap \mathcal X \backslash \texttt {Frozen}) do

\widetilde {T}_{\mathrm {i}} \leftarrow SolveEikonal { \mathbf {x}_{\mathrm {j}}, \mathcal T, \mathcal F }

if \widetilde {T}_{\mathrm {i}}< T_{\mathrm {i}} then

T_{\mathrm {i}}\leftarrow \widetilde {T}_{\mathrm {i}}

end if

if \mathbf {x}_{\mathrm {i}}\in \texttt {Unknown} then

\texttt {Unknown} \leftarrow \texttt {Unknown} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {i}}\}

end if

end for

\texttt {Narrow} \leftarrow \texttt {Narrow} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Frozen} \leftarrow \texttt {Frozen} \cup \{ \mathbf {x}_{\mathrm {i}}\}

end for

end while

return \mathcal T

end procedure

It is important to point out that that GMM returns the same solution as FMM. In GMM, every node is evaluated twice before inserting it into \texttt {Frozen} , whereas in FMM it is done only once. However, GMM does not require any sorting of the \texttt {Narrow} set, therefore, it is an \mathcal {O}(n) iterative algorithm that converges in only two iterations (traversals). The value of \delta _\tau could be modified by the user keeping in mind that a higher \delta _\tau would require more iterations to converge. However, a smaller \delta _\tau would require also two traversals, but the group G will be composed by fewer cells. Therefore, as the authors of GMM mention, GMM can be interpreted as an intermediary point between FMM ( \delta _\tau = 0 ) and a purely iterative method [49] ( \delta _\tau = \infty ).
B. Dynamic Double Queue Method

The Dynamic Double Queue Method (DDQM) [27] is inspired by LSM but resembles GMM. DDQM is conceptually simple: the \texttt {Narrow} set is divided into two non-sorted FIFO queues: one with cells to be evaluated sooner and the other one with cells to be evaluated later. Every iteration, an element from the first queue is evaluated. If its arrival time is improved, the neighboring cells with higher time are unlocked and added to the first or second queue, depending on the value of the updated cell. Once the first queue is empty, the queues are swapped and the algorithm continues. The purpose is to achieve a pseudo-ordering of the cells, so that cells with lower value are evaluated first.

Since the queues are not sorted, the arrival time of the same cell could require being solved many times until its value converges. DDQM dynamically computes the threshold value, which sets the division of the two queues, depending on the number of points of each queue, trying to reach an equilibrium. Reference [27] includes an in depth analysis of the update of the threshold in each iteration. In this work, the initial value of the step of the threshold is increased every iteration according to: \begin{equation*} step = \frac {1.5hn}{\sum \limits _{i} F_{\mathrm {i}}} \tag{17}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} step = \frac {1.5hn}{\sum \limits _{i} F_{\mathrm {i}}} \tag{17}\end{equation*} where n is the total number of cells in the grid. Originally, it was suggested to compute this step as step = \frac {1.5n}{h\sum \limits _{i}\frac {1}{ F_{\mathrm {i}}}} . However, the step value should have time units whereas this expression has [t^{-1}] units (probably an error due to the ambiguity of using speed F or slowness f=\frac {1}{F} ). Therefore, (17) is proposed as an alternative in this work.

While the algorithm evolves, every time the first queue is emptied UpdateStep() (see Algorithm 11 ) is called, using the value of the current step , the number of cells inserted in the first queue c_{1} , and the total number of cells inserted c_{\mathrm {total}} as inputs. Then, step is modified so that the number of cells inserted in the first queue is between 65% and 75% of the total inserted cells. This is a conservative approach, since the closer this percentage is to 50% the faster DDQM is. However, the penalization provoked by percentages lower than 50% is much more significant than for higher percentages. Note that in Algorithm 11 the step is increased by a factor of 1.5 but decreased by a factor of 2. This makes step to converge to a value instead of overshooting around the optimal value. Dividing by a larger number causes the first queue to become empty earlier. Thus, the next iteration will finish faster and a better step value can be computed.
Algorithm 11 DDQM Threshold Increase

procedure UPDATE STEP( step, c_{1}, c_{\mathrm {total}} )

\texttt {m} \leftarrow 0.65

\texttt {M} \leftarrow 0.75

Perc \leftarrow 1

if c_{1} > 0 then

Perc \leftarrow \frac {c_{1}}{c_{\mathrm {total}}}

end if

if Perc \leq m then

step \leftarrow step*1.5

else if Perc \geq M then

step \leftarrow \frac {step}{2}

end if

return step

end procedure

The method of DDQM is detailed in Algorithm 12 . As in LSM, points are divided into the locked ( \texttt {Frozen} ) or unlocked ( \texttt {Narrow} ) sets. The initialization labels all the points as frozen except for the neighbors of the start points, which are added to the first queue (lines 2–14). While the first queue is not empty, its front element is extracted and evaluated (lines 18–20). If its time value is improved, all its locked neighbors with higher value are unlocked and added to their corresponding queue.
Algorithm 12 Double Dynamic Queue Method

procedure DDQM( \mathcal X, \mathcal T, \mathcal F, \mathcal X_{\mathrm {s}} )

Initialization:

\texttt {Frozen} \leftarrow \mathcal X, \texttt {Narrow} \leftarrow \emptyset

\mathcal {Q}_{1} \leftarrow \emptyset, \mathcal {Q}_{2} \leftarrow \emptyset

c_{1} \leftarrow 0~\triangleright Counters

c_{\mathrm {total}} \leftarrow 0

step = \frac {1.5hn}{\sum \limits _{i} F_{\mathrm {i}}}\,\,\triangleright n is the total number of cells.

th \leftarrow step

T_{\mathrm {i}}\leftarrow \infty ~\forall \mathbf {x}_{\mathrm {i}} \in \mathcal X

for \mathbf {x}_{\mathrm {i}}\in \mathcal X_{\mathrm {s}} do

T_{\mathrm {i}}\leftarrow 0

for \mathbf {x}_{\mathrm {j}}\in \mathcal {N}(\mathbf {x}_{\mathrm {i}}) do

\mathcal {Q}_{1} \leftarrow \mathcal {Q}_{1} \cup \{ \mathbf {x}_{\mathrm {j}}\}

\texttt {Unknown} \leftarrow \texttt {Unknown} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {i}}\}

end for

end for

while \mathcal {Q}_{1} \neq \emptyset ~\textbf {or}~\mathcal {Q}_{2} \neq \emptyset do

while \mathcal {Q}_{1} \neq \emptyset do

\mathbf {x}_{\mathrm {i}}\leftarrow \mathcal {Q}_{1} . front \triangleright Extracts the front element.

\widetilde {T}_{\mathrm {i}} \leftarrow SolveEikonal { \mathbf {x}_{\mathrm {i}}, \mathcal T, \mathcal F }

if \widetilde {T}_{\mathrm {i}}< T_{\mathrm {i}} then

T_{\mathrm {i}}\leftarrow \widetilde {T}_{\mathrm {i}}

for \mathbf {x}_{\mathrm {j}}\in (\mathcal {N}(\mathbf {x}_{\mathrm {i}})\cap \texttt {Frozen} ) do

\triangleright Add improvable neighbors to their queue.

if T_{\mathrm {i}}< T_{\mathrm {j}} then

\texttt {Frozen} \leftarrow \texttt {Frozen} \backslash \{ \mathbf {x}_{\mathrm {j}}\}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {j}}\}

c_{\mathrm {total}} \leftarrow c_{\mathrm {total}}+1

if T_{\mathrm {i}}\leq th then

\mathcal {Q}_{1} \leftarrow \mathcal {Q}_{1} \cup \{ \mathbf {x}_{\mathrm {j}}\}

c_{1} \leftarrow c_{1}+1

else

\mathcal {Q}_{2} \leftarrow (\mathcal {Q}_{2} \cup \{ \mathbf {x}_{\mathrm {j}}\})

end if

end if

end for

end if

\texttt {Narrow} \leftarrow \texttt {Narrow} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Frozen} \leftarrow \texttt {Frozen} \cup \{ \mathbf {x}_{\mathrm {i}}\}

end while

step \leftarrow UpdateStep {step, c_{1}, c_{\mathrm {total}}}

{SWAP}{\mathcal {Q}_{1}, \mathcal {Q}_{2}}

c_{1} \leftarrow 0

c_{\mathrm {total}} \leftarrow 0

th \leftarrow th+step

end while

return \mathcal T

end procedure

In [27] , three methods were proposed: 1) single-queue (SQ), and therefore a simpler algorithm, 2) two-queue static (TQS), where the step is not updated, and 3) two-queue dynamic (which we call DDQM). SQ and TQS slightly improve on DDQM in some experiments, but when DDQM improves on SQ and TQS (for instance, in environments with noticeable speed changes) the difference can reach one order of magnitude. Therefore, we decided to include DDQM instead of SQ and TQS since it has shown a more adaptive behavior. In any case, any of these methods returns the same solution as FMM.

Regarding its complexity, in the worst case, the whole grid is contained in both queues and traversed many times during the propagation. However, since queue insertion and deletion are \mathcal {O}(1) operations, the overall complexity is \mathcal {O}(n) . Note that SWAP() can be efficiently implemented in \mathcal {O}(1) as a circular binary index, or updating references (or pointers), and therefore there is no need for a real swap operation.
C. Fast Iterative Method

The Fast Iterative Method (FIM) [28] is based on the iterative method proposed by [49] but inspired by FMM. It also resembles DDQM (concretely, its single queue variant). It iteratively evaluates every point in \texttt {Narrow} until it converges. Once a node has converged its neighbors are inserted into \texttt {Narrow} and the process continues. Narrow is implemented as a non-sorted list. The algorithm requires a convergence parameter \epsilon : if T_{\mathrm {i}} is improved less than \epsilon , it is considered as converged. As a result of FIM, if a small enough \epsilon (depending on the environment) is chosen, the same solution as FMM is returned. However, it can be sped up allowing small errors bounded by \epsilon . FIM is designed to be efficient for parallel computing, since all the elements in \texttt {Narrow} can be evaluated simultaneously. However, we are focusing on its sequential implementation in order to have a fair comparison with the other methods.

Algorithm 13 details FIM steps. Its initialization is the same as FMM. Then, for each element in \texttt {Narrow} , its value is updated (lines 13–14). If the value difference is less than \epsilon , the neighbors are evaluated and, in case their value is improved, they are added to \texttt {Narrow} (lines 15–22). Since \texttt {Narrow} is a list, the new elements should be inserted just before the point being currently evaluated, \mathbf {x}_{\mathrm {i}} . Finally, this point is removed from \texttt {Narrow} and labeled as \texttt {Frozen} (lines 25 and 26).
Algorithm 13 Fast Iterative Method

procedure FIM( \mathcal X, \mathcal T, \mathcal F, \mathcal X_{\mathrm {s}}, \epsilon )

Initialization:

\texttt {Frozen} \leftarrow \mathcal X, \texttt {Narrow} \leftarrow \emptyset

T_{\mathrm {i}}\leftarrow \infty ~\forall \mathbf {x}_{\mathrm {i}} \in \mathcal X

for \mathbf {x}_{\mathrm {i}}\in \mathcal X_{\mathrm {s}} do

T_{\mathrm {i}}\leftarrow 0

for \mathbf {x}_{\mathrm {j}}\in (\mathcal {N}(\mathbf {x}_{\mathrm {i}})\cap \texttt {Unknown}) do

\texttt {Frozen} \leftarrow \texttt {Frozen} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {i}}\}

end for

end for

while \texttt {Narrow} \neq \emptyset do

for \mathbf {x}_{\mathrm {i}}\in \texttt {Narrow} do

\widetilde {T}_{\mathrm {i}}\leftarrow T_{\mathrm {i}}

T_{\mathrm {i}} \leftarrow SolveEikonal { \mathbf {x}_{\mathrm {i}}, \mathcal T, \mathcal F }

if | T_{\mathrm {i}}- \widetilde {T}_{\mathrm {i}}| < \epsilon then

for \mathbf {x}_{\mathrm {j}}\in (\mathcal {N}(\mathbf {x}_{\mathrm {i}})\cap \texttt {Frozen}) do

\widetilde {T}_{\mathrm {j}} \leftarrow SolveEikonal { \mathbf {x}_{\mathrm {j}}, \mathcal T, \mathcal F }

if \widetilde {T}_{\mathrm {j}}< T_{\mathrm {j}} then

T_{\mathrm {j}}\leftarrow \widetilde {T}_{\mathrm {j}}

\texttt {Frozen} \leftarrow \texttt {Frozen} \backslash \{ \mathbf {x}_{\mathrm {j}}\}

\triangleright Insert in the Narrow band just before \mathbf {x}_{\mathrm {i}}

\texttt {Narrow} \leftarrow \texttt {Narrow} \cup \{ \mathbf {x}_{\mathrm {j}}\}

end if

end for

\texttt {Narrow} \leftarrow \texttt {Narrow} \backslash \{ \mathbf {x}_{\mathrm {i}}\}

\texttt {Frozen} \leftarrow \texttt {Frozen} \cup \{ \mathbf {x}_{\mathrm {i}}\}

end if

end for

end while

return \mathcal T

end procedure

During the different iterations of the algorithm, a node can be added several times to the \texttt {Narrow} set, since every time an upwind (parent) neighbor is updated, the node can improve its value. In the worst case, \texttt {Narrow} contains the whole grid and the loop would go through all the points several times. Operations on the list are \mathcal {O}(1) , therefore, the overall computational complexity of FIM is \mathcal {O}(n) .
SECTION VI.
Experimental Comparison
A. Experimental Setup

In order to make an impartial and meaningful comparison, all the algorithms have been implemented from scratch, in C++11 using the Boost Heap library. An automatic benchmark application has been created so that the experiments can be carried out and evaluated in the most systematic possible way. This implementation is focused on time performance, and was compiled using G++4.8.4 with optimization flag -Ofast. However, no special optimizations have been included. All algorithms use the same primitive functions for the grid and cell computations. The times reported correspond to an Ubuntu 14.04 64 bits on a virtual machine using 6 cores of 4GHz with 8GB of RAM. However, all experiments were carried out in one core. In order to calculate the time used by each algorithm, only propagation times are taken into account. The computation time used in the initialization has been omitted since it can be done offline, besides it is similar for all algorithms and is only a small percentage of the total computation time.

Since the algorithms are deterministic, the deviation in the computation time between different runs is theoretically 0. In fact, this deviation mostly depends on the OS scheduler and not on the algorithm, as this will perform the exact same number of operations in all the runs. However, the results shown are the mean of ten runs for every algorithm, and the deviation of the results is practically zero.

For UFMM, the default parameters are a maximum range of \mathcal T of 2 units and 1000 buckets (the checkerboard experiment required different ones, see Section VI-A.4 ). The \epsilon parameter for FIM is set to 0 (actually 10^{-47} to provide robust 64 bit double comparison).

Although error analysis is not within the scope of this paper, it can be compared using the results in the existing literature since it is implementation-independent. UFMM errors are reported in those experiments with non-constant speed. Usually, the L_{1} and L_{\infty } norms of the error are reported. Most of the literature computes norm L_{1} as: \begin{equation*} | \mathcal T |_{1} = \sum \limits _{ \mathbf {x}_{\mathrm {i}}\in \mathcal X } | T_{\mathrm {i}}| \tag{18}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} | \mathcal T |_{1} = \sum \limits _{ \mathbf {x}_{\mathrm {i}}\in \mathcal X } | T_{\mathrm {i}}| \tag{18}\end{equation*} where \mathcal X is treated as a regular vector. However, following [7] , we treat the numerical solutions as elements of L_{p} spaces (a generalization of the p -norm to vector spaces), in which the L_{1} norm is defined as an integral over the function. The result is a norm closely related to its physical meaning and independent of the cell size. L_{1} is numerically integrated over the domain and therefore computed as (assuming hypercubic cells and grids): \begin{equation*} | \mathcal T |_{1} = \sum \limits _{ \mathbf {x}_{\mathrm {i}}\in \mathcal X } | T_{\mathrm {i}} h^{N}| = h^{N}\sum \limits _{ \mathbf {x}_{\mathrm {i}}\in \mathcal X } | T_{\mathrm {i}}| \tag{19}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} | \mathcal T |_{1} = \sum \limits _{ \mathbf {x}_{\mathrm {i}}\in \mathcal X } | T_{\mathrm {i}} h^{N}| = h^{N}\sum \limits _{ \mathbf {x}_{\mathrm {i}}\in \mathcal X } | T_{\mathrm {i}}| \tag{19}\end{equation*}

Four different experiments have been carried out, which represent the most characteristic cases for the Fast Methods. They have been chosen so that the advantages and disadvantages of each algorithm can be remarked. These experiments were chosen attending to the most common situations tested in the literature. By combining the characteristics of these problems, it is possible to obtain similar features to those found in real applications. Furthermore, two more experiments have been included to test the application of the Fast Methods to real applications such as path planning or medical imaging.
1) Empty Map

This experiment is designed to show the performance of the methods in the most basic situation, where most of the algorithms perform best. An empty map with constant speed represents the simplest possible case for the Fast Methods. In fact, analytical methods could be implemented by computing the Euclidean distance from every point to the initial point. However, it is interesting because it shows the performance of the algorithms on open spaces which, in a real application, can be part of large environments.

The same environment is divided into a different number of cells to study how the algorithms behave as the number of cells increases. Composed of empty 2D, 3D and 4D hyper-cubical environments of size [{0,1}]^{N} , with N=2,3,4 . The speed is a constant F_{\mathrm {i}}= 1 on \mathcal X . The wavefront starts at the center of the grid. This experimental setup can be found in previous publications such as [27] , [28] , and [50] .

The number of cells was chosen so that an experiment has the same (or as close as possible) number of cells in all dimensions. For instance, a 50\times50 2D grid has 2500 cells. Therefore, the equivalent 3D grid is 14\times14\times14 (2744) and in 4D is 7\times7\times7\times7 (2401). This way, it is possible to also analyze the performance of the algorithms for different numbers of dimensions. Thus, we have chosen the following number of cells for each dimension for 2D grid: \begin{align*}&\hspace {-0.8pc}2D: \{50, 100, 200, 400, 800, 1000, \\&~\qquad \qquad ~\qquad \qquad \qquad 1500, 2000, 2500, 3000, 4000\}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*}&\hspace {-0.8pc}2D: \{50, 100, 200, 400, 800, 1000, \\&~\qquad \qquad ~\qquad \qquad \qquad 1500, 2000, 2500, 3000, 4000\}\end{align*}

Consequently, the 3D and 4D cells are: \begin{align*} 3D:&\{14, 22, 34, 54, 86, 100, 131, 159, 184, 208, 252\}\\ 4D:&\{7, 10, 14, 20, 28, 32, 39, 45, 50, 55, 63\}\end{align*} View Source Right-click on figure for MathML and additional features. \begin{align*} 3D:&\{14, 22, 34, 54, 86, 100, 131, 159, 184, 208, 252\}\\ 4D:&\{7, 10, 14, 20, 28, 32, 39, 45, 50, 55, 63\}\end{align*}
2) Alternating Barriers

In this case, we want to analyze how the algorithms behave with obstacles ( F_{\mathrm {i}}= 0 ) in a constant speed environment ( F_{\mathrm {i}}= 1 ). The obstacles cause the characteristics to change.

The experiment contains a 2D environment of constant size [{0,1}]\times[{0,2}] discretized in a 1000\times2000 grid. A variable number of alternating barriers are equally distributed along the longest dimension. The number of barriers goes from 0 to 9. Examples are shown in Fig. 4 . Analogously, in 3D, a [{0,1}]\times[{0,1}]\times[{0,2}] environment represented by a 100\times100\times200 grid is chosen, with equally distributed alternating barriers (from 0 to 9) along the z -axis. In all cases, the wavefront starts close to a corner of the map. Similar experimental setups can be found in the literature [7] , [26] , [28] .
FIGURE 4.

2D alternating barriers environments. (a) 1 barrier. (b) 5 barriers. (c) 9 barriers.

Show All

3) Random Speed Function

This experiment aims to test the performance of the algorithms with random speed function (similar to noisy images, as in the case of medical computer vision). It creates 2D, 3D and 4D environments of size [{0,1}]^{N} with N=2,3,4 discretized in a 2000\times2000 grid in 2D, 159\times159\times159 in 3D, and 45\times45\times45\times45 in 4D. These discretizations are chosen so that it is possible to make a direct comparison with the empty map problem for the corresponding grid sizes. The wavefront starts in the center of the grid. This setup is inspired by the experiments carried out in [27] , [28] , and [50] .

Additionally, the maximum speed is increased from 1 to 100 (in steps of 10 units) to analyze how the algorithms behave with increasing speed changes. 2D examples are shown in Fig. 5 .
FIGURE 5.

2D random speed function environments. Lighter color means faster wave propagation. (a) Max. speed = 30. (b) Max. speed = 60. (c) Max. speed = 100.

Show All

4) Checkerboard

The random speed function experiment already tested changes in the speed. However, those are high-frequency changes because it is unlikely to have two adjacent cells with the same speed. In this experiment low-frequency changes are studied. The same environment and discretizations as with random speed function are now divided like a checkerboard, alternating minimum and maximum speed. Analogously, the maximum speed is increased from 1 to 100, while the minimum speed is always 1. There are 10 checkerboard divisions for each dimension. The wavefront starts in the center of the grid. 2D examples are shown in Fig. 6 . This experimental setup is inspired by the experiments carried out in [7] .
FIGURE 6.

2D checkerboard environments. Lighter color means faster wave propagation. (a) Max. speed = 30. (b) Max. speed = 60. (c) Max. speed = 100.

Show All

In this case, UFMM in 3D and 4D performed very poorly with the default parameters. Additional tests not included in the present paper show that the best parameters for UFMM are approximately 1000 buckets with a maximum range of 0.01 in 3D. In the 4D case, 20000 buckets and a maximum range of 0.025 are used.
5) Path Planning

Fast Methods are commonly used for low-dimensional path planning problems, given that they produce deterministic results, are complete (will find a solution if there is any), and it is possible to easily influence some properties of the paths, such as their smoothness or obstacle clearance [51] . Fast Methods are applied from a given start point until a goal point is reached and labeled as frozen, then gradient descent is applied from the goal point in order to obtain a path to the global minimum (the start point). The gradient descent step is omitted in these experiments as it is out of the scope of this paper.

Experiments in 2D and 3D maps are included. It is important to remark that since the Fast Methods are based on grids, they scale exponentially with the number of dimensions. To the best of the authors’ knowledge, in path planning, there are no practical applications of Fast Methods further than 3D.

For 2D, the chosen map shown in Fig. 7 belongs to the Intel Research Laboratory building in Seattle. This map is commonly used in path planning and robot navigation research [52] , [53] . The map is square-shaped, and contains binary values (1 for free cells, 0 for occupied cells). Similarly to the empty map experiment, different resolutions have been chosen for the sides of the square: \begin{equation*} \{400, 800, 1000, 1500, 2000, 2500, 3000, 4000\}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \{400, 800, 1000, 1500, 2000, 2500, 3000, 4000\}\end{equation*}
FIGURE 7.

2D gridmap: Intel Research Lab building in Seattle, 2500\times2500 px.

Show All

For the 3D case, binary 3D gridmaps with different resolutions are created from a 3D model of a building used for video games, shown in Fig. 8 , which has been taken from the Internet 4 with slight modifications (such as adding more pieces of furniture or more internal rooms). This model is composed of two floors. The first floor is divided into three rooms and all of them contain some furniture. The second floor has one large room. The Fast Methods are applied to the whole environment with the center of the first floor as starting point. The maximum size of the gridmap created is 355\times190\times237 . In this case, the following resolutions are chosen for the first dimension (the other dimensions are linearly scaled): \begin{equation*} \{19, 30, 48, 76, 122, 141, 185, 224, 260, 293, 355\}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} \{19, 30, 48, 76, 122, 141, 185, 224, 260, 293, 355\}\end{equation*}
FIGURE 8.

3D mesh of the building used to generate 3D gridmaps. (a) Exterior view. (b) Interior view (internal walls are omitted).

Show All

The initial point for the propagation has been arbitrarily chosen in both cases. However, as the algorithms are run without a goal point, all the reachable cells in the map are evaluated. Therefore, although the choice of the initial point can modify the results, the experimental setup guarantees that the impact is negligible.
6) Vessel Segmentation

Another of the main uses of Fast Methods is computer vision for medical applications, as intermediary steps in more complex algorithms. For example, in [54] , FMM is used for 2D vessel segmentation in retina images based on the assumption that vessels usually follow a smooth path. Flórez-Valencia et al. [55] perform a center line extraction in arteries using FMM, after defining an appropriate speed function and stopping criterion. It is then used to extract 2D contours in cross-sectional planes. The contours are finally used to progressively reconstruct a regularized continuous 3D surface.

This section aims to show the performance of the Fast Methods in such applications. More concretely, a vessel segmentation algorithm similar to the one used in [54] is implemented. When using a Fast Method for segmentation, the main goal is to define a speed function, F(\mathbf {x}) , which provokes the wave expansion to happen faster in those areas which have to be segmented. In this case, the image is first processed with a high pass filter in order to subtract the smoothly varying background. Then, a speed function, in which the pixels corresponding to vessels have a higher value, is computed. Using this function, the Fast Method is performed using a central point of a vessel as starting point. Finally, gradient descent is used to extract the geodesics, which correspond to center lines of the vessels.

This experiment involves the use of the Fast Methods in grids with slightly structured propagation speed, which can be considered as a mix between the random speed function and checkerboard experiments. In this case, two examples are covered: 2D and 3D vessel segmentation, using as input for the algorithms the images shown in Fig. 9 , taken from the DRIVE database of retinal vessels [56] . The 2D image has a resolution of 2560\times2560 pixels with a range of speed [1.7234, 100] (of the preprocessed image according the aforementioned segmentation algorithm), whereas the 3D case is composed of a grid of 128\times128\times128 voxels with a range of speed [1, 100]. As in previous cases, a total of ten runs per algorithm are executed for each case. The starting point in the 3D case is close to one of the bottom corners.
FIGURE 9.

Grids used as inputs for the vessels segmentation algorithm. (a) 2D vessels initial grid. (b) Binarized 3D vessels grid after segmentation.

Show All

In this experiment, as it focuses on a real application, the range of speed values used is smaller than those presented in the experiments above, since these values are due to the real image values. Besides, after the application of the Fast Methods, the necessary steps to complete the segmentation algorithm are not included in the experiment, because they are out of the scope of this study. It is also important to remark that in this kind of application, the Fast Methods employed are usually those based on high-accuracy solutions [57] , which are generally slower than the methods included in this paper.
B. Results

The next sections present the results of the experiments explained in Section VI-A . Their analysis is grouped attending to the classification given in Sections III , IV , and V .
1) Empty Map

An example of the time-of-arrival field computed by FMM is shown in Fig. 10 . Note that all algorithms provide the same exact solution in this case. The higher the resolution, the better the accuracy.
FIGURE 10.

Example of the resulting time-of-arrival maps applying FMM to the empty environment in 2D. (a) 50\times50 . (b) 800\times800 . (c) 4000\times4000 .

Show All

The results for the empty map experiment are shown in Fig. 11 for 2D, Fig. 12 for 3D, and Fig. 13 for 4D. In all cases two plots are included: raw computation times for each algorithm, and time ratios computed as: \begin{equation*} ratio = \frac {\text {FMM Time}}{\text {Alg. Time}}\tag{20}\end{equation*} View Source Right-click on figure for MathML and additional features. \begin{equation*} ratio = \frac {\text {FMM Time}}{\text {Alg. Time}}\tag{20}\end{equation*} so that larger ratios represent better performances.
FIGURE 11.

Computation times and ratios for the empty map environment in 2D. (a) Computation times. (b) Time ratios against FMM.

Show All
FIGURE 12.

Computation times and ratios for the empty map experiment in 3D. (a) Computation times. (b) Time ratios against FMM.

Show All
FIGURE 13.

Computation times and ratios for the empty map experiment in 4D. (a) Computation times. (b) Time ratios against FMM.

Show All

FMM is, as expected, the slowest algorithm in almost all cases, because the rest of the algorithms were proposed as improvements of FMM. Besides, an empty map environment is the most favorable case for any of the algorithms. As the number of cells increases, FMMFib quickly outperforms FMM since the number of elements in the narrow band increases exponentially with the number of dimensions, and therefore the better amortized times of the Fibonacci heap become useful.

SFMM, the other \mathcal {O}(n\log {n}) Fast Method, is always faster than FMM and most of the times also faster then FMMFib, due to its simpler and faster heap management. However, as the number of cells increases, the tendencies of FMMFib and SFMM are very similar to that of FMM (the ratio remains constant as the number of cells increase). When the number of cells is large enough, it is hard to say whether SFMM or FMMFib are faster.

The sweeping-based methods show a similar behavior to the FMM-based methods. FSM is only slower than FMM-based methods for environments with a small number of cells. However, its linear complexity quickly makes it faster than FMM, FMMFib, and SFMM if the environment becomes larger. In spite of this, when the number of dimensions increases, the number of required sweeps also increases (duplicates) and therefore this penalizes the algorithm, as it will evaluate each cell more times. LSM and DDQM are methods that improve on FSM by avoiding the recomputation of the cells. Therefore, they become the fastest algorithms as they do not deal with heap operations and they minimize cell recomputation. In this case, LSM is faster in most cases (in 4D it is slower than DDQM but presents a better tendency than DDQM). This happens because DDQM maintains two queues. Whereas the operations of these queues are efficient, they still represent some overhead over LSM, which does not update any internal container.

The iterative algorithms, such as GMM and FIM, present moderate results with similar behavior. As they keep simple data structures, they usually are faster than FMM-like methods. However, they do not leverage the brute-force approach followed by sweeping-based methods, adding some additional overhead to the iterations and thus, being usually slower than them. UFMM also provides average computation times for a similar reason: it maintains a heap, something which is more efficient than FMM-like algorithms, but still requires performing additional operations in comparison to sweeping-based methods. As the speed is constant all over the grid, UFMM provides the same solution as the other methods.

In a previous comparison between GMM and FMM [28] , GMM was about 50% faster than FMM in all cases. In the results presented here, GMM is at most 40% better. We attribute this difference to the implementation, as the heaps for FMM and FMMFib are highly optimized. Therefore, it is worth mentioning that the results shown here are also slightly subject to implementation details and, sometimes, details that are out of the reach of regular users, such as internal cache memory management, prefetchers, and other low-level details of the hardware used.

The conclusions from these experiments are that, in the absence of obstacles and propagation speed modifications, sweeping-like methods perform the best. Although this setup is unlikely to be present in a practical scenario, the results allow understanding the behavior of the algorithms in ideal situations and their major advantages.
2) Alternating Barriers

An example of the results of FMM in some of the alternating barriers environments is shown in Fig. 14 , whereas its performance results (times and ratios) for 2D and 3D are shown in Fig. 15 and Fig. 16 correspondingly.
FIGURE 14.

Example of the resulting time-of-arrival maps applying FMM to some of the alternating barriers environment in 2D. (a) 1 barrier. (b) 5 barriers. (c) 9 barriers.

Show All
FIGURE 15.

Computation times and ratios for the alternating barriers experiment in 2D. (a) Computation times. (b) Time ratios against FMM.

Show All
FIGURE 16.

Computation times and ratios for the alternating barriers experiment in 3D. (a) Computation times. (b) Time ratios against FMM.

Show All

Overall, the results are very similar to the empty map experiment. FMM and FMMFib are again the slowest algorithms when the number of barriers is low, since this setup is very similar to the empty map experiment. SFMM outperforms FMM and FMMFib in all cases, and it again performs better that FIM and GMM. However, in the case of 3D, GMM performs noticeably worse than in 2D due to its overhead while managing the narrow band. The results of FIM and UFMM are also similar to those in the previous experiment.

The most important results are those of the sweep-based methods. This type of map requires more sweeps to cover the full environment as it becomes more complex (i.e., as the number of barriers increases), which negatively affects FSM and LSM. FSM becomes the slowest algorithm as soon as the environment is slightly complex. LSM is still faster than many of the other algorithms since it avoids reevaluating the cells over the sweeps, although its increasing trends show that it is still heavily affected by the environment. In 3D the trend is not as uniform as in 2D. The reason is that in 2D a cell can only be evaluated from four directions. But these directions increase exponentially with the number of dimensions, therefore there are more chances that the sweep directions are aligned with the environment making the sweeps more efficient since each sweep evaluates more unexplored cells.

As the propagation speed of the environment remains constant, the effect on DDQM of the complexity of the map is almost negligible. Also, UFMM provides again the same solution as the other methods.

Finally, the fact that most of the algorithms show a decreasing trend as the number of barriers increase is simply because with more barriers there are fewer cells to be evaluated.
3) Random Speed Function

: The output of the FMM for the random speed map ( Fig. 17 ) is apparently close to the one of the empty map, but with the wavefronts slightly distorted because of the speed changes. However, the performance of the algorithms is greatly modified. The 2D, 3D and 4D results are respectively shown in Fig. 18 , Fig. 19 , and Fig. 20 . In this case, raw computation times are shown together with a zoomed view of the fastest algorithms to make the analysis easier. Note that all the methods become slower with non-constant speed functions. This happens because the narrow band tends to contain more elements in these cases, leading to slower operations. But also the cells have to be reevaluated more times than usual because the values of the neighbors change more frequently.
FIGURE 17.

Example of the resulting time-of-arrival maps applying FMM to the random speed function environment in 2D. (a) Max. speed = 30. (b) Max. speed = 60. (c) Max. speed = 100.

Show All
FIGURE 18.

Computation times for the random speed function experiment in 2D. (a) Computation times. (b) Zoomed in view of the lower part of the chart in a).

Show All
FIGURE 19.

Computation times for the random speed function experiment in 3D. (a) Computation times. (b) Zoomed in view of the lower part of the chart in a).

Show All
FIGURE 20.

Computation times for the random speed function experiment in 4D. (a) Computation times. (b) Zoomed in view of the lower part of the chart in a).

Show All

FMM and FMMFib have the predictable behavior, FMM being faster (just because of the size of the chosen map). As expected, SFMM has a better performance than FMM and similar to FMMFib . These algorithms were designed so that they do not make any assumption on the environment and therefore they are not affected by complexity or propagation speed.

Sweep-based methods become unstable (in terms of asymptotic computational time) with non-uniform speed function. The main reason is that high contrasts in the speed can act as barriers and therefore more sweeps can be required. DDQM is able to maintain the fastest time for slight speed changes, but when the changes are sharper the double-queue threshold becomes unstable. However, for a large number of dimensions, this effect vanishes (its computation time is barely modified with the number of dimensions) and DDQM becomes one of the fastest algorithms. An explanation for this is that the overhead caused by the required sweeps is lower (even if more sweeps are required) than the overhead the other algorithms suffer when increasing the dimensions.

In this case, GMM and FIM are the fastest algorithms. Although FIM requires multiple iterations over the same cells in order to converge to the real value, it only iterates over the narrow band, being faster than sweep-based methods (although these iterations makes it slower in lower dimensions). On the other hand, GMM presents a constant computation time because it will always converge in two iterations, regardless of the speed, thus becoming the most efficient algorithm.

Finally, UFMM requires special attention as it does not provide the same solution as the other Fast Methods. Its performance is very sensitive to the number of dimensions. The main reason is the election of the parameters: they were experimentally chosen to optimize the 2D performance. However, these parameters are no longer useful for different number of dimensions. We consider UFMM parameter tuning to be a complex task.

Table 2 summarizes the largest errors for this experiment. As the number of dimensions increases, the error decreases while the computation time increases exponentially. Therefore, by properly tuning the parameters for 3D and 4D better times could be achieved while keeping a negligible error in most practical cases.
TABLE 2 Largest L_{1} and L_\infty Errors for UFMM in the Random Speed Function Experiment

4) Checkerboard

Different time-of-arrival maps computed by FMM based on the checkerboard grid are shown in Fig. 21 . The numerical results of the computation times are included in Fig. 22 for 2D, Fig. 23 for 3D, and Fig. 24 for 4D.
FIGURE 21.

Example of the resulting time-of-arrival maps applying FMM to the checkerboard environment in 2D. (a) Max. speed = 30. (b) Max. speed = 60. (c) Max. speed = 100.

Show All
FIGURE 22.

Computation times and ratios for the checkerboard experiment in 2D. (a) Computation times. (b) Time ratios against FMM.

Show All
FIGURE 23.

Computation times and ratios for the checkerboard experiment in 3D. (a) Computation times. (b) Time ratios against FMM.

Show All
FIGURE 24.

Computation times and ratios for the checkerboard experiment in 4D. (a) Computation times. (b) Time ratios against FMM.

Show All

The results of FMM, FMMFib and SFMM remain the same as in previous experiments. The main difference between these experiments and the one with random speed function is that the environment presents a well-defined structure and, locally, acts as a constant speed environment as in the empty map experiment.

For sweeping-based methods, the results are relatively close to those in the random speed function experiment. However, the differences between the algorithms are much smaller. DDQM presents a poor performance in 2D, but in 3D and 4D it becomes the fastest algorithm for higher speed modifications. The reason for this behavior is that DDQM sweeps over a queue and, therefore, there is an overhead for maintaining this queue that becomes negligible once the number of elements in the queue is large enough. Besides, if the number of cells of the environment is increased, the same behavior is expected. Also, the gap between FSM/LSM and the remaining algorithms is much smaller than in the random speed function experiment. This is because in this case the environment presents some structure, allowing for more cells to be evaluated at every sweep and thus becoming more efficient, counteracting the overhead of performing more sweeps because of the different speed values.

GMM and FIM perform very similarly to the experiment with random speed function, but in this case their results are closer to each other. FIM requires more evaluations only on those cells whose neighbors have different propagation speed than the evaluated one. Because of the structure of the environment, FIM does not require so many iterations to converge to the cell value. In fact, it can solve most of the cells in only two iterations, as it is very likely that the propagation speed is constant in the neighborhood of the cell to be evaluated. These additional evaluations with respect to GMM are only noticeable in 2D, when the results are more susceptible to internal data structure overheads.

UFMM becomes worse with the number of dimensions but it is among the fastest algorithms in lower dimensions. Again, it is worth noting that these results greatly depend on the parameters chosen, which in this case seem to be close to optimal. UFMM errors are shown in Table 3 .
TABLE 3 Largest L_{1} and L_\infty Errors for UFMM in the Checkerboard Experiment

5) Path Planning

The FMM time-of-arrival field is shown in Fig. 25 for the 2D path planning problem. As the map is composed of binary values, all algorithms provide the exact same solution. Visualization of the results of the 3D path planning problem is not included as they cannot be clearly depicted with a single figure.
FIGURE 25.

Example of the resulting time-of-arrival maps applying FMM to 2D path planning map. (a) 200\times200 . (b) 1500\times1500 . (c) 4000\times4000 .

Show All

The results for the path planning experiments are shown in Fig. 26 for 2D, and Fig. 27 for 3D. In both cases two plots are included: the raw computation times for each algorithm and the time ratios (analogously to the results of the empty map experiment). The results for such a binary and complex map greatly depend on its topology. In this case, the Intel Research Laboratories map has both large clear spaces and cluttered, irregular areas, therefore, the results can be interpreted as a mix between those of the empty map experiment and those from the alternating barriers experiment.
FIGURE 26.

Computation times and ratios for the 2D path planning experiment. (a) Computation times. (b) Time ratios against FMM.

Show All
FIGURE 27.

Computation times and ratios for the 3D path planning experiment. (a) Computation times. (b) Time ratios against FMM.

Show All

For both the 2D and 3D experiments, the results are very similar to the alternating barriers experiments. FMM and FMMFib perform similarly, with SFMM being faster in all cases.

FSM and LSM suffer from the complexity of the environments, being the slowest in many of the cases. As the number of cells in the map increases, the ratio of these methods with respect to FMM decreases because the map becomes relatively simpler. In other words, there is a higher density of free cells (especially in open areas) and, therefore, a larger number of cells can be computed in every sweep even if the proportion to the total map remains the same. This is a manifestation of the fact that the total number of cells increases exponentially, but the complexity of such methods increases linearly. However, DDQM shows the fastest computation times since it is rather immune to the environment complexity.

Both GMM and FIM are among the slowest methods. This behavior has been appreciated already in other experiments with constant propagation speed (empty map and alternating barriers). However, UFMM is among the fastest algorithms because of its efficient heap structure.

In the 3D results there is a bump on the times and ratios for a given resolution. The reason for this irregularity is that in the chosen map, for that given resolution, there are very specific voxels that are difficult to reach, requiring many iterations of the algorithm. This is the reason why it affects only the purely iterative algorithms (FSM, LSM, and FIM). For UFMM, however, these specific voxels cause the untidy heap to be less uniform and therefore also requires more operations.
6) Vessels Segmentation

The time-of-arrival map returned by FMM applied to the 2D vessels grid is shown in Fig. 28 . The numerical results of the computation times for 2D and 3D are included in Table 4 . The results for 2D and 3D are not directly comparable among them as the environments are noticeable different. Nevertheless, some of the behavior can be appreciated and correlated with the generic experiments explained previously. In 2D, the results are in line with those for random speed function for a range of speed values smaller than 20 units. Although to human perception the vessel images seem to be slightly structured, the irregular shapes and the image quality causes the algorithms to behave very similarly to the case of a completely random speed map. In 3D, the results are closer to those observed in the checkerboard experiment.
TABLE 4 Computation Times (Seconds) for the Vessels Segmentation Experiment
FIGURE 28.

Example of the resulting time-of-arrival maps applying FMM to the vessels grid in 2D.

Show All

As in all the other experiments, FMM and FMMFib provide almost identical results, with SFMM outperforming them in both cases.

Since the propagation speed is non-constant, sweep-based methods require many sweeps in order to converge to the final solution. Therefore, FSM and LSM are slower than all the other methods. However, DDQM is again the fastest algorithm in 2D, and among the fastest ones in 3D. The reason for this is that there are numerous changes in the speed of the environment, but not as frequent as in the random speed function experiment, because there is some structure along the lines of the vessels and the surrounding tissues.

As in the 2D random speed function experiment, FIM is among the slowest algorithms because many iterations are required to converge to the final solution due to the frequent changes of the propagation speed. This effect vanishes in 3D as the iterations become more efficient since the narrow band contains more elements. Also, the difference in the environments play an important role in these results.

GMM is one of the fastest methods in 2D, similarly to the other variable speed experiments. However, in 3D its results are similar to those shown in the alternating barriers, showing once again the sensitivity of GMM to the environment.

Finally, UFMM is among the fastest algorithms, mostly due to a lucky selection of parameters for the environments chosen, because in both random speed function and checkerboard experiments UFMM does not show good results.

Considering that the speed in the environments is not constant, the errors for UFMM are included in Table 5 .
TABLE 5 Largest L_{1} and L_\infty Errors for UFMM in the Vessels Segmentation Experiment

SECTION VII.
Discussion

Two different sets of experiments have been carried out. The first set contains canonical problems previously included in the literature that cover the best and worst case scenarios for all the methods. This set consists of: empty map, alternating barriers, random speed function and checkerboard experiment. The main hypothesis considered while designing these experiments is that any other environment can be thought of as a combination of free space with obstacles and high-frequency or low-frequency speed changes of different magnitudes. Consequently, the second set aims to test this hypothesis while applying the Fast Methods to problems which represent real world applications. The second set is composed of the path planning experiment and the vessels experiment, in which the environments are more unpredictable and can contain many of the characteristics of the canonical problems. Note that the different grid sizes used in the experiments vary from extremely small to extraordinarily big grids.

The path planning and vessels results can be correlated with those in the canonical problems, therefore supporting the choice of the latter. However, it is important to remark that these results are very sensitive to the environments chosen. Some algorithms, such as FMM, FMMFib, and SFMM are completely independent of the environment, as their internal data structures behave in the same manner regardless of shape or speed, and therefore show consistent behavior for all the experiments, but they are never among the fastest methods. FIM results can be sped up for non-constant speed problems if larger errors are allowed. UFMM can probably be improved as well. However, our experience is that the configuration of its parameters is complex and requires a deep analysis of the environment to which it is to be applied on.

The results are also subject to low-level factors out of the scope of this paper. These factors are, for example, internal cache levels and memory management, data prefetching, etc. For example, the performance of algorithms that maintain a heap for the narrow band greatly depends on whether the narrow band is small enough that can be stored completely in microprocessor cache memories. Another example is that some algorithms such as DDQM or FIM make it complex for the prefetchers to prefetch the appropriate indices of the narrow band that are going to be evaluated on the following iterations since, commonly, grid neighbors are not contiguously stored in memory.

Taking into account the results from the conducted experiments, several conclusions for the use of the different types of Fast Methods will now be summarized:

    There is no practical reason to use FMM or FMMFib as SFMM is faster in every case. It shows the same behavior as its counterparts regardless of the characteristics of the environment, since the internal narrow band implementation is more efficient.

    If a sweep-based method is to be used, LSM should be always chosen, as it greatly outperforms FSM, since it recomputes cells only if there is a chance of improving their value. In fact, it was not possible to find any case in which FSM performs better than LSM. Therefore, FSM methods are recommended when applied to simple scenarios with constant propagation speed, because they require lower number of sweeps.

    In problems with constant speed and negligible dependence on the environmental complexity, DDQM should be chosen, as it has shown the best performance for the empty map and the alternating barriers environments, given that it combines the advantages of sweep-based and wavefront-based methods. However, for problems with variable speed values, its performance is highly influenced by the distribution of speed changes throughout the environment, as it might require many evaluations of the same cells in order to converge to the final solution.

    For variable speed functions, but simple scenarios, GMM is the algorithm to choose, as it guarantees that only two cell evaluations are required. However, complex environments distort the narrow band, requiring more iterations of its main loop.

    UFMM is hard to tune and its results include errors. Also, it has been outperformed in most of the cases by DDQM in constant speed scenarios, or by SFMM or FIM in experiments with variable speed. In order to evaluate whether this method should be considered, an in-depth study for the specific problem is required.

    There is no clear winner for complex scenarios with variable speed. UFMM can perform well in all cases if tuned properly. Otherwise, SFMM is a safe choice, especially in cases where there is not much information about the environment.

If a goal point is selected, cost-to-go heuristics can be applied [58] , which would greatly affect the results. Heuristics for FMM, FMMFib and SFMM are straightforward and they can be similarly applied to UFMM. They would improve the results in most of the cases. However, it is not clear if they can be applied to other Fast Methods. Anisotropic solutions given to anisotropic problems based on some of the presented methods are also interesting [9] , [47] , [59] , [60] .
SECTION VIII.
Conclusions

In this paper we have introduced the main Fast Methods in a common mathematical framework, adopting a practical point of view. Besides, an exhaustive comparison of the methods has been performed, which allows the users to choose among them depending on the application.

The code is publicly available, 5 as are the automatic benchmark programs. This code has been thoroughly tested and it can serve as a basis for future algorithm design, as it provides all the tools required to easily implement and compare novel Fast Methods.

Future research will focus on three different aspects: developing an analogous review for parallelized Fast Methods [31] , studying the application of these methods to anisotropic problems as well as to the new Fast Marching-based solutions focused on path planning applications [61] , [62] . Finally, the combination of UFMM and SFMM seems straightforward and it would presumably outperform both algorithms.
ACKNOWLEDGMENT

The authors would like to thank the contribution of Pablo Gely Muñoz to the GMM, FIM and UFMM implementations, and to Adam Chacon for the interesting discussions and suggestions towards improving the work.

Authors
Figures
References
Citations
Keywords
Metrics
Footnotes
More Like This
A 1.87-mm2 56.9-GOPS Accelerator for Solving Partial Differential Equations

IEEE Journal of Solid-State Circuits

Published: 2020
Fast Method for Accelerating Convergence in Iterative Solution of Frequency-Domain Partial Differential Equation Methods

2020 IEEE International Symposium on Antennas and Propagation and North American Radio Science Meeting

Published: 2020
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
