This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

1

Gated Adversarial Network Based Environmental Enhancement Method for Driving Safety Under Adverse Weather Conditions
Ke Wang1, Member, IEEE, Liang Pu1, Jian Zhang1, Jianbo Lu2, Fellow, IEEE 1College of Mechanical and Vehicle Engineering, Chongqing University, Chongqing 400044, China
2Research and Advanced Engineering, Ford Motor Company, Dearborn, MI 48121 USA
The adverse weather conditions have brought considerable difﬁculties in vision-based applications, which are closely related to the driving safety of autonomous vehicles. However, to date, the greater part of the existing environmental perception studies are under ordinary conditions, and the method to deal with the adverse weather conditions was ignored. Hence, this paper proposes an all-in-one gated adversarial network (AIO-GAN) to improve the performance of vision-based environment perception algorithms under adverse weather conditions, including in rain, haze, lack of light, etc. Three key technical contributions are made. At ﬁrst, the deep learning based gated transformer module was proposed to classify the input mixed images to different collections by passing them through different branches. Second, the multi-branch based variational autoencoder-generative adversarial network was proposed to solve the ill-pose problem of the solution. Third, high-level weight sharing encoders was given out to guarantee the stability and the high quality of the training process. In this way, the uniﬁed clean-style images can be achieved, even if the mixed multi-modal images are transferred from the source domain of complex weather scenes. Extensive experimental results show that the proposed method has achieved better performance than state-of-the-arts and improved the accuracy of target detection.
Index Terms—Driving safety, Deep generative model, Image-to-Image translation, Generative adversarial network

I. INTRODUCTION
A UTONOMOUS vehicle technology has been rapidly developing and it can signiﬁcantly reduce the accident caused by manual driving, thus improving the driving safety [1]. And object detection is an important part of autonomous vehicles, but existing algorithms ignore the inﬂuence of adverse weather conditions such as rain, haze, lack of light, etc. [2], which presents many difﬁculties for further visual processes. Therefore, to alleviate the burden of visual processing in bad weather conditions, this paper focused on developing an all-in-one style translation approach with the capability of transferring the complex input images into a uniﬁed clean image domain. Until now, a series of progress has been made in the style transfer area and depicted us with a big picture of the further applications. But there are still some challenges to be solved:
1) Most of the existing supervised works did not consider the potential shared space between the paired images during the training process.
2) Most of the state-of-arts can only deal with the image domain translation as a deterministic one-by-one mapping, and
Manuscript received Dec 05, 2020; revised Jun 26, 2021; accepted Jan 07, 2022. Date of publication August 16, 2022; This work was supported in part by the National Natural Science Foundation of China under Grant 51605054, in part by the Natural Science Foundation of Chongqing under Grant cstc2020jcyj-msxmX0575, in part by the Chongqing Technology Innovation and Application Development Project under Grant cstc2020jscx-dxwtBX0029 and Grant cstc2020jscxmsxmX0109, and in part by the Fundamental Research Funds for the Central Universities under Grant 2020CDJ-LHZZ-042. The Associate Editor coordinating the review process was Dr.* (Corresponding author: Ke Wang.)
Ke Wang, Jian Zhang and Liang Pu are with the College of Mechanical and Vehicle Engineering, Chongqing University, Chongqing 400044, China, and also with the Key Laboratory of Mechanical Transmission, Chongqing University, Chongqing 400044, China (e-mail: kw@cqu.edu.cn; masai@cqu.edu.cn).
Jianbo Lu is with the Research and Advanced Engineering, Ford Motor Company, Dearborn, MI 48121 USA (e-mail: jlu10@ford.com).
Digital Object Identiﬁer -

it usually suffers from its ill-posed nature, unstable transfer quality and mode collapse issue.
With the rapid development of artiﬁcial intelligence technologies [3], [4], especially the success of features visualization and representation of deep convolutional neural network [5], it has brought some breakthroughs in this developing area. The DeepDream method proposed an inception learning architecture that achieves the new state of the art creation. The generative Markov random ﬁeld (MRF) model was integrated into the pre-trained Convolutional Neural Networks (CNNs) by Li and Wand for image synthesis using patchbased style translation method [6]. Meanwhile, several works discussed the problem of multi-style translation. For example, the conditional and adaptive instance normalization method was used to adjust the parameters of the content input to match those of the objective styles, including mean, variance, scaling, and shifting, et.al [7]. Right now, it can produce high-quality results and can generate many types of artworks in new abstract and psychedelic styles [8]. To excavate the deeper capacity of the style transfer algorithm, recently, GANbased methods are also explored and employed in the style translation ﬁeld by synthesizing convincing counterfeits [9], [10]. By establishing a zero-sum game liked adversarial neural network, the quality of images generated by GANs had been improved considerably. But such methods can’t deal with the All-in-one translation problem and they usually suffer from its ill-posed nature, not to mention the unstable transfer quality and mode collapse issue.
This paper focuses on improving these challenges and proposed the AIO-GAN, a gated adversarial network for allin-one transfer problems. In detail, a uniﬁed single model was proposed to transfer diverse images with multiple styles into one objective domain and it was constructed by three modules: the gate module, high-level weight sharing encoders, and VAE-GANs. Further, the proposed method was used

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

2

in the environment perception area under complex weather conditions for autonomous vehicle applications. Compared to the previous works, there are three main contributions of this work, which can be summarized as follows:
(1) A novel uniﬁed model of AIO-GAN was proposed with the capability of transferring diverse images with multiple styles into one objective domain with only one network.
(2) Proposing the multi-branch VAE-GAN to get the content code and style code for more stable and effective training.
(3) Using information from clean images in high-level weight sharing encoders to improve the performance of the proposed network.
The remainder of this paper is structured as follows. The assumptions, AIO-GAN approach, and network implementation are described in Section 2. Section 3 discusses the experiment results and comparisons to other state-of-the-art, followed by concluding remarks in Section 4.
II. MATERIAL AND METHODS
A. Mathematical Formulation and Assumptions
Let χm be the mixed image domain, χo be the objective image domain. Here, for simplicity, we assume image samples xm ∈ χm, xo ∈ χo. In a supervised translation problem, given training samples (xm, xo) drawn from a joint distribution Pχm,χo (xm, xo), the translation model can learn from the given joint distribution and give out credible output.
Generally, without any other assumptions, it is hard to infer the joint distribution from the marginal distribution Pχm (xm) and Pχo (xo) in the unsupervised problem. Therefore, some additional assumptions are given here:
1) First, we suppose that there is a ﬁnite number of styles in the input mixed image domain. In this way, we can mark an attribute to each image in the mixed source domain;
2) Second, each image can be encoded into a shared content space and a domain-speciﬁc style space;
3) Third, the connection between χm and χo exists both at the image level and at the perceptual level, so we use a collaborative underlying representation z to associate image pairs xm ∈ χm, xo ∈ χo.
such that, the images can be recovered from the representation z and it can be computed with the shared content in the source space and the target style in the target domain.
In detail, given images xm ∈ χm, xo ∈ χo, the mapping function from marginal distribution Pχm (xm) to Pχo (xo) can be given as xo ≈ φm→o (xm). Under above assumptions, there also exist functions Gate, Em, zm, Gm→o, and Do , such that zm ≈ Em(Gate (xm)) and x0 ≈ Gm→o(zm). Therefore, the mapping model can be represented by the compositon function xo ≈ φm→o (xm) = Gm→o(Em(Gate (xm))), which will be discussed below.
B. AIO-GAN Method
The proposed model was constructed by three modules, including gate module, high-level weight sharing encoders, and VAE-GAN, as illustrated in Fig.1.

1) Gate Module
In multi-collection style transfer, the original GAN is known to be unstable for the reason that there exist non-unique solutions. The situation would be even worse when large differences lie in the mixed image domain. To reduce the distance between multi-modal images in the source domain, the gate module was used to roughly classify the input mixed images into different sets.
Therefore, the adopted gate module aims to output Gate xm, liS by assigning a speciﬁc collection label l to the input image xm, . Speciﬁcally, the gate module has the capability of transforming the input images from mixed domain space into different collections by switching trigger to different branches:

Gate xim, liS = N N xim

(1)

where i is the index of the images, m means the mixed
source domain, S denotes the number of collections, l is the
label of each collection. N N is the classiﬁer of the gate
module. After giving each image a corresponding pre-labeling liS,
we can distribute the mixed image domain into P subdomain {ϑs}Ps=1. In the same subdomain, the image has the same pre-labeling. It should be noted that some images may not be
strictly distinguished and labeled, then, the gate module would
tend to classify them into the most suitable domain with a pre-
trained neural network.
2) Multi-branch VAE-GAN Then, under our shared latent space assumption, image xim can be converted to the content code cim ∈ C, that is shared by both mixed image domain χm and target image domain χo and the style code sim ∈ S, that is shared only in the same subdomain ϑs. While the content code and style conde can be
obtained using the multi-branch VAE-GAN.
Speciﬁcally, the encoder-genenator pair of VAE-GAN
{Em1 , Em2 , Em3 , . . . , Go} constitues a multi-branch VAEs (Variational AutoEncoders), as shown in Fig1. For the input
images from the mixed source domain χm, the gate module
will ﬁrst determine the right branch of subdomain, and choose
one of the corresponding {Em1 , Em2 , Em3 , . . . , } as the encoder, termed VAEmi . It means that, for each input image xm ∈ χm, the VAEmi can map it into a code in the latent space using encoder Emi . Where, the components are assumed to be conditionally independent and Gaussian distributed [11].
Then, the generator Go decodes the random-perturbed code to reconstruct the objective image belong to target domain χo.
In the process, the encoder outputs a mean vector and a variance vector, as Emi (xi) = µk(xi), σk2(xi) . Then, the distribution of the content code zmi ∈ Z can be got by q(zmi |xi) = N (zmi |µi(xi), σi2(xi)). At this end, the decoder G decodes the code zmi to target domain χo, which can be expressed as x˜io→o = G (zmi ). Similarly, for the corresponding input image xo ∈ target image domain χo, the encoder-genenator pair {Eo, G} constitues a VAEo for reconstructing image from χo to χo. The encoder Eo outputs Eo (xo) = µo(xo), σo2(xo) , the distribution of the latent code zo which is given by qo (zo|xo) = N zo|µo(xo), σo2(xo) . And then the decoder G decodes the code zo to translate image

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

3

Fig. 1. Architeture of the proposed AIO-GAN: it has two modules: gate module and Multi-branch VAE-GAN. gate module was used to distinguish the style categories of the input mixed images and to determine the right branch to be chosen.High-level weight sharing encoders can efﬁciently guarantee the stability of the training process, while the Multi-branch VAE-GAN module was used to solve the ill-posed problem of the solution.

Fig. 2. The embodiment of weight sharing: We represent Encoders and Generator using CNNs and implement the shared latent space assumption using a weight sharing constraint where the connection weights of the last few layers (high-level layers) in Encoders are tied (illustrated using red dotted lines).

to χo, that can be expressed as x˜oo→o = G (zo). Therefore, for the generated image x˜o (including image translation x˜io→o and image reconstruction x˜oo→o), the AIO-GAN framework can achieve the goal by three sub-networks, as shown in TABLE
I.

TABLE I THE ROLES OF THE SUBNETWORKS IN AIO-GAN

subnetwork
{Emi , G} {EO, G} {G, D}

role
χm → χo image translator VAE for χo GAN for χo

representation back to data space:

z ∼ E(x) = q(z|x), x ∼ D(z) = p(x|z)

(2)

VAE loss describes the error between reconstructed data and target data by the expected log likelihood and a prior regularization term. The VAE object functions are given by:
LV AEmi = −Eqi(zi|xi)[log(po(xi|zi))] + KL[qi(zi|xi)||p(zi)] (3)

LV AEo = −Eqo(zo|xo)[log(po(xo|zo))] + KL[qo(zo|xo)||p(zo)] (4)

3) Loss function
Our network can be trained through jointly solving the learning problems of the VAEmi , VAEo and GAN for both the image reconstruction and the image translation. Then, we designed 3 losses to train the model including VAE loss, GAN loss and perceptual loss .VAEmi and VAEo encode data samples to the latent space Z, and decode the latent

LV AE = LV AEmi + LV AEo

(5)

where KL is the Kullback-Leibler divergence. KL divergence describes the information bias of the distribution of the latent code and the prior distribution [11] and we need to minimize it. Meanwhile, we maximize another item to make the rebuild image similar to the image of the target domain. The prior

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

4

distribution is p(z) = N (z|0, I). The GAN objective function different encoders were strictly equal, which would cause great

is given by:

problems in the training process, such as causing the generated

LGAN

=

Exo∼P χo [log(D(xo))]

+

Eqo(zo|xo)[log(1

−

D(G(zo)))it]mraiangiensg

to be all black or all white. we against the generation network.

avoid this we show

problem by that through

+Eqi(zi|xi)[log(1 − D(G(zi)))a]dversarial training, a pair of corresponding images in the two

(6) domains can be mapped to common latent code and the latent

where the objective function aims to encourage G to generate code will be mapped to a pair of corresponding images in the images of which the distribution is similar to that of domain two domains.

χo. the two distributions are qo(zo|xo) (the distribution of the reconstructed images for the input images in χi) and qi(zi|xi) (the distribution of the translated images for the input images

in χo). In addition, we add the perceptual loss as well. Aim to

generate a more realistic picture and promote the output image

and the target image to have similar feature representations, the

perceptual loss function based on the pre-trained VGG (Visual

Geometry Group) feature network is used, and the formulation

is as follows:

1 Lp = CkHkWk

φk(x˜o) − φk(xo)

2 2

(7)

C. Implementation
The network architecture of the AIO-GAN is shown in Table.II. For training details, our models are trained using Adam [12] with β1=0.5 and β2=0.999. We set the learning rate to 0.0001. For the lack of memory size, the batch size was set to 4. If small resolution images (≤256*256) are translated, the batch size can be increased. Training takes about one day on a single NVIDIA TITAN Xp. In the test, all models are trained using the same number of iterations, and Fig.4 shows the available number of iterations is 100000.

Ck,Hk and Wk are the size information of the kth feature map, and φk is the feature maps in the kth layer of the VGG network.

Combining all the above losses, we obtain the total loss

Ltotal. which is deﬁned as:

Ltotal = γ1LV AE + γ2LGAN + γ3Lp

(8)

where γ1,γ2 and γ3 are the positive weights.Weighting factors γ1=1,γ2=1 and γ3=1. The generator is trained to minimize Eq 8. γ1 mainly tunes the ratio of KL divergence, γ2 adjusts the GAN’s functional percentage. Usually we seldom change the

total weight of the perceptual loss γ3. GAN and KL divergence have different effect for training the model.

4) High-level Weight-sharing Scheme

From the intuition that the high-level representation of

corresponding images should be cognate. Therefore, a weight-

sharing constraint is enforced upon the multi-branch VAEs (in-

cluding VAEmi , VAEo) to relate the representations, as shown in Fig2. The weights of the last few layers of encoders of the

mixed domain {Em1 , Em2 , Em3 , . . . }, which are responsible for extracting high-level representations of input images, are

shared for the implementation of weight-sharing constraint.

Meanwhile, in order to make the weights solving process more

accurate and fast, we absorb the target domain encoder Eo into the scope of weight-sharing constraint. Since that Eo are responsible for encoding the images from objection domain

χo to objection domain χo. With the weight-sharing

constraint,

an

encoder

E

=.

{Emi , Eo} can be express as Ep ⊕ Es, where Es are the

shared layers of encoder, Ep are the independent layers, and

⊕ is the connection of convolution layer. Usually, Es are

set to code z

th=.e

last {zmi

few , zo}

layers from

of all encoders to ensure different domains through

that the different

encoders belongs to the same latent space.

It should be noticed that only using the weight-sharing con-

straint, we can’t guarantee that the content code z encoded by

Fig. 3. Sample of the dataset. From left to right: (a)rain, (b)dark, (c)haze, (d)clear
III. RESULTS AND DISCUSSION
A. Dataset and Evaluation Metrics
1) Dataset Our collected dataset includes images from different weather conditions, such as rain, haze, insufﬁcient light, etc. In the dataset, the haze data consists of both synthetic and realworld hazy images, called Realistic Single-Image Dehazing (RESIDE). This dataset not only includes synthetic hazy remote sensing images, which are labeled with 15 categories for evaluation of detection accuracy but also includes realworld haze images obtained from Landsat 8 OLI and UAV. [13]. While, the raindrop data is created by [14], using two identical cups: one sprinkled with water and the other kept clean. Using two pieces of glass can avoid misalignment, because the refractive index of glass is different from that of air, so it can refract light. The low-light data is on the one hand created by BiGAN [15] and on the other hand constructed by ExDark dataset [16]to ensure that the evaluation of different algorithms during the experiment is objective. [16] After that, the visual evaluation method and the NIQE index (a Completely Blind Image Quality Analyzer) [17] were

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

5

TABLE II GENERATOR AND DISCRIMINATOR NETWORK OF AIO-GAN

Encoders Generator Discriminator

layer 1 2 3 4 µ σ2
1 2 3 4 5 1 2 3 4 5

Operation Convolution Convolution Convolution Convolution Convolution Convolution Deconvolution Deconvolution Deconvolution Deconvolution Deconvolution Convolution Convolution Convolution Convolution Convolution

K=Kernel size,S=Stride size K5,S2 K5,S2 K8,S1 K1,S1 K1,S1 K1,S1 K4,S2 K4,S2 K4,S2 K4,S2 K1,S1 K5,S2 K5,S2 K8,S1 K1,S1 K1,S1

Normalization Batch Normalization Batch Normalization Batch Normalization Batch Normalization
Batch Normalization Batch Normalization Batch Normalization Batch Normalization Batch Normalization Batch Normalization Batch Normalization Batch Normalization Batch Normalization Batch Normalization

Nonelinearity ReLU ReLU ReLU ReLU
ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU

Shared? No Yes Yes Yes Yes Yes None None None None None None None None None None

Fig. 4. The iteration versus translation performance

also used to reassess the image quality. During the step the distorted images were excluded and the rest are kept. Finally, our dataset contains about 2400 image pairs with different background scenes and different weather. Among them, there are about 800 image pairs in rain, fog and lack of light conditions respectively. Fig.3 shows some samples of our collected dataset.
2) Evaluation Metrics
The quality of the proposed AIO-GAN network was evaluated from the quality of image generation and environmental perception.
Two fully referenced metrics including PSNR (Peak Signal to Noise Ratio) and SSIM (Structural Similarity) [18] were used as the quantitative metric to evaluate the quality of the generated images. Since, the PSNR is the most common and widely used objective evaluation index, which is based on the errors between corresponding pixels, while the index of SSIM measures image similarity from brightness, contrast, and structure. The calculation formula of PSNR and SSIM

are shown as follows:

H × W × (2n − 1)2

P SN R = 10 log10[

H i=1

W j=1

(X

(i,

j

)

−

Y

(i,

j

))

]

(9)

SSIM

=

2µX µY + C1 µ2X + µ2Y + C1

×

2ρX ρY + C2 ρ2X + ρ2Y + C2

×

2ρXY ρX ρY

+ C3 + C3

(10)

Where H and W are the height and width of the image

respectively, and µX , µY and ρX , ρY are the mean values and

variances of image X and image Y, and ρXY is the covariance

of two images.

And then, at the environmental perception level, we use

different object detection algorithms to evaluate whether our

method can signiﬁcantly improve the performance of auto-

matic driving and driving assistance.

B. Evaluation of Proposed AIO-GAN
We evaluated the proposed method on our collected dataset and compared its performance with other state-of-the-art systems. Some key sub-modules, including gate module and high-level weight-sharing, are carried out and comprehensively

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

6

Fig. 5. Results different number of shared layers. Here, λ is the number of shared high-level layers.

Fig. 6. The number of weight-sharing layers versus translation performance

evaluated. Meanwhile, depending on the task, we compare it to both general algorithms and specialized algorithms.
1) Effectiveness of high-level weight-sharing
The weight-sharing scheme aims to look for the similarity between different domains and to encode different domains into the same latent space. In order to prove the effectiveness of the high-level weight-sharing scheme, a set of networks with different numbers of weight-sharing layers was trained. The networks used in the experiment were all based on the architecture in Table.II (except the number of shared layers).
As is shown in Fig.5, under each task, when the number of high-level shared layers is 0 or 1, the images generated by the model are chaotic. This is because the high-level information between different image domains cannot be mapped to the shared latent space, and when the latent code z is encoded by various encoders input into the same generator, the output will be blurred. While the number of high-level shared layers is greater than 1, the mechanism of weight sharing comes into play, and with the number of high-level sharing layers increasing, the translated images become clearer and closer to the ground truth.
Fig.6 shows the relationship between image quantitative evaluation (PSNR and SSIM) and the number of shared layers. It supports the previous conclusion. When weight sharing fully works, the SSIM and PSNR of test images can achieve a state of optimal respectively. It is worth noting that when all layers of encoders are shared, the translation ability of AIO-GAN would greatly be reduced. This is because when all layers are Shared, the entire network is equivalent to that without

gate module, and there is no difference between AIO-GAN and ordinary VAE-GAN [19], which also indicates that our method has much better performance than the original VAEGAN. From the results, we can conclude that the weightsharing constraint to the encoders in our proposed method is essential.
2) Effectiveness of loss function
In order to evaluate the effects of loss functions, we did loss function ablation experiment based on the test data set with the same computing settings for fair comparisons, as shown in Table.IV
The results from the ﬁrst column show the effect of LGAN and Lp on PSNR AVG and SSIM AVG. By contrasted with the ﬁrst colume,when loss function add the LV AE, the value of PSNR AVG and SSIM AVG get some increase.That is because using LV AE loss helps to get more information from the shared-latent space so that the decoder can generate robust and abstract hierarchical feature maps which improves the performance of the model.
3) Quality evaluation of generated images
To illustrate the efﬁciency of the proposed model, we evaluate it on the test dataset including haze, raindrop and dark.
At ﬁrst, for haze removal, as is shown in Table.III, Our model performs well in the average PSNR and SSIM results of the synthetic dataset. Compared with the general image translation methods, like pix2pix [20], CycleGAN [21], the PSNR and SSIM values of the images generated by our model are the best. Even compared with the special methods for

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

7

TABLE III THE AVERAGE RESULTS OF PSNR AND SSIM FOR DEHAZING.

Metrics PSNR SSIM

ATM 15.3262 0.7301

DCP 17.6734 0.7863

DehazeNet 19.1763 0.8701

CAP 18.9653 0.7932

Pix2pix 17.6933 0.8242

CycleGAN 19.3274 0.8451

FS-GAN 19.8263 0.8637

Multi-Scale Dehazing 20.0375 0.8712

Ours 20.6431 0.8648

Fig. 7. Our instances of hazy removal results using several state-of-art dehazing methods. From left to right: (a)input(b)ATM(c)DCP (d)DehazeNet(e)CAP(f)Pixpix(g)CycleGAN (h)Ours (i)Ground truth

Fig. 8. Our instances of rain removal results using several state-of-art methods. From left to right: (a)input (b)Eigen13 (c)DualGAN (d)AttentiveGAN (e)IDCGAN (f)Pix2pix (g)CycleGAN (h)Ours (i)Ground truth

TABLE IV QUANTITATIVELY EVALUATE THE EFFECT OF THE LOSS FUNCTIONS ON
TEST DTASET .

Metrics PSNR AVG SSIM AVG

LGAN +Lp 22.703 0.839

LGAN +Lp+LV AE 22.924 0.842

dehazing, our method has the highest PSNR value and good SSIM value. Speciﬁcally, due to the adoption of adversarial learning and weight sharing, our model gets the highest score in PSNR and the second best score in SSIM, which means that our method is effective in dehazing. Fig.7 shows the visible results of other algorithms in comparison to our re-

sults. The traditional dehazing algorithms, like ATM [22] and DCP [23], generate unrealistic tones which cause the images color-distorted especially the ATM. Compared with general image translation methods, such as pix2pix and CycleGAN, our method produces better fogless images. In addition, our method is comparable to the most advanced deep learning dehazing algorithms like DehazeNet [24], CAP [25],FS-GAN [9],and Multi-Scale Dehazing [26].
Second, for raindrop removal, we evaluate it on the synthetic dataset and compare it with some other state-of-the-art methods using PSNR and SSIM. Table.V shows the quantitative comparisons between our method and other existing methods: Eigen13 [27], DualGAN [28], AttentiveGAN [14], ID-CGAN

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

8

TABLE V THE AVERAGE RESULTS OF PSNR AND SSIM FOR RAINDROP REMOVAL.

Metrics PSNR SSIM

Eigen13 16.3276 0.7019

DualGAN 18.0631 0.7843

AttentiveGAN 22.8593 0.9212

ID-CGAN 18.8153 0.8121

Pix2pix 18.7653 0.7932

CycleGAN 18.6933 0.8242

Ours 21.1879 0.9224

TABLE VI THE AVERAGE RESULTS OF PSNR AND SSIM FOR DARK REMOVAL.

Metrics PSNR SSIM

DualGAN 18.1221 0.7911

CoGAN 20.7592 0.9243

Pix2pix 20.5243 0.9312

CycleGAN 19.2866 0.8783

Ours 22.8752 0.9414

Fig. 9. Our instances of dark removal results using several state-of-art methods. From left to right: (a)input (b)Pix2pix (c)CoGAN (d)ConditianalGAN (e)CycleGAN (f)Ours (g)Ground truth

[29], Pix2pix [20], CycleGAN [21]. As shown in the table, compared to these six methods, our SSIM value is higher, although our PSNR value is second. This indicates that our method can generate results more similar to the ground truth. Fig.8 shows some visible results of some other state-of-the-art algorithms in comparison to our results. As can be seen, our method is considerably more effective in removing raindrops compared to the algorithms for raindrop removal, like Eigen13 and IDCGAN. Compared with the general image translation algorithms, like DualGAN, pix2pix and CycleGAN, our outputs have less artifacts and have better restored structures. Furthermore, our method is comparable to AttentiveGAN, which is a professional method with the best image raindrop removal effect, and even the image generated by our algorithm is closer to ground truth in certain details.
Third, for the task of dark removal, we evaluate our model on the synthetic dataset and compare it with some other stateof-the-art methods, such as DualGAN [28], CoGAN [30], Pix2pix [20], CycleGAN [21]. In Table.VI, the average PSNR and SSIM values of the images generated by our model are the highest, and our method outperforms the others at least 9.11% and 1.85% in terms of PSNR and SSIM. As shown in Fig.9, the images generated by CycleGAN always produces blurred shadows in the upper right corner, and the image is distorted. CoGAN and ConditianalGAN produce noise while removing the dark. The image produced by the pix2pix would be distorted in some colors. Compared with those methods,

our method can generate images closer to ground truth while removing the dark.
4) Improve the performance of environmental perception system
In order to demonstrate that AIO-GAN can improve the driving safety of self-driving vehicles by improving the accuracy of environmental perception under adverse weather conditions, we applied our proposed method on object detection using our collected dataset. Since, the performance of those high-level algorithms might be greatly jeopardized by diverse degradations in complex driving environments, such as rain, haze, and dark. We use different object detection algorithms YOLOv3 [31] and SSD [32]) to show the improvement using our method. Fig.10 demonstrates that all of the dark, haze and raindrop have a great negative impact on target recognition. As the intensity of dark, haze and raindrop increases from light to heavy, the accuracy of visual environment perception would gradually decreases, and the average mAP can decrease by more than 10%. This suggests that if we can improve the adverse weather conditions, even if we only transfer the intensity from heavy to light, it would have a positive impact on the safety of self-driving vehicles. The proposed algorithm is implemented in Pytorch on a computer with an Nvidia TitanXP GPU.As a pre-processing step, our proposed method takes nearly about 90-120ms to preprocess per image taking from the adverse environment from the onboard camera, while the total time is mainly detemianed by the detector choosed.

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

9

TABLE VII MAP COMPARISON ON ALL SEVEN SETTINGS: “RAIN + A” “HAZE + A” AND“DARK+ A” ARE SHORT FOR “RAIN + AIO-GAN” “HAZE + AIO-GAN”
AND“DARK+ AIO-GAN”, RESPECTIVELY

Setting Yolo v3
SSD

Groundtruth 81.14% 79.53%

Rain 73.01% 71.71%

Rain + A 76.34% 75.21%

Haze 68.76% 67.01%

Haze + A 76.51% 74.32%

Dark 72.24% 68.69%

Dark + A 77.82% 75.32%

Fig. 10. Environmental disturbances of different intensities versus detection performance

The detection results are shown in Fig.11. As can be seen, using our algorithm, the general recognition is better than without our visibility enhancement process. The problem of low accuracy of detection and missing detection has been improved to some extent. For example, under the hazy and gloomy weather, the detection accuracy of the vehicle has been greatly improved. Meanwhile, pedestrians have been successfully detected. Furthermore, the following conclusions can be drawn through Table.VII: environmental interference would signiﬁcantly reduce the accuracy of object recognition, however, the combined application of our method and object detection algorithms can effectively reduce the impact of environmental noise and notably improve the detection accuracy. Which can ensure the safe driving of autonomous vehicles under adverse weather conditions.
IV. CONCLUSION
This study provided a novel uniﬁed model called AIOGAN, which has the capability of transferring diverse images with multiple styles into one objective domain using only one network. Different from traditional methods, we utilized the outstanding features of the gate module, high-level weight sharing encoders, and multi-branch VAE-GAN. Where gate module was used to distinguish the style categories of the input images and to determine the right branch to be chosen. High-level weight sharing encoders can efﬁciently guarantee the stability of the training process, while the VAE-GAN module was used to solve the ill-pose problem. The content and style in image space can be carefully distinguished by conjunction use of different layers of neural networks. Further, the proposed method was used in the environmental perception

Fig. 11. Comparison of detection and recognition results on natural hazy images, using a threshold of 0.6.From up to down: YOLOv3 alone, YOLOv3+AIO-GAN
area under complex weather conditions. Quality evaluation and high-level vision tasks demonstrate the stability, functionality, and effectiveness of our model and produce satisfactory results compared with state-of-the-art algorithms. And it proves that our proposed model can improve the safety of the autonomous vehicle.
REFERENCES
[1] K. Wang, G. Li, J. Chen, Y. Long, T. Chen, L. Chen, and Q. Xia, “The adaptability and challenges of autonomous vehicles to pedestrians in urban china,” Accident Analysis & Prevention, vol. 145, p. 105692, 2020.
[2] K. Wang, S. Ma, J. Chen, and J. Lu, “Approaches challenges and applications for deep visual odometry toward to complicated and emerging areas,” IEEE Transactions on Cognitive and Developmental Systems, pp. 1–1, 2020.
[3] C. Szegedy, W. Liu, Y. Q. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” 2015 Ieee Conference on Computer Vision and Pattern Recognition (Cvpr), pp. 1–9, 2015.
[4] K. Wang, X. Tang, S. Zhao, and Y. Zhou, “Simultaneous detection and tracking using deep learning and integrated channel feature for ambint trafﬁc light recognition,” Journal of Ambient Intelligence and Humanized Computing, 2021.

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIV.2022.3142128, IEEE Transactions on Intelligent Vehicles

IEEE TRANSACTIONS ON INTELLIGENT VEHICLES

10

[5] K. Wang, L. Zhang, Q. Xia, L. Pu, and J. Chen, “Cross-domain learning using optimized pseudo labels: toward adaptive car detection in different weather conditions and urban cities,” Neural Computing and Applications, 2021.
[6] C. Li and M. Wand, “Combining markov random ﬁelds and convolutional neural networks for image synthesis,” 2016 Ieee Conference on Computer Vision and Pattern Recognition (Cvpr), pp. 2479–2486, 2016.
[7] X. Huang and S. Belongie, “Arbitrary style transfer in real-time with adaptive instance normalization,” 2017 Ieee International Conference on Computer Vision (Iccv), pp. 1510–1519, 2017.
[8] K. Wang, C. Cao, S. Ma, and F. Ren, “An optimization-based multisensor fusion approach towards global drift-free motion estimation,” IEEE Sensors Journal, vol. 21, no. 10, pp. 12 228–12 235, 2021.
[9] K. Wang, S. Zhang, J. Chen, F. Ren, and L. Xiao, “A feature-supervised generative adversarial network for environmental monitoring during hazy days,” science of The Total Environment, vol. 748, p. 141445, 2020.
[10] K. Wang, S. Ma, F. Ren, and J. Lu, “Sbas: Salient bundle adjustment for visual slam,” IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1–9, 2021.
[11] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013.
[12] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” Computer Science, 2014.
[13] B. Li, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and Z. Wang, “Benchmarking single-image dehazing and beyond,” IEEE Transactions on Image Processing, vol. 28, no. 1, pp. 492–505, 2018.
[14] Q. Rui, R. T. Tan, W. Yang, J. Su, and J. Liu, “Attentive generative adversarial network for raindrop removal from a single image,” IEEE Computer Vision and Pattern Recognition (CVPR), 2017.
[15] J. Donahue, P. Kra¨henbu¨hl, and T. Darrell, “Adversarial feature learning,” arXiv preprint arXiv:1605.09782, 2016.
[16] Y. Sasagawa and H. Nagahara, “Yolo in the dark - domain adaptation method for merging multiple models,” in European Conference on Computer Vision, 2020.
[17] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely blind” image quality analyzer,” IEEE Signal Processing Letters, vol. 20, no. 3, pp. 209–212, 2013.
[18] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE transactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.
[19] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther, “Autoencoding beyond pixels using a learned similarity metric,” arXiv preprint arXiv:1512.09300, 2015.
[20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” 2017, pp. 1125–1134.
[21] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” 2017, pp. 2223– 2232.
[22] M. Sulami, I. Glatzer, R. Fattal, and M. Werman, “Automatic recovery of the atmospheric light in hazy images.” IEEE, 2014, pp. 1–11.
[23] K. He, J. Sun, and X. Tang, “Single image haze removal using dark channel prior,” IEEE transactions on pattern analysis and machine intelligence, vol. 33, no. 12, pp. 2341–2353, 2010.
[24] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, “Dehazenet: An end-to-end system for single image haze removal,” IEEE Transactions on Image Processing, vol. 25, no. 11, pp. 5187–5198, 2016.
[25] Q. Zhu, J. Mai, and L. Shao, “A fast single image haze removal algorithm using color attenuation prior,” IEEE transactions on image processing, vol. 24, no. 11, pp. 3522–3533, 2015.
[26] H. Dong, J. Pan, L. Xiang, Z. Hu, X. Zhang, F. Wang, and M.H. Yang, “Multi-scale boosted dehazing network with dense feature fusion,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 2154–2164.
[27] D. Eigen, D. Krishnan, and R. Fergus, “Restoring an image taken through a window covered with dirt or rain,” 2013, pp. 633–640.
[28] Z. Yi, H. Zhang, P. Tan, and M. Gong, “Dualgan: Unsupervised dual learning for image-to-image translation,” 2017, pp. 2849–2857.
[29] H. Zhang, V. Sindagi, and V. M. Patel, “Image de-raining using a conditional generative adversarial network,” IEEE transactions on circuits and systems for video technology, 2019.
[30] M.-Y. Liu and O. Tuzel, “Coupled generative adversarial networks,” 2016, pp. 469–477.
[31] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” arXiv preprint arXiv:1804.02767, 2018.
[32] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, “Ssd: Single shot multibox detector.” Springer, 2016, pp. 21–37.

Ke Wang was born in Huaian, Jiangsu, China in 1984. He received the B.S. and M.S. degrees in vehicle engineering from the Hunan University, Hunan, China, in 2007 and in 2009 and the Ph.D. degree in mechanical engineering from Hunan University, Hunan, China, in 2013. He ﬁnished his Postdoctoral research at College of Engineering, Michigan University Ann Arbor, USA, in 2016 and 2017. From 2014 to 2016, he was an Assistant Professor with the Automobile Engineering Department. Since 2017, he has been an Associate Professor with the State Key Laboratory of Mechanical Transmission, Chongqing University. He is the author of one book, more than 20 articles, and more than 15 inventions. His research interests are the intelligent vehicle, environment perception and AI.
Liang Pu Liang Pu received the B.S. degree in vehicle engineering from Chongqing University, Chongqing, China,in 2018, where he is currently pursuing the master’s degree. His research interests are computer vision and machine learning, with particular interests in domain adaption, object detection and semantic segmentation.
Jian Zhang was born in Chongqing, China in 1996. He received the B.Sc. degree with honours in vehicle engineering from Chongqing University, Chonqing, China in 2017. He was a research intern at The State Key Laboratory of Vehicle NVH and Safety Technology. He is currently pursuing the M.Sc. degree in vehicle engineering at Chongqing University. His research interests include machine learning, Bayesian statistics, Monte Carlo methods, prob- abilistic graphical models, artiﬁcial neural networks and mobile robot localization.
Jianbo Lu Jianbo Lu (SM 09) received the Ph.D. degree in aeronautics and astronautics from Purdue University, West Lafayette, IN, USA, in 1997. He is currently a Technical Expert in advanced vehicle controls with Controls Research and Advanced Engineering, Research and Innovation Center, Ford Motor Company, Dearborn, MI, USA. He is an Inventor or CoInventor of over 100 U.S. patents. His invented technologies have been widely implemented in tens of millions of vehicles with brand names, such as Ford, Lincoln, Volvo, and Land Rover. He has authored over 70 refereed research articles. His research interests include automotive controls, intelligent and adaptive vehicle systems, integrated sensing systems, driving assistance and active safety systems, and future mobility. Dr. Lu was a two-time recipient of the Henry Ford Technology Award at Ford Motor Company. He is an Associate Editor for IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY. He is on the Editorial Board of International Journal of Vehicle Autonomous Systems and International Journal of Vehicle Performance. He also is the Chair of the Intelligent Vehicular Systems and Control Technical Committee under the IEEE Society of Systems, Man, and Cybernetics.

2379-8858 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 25,2022 at 11:00:47 UTC from IEEE Xplore. Restrictions apply.

