1
A Survey on Rain Removal from Video and Single Image
Hong Wang, Yichen Wu, Minghan Li, Qian Zhao, and Deyu Meng, Member, IEEE

arXiv:1909.08326v2 [eess.IV] 3 Oct 2019

Abstract—Rain streaks might severely degenerate the performance of video/image processing tasks. The investigation on rain removal from video or a single image has thus been attracting much research attention in computer vision and pattern recognition, and various methods have been proposed against this task recently. However, there is still not a comprehensive survey paper to summarize current rain removal methods and fairly compare their generalization performance, and especially, still not a offthe-shelf toolkit to accumulate recent representative methods for easy performance comparison and capability evaluation. Aiming at this meaningful task, in this study we present a comprehensive review for current rain removal methods for video and a single image. Speciﬁcally, these methods are categorized into modeldriven and data-driven approaches, and more elaborate branches of each approach are further introduced. Intrinsic capabilities, especially generalization, of representative state-of-the-art methods of each approach have been evaluated and analyzed by experiments implemented on synthetic and real data both visually and quantitatively. Furthermore, we release a comprehensive repository, including direct links to 74 rain removal papers, source codes of 9 methods for video rain removal and 20 ones for single image rain removal, 19 related project pages, 6 synthetic datasets and 4 real ones, and 4 commonly used image quality metrics, to facilitate reproduction and performance comparison of current existing methods for general users. Some limitations and research issues worthy to be further investigated have also been discussed for future research of this direction.
Index Terms—Rain removal, maximum a posterior estimation, deep learning, generalization.
I. INTRODUCTION
I MAGES and videos captured from outdoor vision systems are often affected by rain. Speciﬁcally, as a complicated atmospheric process, rain can cause different types of visibility degradations. Typically, nearby rain drops/streaks incline to obstruct or distort background scene contents and distant rain streaks tend to generate atmospheric veiling effects like mist or fog and blur the image contents [1]–[3]. Rain removal has thus become a necessary preprocessing step for subsequent tasks, like object tracking [4], scene analysis [5], person reidentiﬁcation [6], and event detection [7], to further enhance their performance. Therefore, as an important research topic, removing rain streaks from videos and images has been attracting much attention recently in the ﬁled of computer vision and pattern recognition [8]–[12].
In the recent years, various methods have been proposed for this rain removal task for both video and single image [13]–[20]. Comparatively, removing rain from an indi-
H. Wang, Y. Wu, M. Li, Q. Zhao, and D. Meng (corresponding author) are with Institute for Information and System Sciences and Ministry of Education Key Lab of Intelligent Networks and Network Security, Xi’an Jiaotong University, Shaan’xi, 710049 P.R. China

vidual image is evidently more challenging than that from a video composing of a sequence of image frames, due to the lack of beneﬁcial temporal information in the former case [21]–[23]. The methodologies designed for two cases are thus with signiﬁcant distinction. Yet similarly for both issues, conventional methods mainly adopt the model-driven methodology and especially focus on sufﬁciently utilizing and encoding physical properties of rain and prior knowledge of background scenes into an optimization problem and designing rational algorithms to solve it, while more recently raised methods often employ the data-driven manner by designing speciﬁc network architectures and pre-collecting rainy-clean image pairs to learn network parameters to attain complex rain removal functions [24], [25]. Most of these methods have targeted certain insightful aspects of the rain removal issue and have their suitability and superiority on speciﬁc occasions.
Albeit raising so many methods for rain removal from both video and single image, to the best of our knowledge, there is still not a comprehensive survey paper to speciﬁcally summarize and categorize current developments along this research line. Especially, there does still not exist a easily-usable source which could provide an off-the-shelf platform for general users to attain the source codes of current methods presented along this research line for easy performance comparison and capability evaluation for these methods. This, however, should be very meaningful for further prompting the frontier of this research issue and for facilitating an easy performance reproduction of previous algorithms and discovery of more intrinsic problems existing in current methods.
Against this meaningful task, in this study we aim at presenting a possibly comprehensive review for current rain removal methods for video and single image, as well as evaluating and analyzing the intrinsic capabilities, especially generalization, of representative state-of-the-art methods. In all, our contributions can be mainly summarized as follows:
Firstly, we comprehensively introduce the main ideas of the current rain removal methods for both video and single image. In speciﬁc, we summarize the physical properties of rain commonly used for rain modeling by previous research. For video and single image rain removal methods raised in both conventional model-driven and latest data-driven manners, we elaborately categorize them into several hierarchal branches, as shown in Fig. 1, and introduce the main methodology and representative methods of each branch.
Secondly, we provide a comprehensive performance comparison on representative rain removal methods and evaluate their respective capacity, especially generalization capability, both visually and quantitatively, based on typical synthetic

2

polar angel of elevation. θ = 0 represents the direction of the

Rain Removal

rainfall and r (θ) is the polar radius in the direction of θ. As a raindrop falls, it attains a constant velocity, called ter-

Video based Methods

Single Image based Methods

minal velocity [30]. By ﬁtting a large amount of experimental data with the least squares, Foote and Toit [30] obtained the

Physical Property

Low Rank and Sparsity

Deep Learning

Guided

Prior

Deep

Filter Information Learning

relationship between the terminal velocity v (m/s) of a raindrop and its diameter d (mm) as:

Time Domain

Frequency Domain

Model-driven

Data-driven Model-driven Data-driven

v0 = −0.2 + 5d − 0.9d2 + 0.1d3,

v = v0 (ρ0/ρ)0.4 ,

(2)

Fig. 1. A hierarchical categorization of current rain removal methods for where ρ is the air density at the location of the raindrop. ρ0

video and single image.

and v0 are obtained under the 1013 mb atmospheric conditions.

and real datasets containing diverse rain conﬁgurations. The Although a strong wind tends to change the rain orientation, implemented deraining methods, including 7 ones for video the direction of rain streaks captured in the limited range of a

and 10 ones for single image, cover recent state-of-the-art video frame or an image is almost consistent [31].

model-driven and data-driven rain removal algorithms.

2) Brightness Property: Garg and Nayar [8] pointed out

Most importantly, in this study we release a comprehensive that raindrops can be viewed as optical lens that refract repository to facilitate an easy use and performance reproduc- and reﬂect lights, and when a raindrop is passing through tion/comparison of current rain removal methods for general a pixel, the intensity of its image Ir is brighter than the

users. Particularly, this repository includes direct links to 74 background [32]. The imaging process was illustrated as:

rain removal papers, source codes of 9 methods for video deraining and 20 ones for single image deraining, 19 related project pages, 6 synthetic training datasets and 4 real ones, and 4 commonly used image quality metrics.
The rest of the paper is organized as follows. Section II surveys the main contents of recent literatures raised on rain removal issue. Experiments are then presented in Sections III for performance evaluation. Section IV concludes the whole paper, and lists some limitations and research issues worthy to be further investigated for future research of this direction.

τ

T

Ir (x, y) = Er (x, y) dt + Eb (x, y) dt, (3)

0

τ

where τ is the time during which a raindrop projects onto the

pixel location (x, y) and T is the exposure time of a camera.

Er is the irradiance caused by the raindrop and Eb is the average irradiance of the background [8], [33].

3) Chromatic Property: Zhang et al. [9] made further

investigation about the brightness property of rain and showed

that the increase in the intensities of R, G, and B channels is

dependent on the background scene. By empirical examples,

II. REVIEW OF CURRENT RAIN REMOVAL METHODS
In this section, we ﬁrst introduce some physical properties of rain, which constitute the modeling foundation of most rain removal methods, and then review the deraining methods for video and single image, respectively, according to the categorization as displayed in Fig. 1.

they found that the ﬁeld of views (FOVs) of red, green, and blue lights are all around 165o. For ease of computation, the authors directly assumed that the means of ∆R, ∆G, and ∆B are roughly equivalent for pixels covered by raindrops, where ∆R, ∆G, and ∆B denote the changes in color components of one pixel in two consecutive frames.
4) Spatial and Temporal Property: As raindrops are ran-

A. Physical Properties of Rain

domly distributed in space and move at high velocities, they often cause spatial and temporal intensity ﬂuctuations in a video,

A falling raindrop undergoes rapid shape distortions caused by many key factors, such as surface tension, hydrostatic pressure, ambient illumination, and aerodynamic pressure [26], [27]. These distortions will appear in forms of rain streaks with different brightness/directions [28] and distort background objects/scenes of videos/images [29]. In the following, we introduce some intrinsic properties of rain demonstrated in a video or a single image, which represent the typical clues

and a pixel at particular position is not always covered by these raindrops in every frame [9]. Therefore, in a video with stationary scene captured by a stationary camera, the intensity histogram of a pixel sometimes covered by rain exhibits two peaks, one for the background intensity distribution and the other for the rain intensity distribution. However, the intensity histogram of a pixel never covered by rain throughout the entire video exhibits only one peak [9].

for optimization or network modeling for constructing a rain

removal method. 1) Geometric Property: Beard and Chuang [26] described
the shape of small raindrops as a sphere, expressed as:

10

r (θ) = a 1 + cncos (nθ) ,

(1)

n=1

B. Video Rain Removal Methods
Garg and Nayar [8], [29] made early attempt for rain removal from videos, and proposed that by directly increasing the exposure time or reducing the depth of ﬁeld of a camera, the effects of rain can be reduced or even removed without altering the appearance of the scene in a video. However, this

where a is the radius of the undistorted sphere, cn is the shape method fails to deal with heavy rain and fast-moving objects coefﬁcient that depends on the radius of raindrop, and θ is the that are close to the camera, and the camcorder setting cannot

3

be adjusted by this method without substantial performance degradation of videos [27].
In the past few years, more intrinsic properties of rain streaks have been explored and formulated in algorithm designing for rain removal from videos in static/dynamic scenes. These algorithms can be mainly divided into four categories: time domain based ones, frequency domain based ones, low rank and sparsity based ones, and deep learning based ones. The ﬁrst three categories follow the hand-crafting pipelines to model rain context and thus should be seen as modeldriven methodologies, whereas the latter one follows datadriven manner where features are automatically learnt from pre-collected training data (rainy/clean frame pairs) [17], [25].
1) Time domain based methods: Garg and Nayar [34] ﬁrstly presented a comprehensive analysis of visual effects of rain on an imaging system and then developed a rain detection and removal algorithm for videos, which utilized a spacetime correlation model to capture the dynamics of rain and a physics-based motion blur model to explain the photometry of rain. Here the authors assumed that as raindrops fall with some velocity, they affect only a single frame. Hence, the rain streaks can be removed by exploiting the difference between consecutive frames [35].
To further improve the rain detection accuracy, Zhang et al. [9] incorporated both temporal and chromatic properties of rain and utilized K-means clustering to identify the background and rain streaks from videos. The idea works well in handling light and heavy rain, as well as rain in/out of focus. However, the method often tends to blur images due to a temporal average of the background. To alleviate this problem, Park et al. [36] further proposed to estimate the intensity of pixels and then remove rain recursively by Kalman ﬁlter, which performs well in a video with stationary background.
Later, by introducing both optical and physical properties of rain streaks, Brewer et al. [37] proposed to ﬁrst identify rain-affected regions showing a short-duration intensity spike, and then replaced the rain-affected pixel with average value in consecutive frames. Naturally, the method is able to distinguish intensity changes caused by rain from those made by scene motion. Yet, it is not very suitable to detect heavy rain where multiple rain streaks overlap and form undesirable shapes.
Zhao et al. [38] used temporal and spatial properties of rain streaks to design a histogram model for rain detection and removal, which embedded a concise K-means clustering algorithm with low complexity [9]. To handle both dynamic background and camera motion, Bossu et al. [31] utilized a Gaussian mixture model (GMM) and geometric moments to estimate the histogram of orientation of rain steaks.
Inspired by Bayesian theory, Tripathi et al. [28] relied on temporal property of rain to build a probabilistic model for rain streaks removal. Since intensity variations of rain-affected and rain-free pixels differ by the symmetry of waveform, the authors used two statistical features (intensity ﬂuctuation range and spread asymmetry) for distinguishing rain from rain-free moving object. As there is no any assumption about the shape and size of raindrops, the method is robust to rain conditions. To further reduce the usage of consecutive frames, the authors

turned to employing spatio-temporal process [35], which has less detection accuracy but better perceptual quality than [28].
2) Frequency domain based methods: Barnum et al. [39], [40] demonstrated a spatio-temporal frequency based method for globally detecting rain and snow with a physical and statistical model, where the authors utilized a blurred Gaussian model to approximate the blurring effects produced by the raindrops and a frequency-domain ﬁlter to reduce the visibility of raindrops/snow. The idea still works in videos with both scene and camera motions and can efﬁciently analyze repeated rain patterns. Nevertheless, the blurred Gaussian model cannot always cover rain streaks which are not sharp enough. Besides, the frequency-based detection manner often has errors when the frequency components of rain are not in order [28].
3) Low rank and sparsity based methods: In the recent decade, low rank and sparsity properties are extensively studied for rain/snow removal from videos. Chen et al. [41] ﬁrst considered the similarity and repeatability of rain streaks and generalized a low-rank model from matrix to tensor structure to capture the spatio-temporally correlated rain streaks. To deal with highly dynamic scenes [28], [34], [36], [38], Chen et al. further designed an algorithm based on motion segmentation of dynamic scene [42], which ﬁrst utilized photometric and chromatic constraints for rain detection and then applied rain removal ﬁlters on pixels such that their dynamic property as well as motion occlusion clue were incorporated. Spatial and temporal information is thus adaptively exploited during rain pixel recovery by the method, which, however, still cannot ﬁnely ﬁt camera jitters [43].
Later, Kim et al. [44] proposed to subtract temporally warped frames from the current frame to obtain an initial rain map, and then decomposed it into two types of basis vectors (rain streaks and outliers) via SVM. Next, by ﬁnding the rain map to exclude the outliers and executing low rank matrix completion, rain streaks could be removed. Obviously, the method needs extra supervised samples to train SVM.
Considering heavy rain and dynamic scenes, Ren et al. [43] divided rain streaks into sparse and dense layers, and formulated the detection of moving objects and sparse rain streaks as a multi-label Markov random ﬁeld (MRF) and dense ones as Gaussian distribution.
Jiang et al. [45], [46] proposed a novel tensor based video rain streaks removal approach by fully analyzing the discriminatively intrinsic characteristics of rain streaks and clean videos. In speciﬁc, rain streaks are sparse and smooth along the direction of raindrops, and clean videos possess smoothness along the rain-perpendicular direction and global and local correlation along time direction.
Different from previous rain removal methods formulating rain streaks as deterministic message, Wei et al. [17] ﬁrst encoded the rain layer as a patch-based mixture of Gaussian (P-MoG). By integrating the spatio-temporal smoothness conﬁguration of moving objects and low rank structure of background scene, the authors proposed a concise P-MoG model for rain streaks removal. Such stochastic manner makes the model capable of adapting a wider range of rain variations.
Motivated by the work [47], Li et al. [18] considered two intrinsic characteristics of rain streaks in videos, i.e., repetitive

4

local patterns sparsely scattered over different positions of a video and multiscale conﬁgurations due to their occurrence on positions with different distances to the cameras. The authors speciﬁcally formulated such understanding as a multi-scale convolutional sparse coding model (MS-CSC). Similar to [17], the authors separately use L1 and total variation (TV) to regularize the sparsity of feature maps and the smoothness of moving object layer. Such an encoding manner makes the model capable of properly extracting natural rain streaks from rainy videos.
4) Deep learning based methods: Very recently, deep learning based methods have also been investigated for the video rain removal task. For example, Chen et al. [48] proposed a convolutional neural network (CNN) framework for video rain streaks removal, which can handle torrential rain fall with opaque streak occlusions. In the work, superpixel has been utilized as the basic processing unit for content alignment and occlusion removal in a video with highly complex and dynamic scenes.
By exploring the wealth of temporal redundancy in videos, Liu et al. [19] built a hybrid rain model to depict both rain streaks and occlusions as:
Ot = (1 − αt) (Bt + Rt) + αtAt, t = 1, 2, . . . , N, (4)
where t and N signify the current time-step and total number of the frames in a video. Ot ∈ Rh×w, Bt ∈ Rh×w, and Rt ∈ Rh×w are the rainy image, background frame, and rain streak frame, respectively. At is the rain reliance map and αt is an alpha matting map. Based on the model (4), the authors utilized a deep recurrent convolutional network (RNN) to design a joint recurrent rain removal and reconstruction network (J4R-Net) that seamlessly integrated rain degradation classiﬁcation, spatial texture appearances based rain removal, and temporal coherence based background details reconstruction. To address deraining with dynamically detected video contexts, the authors chose a parallel technical route and further developed a dynamic routing residue recurrent network (D3R-Net) as well as a spatial temporal residue learning module for video rain removal [20].
C. Single Image Rain Removal Methods
In contrast to video based deraining methods with temporal redundancy knowledge, removing rain from individual images is more challenging since less information is available. To handle the problem, the algorithm design for single image rain removal has drawn increasingly more research attention. Generally, the existing single image rain removal methods can be divided into three categories: ﬁlter based ones, prior based ones, and deep learning based ones.
1) Filter based methods: Xu et al. [49] proposed a single image rain removal algorithm with guided ﬁlter [50]. In speciﬁc, by using chromatic property of rain streaks, the authors ﬁrst obtained coarse rain-free image (guidance image) and then ﬁltered rainy image to get the rain-removed image. For better visual quality, the authors incorporated brightness property of rain streaks and remended the guidance image [51].
Zheng et al. [52] later presented a multiple guided ﬁltering based single image rain/snow removal method. In the work,

the rain-removed image was acquired by taking the minimum value of rainy image and the coarse recovery image obtained by merging low frequency part (LFP) of rainy image with high frequency part (HFP) of rain-free image. To improve the rain removal performance, Ding et al. [53] designed a guided L0 smoothing ﬁlter to get coarse rain-/snow-free image.
Considering that a typical rain streak has an elongated elliptical shape with a vertical orientation, Kim et al. [54] proposed to detect rain streak regions by analyzing rotation angle and aspect ratio of the elliptical kernel at each pixel, and then executed nonlocal means ﬁltering on the detected regions by adaptively selecting nonlocal neighbor pixels and the corresponding weights.
2) Prior based Methods: In the recent years, maximum a posterior (MAP) has been attracting considerable attention for rain streak removal from a single image [55], [56], which can be mathematically described as:

max p(B, R|O) ∝ p(O|B, R) · p(B)· (R), (5)
B,R∈Ω
where O ∈ Rh×w, B ∈ Rh×w, and R ∈ Rh×w denote the observed rainy image, rain-free image, and rain streaks, respectively. p(B, R|O) is the posterior probability and p(O|B, R) is the likelihood function. Ω := {B, R|0 ≤ Bi, Ri ≤ Oi, ∀i ∈ [1, M × N ]} is the solution space. Generally, the MAP problem can be equivalently reformulated as the following energy minimization problem [55]:

min f (O, B, R) + Ψ(B) + Φ(R),

(6)

B,R∈Ω

where the ﬁrst term f (O, B, R) represents the ﬁdelity term measuring the discrepancy between the input image O and the recovered image B. The two regularization terms Ψ(B) and Φ(R) model image priors on B and R. Since single image rain removal is an ill-posed inverse problem, the priors play important roles in constraining solution space and enforcing desired property of the output [57].
Various methods have been proposed for designing the forms of all terms involved in (6). By using certain optimization algorithms, generally including an iterative process, the recovery image can then be obtained [57]. We introduce representative works presented along this line as follows.
Fu et al. [58] utilized morphological component analysis (MCA) to formulate rain removal as an image decomposition problem. Speciﬁcally, a rainy image was divided into LFP and HFP with a bilateral ﬁlter, and the derained result was obtained by merging the LFP and the rain-free component. Here the component was achieved by performing dictionary learning and sparse coding on the HFP. For more accurate HFP, Chen et al. [59] exploited sparse representation and then separated rain streaks from the HFP by exploiting a hybrid feature set, including histogram of oriented gradients, depth of ﬁeld, and Eigen color. Similarly, Kang et al. [60], [61] exploited histogram of oriented gradients (HOGs) features of rain streaks to cluster into rain and non-rain dictionary.
To remove rain and snow for single image, Wang et al. [62] designed a 3-layer hierarchical scheme. With a guided ﬁlter, the authors obtained the HFP consisting of rain/snow and image details, and then decomposed it into rain/snow-free

5

parts and rain/snow-affected parts via dictionary learning and three classiﬁcations of dictionary atoms. In the end, with the sensitivity of variance of color image (SVCC) map and the combination of rain/snow detection and the guided ﬁlter, the useful image details could be extracted.
Afterwards, Sun et al. [63] intended to exploit the structural similarity of image bases for single image rain removal. By focusing on basis selection and incorporating the strategy of incremental dictionary learning, the idea is robust against rain patterns and can preserve image information well.
To ﬁnely separate rain layer and rain-removed image layer, Luo et al. [10] proposed a dictionary learning based single image rain removal method. The main idea is to sparsely approximate the patches of the two layers by high discriminative codes over a learned dictionary with strong mutual exclusivity. To better remove more rain streaks and preserve background layer, Li et al. [11] introduced GMM based patch prior to accommodate multiple orientations and scales of rain streaks.
For the progressive separation of rain streaks from background details, Zhu et al. [64] modeled three regularization terms in various aspects: integrating local and nonlocal sparsity via a centralized sparse representation, measuring derivation of gradients from the estimated rain direction by analyzing the gradient statistics, and measuring the visual similarity between image patches and rain patches to ﬁlter the rain layer.
Very recently, Gu et al. [12] proposed a joint convolutional analysis and synthesis (JCAS) sparse representation model, where image large-scale structures were approximated by analysis sparse representation (ASR) and image ﬁne-scale textures were described by synthesis sparse representation (SSR). The complementary property of ASR and SSR made the proposed JCAS able to effectively extract image texture layer without oversmoothing the background layer.
Considering the challenge to establish effective regularization priors and optimize the objective function in (6), Mu et al. [55] introduced an unrolling strategy to incorporate datadependent network architectures into the established iterations, i.e., a learning bilevel layer priors method to jointly investigate the learnable feasibility and optimality of rain streaks removal problem. This is a beneﬁcial attempt to integrate both modeldriven and data-driven methodologies for the deraining task.
3) Deep learning based methods: Eigen et al. [65] ﬁrst utilized CNN to remove dirt and water droplets adhered to a glass window or camera lens. However, the method fails to handle relatively large/dense raindrops and dynamic rain streaks, and produces blurry outputs. In order to deal with substantial presence of raindrops, Qian et al. [66] designed an attentive generative network. The basic idea is to inject visual attention into the generative and discriminative networks. Here the generative network focuses on raindrop regions and their surroundings, and the discriminative network mainly assesses the local consistency of restored regions.
To especially deal with single image rain streak removal, Fu et al. [13] ﬁrst designed a CNN based DerainNet, which automatically learnt the nonlinear mapping function between clean and rainy image details from data. To improve the restoration quality, the authors additionally introduced image processing domain knowledge. Motivated by great success

of deep residual network (ResNet) [67], Fu et al. [15] further proposed a deep detail network (DDN) to reduce the mapping range from input to output and then to make the learning process signiﬁcantly easier. Again, Fan and Fu et al. [68] proposed a residual-guided feature fusion network (ResGuideNet), where a coarse to ﬁne estimation of negative residual was progressively obtained as network goes deeper.
Instead of relying on image decomposition framework like [13], [15], Zhang et al. [69] proposed a conditional generative adversarial networks (GAN) for single image deraining which incorporated quantitative, visual, and discriminative performance into objective function. Since a single network may not learn all patterns in training samples, the authors [14] further presented a density-aware image deraining method using a multistream dense network (DID-MDN). By integrating a residual-aware classiﬁer process, DID-MDN can adaptively determine the rain-density information (heavy/medium/light).
Recently, Yang et al. [1] reformulated the atmospheric process of rain as a new model, expressed as:

s

O = α B + StR + (1 − α)A,

(7)

t=1

where R denotes the locations of individually visible rain streaks. Each St is a rain streak layer with the same direction and s is the maximum number of layers. A is the global atmospheric light and α is the atmospheric transmission. Based on the generative model, the authors developed a multitask architecture that successively learnt binary rain streak map, appearance of rain streaks, and clean background. By utilizing a RNN and a contextualized dilated network [70], the method can remove rain streak and rain accumulation iteratively and progressively, even in the presence of heavy rain. For better deraining performance, the authors further proposed an enhanced version–JORDER E, which included an extra detail preserving step [2].
Similarly, Li et al. [16] proposed a recurrent squeeze-andexcitation (SE) based context aggregation network (CAN) for single image rain removal, where SE block assigned different alpha-values to various rain streak layers and CAN acquired large receptive ﬁeld and better ﬁt the rain removal task.
Existing deep learning methods usually treated network as an encapsulated end-to-end mapping module without deepening into the rationality and superiority towards more effective rain streaks removal [23], [71]. Li et al. [72] proposed a nonlocally enhanced encoder-decoder network to efﬁciently learn increasingly abstract feature representation for more accurate rain streaks and then ﬁnely preserve image details.
As seen, the constructed deep network structures become more and more complicated, making network designing hardly reproducible and attainable to many beginners in this area. To alleviate this issue, Ren et al. [21] presented a simple and effective progressive recurrent deraining network (PReNet) by repeatedly unfolding a shallow ResNet with a recurrent layer.
A practical issue for data-driven single image rain removal methods is the requirement of synthetic rainy/clean image pairs, which cannot sufﬁciently cover wider range of rain streak patterns in real rainy image such as rain shape, direction

6

and intensity. In addition, there are no public benchmarks for quantitative comparisons on real rainy images, which makes current evaluation less objective. To handle these problems, Wang et al. [22] semi-automatically constructed a large-scale dataset of rainy/clean image pairs that covers a wide range of natural rain scenes, and proposed a spatial attentive network (SPANet) to remove rain streaks in a local-to-global manner.
As we know, the main problem in recent data-driven single image rain removal methods is that they generally need to precollect sufﬁcient supervised samples, which is time-consuming and cumbersome. Besides, most of these methods are trained on synthetic samples, making themselves less able to well generalize to real test samples with complicated rain distribution. To alleviate these problems, Wei et al. [24] adopted DDN as the backbone (supervised part) and regularized rain layer with GMM to feed unsupervised rainy images. In this semisupervised manner, the method ameliorates the hard-to-collecttraining-sample and overﬁtting-to-training-sample issues.
D. A Comprehensive Repository for Rain Removal
To facilitate an easy usage and performance reproduction/comparison of current rain removal methods for general users, we build a repository for current research development of rain removal1. Speciﬁcally, this repository includes direct links to 74 rain removal papers, source codes of 9 methods for video rain removal and 20 ones for single image rain removal, 19 related project pages, 6 synthetic datasets and 4 real ones, and 4 commonly used image quality metrics as well as their computation codes including peak-signal-to-noise ratio (PSNR) [73], structure similarity (SSIM) [74], visual quality (VIF) [75], and feature similarity (FSIM) [76]. The state-ofthe-art performance for the rain removal problems can thus be easily obtained by general users. All our experiments were readily implemented by using this repository.
III. EXPERIMENTS AND ANALYSIS
In this section, we compare the performance of different competing deraining methods for video and a single image. The implementation environment is Windows10, Matlab (R2018b), PyTorch (version 1.0.1) [77], and Tensorﬂow (version 1.12.0) with an Intel (R) Core(TM) i7-8700K at 3.70GHZ, 32GM RAM, and two Nvidia GeForce GTX 1080Ti GPUs.
A. Video Deraining Experiments
In this section, we evaluate the video deraining performance of the recent state-of-the-art methods on synthetic and real benchmark datasets. These methods include Garg et al. [8], Kim et al. [44], Jiang et al. [45], Ren et al. [43], Wei et al. [17], Li et al. [18], and Liu et al. [19].
1) Synthetic Data: Here we utilize the dataset released by [18]. The paper chose two videos from CDNET database [78], containing varying forms of moving objects and background scenes, and added different types of rain streaks under black background of these videos, varying from tiny drizzling to heavy rain storm and vertical rain to slash line. For synthetic
1https://github.com/hongwang01/Video-and-Single-Image-Deraining

TABLE I PERFORMANCE COMPARISONS OF ALL COMPETING VIDEO RAIN REMOVAL
METHODS IN SYNTHETIC RAIN.

Datasets Metrics PSNR
Input 28.22 Garg [8] 29.83 Kim [44] 30.44 Jiang [45] 31.93 Ren [43] 28.26 Wei [17] 29.76 Li [18] 33.89 Liu [19] 27.56

Fig. 2 VIF FSIM
0.637 0.935 0.661 0.955 0.602 0.958 0.745 0.971 0.685 0.970 0.830 0.992 0.865 0.992 0.626 0.995

SSIM 0.927 0.946 0.952 0.974 0.962 0.988 0.992 0.941

PSNR 23.82 24.15 22.39 24.32 23.52 24.47 25.37 22.19

Fig. 3 VIF FSIM
0.766 0.970 0.611 0.960 0.526 0.932 0.713 0.966 0.681 0.966 0.779 0.980 0.790 0.980 0.555 0.946

SSIM 0.929 0.911 0.886 0.938 0.927 0.951 0.957 0.895

data, since the rain-free groundtruth videos are available, we can compare all competing methods both visually and quantitatively. Four typical metrics for video have been employed, including PSNR, SSIM, VIF, and FSIM.
Fig. 2 illustrates the deraining performance of all compared methods on videos with usual rain. As displayed in the ﬁrst row, the rain removal results show that Garg et al.’s, Kim et al.’s, Jiang et al.’s, and Liu et al.’s methods do not ﬁnely detect rain streaks, and Ren et al.’s method improperly removes moving objects and rain streaks. The corresponding rain layers provided in the second row depict that apart from Li et al.’s method which can preserve texture details well, the rain layers extracted by the other methods contain different degrees of background information.
We also evaluate all competing methods under heavy rain scenario as shown in Fig. 3. The rain removal results displayed in the ﬁrst row indicate that Garg et al.’s, Kim et al.’s, Jiang et al.’s, and Liu et al.’s methods do not well detect heavy rain streaks. In comparison with Wei et al.’s method, which treats rain streaks as aggregation of noise rather than natural streamline, Li et al.’s method presents natural rain patterns and has a better visual effect.
The quantitative comparisons are listed in Table I, which shows that among these competing methods, Li et al.’s method achieves a relatively better performance in terms of used quality metrics. Note that the performance of Liu et al’s method is not very satisfactory since there is an evident bias between the training data and our testing cases. The overﬁtting issue inevitably occurs.
2) Real-World Data: We then show the rain streak removal results on real videos. As we have no groundtruth knowledge in this case, we only provide the visual effect comparisons.
Fig. 4 presents the deraining results on a video with complex moving objects, including walking pedestrian and moving vehicles, which is captured by surveillance systems on street. It is seen that Garg et al.’s, Kim et al.’s, Jiang et al.’s, and Wei et al.’s methods cause different degrees of artifacts at the location of the moving car. Comparatively, Li et al.’s method performs relatively well in this complicated scenario.
Fig. 5 displays the rain removal performance on a real video obtained at night. Comparatively, Wei et al.’s and Li et al.’s methods can better detect all rain streaks.

B. Single Image Deraining Experiments
In this section, we evaluate the single image deraining performance of the recent state-of-the-art methods, including

7

Input/Groundtruth

Garg et al. [8]

Kim et al. [44]

Jiang et al. [45]

Ren et al. [43]

Wei et al. [17]

Li et al. [18]

Liu et al. [19]

Fig. 2. The ﬁrst column: input rainy frame (upper) and groundtruth (lower). From the second to the eighth columns: derained frames and extracted rain layers

by 7 competing methods.

typical model-driven methods: Luo et al. [10] (denoted as DSC), Li et al. [11] (denoted as GMM), and Gu et al. [12] (denoted as JCAS), and representative data-driven methods: Fu et al. [13] (denoted as Clear), Fu et al. [15] (denoted as DDN), Li et al. [16] (denoted as RESCAN), and Ren et al. [21] (denoted as PReNet), Wang et al. [22] (denoted as SPANet), Yang et al. [2] (denoted as JORDER E), and semi-supervised method: Wei et al. [24] (denoted as SIRR).
1) Synthetic Data: For synthetic data, we utilized four frequently-used benchmark datasets: Rain1400 synthesized by Fu et al. [15], Rain12 provided by Li et al. [11], Rain100L and Rain100H provided by Yang et al. [1]. Speciﬁcally, Rain1400 includes 14000 rainy images synthesized from 1000 clean images with 14 kinds of different rain streak orientations and magnitudes. Among these images, 900 clean images (12600 rainy images) are chosen for training and 100 clean images (1400 rainy images) are selected as testing samples. Rain12 consists of 12 rainy/clean image pairs. Rain100L is selected from BSD200 [79] with only one type of rain streaks, which consists of 200 image pairs for training and 100 image pairs for testing. Compared with Rain100L, Rain100H with ﬁve types of streak directions is more challenging, which contains 1800 image pairs for training and 100 image pairs for testing. As for SIRR, we use the real 147 rainy images released by Wei et al. [24] as unsupervised training data. Since Rain12 has few samples, like [21], we directly adopt the trained model on Rain100L to do an evaluation on Rain12.

As the groundtruth in synthetic datasets is available, we will evaluate all competing methods by two commonly used metrics PSNR and SSIM. Since human visual system is sensitive to Y channel in YCbCr space, we utilize the luminance channel to compute all quantitative results.
Fig. 6 shows the visual and quantitative comparisons of rain streak removal results for one synthesized rainy image from Rain100L. As displayed, three model-driven methods: DSC, GMM, and JCAS, leave many rain streaks in the recovered image. Especially, JCAS tends to oversmooth the background details. It implies that model prior is not sufﬁcient enough to convey complex rain streak shapes in synthetic dataset. Compared with these conventional model-driven methods, six datadriven methods, Clear, DDN, RESCAN, PReNet, SPANet, and JORDER E, have the ability to more completely remove the rain streaks. However, they damage the image content and lose detail information to a certain extent. Although SIRR focus on domain adaption, it fails to remove most rain streaks. This can be explained by the fact that there exists an obvious difference in distribution between Rain100L and real rainy images.
We further evaluate these competing methods on Rain100H. As shown in Fig. 7, due to complicated rain patterns in heavy rain cases, the rain detection capability of most competing methods is weakened. By observing zoomed red boxes, we can ﬁnd that for almost all competing methods, the rain removal results are not very satisfactory when rain streaks and the image background merge with each other. More rational and

8

Input/Groundtruth

Garg et al. [8]

Kim et al. [44]

Jiang et al. [45]

Ren et al. [43]

Wei et al. [17]

Li et al. [18]

Liu et al. [19]

Fig. 3. The ﬁrst column: input rainy frame with synthetic heavy rain (upper) and groundtruth (lower). From the second to the eighth columns: derained frames and extracted rain layers by 7 competing methods.

insightful understanding for intrinsic imaging process of rain streaks is still required to be further discovered and utilized [1].
We additionally made an evaluation based on Rain1400 and Rain12 with different rain patterns as presented in Fig. 8 and Fig. 9. From these, we can easily understand that generally the data-driven methods can achieve better rain removal effect than model-driven methods. However, due to the overﬁttingto-training-samples issue, these deep learning methods make derained results lack of some image details.
Table II demonstrates quantitative results of all competing methods on synthetic datasets. From the table, we can conclude that due to the strong nonlinear ﬁtting ability of deep networks, the rain removal effect of most data-driven methods is evidently superior than those of model-driven methods. Besides, compared with the backbone network–DDN, SIRR hardly obtains any performance gain on these datasets. This can be explained by the fact that the usage of real unsupervised training samples makes the data distribution deviate from synthetic datasets.
2) Real-World Data: For real application, what we really care about is the deraining ability of all competing methods on real rainy images. Here we will give a fair evaluation based on two real-world datasets: the one with 147 rainy images

TABLE II PSNR AND SSIM COMPARISONS ON FOUR SYNTHETIC BENCHMARK
DATASETS.

Datasets

Rain100L

Metrics PSNR SSIM

Input

26.90 0.838

DSC [10] 27.34 0.849

GMM [11] 29.05 0.872

JCAS [12] 28.54 0.852

Clear [13] 30.24 0.934

DDN [15] 32.38 0.926

RESCAN [16] 38.52 0.981

PReNet [21] 37.45 0.979

SPANet [22] 34.46 0.962

JORDER E [2] 38.61 0.982

SIRR [24] 32.37 0.926

Rain100H PSNR SSIM
13.56 0.371 13.77 0.320 15.23 0.450 14.62 0.451 15.33 0.742 22.85 0.725 29.62 0.872 30.11 0.905 25.11 0.833 30.04 0.906 22.47 0.716

Rain1400

Rain12

PSNR SSIM PSNR SSIM

25.24 0.810 30.14 0.856

27.88 0.839 30.07 0.866

27.78 0.859 32.14 0.916

26.20 0.847 33.10 0.931

26.21 0.895 31.24 0.935

28.45 0.889 34.04 0.933

32.03 0.931 36.43 0.952

32.55 0.946 36.66 0.961

29.76 0.908 34.63 0.943

32.68 0.943 36.69 0.962

28.44 0.889 34.02 0.935

released by Wei et al. [24], called Internet-Data, and the other with 1000 image pairs collected by Wang et al. [22], called SPA-Data. Note that as Internet-Data has no groundtruth, we can only provide visual comparison.
Fig. 10 demonstrates a hard sample with various rain densities selected from Internet-Data. As seen, almost all competing methods cannot completely remove rain streaks and perfectly clear up rain accumulation effect. Even though PReNet, RESCAN, and JORDER E achieve signiﬁcant de-

9

Input

Garg et al. [8]

Kim et al. [44]

Jiang et al. [45]

Ren et al. [43]

Wei et al. [17]

Li et al. [18]

Fig. 4. Rain removal performance of all competing methods on a real video with complex moving objects.

Liu et al. [19]

Input

Garg et al. [8]

Kim et al. [44]

Jiang et al. [45]

Ren et al. [43]

Wei et al. [17]

Fig. 5. Rain removal performance of all competing methods on a real video at night.

Li et al. [18]

Liu et al. [19]

raining performance on synthetic datasets, they oversmooth the background information to some extent. This can be interpreted as that for model-driven methods, the priors they adopt have not comprehensively covered the complicated distribution of real rain, and for data-driven methods, they tend to learn speciﬁc rain patterns in synthesized data while cannot properly generalize to real test samples with diverse rain types.
Further, we utilize SPA-Data to more objectively analyze the generalization ability of all competing methods as displayed in Fig. 11 and Table III. These comparisons tell us that in this case, the model-driven method JCAS with meaningful priors even performs better than some data-driven works, i.e., DDN and RESCAN. It is worth mentioning that although the rain removal performance of SPANet on synthesized datasets with imprecise rain mask is not very satisfying, it obtains an outstanding generalization ability on the real dataset with easily extracted rain mask. Additionally, compared with DDN, SIRR accomplishes a better transfer learning effect, which beneﬁt from the unsupervised module.
IV. CONCLUSIONS AND FUTURE WORKS
In this paper, we have presented a comprehensive survey on the rain removal methods for video and a single image in the past few years. Both conventional model-driven and

TABLE III PSNR AND SSIM COMPARISONS ON SPA-DATA [22].

Methods Input
DSC [10] GMM [11] JCAS [12] Clear [13] DDN [15]

PSNR 34.15 34.95 34.30 34.95 32.66 34.70

SSIM 0.927 0.942 0.943 0.945 0.942 0.934

Methods RESCAN [16] PReNet [21] SPANet [22] JORDER E [2]
SIRR [24] /

PSNR 34.70 35.08 35.24 34.34 34.85
/

SSIM 0.938 0.942 0.945 0.936 0.936
/

latest data-driven methodologies raised for the deraining task have been thoroughly introduced. Recent representative stateof-the-art algorithms have been implemented on both synthetic and real benchmark datasets, and the deraining performance, especially the generalization capability have been empirically compared and quantitatively analyzed. Especially, to make general users easily attain rain removal resources, we release a repository, including direct links to 74 rain removal papers, source codes of 9 methods for video rain removal and 20 ones for single image rain removal, 19 related project pages, 6 synthetic datasets and 4 real ones, and 4 commonly used image quality metrics. We believe this repository should be beneﬁcial to further prompt the further advancement of this meaningful research issue. Here we want to summarize some limitations still existing in current deraining methods as follows:

10

Input 27.37 / 0.815

Groundtruth
/

DSC [10] 29.34 / 0.848

GMM [11] 32.38 / 0.931

JCAS [12] 31.45 / 0.915

Clear [13] 31.59 / 0.938

DDN [15] 37.31 / 0.970

RESCAN [16] 41.26 / 0.989

PReNet [21] 37.27 / 0.979

SPANet [22] 35.67 / 0.970

JORDER_E [2] 41.72 / 0.991

SIRR [24] 36.99 / 0.969

Fig. 6. Rain removal performance of all competing methods on a synthetic test image from Rain100L. PSNR/SSIM results are included below the corresponding

recovery image for reference.

Input 14.66 / 0.422

Groundtruth
/

DSC [10] 16.35 / 0.422

GMM [11] 15.90 / 0.438

JCAS [12] 16.84 / 0.565

Clear [13] 17.47 / 0.787

DDN [15]

RESCAN [16]

PReNet [21]

SPANet [22]

JORDER_E [2]

SIRR [24]

26.11 / 0.790

29.89 / 0.885

30.92 / 0.920

26.66 / 0.837

31.59 / 0.919

25.11 / 0.757

Fig. 7. Rain removal performance of competing methods on a synthetic test image from Rain100H. PSNR/SSIM results are included below the corresponding

recovery image for reference.

Input 20.17 / 0.520

Groundtruth /

DSC [10] 23.92 / 0.587

GMM [11] 23.69 / 0.713

JCAS [12] 22.09 / 0.657

Clear [13] 22.05 / 0.795

DDN [15] 25.66 / 0.774

RESCAN [16] 31.77 / 0.887

PReNet [21] 32.65 / 0.907

SPANet [22] 29.73 / 0.861

JORDER_E [2] 32.61/ 0.899

SIRR [24] 25.74 / 0.771

Fig. 8. Rain removal performance of competing methods on a synthetic test image from Rain1400. PSNR/SSIM results are included below the corresponding

recovery image for reference.

1) Due to the intrinsic overlapping between rain streaks and background texture patterns, most of deraining methods tend to more or less remove texture details in rain-free regions, thus resulting in oversmoothing effect in the recovered background.
2) As aforementioned, the imaging process of rain in natural scenes is very complex [1], [3], [23]. However, the rain model widely used in most existing methods has not sufﬁciently describe such intrinsic mechanism, like the mist/fog effect formed by rain streak accumulation.
3) Although current model-driven methods try to portray complex rain streaks by diverse well-designed priors, they are only applicable to speciﬁc patterns instead of irregular distribution in real rainy images. Another

obvious drawback is that the optimization algorithms employed by these methods generally involve many iterations of computation, causing their inefﬁciency in real scenarios [10]–[12], [43], [55]. 4) Most data-driven methods require a great deal of training samples, which is time-consuming and cumbersome to collect [1], [15], [22]. And they generally have unsatisfactory generalization capability because of the overﬁtting-to-training-sample issue. Besides, the designed networks are always like black boxes with less interpretability and few insights [25], [80]. 5) For the video deraining task, most model-driven methods cannot directly apply to streaming video data [17], [18], [43] in real-time. Meanwhile, the deep learning methods

11

Input 25.49 / 0.751

Groundtruth /

DSC [10] 25.63 / 0.757

GMM [11] 30.31 / 0.931

JCAS [12] 28.27 / 0.900

Clear [13] 27.21 / 0.893

DDN [15] 28.93 / 0.872

RESCAN [16] 32.36 / 0.936

PReNet [21] 31.63 / 0.934

SPANet [22] 29.50 / 0.907

JORDER_E [2] 33.87 / 0.952

SIRR [24] 28.86 / 0.877

Fig. 9. Rain removal performance of competing methods on a synthetic test image from Rain12. PSNR/SSIM results are included below the corresponding

recovery image for reference.

Input

DSC [10]

GMM [11]

JCAS [12]

Clear [13]

DDN [15]

RESCAN [16]

PReNet [21]

SPANet [22]

Fig. 10. Rain removal performance of different methods on a real rainy image from [24].

JORDER_E [2]

SIRR [24]

need a large amount of supervised videos, which exhibits high computational complexity in training stage, and they cannot guarantee favorable rain removal performance especially in complex scenes [19], [48].
The rain removal for video and a single image is thus still an open and worthy to be further investigated problem. Based on our evaluation and research experience, we also try to present the following remarks to illustrate some meaningful future research directions along this line:
1) Due to the diversity and the complexity of real rain, a meaningful scope is to skillfully combine modeldriven and data-driven methodologies into a unique framework to make it possess both superiority of two learning manners. A hopeful direction is the deep unroll strategy, which might conduct networks with both better interpretability and generalization ability [80].
2) To deal with the hard-to-collect-training-example and overﬁtting-to-training-example issues, semisupervised/unsupervised learning as well as domain adaption and transfer learning regimes should be necessary to be explored to transfer the learned knowledge from limited training cases to wider range of diverse testing scenarios [24], [81].
3) To better serve real applications, we should emphasize efﬁciency and real-time requirement. Especially for videos, it is necessary to construct online rain removal techniques which meet three crucial properties: persis-

tence (process steaming video data in real time), low time and space complexity, universality (available to complex video scenes). Similarly, fast test speed for a single image is also required. 4) Generally, deraining is served as a pre-processing step for certain subsequent computer vision tasks. It is critical to develop task-speciﬁc deraining algorithm [3]. 5) Due to the fact that rain is closely related to other weather conditions, such as haze and snow [1], [18], multi-task learning by accumulating multiple sample sources collected in bad weathers for performance enhancement may also be worth exploring in future research.
REFERENCES
[1] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint rain detection and removal from a single image,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1357–1366.
[2] W. Yang, R. T. Tan, J. Feng, J. Liu, S. Yan, and Z. Guo, “Joint rain detection and removal from a single image with contextualized deep networks,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PP, no. 99, pp. 1–1, 2019.
[3] S. Li, I. B. Araujo, W. Ren, Z. Wang, E. K. Tokuda, R. H. Junior, R. Cesar-Junior, J. Zhang, X. Guo, and X. Cao, “Single image deraining: A comprehensive benchmark analysis,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 3838–3847.
[4] D. Comaniciu, V. Ramesh, and P. Meer, “Kernel-based object tracking,” IEEE Transactions on Pattern Analysis & Machine Intelligence, no. 5, pp. 564–575, 2003.

12

Input 34.86 / 0.937

Groundtruth /

DSC [10] 35.02 / 0.939

GMM [11] 37.51 / 0.966

JCAS [12] 38.60 / 0.972

Clear [13] 36.51 / 0.974

DDN [15] 36.80 / 0.955

RESCAN [16] 37.77 / 0.971

PReNet [21] 40.55 / 0.983

SPANet [22] 41.14 / 0.987

JORDER_E [2] 40.04 / 0.985

SIRR[24] 36.91 / 0.955

Fig. 11. Rain removal performance of different competing methods on a real rainy image from [22]. PSNR/SSIM results are included below the corresponding

recovery image for reference.

[5] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual attention for rapid scene analysis,” IEEE Transactions on Pattern Analysis & Machine Intelligence, no. 11, pp. 1254–1259, 1998.
[6] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and M. Cristani, “Person re-identiﬁcation by symmetry-driven accumulation of local features,” in IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010, pp. 2360–2367.
[7] M. S. Shehata, J. Cai, W. M. Badawy, T. W. Burr, M. S. Pervez, R. J. Johannesson, and A. Radmanesh, “Video-based automatic incident detection for smart roads: The outdoor environmental challenges regarding false alarms,” IEEE Transactions on Intelligent Transportation Systems, vol. 9, no. 2, pp. 349–360, 2008.
[8] K. Garg and S. K. Nayar, “Vision and rain,” International Journal of Computer Vision, vol. 75, no. 1, pp. 3–27, 2007.
[9] X. Zhang, H. Li, Y. Qi, W. K. Leow, and T. K. Ng, “Rain removal in video by combining temporal and chromatic properties,” in IEEE International Conference on Multimedia and Expo, 2006, pp. 461–464.
[10] Y. Luo, Y. Xu, and H. Ji, “Removing rain from a single image via discriminative sparse coding,” in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 3397–3405.
[11] Y. Li, R. T. Tan, X. Guo, J. Lu, and M. S. Brown, “Rain streak removal using layer priors,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2736–2744.
[12] S. Gu, D. Meng, W. Zuo, and L. Zhang, “Joint convolutional analysis and synthesis sparse representation for single image layer separation,” in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1708–1716.
[13] X. Fu, J. Huang, X. Ding, Y. Liao, and J. Paisley, “Clearing the skies: A deep network architecture for single-image rain removal,” IEEE Transactions on Image Processing, vol. 26, no. 6, pp. 2944–2956, 2017.
[14] H. Zhang and V. M. Patel, “Density-aware single image de-raining using a multi-stream dense network,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 695–704.
[15] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Paisley, “Removing rain from single images via a deep detail network,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 3855–3863.
[16] X. Li, J. Wu, Z. Lin, H. Liu, and H. Zha, “Recurrent squeezeand-excitation context aggregation net for single image deraining,” in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 254–269.
[17] W. Wei, L. Yi, Q. Xie, Q. Zhao, D. Meng, and Z. Xu, “Should we encode rain streaks in video as deterministic or stochastic?” in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 2516– 2525.
[18] M. Li, Q. Xie, Q. Zhao, W. Wei, S. Gu, J. Tao, and D. Meng, “Video rain streak removal by multiscale convolutional sparse coding,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 6644–6653.
[19] J. Liu, W. Yang, S. Yang, and Z. Guo, “Erase or ﬁll? deep joint recurrent rain removal and reconstruction in videos,” in Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, 2018, pp. 3233–3242. [20] ——, “D3r-net: Dynamic routing residue recurrent network for video rain removal,” IEEE Transactions on Image Processing, vol. 28, no. 2, pp. 699–712, 2018. [21] D. Ren, W. Zuo, Q. Hu, P. Zhu, and D. Meng, “Progressive image deraining networks: a better and simpler baseline,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 3937–3946. [22] T. Wang, X. Yang, K. Xu, S. Chen, Q. Zhang, and R. W. Lau, “Spatial attentive single-image deraining with a high quality real rain dataset,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 12 270–12 279. [23] J. Pan, S. Liu, D. Sun, J. Zhang, Y. Liu, J. Ren, Z. Li, J. Tang, H. Lu, Y.-W. Tai et al., “Learning dual convolutional neural networks for lowlevel vision,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3070–3079. [24] W. Wei, D. Meng, Q. Zhao, Z. Xu, and Y. Wu, “Semi-supervised transfer learning for image rain removal,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 3877–3886. [25] C. Yang, R. Liu, L. Ma, X. Fan, H. Li, and M. Zhang, “Unrolled optimization with deep priors for intrinsic image decomposition,” in IEEE Fourth International Conference on Multimedia Big Data (BigMM), 2018, pp. 1–7. [26] K. V. Beard and C. Chuang, “A new model for the equilibrium shape of raindrops,” Journal of the Atmospheric sciences, vol. 44, no. 11, pp. 1509–1524, 1987. [27] A. K. Tripathi and S. Mukhopadhyay, “Removal of rain from videos: a review,” Signal, Image and Video Processing, vol. 8, no. 8, pp. 1421– 1430, 2014. [28] ——, “A probabilistic approach for detection and removal of rain from videos,” IETE Journal of Research, vol. 57, no. 1, pp. 82–91, 2011. [29] K. Garg and S. K. Nayar, “When does a camera see rain?” in Tenth IEEE International Conference on Computer Vision (ICCV’05) Volume 1, vol. 2, 2005, pp. 1067–1074. [30] G. B. Foote and P. Du Toit, “Terminal velocity of raindrops aloft,” Journal of Applied Meteorology, vol. 8, no. 2, pp. 249–253, 1969. [31] J. Bossu, N. Hautie`re, and J.-P. Tarel, “Rain or snow detection in image sequences through use of a histogram of orientation of streaks,” International journal of computer vision, vol. 93, no. 3, pp. 348–367, 2011. [32] K. Garg and S. K. Nayar, “Photometric model of a rain drop,” in CMU Technical Report, 2003. [33] P. Liu, J. Xu, J. Liu, and X. Tang, “Pixel based temporal analysis using chromatic property for removing rain from videos.” Computer and information science, vol. 2, no. 1, pp. 53–60, 2009. [34] K. Garg and S. K. Nayar, “Detection and removal of rain from videos,” in Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., vol. 1, 2004, pp. I–I.

13

[35] A. Tripathi and S. Mukhopadhyay, “Video post processing: low-latency spatiotemporal approach for detection and removal of rain,” IET image processing, vol. 6, no. 2, pp. 181–196, 2012.
[36] W.-J. Park and K.-H. Lee, “Rain removal using kalman ﬁlter in video,” in 2008 International Conference on Smart Manufacturing Application, 2008, pp. 494–497.
[37] N. Brewer and N. Liu, “Using the shape characteristics of rain to identify and remove rain from video,” in Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR), 2008, pp. 451–458.
[38] X. Zhao, P. Liu, J. Liu, and T. Xianglong, “The application of histogram on rain detection in video,” in 11th Joint International Conference on Information Sciences, 2008.
[39] P. Barnum, T. Kanade, and S. Narasimhan, “Spatio-temporal frequency analysis for removing rain and snow from videos,” in International Workshop on Photometric Analysis for Computer Vision, 2007.
[40] P. C. Barnum, S. Narasimhan, and T. Kanade, “Analysis of rain and snow in frequency space,” International journal of computer vision, vol. 86, no. 2-3, p. 256, 2010.
[41] Y.-L. Chen and C.-T. Hsu, “A generalized low-rank appearance model for spatio-temporally correlated rain streaks,” in Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 1968–1975.
[42] J. Chen and L.-P. Chau, “A rain pixel recovery algorithm for videos with highly dynamic scenes,” IEEE Transactions on Image Processing, vol. 23, no. 3, pp. 1097–1104, 2013.
[43] W. Ren, J. Tian, Z. Han, A. Chan, and Y. Tang, “Video desnowing and deraining based on matrix decomposition,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 4210–4219.
[44] J.-H. Kim, J.-Y. Sim, and C.-S. Kim, “Video deraining and desnowing using temporal correlation and low-rank matrix completion,” IEEE Transactions on Image Processing, vol. 24, no. 9, pp. 2658–2670, 2015.
[45] T.-X. Jiang, T.-Z. Huang, X.-L. Zhao, L.-J. Deng, and Y. Wang, “A novel tensor-based video rain streaks removal approach via utilizing discriminatively intrinsic priors,” in Proceedings of the ieee conference on computer vision and pattern recognition, 2017, pp. 4057–4066.
[46] ——, “Fastderain: A novel video rain streak removal method using directional gradient priors,” IEEE Transactions on Image Processing, vol. 28, no. 4, pp. 2089–2102, 2018.
[47] H. Zhang and V. M. Patel, “Convolutional sparse and low-rank codingbased rain streak removal,” in 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), 2017, pp. 1259–1267.
[48] J. Chen, C.-H. Tan, J. Hou, L.-P. Chau, and H. Li, “Robust video content alignment and compensation for rain removal in a cnn framework,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 6286–6295.
[49] J. Xu, W. Zhao, P. Liu, and X. Tang, “Removing rain and snow in a single image using guided ﬁlter,” in IEEE International Conference on Computer Science and Automation Engineering (CSAE), vol. 2, 2012, pp. 304–307.
[50] K. He, J. Sun, and X. Tang, “Guided image ﬁltering,” IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 6, pp. 1397– 1409, 2012.
[51] J. Xu, W. Zhao, P. Liu, and X. Tang, “An improved guidance image based method to remove rain and snow in a single image,” Computer and Information Science, vol. 5, no. 3, p. 49, 2012.
[52] X. Zheng, Y. Liao, W. Guo, X. Fu, and X. Ding, “Single-imagebased rain and snow removal using multi-guided ﬁlter,” in International Conference on Neural Information Processing, 2013, pp. 258–265.
[53] X. Ding, L. Chen, X. Zheng, Y. Huang, and D. Zeng, “Single image rain and snow removal via guided l0 smoothing ﬁlter,” Multimedia Tools and Applications, vol. 75, no. 5, pp. 2697–2712, 2016.
[54] J.-H. Kim, C. Lee, J.-Y. Sim, and C.-S. Kim, “Single-image deraining using an adaptive nonlocal means ﬁlter,” in IEEE International Conference on Image Processing, 2013, pp. 914–917.
[55] P. Mu, J. Chen, R. Liu, X. Fan, and Z. Luo, “Learning bilevel layer priors for single image rain streaks removal,” IEEE Signal Processing Letters, vol. 26, no. 2, pp. 307–311, 2019.
[56] J.-L. Gauvain and C.-H. Lee, “Maximum a posteriori estimation for multivariate gaussian mixture observations of markov chains,” IEEE Transactions on Speech and Audio Processing, vol. 2, no. 2, pp. 291– 298, 1994.
[57] K. Zhang, W. Zuo, S. Gu, and L. Zhang, “Learning deep cnn denoiser prior for image restoration,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 3929–3938.

[58] Y.-H. Fu, L.-W. Kang, C.-W. Lin, and C.-T. Hsu, “Single-frame-based rain removal via image decomposition,” in 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2011, pp. 1453–1456.
[59] D.-Y. Chen, C.-C. Chen, and L.-W. Kang, “Visual depth guided color image rain streaks removal using sparse coding,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 24, no. 8, pp. 1430– 1455, 2014.
[60] L.-W. Kang, C.-W. Lin, and Y.-H. Fu, “Automatic single-image-based rain streaks removal via image decomposition,” IEEE Transactions on Image Processing, vol. 21, no. 4, pp. 1742–1755, 2011.
[61] L.-W. Kang, C.-W. Lin, C.-T. Lin, and Y.-C. Lin, “Self-learning-based rain streak removal for image/video,” in IEEE International Symposium on Circuits and Systems, 2012, pp. 1871–1874.
[62] Y. Wang, S. Liu, C. Chen, and B. Zeng, “A hierarchical approach for rain or snow removing in a single color image,” IEEE Transactions on Image Processing, vol. 26, no. 8, pp. 3936–3950, 2017.
[63] S.-H. Sun, S.-P. Fan, and Y.-C. F. Wang, “Exploiting image structural similarity for single image rain removal,” in IEEE International Conference on Image Processing (ICIP), 2014, pp. 4482–4486.
[64] L. Zhu, C.-W. Fu, D. Lischinski, and P.-A. Heng, “Joint bi-layer optimization for single-image rain streak removal,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2526– 2534.
[65] D. Eigen, D. Krishnan, and R. Fergus, “Restoring an image taken through a window covered with dirt or rain,” in Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 633–640.
[66] R. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu, “Attentive generative adversarial network for raindrop removal from a single image,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 2482–2491.
[67] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.
[68] Z. Fan, H. Wu, X. Fu, Y. Hunag, and X. Ding, “Residual-guide feature fusion network for single image deraining,” arXiv preprint arXiv:1804.07493, 2018.
[69] H. Zhang, V. Sindagi, and V. M. Patel, “Image de-raining using a conditional generative adversarial network,” IEEE Transactions on Circuits and Systems for Video Technology, 2019.
[70] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated convolutions,” arXiv preprint arXiv:1511.07122, 2015.
[71] Y.-T. Wang, X.-L. Zhao, T.-X. Jiang, L.-J. Deng, Y. Chang, and T.-Z. Huang, “Rain streak removal for single image via kernel guided cnn,” arXiv preprint arXiv:1808.08545, 2018.
[72] G. Li, X. He, W. Zhang, H. Chang, L. Dong, and L. Lin, “Non-locally enhanced encoder-decoder network for single image de-raining,” arXiv preprint arXiv:1808.01491, 2018.
[73] Q. Huynh-Thu and M. Ghanbari, “Scope of validity of psnr in image/video quality assessment,” Electronics letters, vol. 44, no. 13, pp. 800–801, 2008.
[74] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli et al., “Image quality assessment: from error visibility to structural similarity,” IEEE transactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.
[75] H. R. Sheikh and A. C. Bovik, “Image information and visual quality,” IEEE Transactions on image processing, vol. 15, no. 2, pp. 430–444, 2006.
[76] L. Zhang, L. Zhang, X. Mou, and D. Zhang, “Fsim: A feature similarity index for image quality assessment,” IEEE transactions on Image Processing, vol. 20, no. 8, pp. 2378–2386, 2011.
[77] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in pytorch,” 2017.
[78] N. Goyette, P.-M. Jodoin, F. Porikli, J. Konrad, and P. Ishwar, “Changedetection. net: A new change detection benchmark dataset,” in IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2012, pp. 1–8.
[79] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics,” in IEEE International Conference on Computer Vision, 2002.
[80] R. Liu, S. Cheng, L. Ma, X. Fan, and Z. Luo, “Deep proximal unrolling: Algorithmic framework, convergence analysis and applications,” IEEE Transactions on Image Processing, 2019.
[81] X. Jin, Z. Chen, J. Lin, Z. Chen, and W. Zhou, “Unsupervised single image deraining with self-supervised constraints,” in IEEE International Conference on Image Processing (ICIP), 2019, pp. 2761–2765.

