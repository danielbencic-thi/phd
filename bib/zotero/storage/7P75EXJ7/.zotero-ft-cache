BAYESIAN OPTIMIZATION
ROMAN GARNETT

BAYESIAN OPTIMIZATION
ROMAN GARNETT
Cambridge University Press

CONTENTS

ix xiii

. Formalization of optimization . The Bayesian approach

. De nition and basic properties . Inference with exact and noisy observations . Overview of remainder of chapter . Joint Gaussian processes . Continuity . Di erentiability . Existence and uniqueness of global maxima . Inference with non-Gaussian observations and constraints . Summary of major ideas

. The prior mean function . The prior covariance function . Notable covariance functions . Modifying and combining covariance functions . Modeling functions on high-dimensional domains . Summary of major ideas

,

,

. Models and model structures

. Bayesian inference over parametric model spaces

. Model selection via posterior maximization

. Model averaging

. Multiple model structures

. Automating model structure search

. Summary of major ideas

. Introduction to Bayesian decision theory . Sequential decisions with a xed budget . Cost and approximation of the optimal policy . Cost-aware optimization and termination as a decision . Summary of major ideas

. Expected utility of terminal recommendation . Cumulative reward

This material will be published by Cambridge University Press as Bayesian Optimization.

v

This prepublication version is free to view and download for personal use only. Not for

redistribution, resale, or use in derivative works. Â©Roman Garnett .

. Information gain . Dependence on model of objective function . Comparison of utility functions . Summary of major ideas
. Example optimization scenario . Decision-theoretic policies . Expected improvement . Knowledge gradient . Probability of improvement . Mutual information and entropy search . Multi-armed bandits and optimization . Maximizing a statistical upper bound . Thompson sampling . Other ideas in policy construction . Summary of major ideas
. Notation for objective function model . Expected improvement . Probability of improvement . Upper con dence bound . Approximate computation for one-step lookahead . Knowledge gradient . Thompson sampling . Mutual information with ğ‘¥âˆ— . Mutual information with ğ‘“ âˆ— . Averaging over a space of Gaussian processes . Alternative models: Bayesian neural networks, etc. . Summary of major ideas
. Gaussian process inference, scaling, and approximation . Optimizing acquisition functions . Starting and stopping optimization . Summary of major ideas
. Regret . Useful function spaces for studying convergence . Relevant properties of covariance functions . Bayesian regret with observation noise . Worst-case regret with observation noise . The exact observation case . The e ect of unknown hyperparameters . Summary of major ideas
vi

. Unknown observation costs . Constrained optimization and unknown constraints . Synchronous batch observations . Asynchronous observation with pending experiments . Multi delity optimization . Multitask optimization . Multiobjective optimization . Gradient observations . Stochastic and robust optimization . Incremental optimization of sequential procedures . Non-Gaussian observation models and active search
. Historical precursors and optimal design . Sequential analysis and Bayesian experimental design . The rise of Bayesian optimization . Later rediscovery and development . Multi-armed bandits to in nite-armed bandits . Whatâ€™s next?

Draft as of January . Feedback welcome: https://bayesoptbook.com/

vii

PREFACE

My interest in Bayesian optimization began in at the start of my

doctoral studies. I was frustrated that there seemed to be a Bayesian

approach to every task I cared about, except optimization. Of course, as

was often the case at that time (not to mention now!), I was mistaken in

this belief, but one should never let ignorance impede inspiration.

Meanwhile, my labmate and soon-to-be frequent collaborator Mike

Osborne had a fresh copy of

and

â€™s Gaussian Pro-

cesses for Machine Learning and just would not stop talking about s at

our lab meetings. Through sheer brute force of repetition, I slowly built

a hand-wavy intuition for Gaussian processes â€“ my mental model was

the â€œsausage plotâ€ â€“ without even being sure about their precise de -

nition. However, I was pretty sure that marginals were Gaussian (what

else?), and one day it occurred to me that one could achieve Bayesian

optimization by maximizing the probability of improvement. This was

the algorithm I was looking for! In my excitement I shot o an email to

Mike that kicked o years of fruitful collaboration:

Can I ask a dumb question about s? Letâ€™s say that Iâ€™m doing function approximation on an interval with a . So Iâ€™ve got this mean function ğ‘š(ğ‘¥) and a variance function ğ‘£ (ğ‘¥). Is it true that if I pick a particular point ğ‘¥, then ğ‘ ğ‘“ (ğ‘¥) âˆ¼ N ğ‘š(ğ‘¥), ğ‘£ (ğ‘¥) ? Please say yes.
If this is true, then I think the idea of doing Bayesian optimization using s is, dare I say, trivial.

The hubris of youth!

Well, it turned out I was years too late in proposing this algo-

rithm, and that it only seemed â€œtrivialâ€ because I had no appreciation for

its theoretical foundation. However, truly great ideas are rediscovered

many times, and my excitement did not fade. Once I developed a deeper

understanding of Gaussian processes and Bayesian decision theory, I

came to see them as a â€œBayesian crankâ€ I could turn to realize adaptive

algorithms for any task. I have been repeatedly astonished to nd that

the resulting algorithms â€“ seemingly by magic â€“ automatically display

intuitive emergent behavior as a result of their careful design. My goal

with this book is to paint this grand picture. In e ect, it is a gift to my

former self: the book I wish I had in the early years of my career.

In the context of machine learning, Bayesian optimization is an

ancient idea â€“

â€™s paper appeared only three years after the

term â€œmachine learningâ€ was coined! Despite its advanced age, Bayesian

optimization has been enjoying a period of revitalization and rapid

progress over the past ten years. The primary driver of this renaissance

has been advances in computation, which have enabled increasingly

sophisticated tools for Bayesian modeling and inference.

Ironically, however, perhaps the most critical development was not

Bayesian at all, but the rise of deep neural networks, another old idea

This material will be published by Cambridge University Press as Bayesian Optimization. This prepublication version is free to view and download for personal use only. Not for redistribution, resale, or use in derivative works. Â©Roman Garnett .

The rst of many â€œsausage plotsâ€ to come.

..

( ). A Versatile Stochastic

Model of a Function of Unknown and Time

Varying Form. Journal of Mathematical Analy-

sis and Applications ( ): â€“ .

ix

.

et al. ( ). Practical Bayesian Op-

timization of Machine Learning Algorithms.

eur

.

intended audience prerequisites
chapters â€“ : modeling the objective function with Gaussian processes
chapters â€“ : sequential decision making and policy building
chapters â€“ : Bayesian optimization with Gaussian processes
x

granted new life by modern computation. The extreme cost of training

these models demands e cient routines for hyperparameter tuning, and

in a timely and in uential paper,

et al. demonstrated (dramati-

cally!) that Bayesian optimization was up to the task. Hyperparameter

tuning proved to be a â€œkiller appâ€ for Bayesian optimization, and the en-

suing surge of interest has yielded a mountain of publications developing

new algorithms and improving old ones, exploring countless variations

on the basic setup, establishing theoretical guarantees on performance,

and applying the framework to a huge range of domains.

Due to the nature of the computer science publication model, these

recent developments are scattered across dozens of brief papers, and the

pressure to establish novelty in a limited space can obscure the big picture

in favor of minute details. This book aims to provide a self-contained and

comprehensive introduction to Bayesian optimization, starting â€œfrom

scratchâ€ and carefully developing all the key ideas along the way. This

bottom-up approach allows us to identify unifying themes in Bayesian

optimization algorithms that may be lost when surveying the literature.

The intended audience is graduate students and researchers in ma-

chine learning, statistics, and related elds. However, it is also my sincere

hope that practitioners from more distant elds wishing to harness the

power of Bayesian optimization will also nd some utility here.

For the bulk of the text, I assume the reader is comfortable with di er-

ential and integral calculus, probability, and linear algebra. On occasion

the discussion will meander to more esoteric areas of mathematics, and

these passages can be safely ignored and returned to later if desired. A

good working knowledge of the Gaussian distribution is also essential,

and I provide an abbreviated but su cient introduction in appendix .

The book is divided into three main parts. Chapters â€“ cover theo-

retical and practical aspects of modeling with Gaussian processes. This

class of models is the overwhelming favorite in the Bayesian optimiza-

tion literature, and the material contained within is critical for several

following chapters. It was daunting to write this material in light of

the many excellent references already available, in particular the afore-

mentioned Gaussian Processes for Machine Learning. However, I heavily

biased the presentation in light of the needs of optimization, and even

experts may nd something new.

Chapters â€“ develop the theory of sequential decision making and

its application to optimization. Although this theory requires a model

of the objective function and our observations of it, the presentation is

agnostic to the choice of model and may be read independently from the

preceding chapters on Gaussian processes.

These threads are uni ed in chapters â€“ , which discuss the partic-

ulars of Bayesian optimization with Gaussian process models. Chapters

â€“ cover details of computation and implementation, and chapter

discusses theoretical performance bounds on Bayesian optimization al-

gorithms, where most results depend intimately on a Gaussian process

model of the objective function or the associated reproducing kernel

Hilbert space.

The nuances of some applications require modi cations to the basic

sequential optimization scheme that is the focus of the bulk of the book,

and chapter introduces several notable extensions to this basic setup.

Each is systematically presented through the unifying lens of Bayesian

decision theory to illustrate how one might proceed when facing a novel

situation.

Finally, chapter provides a brief and standalone history of Bayesian

optimization. This was perhaps the most fun chapter for me to write,

if only because it forced me to plod through old Soviet literature (in an

actual library! what a novelty these days!). To my surprise I was able to

antedate many Bayesian optimization policies beyond their commonly

attested origin, including expected improvement, knowledge gradient,

probability of improvement, and upper con dence bound. (A reader

familiar with the literature may be surprised to learn the last of these

was actually the rst policy discussed by

in his paper.)

Despite my best e orts, there may still be stones left to be overturned

before the complete history is revealed.

Dependencies between the main chapters are illustrated in the mar-

gin. There are two natural linearizations of the material. The rst is the

one I adopted and personally prefer, which covers modeling prior to

decision making. However, one could also proceed in the other order,

reading chapters â€“ rst, then looping back to chapter . After covering

the material in these chapters (in either order), the remainder of the book

can be perused at will. Logical partial paths through the book include:

â€¢ a minimal but self-contained introduction: chapters â€“ , â€“ â€¢ a shorter introduction requiring leaps of faith: chapters and â€¢ a crash course on the underlying theory: chapters â€“ , â€“ , â€¢ a head start on implementing a software package: chapters â€“

A reader already quite comfortable with Gaussian processes might wish to skip over chapters â€“ entirely.
I struggled for some time over whether to include a chapter on applications. On the one hand, Bayesian optimization ultimately owes its popularity to its success in optimizing a growing and diverse set of dif-
cult objectives. However, these applications often require extensive technical background to appreciate, and an adequate coverage would be tedious to write and tedious to read. As a compromise, I provide an annotated bibliography outlining the optimization challenges involved in notable domains of interest and pointing to studies where these challenges were successfully overcome with the aid of Bayesian optimization.
The sheer size of the Bayesian optimization literature â€“ especially the output of the previous decade â€“ makes it impossible to provide a complete survey of every recent development. This is especially true for the extensions discussed in chapter and even more so for the bibliography on applications, where work has proliferated in myriad branching directions. Instead I settled for presenting what I considered to be the most important ideas and providing pointers to entry points

Draft as of January . Feedback welcome: https://bayesoptbook.com/

chapter : extensions chapter : brief history of Bayesian optimization
A dependency graph for chapters â€“ . Chapter is a universal dependency. annotated bibliography of applications: appendix , p.
xi

Thank you!

for the relevant literature. The reader should not read anything into any omissions; there is simply too much high-quality work to go around.
Additional information about the book, including a list of errata as they are discovered, may be found at the companion webpage:

bayesoptbook.com

I encourage the reader to report any errata or other issues to the companion GitHub repository for discussion and resolution:

github.com/bayesoptbook/bayesoptbook.github.io

Preparation of this manuscript was facilitated tremendously by nu-

merous free and open source projects, and the creators, developers, and

maintainers of these projects have my sincere gratitude. The manuscript

was typeset in LATEX using the excellent and extremely exible memoir class. The typeface is Linux Libertine. Figures were laid out in

and converted to TikZ/ /

for further tweaking and typeset-

ting via the mat ab2tikz script. The colors used in gures were based

on www.co orbrewer.org by Cynthia A. Brewer, and I endeavored to the

best of my ability to ensure that the gures are colorblind friendly. The

colormap used in heat maps is a slight modi cation of the Matplotlib

viridis colormap where the â€œbrightâ€ end is pure white.

I would like to thank Eric Brochu, Nando de Freitas, Matt Ho man,

Frank Hutter, Mike Osborne, Bobak Shahriari, Jasper Snoek, Kevin Swer-

sky, and Ziyu Wang, who jointly provided the activation energy for this

undertaking. I would also like to thank Eytan Bakshy, Ivan Barrientos,

George De Ath, Peter Frazier, Lukas FrÃ¶hlich, Ashok Gautam, Jake Gard-

ner, Javier GonzÃ¡lez, Eugen Hotaj, Frank Hutter, Jungtaek Kim, Bryan

Low, Ruben Martinez-Cantin, Keita Mori, Kevin Murphy, Mike Osborne,

Matthias Poloczeck, Jon Scarlett, Bobak Shahriari, Jasper Snoek, Sebas-

tian Tay, Sattar Vakili, Qiuyi Zhang, and GitHub users cgob e001 and

chaos-and-patterns for their suggestions, corrections, and valuable

discussions along the way, as well as David Tranah and Anna Scriven

at Cambridge University Press for their support and patience as I con-

tinually missed deadlines. Special thanks are due to the students of two

seminars run at Washington University covering the material in this

book; their feedback was also instrumental in shaping the bookâ€™s content.

Funding support was provided by the United States National Science

Foundation ( ) under award number

. Any opinions, ndings,

and conclusions or recommendations expressed in this book are those

of the author and do not necessarily re ect the views of the .

This book took far more time than I initially anticipated, and I would

especially like to thank my wife Marion and son Max (arg Max?) for their

understanding and support during this long journey.

Roman Garnett St. Louis, Missouri January

xii

NOTATION

All vectors are column vectors and are denoted in lowercase bold: x âˆˆ â„ğ‘‘. Matrices are denoted in uppercase bold: A.
We adopt the â€œnumerator layoutâ€ convention for matrix calculus:
the derivative of a vector by a scalar is a (column) vector, whereas the
derivative of a scalar by a vector is a row vector. This results in the chain rule proceeding from left-to-right; for example, if a vector x(ğœƒ ) depends on a scalar parameter ğœƒ , then for a function ğ‘“ (x), we have:

ğœ•ğ‘“ ğœ•ğœƒ

=

ğœ•ğ‘“ ğœ•x

ğœ•x ğœ•ğœƒ

.

When an indicator function is required, we use the Iverson bracket notation. For a statement ğ‘ , we have:

[ğ‘ ] = 1 if ğ‘  is true; 0 otherwise.

The statement may depend on a parameter: [ğ‘¥ âˆˆ ğ´], [ğ‘¥ â‰¥ 0], etc. Logarithms are taken with respect to their natural base, ğ‘’. Quantities
in log units such as log likelihoods or entropy thus have units of nats, the base-ğ‘’ analogue of the more familiar base- bits.

vectors and matrices matrix calculus convention chain rule
indicator functions
logarithms nats

There is one notational innovation in this book compared with the Gaussian process and Bayesian optimization literature at large: we make heavy use of symbols for quantities that depend implicitly on a putative (and arbitrary) input location ğ‘¥. Most importantly, to refer to the value of an objective function ğ‘“ at a given location ğ‘¥, we introduce the symbol ğœ™ = ğ‘“ (ğ‘¥). This avoids clash with the name of the function itself, ğ‘“, while avoiding an extra layer of brackets. We use this scheme throughout the book, including variations such as:
ğœ™ = ğ‘“ (ğ‘¥ ); ğ“ = ğ‘“ (x); ğ›¾ = ğ‘”(ğ‘¥); etc.
To refer to the outcome of a (possibly inexact) measurement at ğ‘¥, we use the symbol ğ‘¦; the distribution of ğ‘¦ presumably depends on ğœ™.
We also allocate symbols to describe properties of the marginal predictive distributions for the objective function value ğœ™ and observed value ğ‘¦, all of which also have implicit dependence on ğ‘¥. These appear in the table below.

A list of important symbols appears on the following pages, arranged roughly in alphabetical order.

This material will be published by Cambridge University Press as Bayesian Optimization.

xiii

This prepublication version is free to view and download for personal use only. Not for

redistribution, resale, or use in derivative works. Â©Roman Garnett .

symbol
â‰¡ âˆ‡
â‰º
ğœ” âˆ¼ ğ‘ (ğœ”) ğ‘– Xğ‘–
|A| |x|
ğ‘“ Hğ¾ Aâˆ’1 x 0 A ğ›¼ (ğ‘¥; D) ğ›¼ğœ (ğ‘¥; D)
ğ›¼ğœâˆ— (D)
ğ›¼ ğ›¼ğ‘“ âˆ— ğ›¼ ğ›¼ ğ›¼ğ‘¥âˆ— ğ›¼ ğ›¼ ğ›½ ğ›½ (x; D) C ğ‘ (D) chol A corr[ğœ”,ğœ“ ] cov[ğœ”,ğœ“ ] D D, D1 Dğœ ğ· [ğ‘ ğ‘] Î”(ğ‘¥, ğ‘¦) ğ›¿ (ğœ” âˆ’ ğ‘) diag x ğ”¼, ğ”¼ğœ” ğœ€ ğ‘“ ğ‘“ |Y ğ‘“âˆ— ğ›¾ğœ
xiv

description
identical equality of functions; for a constant ğ‘, ğ‘“ â‰¡ ğ‘ is a constant function
gradient operator
termination option: the action of immediately terminating optimization either Pareto dominance or the LÃ¶wner order: for symmetric A, B, A â‰º B if and only if B âˆ’ A is positive de nite is sampled according to: ğœ” is a realization of a random variable with probability density ğ‘ (ğœ”) disjoint union of {Xğ‘– }: ğ‘– Xğ‘– = ğ‘– (ğ‘¥, ğ‘–) | ğ‘¥ âˆˆ Xğ‘– determinant of square matrix A Euclidean norm of vector x; |x âˆ’ y| is thus the Euclidean distance between vectors x and y norm of function ğ‘“ in reproducing kernel Hilbert space Hğ¾ inverse of square matrix A transpose of vector x vector or matrix of zeros
action space for a decision acquisition function evaluating ğ‘¥ given data D expected marginal gain in ğ‘¢ (D) after observing at ğ‘¥ then making ğœ âˆ’ 1 additional optimal
observations given the outcome value of D with horizon ğœ: expected marginal gain in ğ‘¢ (D) from ğœ additional optimal obser-
vations
expected improvement mutual information between ğ‘¦ and ğ‘“ âˆ—
knowledge gradient
probability of improvement mutual information between ğ‘¦ and ğ‘¥âˆ—
upper con dence bound Thompson sampling â€œacquisition function:â€ a draw ğ‘“ âˆ¼ ğ‘ (ğ‘“ | D)
con dence parameter in Gaussian process upper con dence bound policy batch acquisition function evaluating x given data D; may have modi ers analogous to ğ›¼ prior covariance matrix of observed values y: C = cov[y] cost of acquiring data D Cholesky decomposition of positive de nite matrix A: if ğš² = chol A, then A = ğš²ğš² correlation of random variables ğœ” and ğœ“ ; with a single argument, corr[ğœ”] = corr[ğœ”, ğœ”] covariance of random variables ğœ” and ğœ“ ; with a single argument, cov[ğœ”] = cov[ğœ”, ğœ”] set of observed data, D = (x, y) set of observed data after observing at ğ‘¥: D = D âˆª (ğ‘¥, ğ‘¦) = (x, y ) set of observed data after ğœ observations Kullbackâ€“Leibler divergence between distributions with probability densities ğ‘ and ğ‘ marginal gain in utility after acquiring observation (ğ‘¥, ğ‘¦): Î”(ğ‘¥, ğ‘¦) = ğ‘¢ (D ) âˆ’ ğ‘¢ (D) Dirac delta distribution on ğœ” with point mass at ğ‘ diagonal matrix with diagonal x expectation, expectation with respect to ğœ” measurement error associated with an observation at ğ‘¥: ğœ€ = ğ‘¦ âˆ’ ğœ™ objective function; ğ‘“ : X â†’ â„ the restriction of ğ‘“ onto the subdomain Y âŠ‚ X globally maximal value of the objective function: ğ‘“ âˆ— = max ğ‘“ information capacity of an observation process given ğœ iterations

symbol
GP (ğ‘“ ; ğœ‡, ğ¾) Hğ¾ Hğ¾ [ğµ] ğ» [ğœ”] ğ» [ğœ” | D] ğ¼ (ğœ”;ğœ“ ) ğ¼ (ğœ”;ğœ“ | D) I ğ¾ ğ¾D ğ¾ ğ¾ ğœ… â„“ ğœ† M m ğœ‡ ğœ‡D N (ğ“; ğ, ğšº) N O
Oâˆ— Î© ğ‘ ğ‘ Î¦(ğ‘§) ğœ™ ğœ™ (ğ‘§) Pr
â„ ğ‘…ğœ ğ‘…Â¯ğœ [ğµ] ğ‘Ÿğœ ğ‘ŸÂ¯ğœ [ğµ] P ğœŒ ğœŒğœ ğ‘ 2
ğšº
ğœ2 ğœğ‘›2 std [ğœ” ] T (ğœ™; ğœ‡, ğœ,2 ğœˆ) TN (ğœ™; ğœ‡, ğœ,2 ğ¼ )

description

Gaussian process on ğ‘“ with mean function ğœ‡ and covariance function ğ¾

reproducing kernel Hilbert space associated with kernel ğ¾

ball of radius ğµ in Hğ¾ : {ğ‘“ | ğ‘“ Hğ¾ â‰¤ ğµ} discrete or di erential entropy of random variable ğœ”

discrete or di erential of random variable ğœ” after conditioning on D

mutual information between random variables ğœ” and ğœ“

mutual information between random variables ğœ” and ğœ“ after conditioning on D

identity matrix

prior covariance function: ğ¾ = cov[ğ‘“ ]

posterior covariance function given data D: ğ¾D = cov[ğ‘“ | D]

MatÃ©rn covariance function

squared exponential covariance function

cross covariance between ğ‘“ and observed values y: ğœ… (ğ‘¥) = cov[y, ğœ™ | ğ‘¥]

either a length-scale parameter or the lookahead horizon

output-scale parameter

space of models indexed by the hyperparameter vector ğœ½

prior expected value of observed values y, m = ğ”¼[y]

either the prior mean function, ğœ‡ = ğ”¼[ğ‘“ ], or the predictive mean of ğœ™: ğœ‡ = ğ”¼[ğœ™ | ğ‘¥, D] = ğœ‡D (ğ‘¥) posterior mean function given data D: ğœ‡D = ğ”¼[ğ‘“ | D]

multivariate normal distribution on ğ“ with mean vector ğ and covariance matrix ğšº

measurement error covariance corresponding to observed values y

is asymptotically bounded above by: for nonnegative functions ğ‘“, ğ‘” of ğœ, ğ‘“ = O(ğ‘”) if ğ‘“ /ğ‘” is

asymptotically bounded by a constant as ğœ â†’ âˆ

as above with logarithmic factors suppressed: ğ‘“ = Oâˆ— (ğ‘”) if ğ‘“ (ğœ) (log ğœ)ğ‘˜ = O(ğ‘”) for some ğ‘˜

is asymptotically bounded below by: ğ‘“ = Î©(ğ‘”) if ğ‘” = O(ğ‘“ )

probability density

either an standard

approximation to probability density ğ‘ or normal cumulative density function: Î¦(ğ‘§)

a =

qâˆ«uâˆ’ğ‘§aâˆnğœ™ti(lğ‘§e

function ) dğ‘§

value of the objective function at ğ‘¥: ğœ™ = ğ‘“ (ğ‘¥) standard normal probability density function:

ğœ™

(ğ‘§)

=

âˆš ( 2ğœ‹

)âˆ’1

exp(âˆ’

1 2

ğ‘§2

)

probability

set of real numbers

cumulative regret after ğœ iterations

worst-case cumulative regret after ğœ iterations on the simple regret after ğœ iterations

ball Hğ¾ [ğµ]

worst-case simple regret after ğœ iterations on the

ball Hğ¾ [ğµ]

a correlation matrix

a scalar correlation

instantaneous regret on iteration ğœ

predictive variance of ğ‘¦; for additive Gaussian noise, ğ‘ 2 = var[ğ‘¦ | ğ‘¥, D] = ğœ2 + ğœğ‘›2 a covariance matrix, usually the Gram matrix associated with x: ğšº = ğ¾D (x, x) predictive variance of ğœ™: ğœ2 = ğ¾D (ğ‘¥, ğ‘¥) variance of measurement error at ğ‘¥: ğœğ‘›2 = var[ğœ€ | ğ‘¥] standard deviation of random variable ğœ”

Student-ğ‘¡ distribution on ğœ™ with ğœˆ degrees of freedom, mean ğœ‡, and variance ğœ2

truncated normal distribution, N (ğœ™; ğœ‡, ğœ2) truncated to interval ğ¼

Draft as of January . Feedback welcome: https://bayesoptbook.com/

xv

symbol
ğœ
Î˜ ğœ½ tr A ğ‘¢ (D) var [ğœ” ] ğ‘¥ x
ğ‘¥âˆ— X ğ‘¦ y ğ‘§

description
either decision horizon (in the context of decision making) or number of optimization iterations passed (in the context of asymptotic analysis) is asymptotically bounded above and below by: ğ‘“ = Î˜(ğ‘”) if ğ‘“ = O(ğ‘”) and ğ‘“ = Î©(ğ‘”) vector of hyperparameters indexing a model space M trace of square matrix A utility of data D variance of random variable ğœ” putative input location of the objective function either a sequence of observed locations x = {ğ‘¥ğ‘– } or (when the distinction is important) a vector-valued input location a location attaining the globally maximal value of ğ‘“ : ğ‘¥âˆ— âˆˆ arg max ğ‘“ ; ğ‘“ (ğ‘¥âˆ—) = ğ‘“ âˆ— domain of objective function value resulting from an observation at ğ‘¥ observed values resulting from observations at locations x ğ‘§-score of measurement ğ‘¦ at ğ‘¥: ğ‘§ = (ğ‘¦ âˆ’ ğœ‡)/ğ‘ 

xvi

INTRODUCTION

Optimization is an innate human behavior. On an individual level, we

all strive to better ourselves and our surroundings. On a collective level,

societies struggle to allocate limited resources seeking to improve the

welfare of their members, and optimization has been an engine of societal

progress since the domestication of crops through selective breeding

over

years ago â€“ an e ort that continues to this day.

Given its pervasiveness, it should perhaps not be surprising that

optimization is also di cult. While searching for an optimal design,

we must spend â€“ sometimes quite signi cant â€“ resources evaluating

suboptimal alternatives along the way. This observation compels us to

seek methods of optimization that, when necessary, can carefully allocate

resources to identify optimal parameters as e ciently as possible. This

is the goal of mathematical optimization.

Since the s, the statistics and machine learning communities

have re ned a Bayesian approach to optimization that we will develop

and explore in this book. Bayesian optimization routines rely on a statis-

tical model of the objective function, whose beliefs guide the algorithm

in making the most fruitful decisions. These models can be quite so-

phisticated, and maintaining them throughout optimization may entail

signi cant cost of its own. However, the reward for this e ort is unparal-

leled sample e ciency. For this reason, Bayesian optimization has found

a niche in optimizing objectives that:

â€¢ are costly to compute, precluding exhaustive evaluation, â€¢ lack a useful expression, causing them to function as â€œblack boxes,â€ â€¢ cannot be evaluated exactly, but only through some indirect or noisy
mechanism, and/or â€¢ o er no e cient mechanism for estimating their gradient.

Let us consider an example setting motivating the machine learning communityâ€™s recent interest in Bayesian optimization. Consider a data scientist crafting a complex machine learning model â€“ say a deep neural network â€“ from training data. To ensure success, the scientist must carefully tune the modelâ€™s hyperparameters, including the network architecture and details of the training procedure, which have massive in uence on performance. Unfortunately, e ective settings can only be identi ed via trial-and-error: by training several networks with di erent settings and evaluating their performance on a validation dataset.
The search for the best hyperparameters is of course an exercise in optimization. Mathematical optimization has been under continual development for centuries, and numerous o -the-shelf procedures are available. However, these procedures usually make assumptions about the objective function that may not always be valid. For example, we might assume that the objective is cheap to evaluate, that we can easily compute its gradient, or that it is convex, allowing us to reduce from global to local optimization.

This material will be published by Cambridge University Press as Bayesian Optimization. This prepublication version is free to view and download for personal use only. Not for redistribution, resale, or use in derivative works. Â©Roman Garnett .

.

et al. ( ). Bayesian Optimization

is Superior to Random Search for Machine

Learning Hyperparameter Tuning: Analysis of

the Black-Box Optimization Challenge .

Proceedings of the Neur

Competition

and Demonstration Track.

.

and .

( ). What are

the most important statistical ideas of the past

years? Journal of the American Statistical

Association.

annotated bibliography of applications: appendix , p.

outline and reading guide: p. x

In hyperparameter tuning, all of these assumptions are invalid. Train-

ing a deep neural network can be extremely expensive in terms of both

time and energy. When some hyperparameters are discrete â€“ as many

features of network architecture naturally are â€“ the gradient does not

even exist. Finally, the mapping from hyperparameters to performance

may be highly complex and multimodal, so local re nement may not

yield an acceptable result.

The Bayesian approach to optimization allows us to relax all of these

assumptions when necessary, and Bayesian optimization algorithms can

deliver impressive performance even when optimizing complex â€œblack

boxâ€ objectives under severely limited observation budgets. Bayesian

optimization has proven successful in settings spanning science, engi-

neering, and beyond, including of course hyperparameter tuning. In

light of this broad success,

and

identi ed adaptive

decision analysis â€“ and Bayesian optimization in particular â€“ as one of

the eight most important statistical ideas of the past years.

Covering all these applications and their nuances could easily ll a

separate volume (although we do provide an overview of some impor-

tant application domains in an annotated bibliography), so in this book

we will settle for developing the mathematical foundation of Bayesian

optimization underlying its success. In the remainder of this chapter we

will lay important groundwork for this discussion. We will rst establish

the precise formulation of optimization we will consider and important

conventions of our presentation, then outline and illustrate the key as-

pects of the Bayesian approach. The reader may nd an outline of and

reading guide for the chapters to come in the preface.

objective function, ğ‘“ domain of objective function, X ğ‘“âˆ—
ğ‘¥âˆ— An objective function with the location, ğ‘¥,âˆ— and value, ğ‘“,âˆ— of the global optimum marked.
A skeptical reader may object that, without further assumptions, a global maximum may not exist at all! We will sidestep this issue for now and pick it up again in Â§ . , p. .

.

Throughout this book we will consider a simple but exible formulation of sequential global optimization outlined below. There is nothing inherently Bayesian about this model, and countless solutions are possible.
We begin with a real-valued objective function de ned on some domain X ; ğ‘“ : X â†’ â„. We make no assumptions regarding the nature of the domain. In particular, it need not be Euclidean but might instead, for example, comprise a space of complex structured objects. The goal of optimization is to systematically search the domain for a point ğ‘¥âˆ— attaining the globally maximal value ğ‘“ âˆ—:

ğ‘¥âˆ— âˆˆ arg max ğ‘“ (ğ‘¥);

ğ‘“ âˆ— = max ğ‘“ (ğ‘¥) = ğ‘“ (ğ‘¥âˆ—).

(.)

ğ‘¥ âˆˆX

ğ‘¥ âˆˆX

Before we proceed, we note that our focus on maximization rather than minimization is entirely arbitrary; the author simply judges maximization to be the more optimistic choice. If desired, we can freely transform one problem to the other by negating the objective function. We caution the reader that some translation may be required when comparing expressions derived here to what may appear in parallel texts focusing on minimization.

input: initial dataset D

can be empty

repeat

ğ‘¥â†

(D)

select the next observation location

ğ‘¦â†

(ğ‘¥ )

observe at the chosen location

D â† D âˆª (ğ‘¥, ğ‘¦)

update dataset

until termination condition reached

e.g., budget exhausted

return D

.. Algorithm . : Sequential optimization.

In a signi cant departure from classical mathematical optimization, we do not require that the objective function have a known functional form or even be computable directly. Rather, we only require access to a mechanism revealing some information about the objective function at identi ed points on demand. By amassing su cient information from this mechanism, we may hope to infer the solution to ( . ). Avoiding the need for an explicit expression for ğ‘“ allows us to consider so-called â€œblack boxâ€ optimization, where a system is optimized through indirect measurements of its quality. This is one of the greatest strengths of Bayesian optimization.
Optimization policy
Directly solving for the location of global optima is infeasible except in exceptional circumstances. The tools of traditional calculus are virtually powerless in this setting; for example, enumerating and classifying every stationary point in the domain would be tedious at best and perhaps even impossible. Mathematical optimization instead takes an indirect approach: we design a sequence of experiments to probe the objective function for information that, we hope, will reveal the solution to ( . ).
The iterative procedure in algorithm . formalizes this process. We begin with an initial (possibly empty) dataset D that we grow incrementally through a sequence of observations of our design. In each iteration, an optimization policy inspects the available data and selects a point ğ‘¥ âˆˆ X where we make our next observation. This action in turn reveals a corresponding value ğ‘¦ provided by the system under study. We append the newly observed information to our dataset and nally decide whether to continue with another observation or terminate and return the current data. When we inevitably do choose to terminate, the returned data can be used by an external consumer as desired, for example to inform a subsequent decision.
We place no restrictions on how an optimization policy is implemented beyond mapping an arbitrary dataset to some point in the domain for evaluation. A policy may be deterministic or stochastic, as demonstrated respectively by the prototypical examples of grid search and random search. In fact, these popular policies are nonadaptive and completely ignore the observed data. However, when observations only come at signi cant cost, we will naturally prefer policies that adapt their behavior in light of evolving information. The primary challenge in opti-
Draft as of January . Feedback welcome: https://bayesoptbook.com/

Of course, we do not require but merely allow that the objective function act as a black box. Access to a closed-form expression does not preclude a Bayesian approach!
Here â€œpolicyâ€ has the same meaning as in other decision-making contexts: it maps our state (indexed by our data, D) to an action (the location of our next observation, ğ‘¥).
terminal recommendations: Â§ . , p.

Inexact observations of an objective function corrupted by additive noise.
measured value, ğ‘¦ observation location, ğ‘¥ objective function value, ğœ™ = ğ‘“ (ğ‘¥)
conditional independence of observations given objective values
ğ‘(ğ‘¦ | ğ‘¥,ğœ™)
ğ‘¦ ğœ™ Additive Gaussian noise: the distribution of the value ğ‘¦ observed at ğ‘¥ is Gaussian, centered on the objective function value ğœ™.
observation noise scale, ğœğ‘› heteroskedastic noise: Â§ . , p.

mization is designing policies that can rapidly optimize a broad class of objective functions, and intelligent policy design will be our focus for the majority of this book.

Observation model

For optimization to be feasible, the observations we obtain must provide

information about the objective function that can guide our search and in

aggregate determine the solution to ( . ). A near-universal assumption in

mathematical optimization is that observations yield exact evaluations of

the objective function at our chosen locations. However, this assumption

is unduly restrictive: many settings feature inexact measurements due

to noisy sensors, imperfect simulation, or statistical approximation. A

typical example featuring additive observation noise is shown in the

margin. Although the objective function is not observed directly, the

noisy measurements nonetheless constrain the plausible options due to

strong dependence on the objective.

We thus relax the assumption of exact observation and instead as-

sume that observations are realized by a stochastic mechanism depending

on the objective function. Namely, we assume that the value ğ‘¦ resulting

from an observation at some point ğ‘¥ is distributed according to an ob-

servation model depending on the underlying objective function value

ğœ™ = ğ‘“ (ğ‘¥):

ğ‘ (ğ‘¦ | ğ‘¥, ğœ™).

(. )

Through judicious design of the observation model, we may consider a wide range of observation mechanisms.
As with the optimization policy, we do not make any assumptions about the nature of the observation model, save one. Unless otherwise mentioned, we assume that a set of multiple measurements y are conditionally independent given the corresponding observation locations x and objective function values ğ“ = ğ‘“ (x):

ğ‘ (y | x, ğ“) = ğ‘ (ğ‘¦ğ‘– | ğ‘¥ğ‘–, ğœ™ğ‘– ).

(. )

ğ‘–

This is not strictly necessary but is overwhelmingly common in practice and will simplify our presentation considerably.
One particular observation model will enjoy most of our attention in this book: additive Gaussian noise. Here we model the value ğ‘¦ observed at ğ‘¥ as
ğ‘¦ = ğœ™ + ğœ€,

where ğœ€ represents measurement error. Errors are assumed to be Gaussian distributed with mean zero, implying a Gaussian observation model:

ğ‘ (ğ‘¦ | ğ‘¥, ğœ™, ğœğ‘›) = N (ğ‘¦; ğœ™, ğœğ‘›2).

(. )

Here the observation noise scale ğœğ‘› may optionally depend on ğ‘¥, allowing us to model both homoskedastic or heteroskedastic errors.

If we take the noise scale to be identically zero, we recover the special case of exact observation, where we simply have ğ‘¦ = ğœ™ and the observation model collapses to a Dirac delta distribution:
ğ‘ (ğ‘¦ | ğœ™) = ğ›¿ (ğ‘¦ âˆ’ ğœ™).
Although not universally applicable, many settings do feature exact observations such as optimizing the output of a deterministic computer simulation. We will sometimes consider the exact case separately as some results simplify considerably in the absence of measurement error.
We will focus on additive Gaussian noise as it is a reasonably faithful model for many systems and o ers considerable mathematical convenience. This observation model will be most prevalent in our discussion on Gaussian processes in the next three chapters and on the explicit computation of Bayesian optimization policies with this model class in chapter . However, the general methodology we will build in the remainder of this book is not contingent on this choice, and we will occasionally address alternative observation mechanisms.
Termination
The nal decision we make in each iteration of optimization is whether to terminate immediately or continue with another observation. As with the optimization policy, we do not assume any particular mechanism by which this decision is made. Termination may be deterministic â€“ such as stopping after reaching a certain optimization goal or exhausting a preallocated observation budget â€“ or stochastic, and may optionally depend on the observed data. In many cases, the time of termination may in fact not be under the control of the optimization routine at all but instead decided by an external agent. However, we will also consider scenarios where the optimization procedure can dynamically choose when to return based upon inspection of the available data.
.
Bayesian optimization does not refer to one particular algorithm but rather to a philosophical approach to optimization grounded in Bayesian inference from which an extensive family of algorithms have been derived. Although these algorithms display signi cant diversity in their details, they are bound by common themes in their design.
Optimization is fundamentally a sequence of decisions: in each iteration, we must choose where to make our next observation and then whether to terminate depending on the outcome. As the outcomes of these decisions are governed by the system under study and outside our control, the success of optimization rests entirely on e ective decision making.
Increasing the di culty of these decisions is that they must be made under uncertainty, as it is impossible to know the outcome of an observation before making it. The optimization policy must therefore design each
Draft as of January . Feedback welcome: https://bayesoptbook.com/

..
ğ‘(ğ‘¦ | ğœ™)
ğ‘¦ ğœ™ Exact observations: every value measured equals the corresponding function value, yielding a Dirac delta observation model.
inference with non-Gaussian observations: Â§ . , p. optimization with non-Gaussian observations: Â§ . , p.
optimal termination: Â§ . , p. practical termination: Â§ . , p.

The literature is vast. The following references are excellent, but no list can be complete:

.. .

( ). Information Theory, In-

ference, and Learning Algorithms. Cambridge

University Press.

.â€™

and .

( ). Kendallâ€™s

Advanced Theory of Statistics. Vol. : Bayes-

ian Inference. Arnold.

..

( ). Statistical Decision Theory

and Bayesian Analysis. Springerâ€“Verlag.

prior distribution, ğ‘ (ğœ™ | ğ‘¥)
Here we assume the location of interest ğ‘¥ is known, hence our conditioning the prior on its value.

observation with some measure of faith that the outcome will ultimately prove bene cial and justify the cost of obtaining it. The sequential nature of optimization further compounds the weight of this uncertainty, as the outcome of each observation not only has an immediate impact, but also forms the basis on which all future decisions are made. Developing an e ective policy requires somehow addressing this uncertainty.
The Bayesian approach systematically relies on probability and Bayesian inference to reason about the uncertain quantities arising during optimization. This critically includes the objective function itself, which is treated as a random variable to be inferred in light of our prior expectations and any available data. In Bayesian optimization, this belief then takes an active role in decision making by guiding the optimization policy, which may evaluate the merit of a proposed observation location according to our belief about the value we might observe. We introduce the key ideas of this process with examples below, starting with a refresher on Bayesian inference.
Bayesian inference
To frame the following discussion, we o er a quick overview of Bayesian inference as a reminder to the reader. This introduction is far from complete, but there are numerous excellent references available.
Bayesian inference is a framework for inferring uncertain features of a system of interest from observations grounded in the laws of probability. To illustrate the basic ideas, we may begin by identifying some unknown feature of a given system that we wish to reason about. In the context of optimization, this might represent, for example, the value of the objective function at a given location, or the location ğ‘¥âˆ— or value ğ‘“ âˆ— of the global optimum ( . ). We will take the rst of these as a running example: inferring about the value of an objective function at some arbitrary point ğ‘¥, ğœ™ = ğ‘“ (ğ‘¥). We will shortly extend this example to inference about the entire objective function.
In the Bayesian approach to inference, all unknown quantities are treated as random variables. This is a powerful convention as it allows us to represent beliefs about these quantities with probability distributions re ecting their plausible values. Inference then takes the form of an inductive process where these beliefs are iteratively re ned in light of observed data by appealing to probabilistic identities.
As with any induction, we must start somewhere. Here we begin with a so-called prior distribution (or simply prior) ğ‘ (ğœ™ | ğ‘¥), which encodes what we consider to be plausible values for ğœ™ before observing any data. The prior distribution allows us to inject our knowledge about and experience with the system of interest into the inferential process, saving us from having to begin â€œfrom scratchâ€ or entertain patently absurd possibilities. The left panel of gure . illustrates a prior distribution for our example, indicating support over a range of values.
Once a prior has been established, the next stage of inference is to re ne our initial beliefs in light of observed data. Suppose in our

prior, ğ‘ (ğœ™ | ğ‘¥)

likelihood, ğ‘ (ğ‘¦ | ğ‘¥, ğœ™)

.. posterior, ğ‘ (ğœ™ | D)

ğœ™

ğœ™

ğœ™

ğ‘¦

ğ‘¦

Figure . : Bayesian inference for an unknown function value ğœ™ = ğ‘“ (ğ‘¥). Left: a prior distribution over ğœ™; middle: the likelihood of the marked observation ğ‘¦ according to an additive Gaussian noise observation model ( . ) (prior shown for reference); right: the posterior distribution in light of the observation and the prior (prior and likelihood shown for reference).

example we make an observation of the objective function at ğ‘¥, revealing a measurement ğ‘¦. In our model of optimization, the distribution of this measurement is assumed to be determined by the value of interest ğœ™ through the observation model ğ‘ (ğ‘¦ | ğ‘¥, ğœ™) ( . ). In the context of Bayesian inference, a distribution explaining the observed values (here ğ‘¦) in terms of the values of interest (here ğœ™) is known as a likelihood function or simply a likelihood. The middle panel of gure . show the likelihood â€“ as a function of ğœ™ â€“ for a given measurement ğ‘¦, here assumed to be generated by additive Gaussian noise ( . ).
Finally, given the observed value ğ‘¦, we may derive the updated posterior distribution (or simply posterior) of ğœ™ by appealing to Bayesâ€™ theorem:

ğ‘ (ğœ™

|

ğ‘¥,ğ‘¦)

=

ğ‘ (ğœ™

| ğ‘¥) ğ‘ (ğ‘¦ | ğ‘ (ğ‘¦ | ğ‘¥)

ğ‘¥, ğœ™)

.

(. )

The posterior is proportional to the prior weighted by the likelihood of

the observed value. The denominator is a constant with respect to ğœ™ that

ensures normalization: âˆ«

ğ‘ (ğ‘¦ | ğ‘¥) = ğ‘ (ğ‘¦ | ğ‘¥, ğœ™) ğ‘ (ğœ™ | ğ‘¥) dğœ™ .

(. )

The right panel of gure . shows the posterior resulting from the measurement in the middle panel. The posterior represents a compromise between our experience (encoded in the prior) and the information contained in the data (encoded in the likelihood).
Throughout this book we will use the catchall notation D to represent all the information in uencing a posterior belief; here the relevant information is D = (ğ‘¥, ğ‘¦), and the posterior distribution is then ğ‘ (ğœ™ | D).
As mentioned previously, Bayesian inference is an inductive process whereby we can continue to re ne our beliefs through additional observation. At this point, the induction is trivial: to incorporate a new

Draft as of January . Feedback welcome: https://bayesoptbook.com/

likelihood function (observation model), ğ‘(ğ‘¦ | ğ‘¥,ğœ™) posterior distribution, ğ‘ (ğœ™ | ğ‘¥, ğ‘¦)
data informing posterior belief, D

posterior predictive, ğ‘ (ğ‘¦ | ğ‘¥, D)
ğ‘¦ ğ‘¦ Posterior predictive distribution for a repeated measurement at ğ‘¥ for our running example. The location of our rst measurement ğ‘¦ and the posterior distribution of ğœ™ are shown for reference. There is more uncertainty in ğ‘¦ than ğœ™ due to the e ect of observation noise.
This expression takes the same form as ( . ), which is simply the (prior) predictive distribution evaluated at the actual observed value.

observation, what was our posterior serves as the prior in the context

of the new information, and multiplying by the likelihood and renor-

malizing yields a new posterior. We may continue in this manner as

desired.

The posterior distribution is not usually the end result of Bayesian

inference but rather a springboard enabling follow-on tasks such as

prediction or decision making, both of which are integral to Bayesian

optimization. To address the former, suppose that after deriving the

posterior ( . ), we wish to predict the result of an independent, repeated

noisy observation at ğ‘¥, ğ‘¦. Treating the outcome as a random variable,

we may derive its distribution by integrating our posterior belief about

ğœ™ against the observation model ( . ): âˆ«

ğ‘ (ğ‘¦ | ğ‘¥, D) = ğ‘ (ğ‘¦ | ğ‘¥, ğœ™) ğ‘ (ğœ™ | ğ‘¥, D) dğœ™;

(. )

this is known as the posterior predictive distribution for ğ‘¦. By integrating over all possible values of ğœ™ weighted by their plausibility, the posterior predictive distribution naturally accounts for uncertainty in the unknown objective function value; see the gure in the margin.
The Bayesian approach to decision making also relies on a posterior belief about unknown features a ecting the outcomes of our decisions, as we will discuss shortly.

stochastic process
objective function prior, ğ‘ (ğ‘“ )
chapter : Gaussian processes, p. chapter : modeling with Gaussian processes,
p. chapter : model assessment, selection, and
averaging, p.
di erentiability: Â§ . , p. characteristic length scales: Â§ . , p.
stationarity: Â§ . , p.

Bayesian inference of the objective function

At the heart of any Bayesian optimization routine is a probabilistic belief

over the objective function. This takes the form of a stochastic process, a

probability distribution over an in nite collection of random variables â€“

here the objective function value at every point. The reasoning behind

this inference is, in essence, the same as our single-point example above.

We begin by encoding any assumptions we may have about the ob-

jective function, such as smoothness or other features, in a prior process

ğ‘ (ğ‘“ ). Conveniently, we can specify a stochastic process via the distribu-

tion of the function values ğ“ corresponding to an arbitrary nite set of

locations x:

ğ‘ (ğ“ | x).

(. )

The family of Gaussian processes â€“ where these nite-dimensional distributions are multivariate Gaussian â€“ is especially convenient and widely used in Bayesian optimization. We will explore this model class in depth in the following three chapters; here we provide a motivating illustration.
Figure . shows a Gaussian process prior on a one-dimensional objective function, constructed to re ect a minimal set of assumptions we will elaborate on later in the book:

â€¢ that the objective function is smooth (that is, in nitely di erentiable),
â€¢ that correlations among function values have a characteristic scale, and
â€¢ that the functionâ€™s expected behavior does not depend on location (that is, the prior process is stationary).

prior mean

prior % credible interval

.. samples

Figure . : An example prior process for an objective de ned on an interval. We illustrate the marginal belief at every location with its mean and a % credible interval and also show three example functions sampled from the prior process.

We summarize the marginal belief of the model, for each point in the domain showing the prior mean (dark blue) and % credible interval (light blue) for the corresponding function value. We also show three functions sampled from the prior process, each exhibiting the assumed behavior. We encourage the reader to become comfortable with this plotting convention, as we will use it throughout this book. In particular we eschew axis labels, as they are always the same: the horizontal axis represents the domain X and the vertical axis the function value. Further, we do not mark units on axes to stress relative rather than absolute behavior, as scale is arbitrary in this illustration.
We can encode a vast array of information into the prior process and can model signi cantly more complex structure than in this simple example. We will explore the world of possibilities in chapter , including interaction at di erent scales, nonstationarity, low intrinsic dimensionality, and more.
With the prior process in hand, suppose we now make a set of observations at some locations x, revealing corresponding values y; we aggregate this information into a dataset D = (x, y). Bayesian inference accounts for these observations by forming the posterior process ğ‘ (ğ‘“ | D).
The derivation of the posterior process can be understood as a twostage process. First we consider the impact of the data on the corresponding function values ğ“ alone ( . ):

ğ‘ (ğ“ | D) âˆ ğ‘ (ğ“ | x) ğ‘ (y | x, ğ“).

(. )

The quantities on the right-hand side are known: the rst term is given

by the prior process ( . ), and the second by the observation model ( . ),

which serves the role of a likelihood. We now extend the posterior on ğ“

to all of ğ‘“ :

âˆ«

ğ‘ (ğ‘“ | D) = ğ‘ (ğ‘“ | x, ğ“) ğ‘ (ğ“ | D) dğ“.

(. )

The posterior encapsulates our belief regarding the objective in light of the data, incorporating both the assumptions of the prior process and the information contained in the observations.
We illustrate an example posterior in gure . , where we have conditioned our prior from gure . on three exact observations. As the

Draft as of January . Feedback welcome: https://bayesoptbook.com/

plotting conventions
nonstationarity, warping: Â§ . , p. low intrinsic dimensionality: Â§ . , p.
observed data, D = (x, y) objective function posterior, ğ‘ (ğ‘“ | D)
The given expression sweeps some details under the rug. A careful derivation of the posterior process proceeds by nding the posterior of an arbitrary nite-dimensional vector ğ“âˆ— = ğ‘“ (xâˆ—):
ğ‘ (ğ“âˆ— | xâˆ—, D) = âˆ« ğ‘ (ğ“âˆ— | xâˆ—, x, ğ“) ğ‘ (ğ“ | D) dğ“,
which speci es the process. The distributions on the right-hand side are known: the posterior on ğ“ is in ( . ), and the posterior on ğ“âˆ— given the exact function values ğ“ can be found by computing their joint prior ( . ) and conditioning.

observations

posterior mean

posterior % credible interval

samples

Figure . : The posterior process for our example scenario in gure . conditioned on three exact observations.

acquisition function

next observation location

Figure . : A prototypical acquisition function corresponding to our example posterior from gure . .

chapter : decision theory for optimization, p.
chapter : utility functions for optimization, p.
chapter : common Bayesian optimization policies, p.

observations are assumed to be exact, the objective function posterior collapses onto the observed values. The posterior mean interpolates through the data, and the posterior credible intervals re ect increased certainty regarding the function near the observed locations. Further, the posterior continues to re ect the structural assumptions encoded in the prior, demonstrated by comparing the behavior of the samples drawn from the posterior process to those drawn from the prior.
Uncertainty-aware optimization policies
Bayesian inference provides an elegant means of reasoning about an uncertain objective function, but the success of optimization is measured not by the delity of our beliefs but by the outcomes of our actions. These actions are determined by the optimization policy, which examines available data to design each successive observation location. Each of these decisions is fraught with uncertainty, as we must commit to each observation before knowing its result, which will form the context of all following decisions. Bayesian inference enables us to express this uncertainty, but e ective decision making additionally requires us to establish preferences over outcomes and act to maximize those preferences.
To proceed we need to establish a framework for decision making under uncertainty, an expansive subject with a world of possibilities. A natural and common choice is Bayesian decision theory, the subject of chapters â€“ . We will discuss this and other approaches to policy construction at length in chapter and derive popular optimization policies from rst principles.
Ignoring details in policy design, a thread running through all Bayesian optimization policies is a uniform handling of uncertainty in the objective function and the outcomes of observations via Bayesian infer-

ence. Instrumental in connecting our beliefs about the objective function to decision making is the posterior predictive distribution ( . ), representing our belief about the outcomes of proposed observations. Bayesian optimization policies are designed with reference to this distribution, which guides the policy in discriminating between potential actions.
In practice, Bayesian optimization policies are de ned indirectly by optimizing a so-called acquisition function assigning a score to potential observation locations commensurate with their perceived ability to bene t the optimization process. Acquisition functions tend to be cheap to evaluate with analytically tractable gradients, allowing the use of o the-shelf optimizers to e ciently design each observation. Numerous acquisition functions have been proposed for Bayesian optimization, each derived from di erent considerations. However, all notable acquisition functions address the classic tension between exploitation â€“ sampling where the objective function is expected to be high â€“ and exploration â€“ sampling where we are uncertain about the objective function to inform future decisions. These opposing concerns must be carefully balanced for e ective global optimization.
An example acquisition function is shown in gure . , corresponding to the posterior from gure . . Consideration of the exploitationâ€“ exploration tradeo is apparent: this example acquisition function attains relatively large values both near local maxima of the posterior mean and in regions with signi cant marginal uncertainty. Local maxima of the acquisition function represent optimal compromises between these concerns. Note that the acquisition function vanishes at the location of the current observations: the objective function values at these locations are already known, so observing there would be pointless. Maximizing the acquisition function determines the policy; here the policy chooses to search around the local optimum on the right-hand side.
Figure . demonstrates an entire session of Bayesian optimization, beginning from the belief and initial decision from gure . and progressing iteratively following algorithm . . The red function shows the true (unknown) objective function, whose maximum is near the center of the domain. The running marks below each posterior show the locations of each measurement made, progressing in sequence from top to bottom, and we show the objective function posterior at four waypoints.
Dynamic consideration of the exploitationâ€“exploration tradeo is evident in the algorithmâ€™s behavior. The rst two observations map out the neighborhood of the initially best-seen point, exhibiting exploitation. Once su ciently explored, the policy continues exploitation around the second best-seen point, discovering and re ning the global optimum in iterations â€“ . Finally, the policy switches to exploration in iterations
â€“ , systematically covering the domain to ensure nothing has been missed. At termination, there is clear bias in the collected data toward higher objective values, and all remaining uncertainty is in regions where the credible intervals indicate the optimum is unlikely to reside.
The â€œmagicâ€ of Bayesian optimization is that the intuitive behavior of this optimization policy is not the result of ad hoc design, but rather
Draft as of January . Feedback welcome: https://bayesoptbook.com/

..
acquisition functions: Â§ , p. example and discussion

objective function

exploitation

exploitation
exploration
Figure . : The posterior after the indicated number of steps of an example Bayesian optimization policy, starting from the posterior in gure . . The marks show the points chosen by the policy, progressing from top to bottom. Observations su ciently close to the optimum are marked in red; the optimum was located on iteration .

.. emerges automatically through the machinery of Gaussian processes and Bayesian decision theory that we will develop over the coming chapters. In this framework, building an optimization policy boils down to: â€¢ choosing a model of the objective function, â€¢ deciding what sort of data we seek to obtain, and â€¢ systematically transforming these beliefs and preferences into an optimization policy. Over the following chapters, we will develop tools for achieving each of these goals: Gaussian processes (chapters â€“ ) for expressing what we believe about the objective function, utility functions (chapter ) for expressing what we value in data, and Bayesian decision theory (chapter ) for building optimization policies aware of the uncertainty encoded in the model and guided by the preferences encoded in the utility function. In chapter we will combine these fundamental components to realize complete Bayesian optimization policies, at which point we will be equipped to replicate this example from rst principles.
Draft as of January . Feedback welcome: https://bayesoptbook.com/

GAUSSIAN PROCESSES

The central object in optimization is an objective function ğ‘“ : X â†’ â„, and

the primary challenge in algorithm design is inherent uncertainty about

this function: most importantly, where is the function maximized and

what is its maximal value? Prior to optimization, we may very well have

no idea. Optimization a ords us the opportunity to acquire information

about the objective â€“ through observations of our own design â€“ to shed

light on these questions. However, this process is itself fraught with

uncertainty, as we cannot know the outcomes and implications of these

observations at the time of their design. Notably, we face this uncertainty

even when we have a closed-form expression for the objective function,

a favorable position as many objectives act as â€œblack boxes.â€

Re ecting on this situation,

posed an intriguing question:

â€œwhat does it mean to â€˜knowâ€™ a function?â€ The answer is unclear when an

analytic expression, which might at rst glance seem to encapsulate the

essence of the function, is insu cient to determine features of interest.

However,

argued that although we may not know everything

about a function, we often have some prior knowledge that can facilitate

a numerical procedure such as optimization. For example, we may expect

an objective function to be smooth (or rough), or to assume values in

a given range, or to feature a relatively simple underlying trend, or to

depend on some hidden low-dimensional representation we hope to

uncover. All of this knowledge could be instrumental in accelerating

optimization if it could be systematically captured and exploited.

Having identi able information about an objective function prior to

optimization motivates the Bayesian approach we will explore through-

out this book. We will address uncertainty in the objective function

through the unifying framework of Bayesian inference, treating ğ‘“ â€“ as well as ancillary quantities such as ğ‘¥âˆ— and ğ‘“ âˆ— ( . ) â€“ as random variables

to be inferred from observations revealed during optimization.

To pursue this approach, we must rst determine how to build useful

prior distributions for objective functions and how to compute a poste-

rior belief given observations. If the system under investigation is well

understood, we may be able to identify an appropriate parametric form

ğ‘“ (ğ‘¥; ğœ½ ) and infer the parameters ğœ½ directly. This approach is likely the

best course of action when possible; however, many objective functions

have no obvious parametric form, and most models used in Bayesian

optimization are thus nonparametric to avoid undue assumptions.

In this chapter we will introduce Gaussian processes ( s), a conve-

nient class of nonparametric regression models widely used in Bayesian

optimization. We will begin by de ning Gaussian processes and deriving

some basic properties, then demonstrate how to perform inference from

observations. In the case of exact observation and additive Gaussian

noise, we can perform this inference exactly, resulting in an updated

posterior Gaussian process. We will continue by considering some the-

oretical properties of Gaussian processes relevant to optimization and

inference with non-Gaussian observation models.

This material will be published by Cambridge University Press as Bayesian Optimization. This prepublication version is free to view and download for personal use only. Not for redistribution, resale, or use in derivative works. Â©Roman Garnett .

.

( ). Bayesian Numerical Anal-

ysis. In: Statistical Decision Theory and Related

Topics .

We will explore all of these possibilities in the next chapter, p. .
Bayesian inference of the objective function: Â§ . , p.

.

et al. ( ). : Building Auto-

Tuners with Structured Bayesian Optimization.

.

The termâ€œnonparametricâ€ is something of a misnomer. A nonparametric objective function model has parameters but their dimension is in nite â€“ we e ectively parametrize the objective by its value at every point.

..

and . . .

( ).

Gaussian Processes for Machine Learning.

Press.

The literature on Gaussian processes is vast, and we do not intend this

chapter to serve as a standalone introduction but rather as companion to

the existing literature. Although our discussion will be comprehensive,

our focus on optimization will sometimes bias its scope. For a broad

overview, the interested reader may consult

and

â€™s

classic monograph.

multivariate normal distribution: appendix , p.

chapter : modeling with Gaussian processes, p.

.

et al. ( ). Probabilistic numerics

and uncertainty in computations. Proceedings

of the Royal Society A: Mathematical, Physical

and Engineering Sciences ( ):

.

.

et al. ( ). Probabilistic Numerics:

Computation as Machine Learning. Cambridge

University Press.

If X is nite, there is no distinction between a Gaussian process and a multivariate normal distribution, so only the in nite case is interesting for this discussion.

.

( ). Stochastic Di erential

Equations: An Introduction with Applications.

Springerâ€“Verlag. [Â§ . ]

Writing the process as if it were a functionvalued probability density function is an abuse of notation, but a useful and harmless one.

.
A Gaussian process is an extension of the familiar multivariate normal distribution suitable for modeling functions on in nite domains. Gaussian processes inherit the convenient mathematical properties of the multivariate normal distribution without sacri cing computational tractability. Further, by modifying the structure of a , we can model functions with a rich variety of behavior; we will explore this capability in the next chapter. This combination of mathematical elegance and exibility in modeling has established Gaussian processes as the workhorse of Bayesian approaches to numerical tasks, including optimization. ,
De nition
Consider an objective function ğ‘“ : X â†’ â„ of interest over an arbitrary in nite domain X. We will take a nonparametric approach and reason about the function as an in nite collection of random variables, one corresponding to the function value at every point in the domain. Mutual dependence between these random variables will then determine the statistical properties of the functionâ€™s shape.
It is perhaps not immediately clear how we can specify a useful distribution over in nitely many random variables, a construction known as a stochastic process. However, a result known as the Kolmogorov extension theorem allows us to construct a stochastic process by de ning only the distribution of arbitrary nite sets of function values, subject to natural consistency constraints. For a Gaussian process, these nite-dimensional distributions are all multivariate Gaussian, hence its name.
In this light, we build a Gaussian process by replacing the parameters in the nite-dimensional case â€“ a mean vector and a positive semide nite covariance matrix â€“ by analogous functions over the domain. We specify a Gaussian process on ğ‘“:

ğ‘(ğ‘“ ) = GP (ğ‘“ ; ğœ‡, ğ¾)

mean function, ğœ‡ covariance function (kernel), ğ¾
value of objective at ğ‘¥, ğœ™

by a mean function ğœ‡ : X â†’ â„ and a positive semide nite covariance function (or kernel) ğ¾ : X Ã— X â†’ â„. The mean function determines the expected function value ğœ™ = ğ‘“ (ğ‘¥) at any location ğ‘¥:

ğœ‡ (ğ‘¥) = ğ”¼[ğœ™ | ğ‘¥],

thus serving as a location parameter representing the functionâ€™s central tendency. The covariance function determines how deviations from the

..

mean are structured, encoding expected properties of the functionâ€™s behavior. De ning ğœ™ = ğ‘“ (ğ‘¥ ), we have:

ğ¾ (ğ‘¥, ğ‘¥ ) = cov[ğœ™, ğœ™ | ğ‘¥, ğ‘¥ ].

( .)

The mean and covariance functions of the process allow us to compute any nite-dimensional marginal distribution on demand. Let x âŠ‚ X be nite and let ğ“ = ğ‘“ (x) be the corresponding function values, a vectorvalued random variable. For the Gaussian process ( . ), the distribution
of ğ“ is multivariate normal with parameters determined by the mean
and covariance functions:

ğ‘ (ğ“ | x) = N (ğ“; ğ, ğšº),

(.)

where

ğ = ğ”¼[ğ“ | x] = ğœ‡ (x); ğšº = cov[ğ“ | x] = ğ¾ (x, x). ( . )

Here ğ¾ (x, x) is the matrix formed by evaluating the covariance function for each pair of points: Î£ğ‘– ğ‘— = ğ¾ (ğ‘¥ğ‘–, ğ‘¥ ğ‘— ), also called the Gram matrix of x.
In many ways, Gaussian processes behave like â€œreally bigâ€ Gaussian
distributions, and one can intuit many of their properties from this
heuristic alone. For example, the Gaussian marginal property in ( . â€“
. ) corresponds precisely with the analogous formula in the nite-
dimensional case ( . ). Further, this property automatically ensures global consistency in the following sense. If x is an arbitrary set of points and x âŠƒ x is a superset, then we arrive at the same belief about ğ“ whether we compute it directly from ( . â€“ . ) or indirectly by rst computing ğ‘ (ğ“ | x ) then marginalizing x \ x ( . ).

value of objective at ğ‘¥, ğœ™
values of objective at x, ğ“ = ğ‘“ (x)
Gram matrix of x, ğšº = ğ¾ (x, x) In fact, this is precisely the consistency required by the Kolmogorov extension theorem mentioned on the facing page. marginalizing multivariate normal distributions, Â§ . , p.

Example and basic properties

Let us construct and explore an explicit Gaussian process for a function on the interval X = [0, 30]. For the mean function we take the zero function ğœ‡ â‰¡ 0, indicating a constant central tendency. For the covariance
function, we take the prototypical squared exponential covariance:

ğ¾ (ğ‘¥, ğ‘¥

)

=

exp

âˆ’

1 2

|ğ‘¥

âˆ’ğ‘¥

|2

.

(.)

Let us pause to consider the implications of this choice. First, note that var[ğœ™ | ğ‘¥] = ğ¾ (ğ‘¥, ğ‘¥) = 1 at every point ğ‘¥ âˆˆ X, and thus the covariance function ( . ) also measures the correlation between the function values ğœ™ and ğœ™. This correlation decreases with the distance between ğ‘¥ and ğ‘¥, falling from unity to zero as these points become increasingly separated; see the illustration in the margin. We can loosely interpret this as a statistical consequence of continuity: function values at nearby locations are highly correlated, whereas function values at distant locations are e ectively independent. This assumption also implies that observing the function at some point ğ‘¥ provides nontrivial information about the function at su ciently nearby locations (roughly when |ğ‘¥ âˆ’ ğ‘¥ | < 3). We will explore this implication further shortly.

Draft as of January . Feedback welcome: https://bayesoptbook.com/

squared exponential covariance: Â§ . , p.
ğ¾ (ğ‘¥, ğ‘¥ ) 1

|ğ‘¥ âˆ’ ğ‘¥ | 0

0

3

The squared exponential covariance ( . ) as a function of the distance between inputs.

prior mean

prior % credible interval

samples

Figure . : Our example Gaussian process on the domain X = [0, 30]. We illustrate the marginal belief at every location with its mean and a % credible interval and also show three example functions sampled from the process.

predictive credible intervals

For a Gaussian process, the marginal distribution of any single function value is univariate normal ( . ):
ğ‘ (ğœ™ | ğ‘¥) = N (ğœ™; ğœ‡, ğœ2); ğœ‡ = ğœ‡(ğ‘¥); ğœ2 = ğ¾ (ğ‘¥, ğ‘¥), ( . )
where we have abused notation slightly by overloading the symbol ğœ‡. This allows us to derive pointwise credible intervals; for example, the familiar ğœ‡ Â± 1.96ğœ is a % credible interval for ğœ™. Examining our example
, the marginal distribution of every function value is in fact standard normal. We provide a rough visual summary of the process via its mean function and pointwise % predictive credible intervals in gure . . There is nothing terribly exciting we can glean from these marginal distributions alone, and no interesting structure in the process is yet apparent.

sampling from a multivariate normal distribution: Â§ . , p.

Sampling
We may gain more insight by inspecting samples drawn from our example process re ecting the joint distribution of function values. Although it is impossible to represent an arbitrary function on X in nite memory, we can approximate the sampling process by taking a dense grid x âŠ‚ X and sampling the corresponding function values from their joint multivariate normal distribution ( . ). Plotting the sampled vectors against the chosen grid reveals curves approximating draws from the Gaussian process. Figure . illustrates this procedure for our example using a grid of equally spaced points. Each sample is smooth and has several local optima distributed throughout the domain â€“ for some applications, this might be a reasonable model for an objective function on X.

example and discussion

.
We now turn to our attention to inference: given a Gaussian process prior on an objective function, how can we condition this initial belief on observations obtained during optimization?
Let us look at an example to build intuition before diving into the details. Figure . shows the e ect of conditioning our example from

observations

posterior mean

.. posterior % credible interval

samples

Figure . : The posterior for our example scenario in gure . conditioned on three exact observations.

the previous section on three exact measurements of the function. The updated belief re ects both our prior assumptions and the information contained in the data, the hallmark of Bayesian inference. To elaborate, the posterior mean smoothly interpolates through the observed values, agreeing with both the measured values and the smoothness encoded in the prior covariance function. The posterior credible intervals are reduced in the neighborhood of the measured locations â€“ where the prior covariance function encodes nontrivial dependence on at least one observed value â€“ and vanish where the function value has been exactly determined. On the other hand, our marginal belief remains e ectively unchanged from the prior in regions su ciently isolated from the data, where the prior covariance function encodes e ectively no correlation.
Conveniently, inference is straightforward for the pervasive observation models of exact measurement and additive Gaussian noise, where the self-conjugacy of the normal distribution yields a Gaussian process posterior with updated parameters we can compute in closed form. The reasoning underlying inference for both observation models is identical and is subsumed by a exible general argument we will present rst.

Inference from arbitrary jointly Gaussian observations
We may exactly condition a Gaussian process ğ‘ (ğ‘“ ) = GP (ğ‘“ ; ğœ‡, ğ¾) on the observation of any vector y sharing a joint Gaussian distribution with ğ‘“ :

ğ‘ (ğ‘“, y) = GP

ğ‘“ y

;

ğœ‡ m

,

ğ¾ ğœ…

ğœ… C

.

(.)

This notation, analogous to ( . ), extends the Gaussian process on ğ‘“ to include the entries of y; that is, we assume the distribution of any nite
subset of function and/or observed values is multivariate normal. We specify the joint distribution via the marginal distribution of y:

ğ‘ (y) = N (y; m, C)

(.)

and the cross-covariance function between y and ğ‘“ :

ğœ… (ğ‘¥) = cov[y, ğœ™ | ğ‘¥].

(.)

Draft as of January . Feedback welcome: https://bayesoptbook.com/

vector of observed values, y
We assume C is positive de nite; if it were only positive semide nite, there would be wasteful linear dependence among observations. observation mean and covariance, m, C
cross-covariance between observations and function values, ğœ…

inference from exact observations: Â§ . , p. a ne transformations: Â§ . , p.
derivatives and expectations: Â§ . , p.
conditioning a multivariate normal distribution: Â§ . , p.
posterior mean and covariance, ğœ‡D, ğ¾D
This is a useful exercise! The result will be a stochastic process with multivariate normal
nite-dimensional distributions, a Gaussian process by de nition ( . ).

Although it may seem absurd that we could identify and observe a vector satisfying such strong restrictions on its distribution, we can already deduce several examples from rst principles, including:

â€¢ any vector of function values ( . ), â€¢ any a ne transformation of function values ( . ), and â€¢ limits of such quantities, such as partial derivatives or expectations.

Further, we may condition on any of the above even if corrupted by
independent additive Gaussian noise, as we will shortly demonstrate. We may condition the joint distribution ( . ) on y analogously to the
nite-dimensional case ( . ), resulting in a Gaussian process posterior on ğ‘“. Writing D = y for the observed data, we have:

where

ğ‘ (ğ‘“ | D) = GP (ğ‘“ ; ğœ‡D, ğ¾D),
ğœ‡D (ğ‘¥) = ğœ‡ (ğ‘¥) + ğœ… (ğ‘¥) Câˆ’1 (y âˆ’ m); ğ¾D (ğ‘¥, ğ‘¥ ) = ğ¾ (ğ‘¥, ğ‘¥ ) âˆ’ ğœ… (ğ‘¥) Câˆ’1ğœ… (ğ‘¥ ).

(.) (. )

This can be veri ed by computing the joint distribution of an arbitrary nite set of function values and y and conditioning on the latter ( . ). The above result provides a simple procedure for posterior infer-
ence from any vector of observations satisfying ( . ):
. compute the marginal distribution of y ( . ),
. derive the cross-covariance function ğœ… ( . ), and
. nd the posterior distribution of ğ‘“ via ( . â€“ . ).

We will realize this procedure for several special cases below. However, we will rst demonstrate how we may seamlessly handle measurements corrupted by additive Gaussian noise and build intuition for the posterior distribution by dissecting its moments in terms of the statistics of the observations and the correlation structure of the prior.

noisy observation of y, z vector of random errors, ğœº noise covariance matrix, N
sums of normal vectors: Â§ . , p.

Corruption by additive Gaussian noise

We pause to make one observation of immense practical importance: any vector satisfying ( . ) would continue to su ce even if corrupted by independent additive Gaussian noise, and thus we can use the above result to condition a Gaussian process on noisy observations as well.
Suppose that rather than observing y exactly, our measurement mechanism only allowed observing z = y + ğœº instead, where ğœº is a vector of random errors independent of y. If the errors are normally distributed with mean zero and known (arbitrary) covariance N:

ğ‘ (ğœº | N) = N (ğœº; 0, N),

(. )

then we have

ğ‘ (z | N) = N (z; m, C + N); cov[z, ğœ™ | ğ‘¥] = cov[y, ğœ™ | ğ‘¥] = ğœ… (ğ‘¥).

..

Thus we can condition on an observation of the corrupted vector z by simply replacing C with C + N in the prior ( . ) and posterior ( . ). Note that the posterior converges to that from a direct observation of y if we take the noise covariance N â†’ 0 in the positive semide nite cone, a reassuring result.

Interpretation of posterior moments

The moments of the posterior Gaussian process ( . ) contain update
terms adjusting the prior moments in light of the data. These updates
have intuitive interpretations in terms of the nature of the prior process
and the observed values, which we may unravel with some care.
We can gain some initial insight by considering the case where we observe a single value with ğ‘¦ distribution N (ğ‘¦; ğ‘š, ğ‘ 2) and breaking down its impact on our belief. Consider an arbitrary function value ğœ™ with prior distribution N (ğœ™; ğœ‡, ğœ2) ( . ) and de ne

ğ‘§

=

ğ‘¦

âˆ’ ğ‘ 

ğ‘š

to be the ğ‘§-score of the observed value ğ‘¦ and

ğœŒ

=

corr[ğ‘¦, ğœ™

|

ğ‘¥]

=

ğœ… (ğ‘¥) ğœğ‘ 

to be the correlation between ğ‘¦ and ğœ™. Then the posterior mean and

standard deviation of ğœ™ are, respectively:

ğœ‡ + ğœğœŒğ‘§;

âˆšï¸ ğœ 1 âˆ’ ğœŒ2.

(. )

The ğ‘§-score of the posterior mean, with respect to the prior distri-

bution of ğœ™, is ğœŒğ‘§. An independent measurement with ğœŒ = 0 thus leaves

the prior mean unchanged, whereas a perfectly dependent measurement

with |ğœŒ | = 1 shifts the mean up or down by ğ‘§ standard deviations (de-

pending on the sign of the correlation) to match the magnitude of the

measurementâ€™s ğ‘§-score. Measurements with partial dependence result in

outcomes between these extremes. Further, surprising measurements â€“

that is, those with large |ğ‘§| â€“ yield larger shifts in the mean, whereas an

entirely expected measurement with ğ‘¦ = ğ‘š leaves the mean unchanged.

Turning to the posterior standard deviation, the measurement re-

duces our uncertainty in ğœ™ by a factor depending on the correlation

ğœŒ, but not on the value observed. An independent measurement again

leaves the prior intact, whereas a perfectly dependent measurement col-

lapses the posterior standard deviation to zero as the value of ğœ™ would be

completely determined. The relative reduction in posterior uncertainty

as a function of the absolute correlation is illustrated in the margin.

In the case of vector-valued observations, we can interpret similar

structure in the posterior, although dependence between entries of y

must also be accounted for. We may factor the observation covariance

matrix as

C = SPS,

(. )

Draft as of January . Feedback welcome: https://bayesoptbook.com/

Assuming zero-mean errors is not strictly nec-
essary but is overwhelmingly common in practice. A nonzero mean ğ”¼[ğœº ] = n is possible by further replacing (y âˆ’ ğ) with y âˆ’ [ğ + n] in ( . ), where ğ + n = ğ”¼[y].

ğ‘§-score of measurement ğ‘¦, ğ‘§
correlation between measurement ğ‘¦ and function value ğœ™, ğœŒ
posterior moments of ğœ™ from a scalar observation
interpretation of moments

âˆšï¸ ğœ 1 âˆ’ ğœŒ2 ğœ

|ğœŒ | 0

0

1

The posterior standard deviation of ğœ™ as a
function of the strength of relationship with ğ‘¦, |ğœŒ |.

It can be instructive to contrast the behavior of the posterior when conditioning on two highly correlated values versus two independent ones. In the former case, the posterior does not change much as a result of the second measurement, as dependence reduces the e ective number of measurements.
P is congruent to C ( . ) and is thus positive de nite from Sylvesterâ€™s law of inertia.
For positive semide nite A, B, |A| â‰¤ |A + B|.

The LÃ¶wner order is the partial order induced
by the convex cone of positive-semide nite matrices. For symmetric A, B, we de ne A â‰º B if and only if B âˆ’ A is positive de nite:

.

( ). Ãœber monotone Matrixfunk-

tionen. Mathematische Zeitschrift : â€“ .

observed data, D = (x, ğ“)

where

S is diagonal with ğ‘†ğ‘–ğ‘–

=

âˆš ğ¶ğ‘–ğ‘–

=

std[ğ‘¦ğ‘– ]

and P

=

corr[y]

is

the

observation correlation matrix. We may then rewrite the posterior mean

of ğœ™ as

ğœ‡ + ğœğ† Pâˆ’1 z,

where z and ğ† represent the vectors of measurement ğ‘§-scores and the cross-correlation between ğœ™ and y, respectively:

ğ‘§ğ‘–

=

ğ‘¦ğ‘–

âˆ’ ğ‘šğ‘– ğ‘ ğ‘–

;

ğœŒğ‘–

=

[ğœ… (ğ‘¥) ğœğ‘ ğ‘–

]ğ‘–

.

The posterior mean is now in the same form as the scalar case ( . ), with the introduction of the observation correlation matrix moderating the ğ‘§-scores to account for dependence between the observed values.
The posterior standard deviation of ğœ™ in the vector-valued case is
âˆšï¸ƒ ğœ 1 âˆ’ ğ† Pâˆ’1ğ†,

again analogous to ( . ). Noting that the inverse correlation matrix Pâˆ’1 is positive de nite, the posterior covariance again re ects a global reduction in the marginal uncertainty of every function value. In fact, the joint distribution of any set of function values has reduced uncertainty in the posterior in terms of the di erential entropy ( . ), as
|ğ¾ (x, x) âˆ’ ğœ… (x) Câˆ’1ğœ… (x)| â‰¤ |ğ¾ (x, x)|.

The reduction of uncertainty again depends on the strength of depen-
dence between function values and the observed data, with independence (ğ† = 0) resulting in no change. The reduction also depends on the precision of the measurements: all other things held equal, observations
with greater precision in terms of the LÃ¶wner order on the precision matrix Câˆ’1 provide a globally better informed posterior. In particular, as (C + N)âˆ’1 â‰º Câˆ’1 for any noise covariance N, noisy measurements ( . ) categorically provide less information about the function than direct
observations, as one should hope.

Inference with exact function evaluations
We will now explicitly demonstrate the general process of Gaussian process inference for important special cases, beginning with the simplest possible observation mechanism: exact observation.
Suppose we have observed ğ‘“ at some set of locations x, revealing the corresponding function values ğ“ = ğ‘“ (x), and let D = (x, ğ“) denote this dataset. The observed vector shares a joint Gaussian distribution with any other set of function values by the assumption on ğ‘“ ( . ), so we may follow the above procedure to derive the posterior. The marginal distribution of ğ“ is Gaussian ( . ):
ğ‘ (ğ“ | x) = N (ğ“; ğ, ğšº),

..

and the cross-covariance between an arbitrary function value and ğ“ is by de nition given by the covariance function:

ğœ… (ğ‘¥) = cov[ğ“, ğœ™ | x, ğ‘¥] = ğ¾ (x, ğ‘¥).

Appealing to ( . â€“ . ) we have:

where

ğ‘ (ğ‘“ | D) = GP (ğ‘“ ; ğœ‡D, ğ¾D),
ğœ‡D (ğ‘¥) = ğœ‡ (ğ‘¥) + ğ¾ (ğ‘¥, x)ğšºâˆ’1 (ğ“ âˆ’ ğ); ğ¾D (ğ‘¥, ğ‘¥ ) = ğ¾ (ğ‘¥, ğ‘¥ ) âˆ’ ğ¾ (ğ‘¥, x)ğšºâˆ’1ğ¾ (x, ğ‘¥ ).

(. )

Our previous gure . illustrates the posterior resulting from conditioning our prior in gure . on three exact measurements, with high-level analysis of its behavior in the accompanying text.

Inference with function evaluations corrupted by additive Gaussian noise
With the notable exception of optimizing the output of a deterministic computer program or simulation, observations of an objective function are typically corrupted by noise due to measurement limitations or statistical approximation; we must be able to handle such noisy observations to maximize utility. Fortunately, in the important case of additive Gaussian noise, we may perform exact inference following the general procedure described above. In fact, the derivation below follows directly from our previous discussion on arbitrary additive Gaussian noise, but the case of additive Gaussian noise in function evaluations is important enough to merit its own discussion.
Suppose we make observations of ğ‘“ at locations x, revealing corrupted values y = ğ“ + ğœº. Suppose the measurement errors ğœº are independent of ğ“ and normally distributed with mean zero and covariance N, which may optionally depend on x:

ğ‘ (ğœº | x, N) = N (ğœº; 0, N).

(. )

As before we aggregate the observations into a dataset D = (x, y). The observation noise covariance can in principle be arbitrary;
however, the most common models in practice are independent homoskedastic noise with scale ğœğ‘›:

N = ğœğ‘›2 I,

(. )

and independent heteroskedastic noise with scale depending on location according to a function ğœğ‘› : X â†’ â„â‰¥0:

N = diag ğœğ‘›2 (x).

(. )

For a given observation location ğ‘¥, we will simply write ğœğ‘› for the associated noise scale, leaving any dependence on ğ‘¥ implicit.

Draft as of January . Feedback welcome: https://bayesoptbook.com/

example and discussion
arbitrary additive Gaussian noise: Â§ . , p. Allowing nondiagonal N departs from our typical convention of assuming conditional independence between observations ( . ), but doing so does not complicate inference, so there is no harm in this generality. special case: independent homoskedastic noise
special case: independent heteroskedastic noise
observation noise scale, ğœğ‘›

observations

posterior mean

posterior % credible interval

Figure . : Posteriors for our example from gure . conditioned on noisy observations with independent homoskedastic noise ( . ). The signal-to-noise ratio is for the top example, for the middle example, and for the bottom example.

homoskedastic example and discussion

The prior distribution of the observations is now multivariate normal

( . , . ):

ğ‘ (y | x, N) = N (y; ğ, ğšº + N).

(. )

Due to independence of the noise, the cross-covariance remains the same as in the exact observation case:

ğœ… (ğ‘¥) = cov[y, ğœ™ | x, ğ‘¥] = ğ¾ (x, ğ‘¥).

Conditioning on the observed value now yields a posterior with

ğœ‡D (ğ‘¥) = ğœ‡ (ğ‘¥) + ğ¾ (ğ‘¥, x) (ğšº + N)âˆ’1 (y âˆ’ ğ); ğ¾D (ğ‘¥, ğ‘¥ ) = ğ¾ (ğ‘¥, ğ‘¥ ) âˆ’ ğ¾ (ğ‘¥, x) (ğšº + N)âˆ’1ğ¾ (x, ğ‘¥ ).

(. )

Figure . shows a sequence of posterior distributions resulting from conditioning our example on data corrupted by increasing levels of homoskedastic noise ( . ). As the noise level increases, the observations have diminishing in uence on our belief, with some extreme values eventually being partially explained away as outliers. As measurements are assumed to be inexact, the posterior mean is not compelled to interpolate perfectly through the observations, as in the exact case ( gure

observations posterior mean

..
posterior % credible interval, ğ‘¦ posterior % credible interval, ğœ™

Figure . : The posterior distribution for our example from gure . conditioned on observations with heteroskedastic observation noise ( . ). We show predictive credible intervals for both the latent objective function and noisy observations; the standard deviation of the observation noise increases linearly from left to right.

. ). Further, with increasing levels of noise, our posterior belief re ects signi cant residual uncertainty in the function, even in regions with multiple nearby observations.
We illustrate an example of Gaussian process inference with heteroskedastic noise ( . ) in gure . , where the signal-to-noise ratio decreases smoothly from left-to-right over the domain. Although the observations provide relatively even coverage, our posterior uncertainty is minimal on the left-hand side of the domain â€“ where the measurements provide maximal information â€“ and increases as our observations become more noisy and less informative.
We will often require the posterior predictive distribution for a noisy measurement ğ‘¦ that would result from observing at a given location ğ‘¥. The posterior distribution on ğ‘“ ( . ) provides the posterior predictive distribution for the latent function value ğœ™ = ğ‘“ (ğ‘¥) ( . ):

ğ‘ (ğœ™ | ğ‘¥, D) = N (ğœ™; ğœ‡, ğœ2); ğœ‡ = ğœ‡D (ğ‘¥); ğœ2 = ğ¾D (ğ‘¥, ğ‘¥),

but does not account for the e ect of observation noise. In the case of independent additive Gaussian noise ( . â€“ . ), deriving the posterior predictive distribution is trivial; we have ( . ):

ğ‘ (ğ‘¦ | ğ‘¥, D, ğœğ‘›) = N (ğ‘¦; ğœ‡, ğœ2 + ğœğ‘›2).

(. )

This predictive distribution is illustrated in gure . ; the credible intervals for noisy measurements re ect in ation of the credible intervals for the underlying function value commensurate with the scale of the noise.
If the noise contains nondiagonal correlation structure, we must account for dependence between training and test errors in the predictive distribution. The easiest way to proceed is to recognize that the noisy observation process ğ‘¦ = ğœ™ + ğœ€, as a function of ğ‘¥, is itself a Gaussian process with mean function ğœ‡ and covariance function

ğ¶ (ğ‘¥, ğ‘¥ ) = cov[ğ‘¦, ğ‘¦ | ğ‘¥, ğ‘¥ ] = ğ¾ (ğ‘¥, ğ‘¥ ) + ğ‘ (ğ‘¥, ğ‘¥ ),

Draft as of January . Feedback welcome: https://bayesoptbook.com/

heteroskedastic example and discussion
posterior predictive distribution for noisy observations
predictive distribution with correlated noise covariance function for noisy measurements, ğ¶

covariance function for observation noise, ğ‘

where ğ‘ is the noise covariance: ğ‘ (ğ‘¥, ğ‘¥ ) = cov[ğœ€, ğœ€ | ğ‘¥, ğ‘¥ ]. The posterior of the observation process is then a with

ğ”¼[ğ‘¦ | ğ‘¥, D] = ğœ‡ (ğ‘¥) + ğ¶ (ğ‘¥, x) (ğšº + N)âˆ’1 (y âˆ’ ğ); cov[ğ‘¦, ğ‘¦ | ğ‘¥, ğ‘¥ , D] = ğ¶ (ğ‘¥, ğ‘¥ ) âˆ’ ğ¶ (ğ‘¥, x) (ğšº + N)âˆ’1ğ¶ (x, ğ‘¥ ),

(. )

from which we can derive predictive distributions via ( . ).

In particular the claims regarding continuity and di erentiability are slightly more complicated than stated below.
multi delity optimization: Â§ . , p. multiobjective optimization: Â§ . , p.
sample path continuity: Â§ . , p.
sample path di erentiability: Â§ . , p.
derivative observations: Â§ . , p.
existence of global maxima: Â§ . , p.
uniqueness of global maxima: Â§ . , p.
inference with non-Gaussian observations and constraints: Â§ . , p.

.
In the remainder of this chapter we will cover some additional, somewhat niche and/or technical aspects of Gaussian processes that see occasional use in Bayesian optimization. Modulo mathematical nuances irrelevant in practical settings, an intuitive (but not entirely accurate!) summary follows:
â€¢ a joint Gaussian process (discussed below) allows us to model multiple related functions simultaneously, which is critical for some scenarios such as multi delity and multiobjective optimization;
â€¢ sample paths are continuous if the mean function is continuous and the covariance function is continuous along the â€œdiagonalâ€ ğ‘¥ = ğ‘¥ ;
â€¢ sample paths are di erentiable if the mean function is di erentiable and the covariance function is di erentiable along the â€œdiagonalâ€ ğ‘¥ = ğ‘¥ ;
â€¢ a function with a su ciently smooth distribution shares a joint distribution with its gradient; among other things, this allows us to condition on (potentially noisy) derivative observations via exact inference;
â€¢ sample paths attain a maximum when sample paths are continuous and the domain is compact,
â€¢ sample paths attain a unique maximum under the additional condition that no two unique function values are perfectly correlated, and
â€¢ several methods are available for approximating the posterior process of a conditioned on information incompatible with exact inference.
If satis ed with the above summary, the reader may safely skip this material for now and move on with the next chapter. For those who wish to see the gritty details, dive in below!

.
In some settings, we may wish to reason jointly about two-or-more related functions, such as an objective function and its gradient or an expensive objective function and a cheaper surrogate. To this end we can extend Gaussian processes to yield a joint distribution over the values assumed by multiple functions. The key to the construction is to â€œpaste togetherâ€ a collection of functions into a single function on a larger domain, then construct a standard on this combined function.

De nition
To elaborate, consider a set of functions {ğ‘“ğ‘– : Xğ‘– â†’ â„} we wish to model. We de ne the disjoint union of these functions ğ‘“ â€“ de ned on the disjoint union of their domains X = Xğ‘– â€“ by insisting its restriction to each domain be compatible with the corresponding function:

ğ‘“ : X â†’ â„;

ğ‘“ |Xğ‘– â‰¡ ğ‘“ğ‘– .

We now can de ne a on ğ‘“ by choosing mean and covariance functions on X as desired:

ğ‘( ğ‘“ ) = GP ( ğ‘“ ; ğœ‡, ğ¾).

(. )

We will call this construction a joint Gaussian process on {ğ‘“ğ‘– }. It is often convenient to decompose the moments of a joint into
their restrictions on relevant subspaces. For example, consider a joint ( . ) on ğ‘“ : F â†’ â„ and ğ‘” : G â†’ â„. After de ning

ğœ‡ğ‘“ â‰¡ ğœ‡|F; ğ¾ğ‘“ â‰¡ ğ¾ |FÃ—F ;

ğœ‡ğ‘” â‰¡ ğœ‡ |G; ğ¾ğ‘” â‰¡ ğ¾ |GÃ—G ;

ğ¾ğ‘“ğ‘” â‰¡ ğ¾ |FÃ—G ;

ğ¾ğ‘”ğ‘“ â‰¡ ğ¾ |GÃ—F ,

we can see that ğ‘“ and ğ‘” in fact have marginal distributions:

ğ‘ (ğ‘“ ) = GP (ğ‘“ ; ğœ‡ğ‘“ , ğ¾ğ‘“ ); ğ‘ (ğ‘”) = GP (ğ‘”; ğœ‡ğ‘”, ğ¾ğ‘”),

(. )

that are coupled by the cross-covariance functions ğ¾ğ‘“ğ‘” and ğ¾ğ‘”ğ‘“ . Given vectors x âŠ‚ F and x âŠ‚ G, these compute the covariance between the corresponding function values ğ“ = ğ‘“ (x) and ğœ¸ = ğ‘”(x ):

ğ¾ğ‘“ğ‘” (x, x ) = cov[ğ“, ğœ¸ | x, x ]; ğ¾ğ‘”ğ‘“ (x, x ) = cov[ğœ¸, ğ“ | x, x ] = ğ¾ğ‘“ğ‘” (x, x ).

(. )

When convenient we will notate a joint in terms of these decomposed functions, here writing:

ğ‘ (ğ‘“, ğ‘”) = GP

ğ‘“ ğ‘”

;

ğœ‡ğ‘“ ğœ‡ğ‘”

,

ğ¾ğ‘“ ğ¾ğ‘”ğ‘“

ğ¾ğ‘“ğ‘” ğ¾ğ‘”

.

(. )

With this notation, the marginal property ( . ) is perfectly analogous
to the marginal property of the multivariate Gaussian distribution ( . ).
We can also use this construction to de ne a on a vector-valued function f: X â†’ â„ğ‘‘ by de ning a joint Gaussian process on its ğ‘‘ coordinate functions {ğ‘“ğ‘– } : X â†’ â„. In this case we typically write the resulting model using the standard notation GP (f; ğœ‡, ğ¾), where the mean and covariance functions are now understood to map to â„ğ‘‘ and â„ğ‘‘. Ã—ğ‘‘

Example We can demonstrate the behavior of a joint Gaussian process by extending our running example on ğ‘“ : [0, 30] â†’ â„. Recall the prior on ğ‘“ has
Draft as of January . Feedback welcome: https://bayesoptbook.com/

..
disjoint union of {ğ‘“ğ‘– }, ğ‘“ disjoint union of {Xğ‘– }, X The domains need not be equal, but they often are in practice. A disjoint union represents a point ğ‘¥ âˆˆ Xğ‘– by the pair (ğ‘¥, ğ‘–), thereby combining the domains while retaining their identities. joint Gaussian process
In fact, any restriction of a -distributed function has a (or multivariate normal) distribution.
We also used this notation in ( . ), where the â€œdomainâ€ of the vector y can be taken to be some nite index set of appropriate size.
extension to vector-valued functions

prior mean

prior % credible interval

sample

Figure . : A joint Gaussian process over two functions on the shared domain X = [0, 30]. The marginal belief over both functions is the same as our example from gure . , but the cross-covariance ( . ) between the functions strongly couples their behavior. We also show a sample from the joint distribution illustrating the strong correlation induced by the joint prior.

zero mean function ğœ‡ â‰¡ 0 and squared exponential covariance function ( . ). We augment our original function with a companion function ğ‘”, de ned on the same domain, that has exactly the same marginal distribution. However, we couple the distribution of ğ‘“ and ğ‘” by de ning a nontrivial cross-covariance function ğ¾ğ‘“ğ‘” ( . ):

ğ¾ğ‘“ğ‘” (ğ‘¥, ğ‘¥ ) = 0.9ğ¾ (ğ‘¥, ğ‘¥ ),

(. )

where ğ¾ is the marginal covariance function of ğ‘“ and ğ‘”. A consequence of this choice is that for any given point ğ‘¥ âˆˆ X, the correlation of the corresponding function values ğœ™ = ğ‘“ (ğ‘¥) and ğ›¾ = ğ‘”(ğ‘¥) is quite strong:

corr[ğœ™, ğ›¾ | ğ‘¥] = 0.9.

(. )

We illustrate the resulting joint in gure . . The marginal credible intervals for ğ‘“ (and now ğ‘”) have not changed from our original example in gure . . However, drawing a sample of the functions from their joint distribution reveals the strong coupling encoded in the prior ( . â€“ . ).

inference from jointly Gaussian distributed observations: Â§ . , p.

Inference for joint Gaussian processes
The construction in ( . ) allows us to reason about a joint Gaussian process as if it were a single . This allows us to condition a joint on observations of jointly Gaussian distributed values following the procedure outlined previously. In gure . , we condition the joint prior from gure . on ten observations: ve exact observations of ğ‘“ on the left-hand side of the domain and ve exact observations of ğ‘” on the right-hand side. Due to the strong correlation between the two functions, an observation of either function strongly informs our belief about the other, even in regions where there are no direct observations.

.
In this and the following sections we will establish some important properties of Gaussian processes determined by the properties of their

observations (direct) observations (other function)

posterior mean posterior % credible interval

.. ğ‘(ğ‘“ | D)

ğ‘ (ğ‘” | D)

Figure . : The joint posterior for our example joint prior in gure . conditioned on ve exact observations of each function.

moments. As a is completely speci ed by its mean and covariance functions, it should not be surprising that the nature of these functions have far-reaching implications regarding properties of the function being modeled. A good familiarity with these implications can help guide model design in practice â€“ the focus of the next two chapters.
To begin, a fundamental question regarding Gaussian processes is whether sample paths are almost surely continuous, and if so how many times di erentiable they may be. This is obviously an important consideration for modeling and is also critical to ensure that global optimization is a well-posed problem, as we will discuss later in this chapter. Fortunately, continuity of Gaussian processes is a well-understood property that can be guaranteed almost surely under simple conditions on the mean and covariance functions.
Suppose ğ‘“ : X â†’ â„ has distribution GP (ğ‘“ ; ğœ‡, ğ¾). Recall that ğ‘“ is continuous at ğ‘¥ if ğ‘“ (ğ‘¥) âˆ’ ğ‘“ (ğ‘¥ ) = ğœ™ âˆ’ ğœ™ â†’ 0 when ğ‘¥ â†’ ğ‘¥. Continuity is thus a limiting property of di erences in function values. But under the Gaussian process assumption, this di erence is Gaussian distributed ( . , . )! We have
ğ‘ (ğœ™ âˆ’ ğœ™ | ğ‘¥, ğ‘¥ ) = N (ğœ™ âˆ’ ğœ™ ;ğ‘š, ğ‘ 2),
where
ğ‘š = ğœ‡ (ğ‘¥) âˆ’ ğœ‡ (ğ‘¥ ); ğ‘ 2 = ğ¾ (ğ‘¥, ğ‘¥) âˆ’ 2ğ¾ (ğ‘¥, ğ‘¥ ) + ğ¾ (ğ‘¥, ğ‘¥ ).
Now if ğœ‡ is continuous at ğ‘¥ and ğ¾ is continuous at ğ‘¥ = ğ‘¥, then both ğ‘š â†’ 0 and ğ‘ 2 â†’ 0 as ğ‘¥ â†’ ğ‘¥, and thus ğœ™ âˆ’ğœ™ converges in probability to 0. This intuitive condition of continuous moments is known as continuity in mean square at ğ‘¥; if ğœ‡ and ğ¾ are both continuous over the entire domain (the latter along the â€œdiagonalâ€ ğ‘¥ = ğ‘¥ ), then we say the entire process is continuous in mean square.
Draft as of January . Feedback welcome: https://bayesoptbook.com/

existence of global maxima: Â§ . , p. continuity in mean square

sample path continuity

..

and . .

( ). Random

Fields and Geometry. Springerâ€“Verlag. [Â§Â§ . â€“

.]

HÃ¶lder continuity is a generalization of Lipschitz continuity. E ectively, the covariance function must, in some sense, be â€œpredictablyâ€ continuous.

.

( ). Principles of Mathematical

Analysis. McGrawâ€“Hill. [theorem . ]

Following the discussion in the next section, they in fact are in nitely di erentiable.

It turns out that continuity in mean square is not quite su cient

to guarantee that ğ‘“ is simultaneously continuous at every ğ‘¥ âˆˆ X with

probability one, a property known as sample path continuity. However,

very slightly stronger conditions on the moments of a are su cient

to guarantee sample path continuity. The following result is adequate

for most settings arising in practice and may be proven as a corollary to

the slightly weaker (and slightly more complicated) conditions assumed

in

and

â€™s theorem . . .

Theorem. Suppose X âŠ‚ â„ğ‘‘ is compact and ğ‘“ : X â†’ â„ has Gaussian process distribution GP (ğ‘“ ; ğœ‡, ğ¾), where ğœ‡ is continuous and ğ¾ is HÃ¶lder
continuous. Then ğ‘“ is almost surely continuous on X.

The condition that X âŠ‚ â„ğ‘‘ be compact is equivalent to the domain being closed and bounded, by the Heineâ€“Borel theorem. Applying this result to our example in gure . , we conclude that samples from the process are continuous with probability one as the domain X = [0, 30] is compact and the squared exponential covariance function ( . ) is HÃ¶lder continuous. Indeed, the generated samples are very smooth.
Sample path continuity can also be guaranteed on non-Euclidean domains under similar smoothness conditions.

.

sequences of normal s: Â§ . , p.

We can approach the question of di erentiability by again reasoning
about the limiting behavior of linear transformations of function values. Suppose ğ‘“ : X â†’ â„ with X âŠ‚ â„ğ‘‘ has distribution GP (ğ‘“ ; ğœ‡, ğ¾), and consider the ğ‘–th partial derivative of ğ‘“ at x, if it exists:

ğœ•ğ‘“ ğœ•ğ‘¥ğ‘–

(x)

=

lim
â„â†’0

ğ‘“

(x

+

â„eğ‘– ) â„

âˆ’

ğ‘“

(x)

,

where eğ‘– is the ğ‘–th standard basis vector. For â„ > 0, the value in the limit is Gaussian distributed as a linear transformation of Gaussian-distributed

random variables ( . ). Assuming the corresponding partial derivative of the mean exists at x and the corresponding partial derivative with respect to each input of the covariance function exists at x = x, then as â„ â†’ 0 the partial derivative converges in distribution to a Gaussian:

di erentiability in mean square joint between function and gradient

ğ‘

ğœ•ğ‘“ ğœ•ğ‘¥ğ‘–

(x)

|

x

=N

ğœ•ğ‘“ ğœ•ğ‘¥ğ‘–

(x);

ğœ•ğœ‡ ğœ•ğ‘¥ğ‘–

(x),

ğœ•2ğ¾ ğœ•ğ‘¥ğ‘– ğœ•ğ‘¥ğ‘–

(x,

x)

.

If this property holds for each coordinate 1 â‰¤ ğ‘– â‰¤ ğ‘‘, then ğ‘“ is said to be di erentiable in mean square at x.
If ğ‘“ is di erentiable in mean square everywhere in the domain, the
process itself is called di erentiable in mean square, and we have the
remarkable result that the function and its gradient have a joint Gaussian
process distribution:

ğ‘ (ğ‘“, âˆ‡ğ‘“ ) = GP

ğ‘“ âˆ‡ğ‘“

;

ğœ‡ âˆ‡ğœ‡

,

ğ¾ âˆ‡ğ¾

ğ¾âˆ‡ âˆ‡ğ¾ âˆ‡

.

(. )

observations

posterior mean

.. posterior % credible interval

ğ‘“

ğ‘‘ğ‘“ ğ‘‘ğ‘¥

Figure . : The joint posterior of the function and its derivative for our example Gaussian process from gure . . The dashed line in the lower plot corresponds to a derivative of zero.

Here by writing the gradient operator âˆ‡ on the left-hand side of ğ¾ we
mean the result of taking the gradient with respect to its rst input, and by writing âˆ‡ on the right-hand side of ğ¾ we mean taking the
gradient with respect to its second input and transposing the result. Thus âˆ‡ğ¾ : X Ã— X â†’ â„ğ‘‘ maps pairs of points to column vectors:

âˆ‡ğ¾ (x, x )

ğ‘–

= cov

ğœ•ğ‘“ ğœ•ğ‘¥ğ‘–

(x),

ğ‘“

(x

)

x, x

=

ğœ•ğ¾ ğœ•ğ‘¥ğ‘–

(x, x

),

and ğ¾ âˆ‡ : X Ã— X â†’ (â„ğ‘‘ )âˆ— maps pairs of points to row vectors:

ğ¾âˆ‡ (x, x ) = âˆ‡ğ¾ (x, x) .

Finally, the function âˆ‡ğ¾ âˆ‡ : X Ã— X â†’ â„ğ‘‘Ã—ğ‘‘ represents the result of applying both operations, mapping a pair of points to the covariance matrix between the entries of the corresponding gradients:

âˆ‡ğ¾ âˆ‡

(x, x )

ğ‘– ğ‘— = cov

ğœ•ğ‘“ ğœ•ğ‘¥ğ‘–

(x),

ğœ•ğ‘“ ğœ•ğ‘¥ğ‘—

(x

)

x, x

=

ğœ•2ğ¾ ğœ•ğ‘¥ğ‘– ğœ•ğ‘¥ğ‘—

(x, x

).

As the gradient of ğ‘“ has a Gaussian process marginal distribution ( . ), we can reduce the question of continuous di erentiability to sample path continuity of the gradient process following the discussion above.
Figure . shows the posterior distribution for the derivative of our example Gaussian process alongside the posterior for the function itself. We can observe a clear correspondence between the two distributions; for example, the posterior mean of the derivative vanishes at critical points of the posterior mean of the function. Notably, we have a great deal of residual uncertainty about the derivative, even at the observed locations. That is because the relatively high spacing between the existing observations limits our ability to accurately estimate the derivative

Draft as of January . Feedback welcome: https://bayesoptbook.com/

covariance between âˆ‡ğ‘“ (x) and ğ‘“ (x ), âˆ‡ğ¾
transpose of covariance between ğ‘“ (x) and âˆ‡ğ‘“ (x ), ğ¾ âˆ‡ covariance between âˆ‡ğ‘“ (x) and âˆ‡ğ‘“ (x ), âˆ‡ğ¾ âˆ‡
continuous di erentiability example and discussion

observations

posterior mean

posterior % credible interval

ğ‘“

ğ‘‘ğ‘“ ğ‘‘ğ‘¥

Figure . : The joint posterior of the derivative of our example Gaussian process after adding a new observation nearby another suggesting a large positive slope. The dashed line in the lower plot corresponds to a derivative of zero.

inference from jointly Gaussian distributed observations: Â§ . , p.
For ğ¾ we again only need to consider the â€œdiagonalâ€ x = x. Recall the Hessian is symmetric (assuming the second partial derivatives are continuous) and thus redundant. The half-vectorization operator vech A maps the upper triangular part of a square, symmetric matrix A to a vector.

anywhere. Adding an observation immediately next to a previous one signi cantly reduces the uncertainty in the derivative in that region by e ectively providing a nite-di erence approximation; see gure . .

Conditioning on derivative observations

However, we can be more direct in specifying derivatives than nite di erencing. We can instead condition the joint ( . ) directly on a derivative observation, as described previously. Figure . shows the joint posterior after conditioning on an exact observation of the derivative at to the left-most observation location, where the uncertainty in the derivative now vanishes entirely. This capability allows the seamless incorporation of derivative information into an objective function model. Notably, we can even condition a Gaussian process on noisy derivative observations as well, as we might obtain in stochastic gradient descent.
We can reason about derivatives past the rst recursively. For example, if ğœ‡ and ğ¾ are twice di erentiable, then the (e.g., half-vectorized ) Hessian of ğ‘“ will also have a joint distribution with ğ‘“ and its gradient. De ning h to be the operator mapping a function to its half-vectorized Hessian:
hğ‘“ = vech âˆ‡âˆ‡ ğ‘“,
for a Gaussian process with suitably di erentiable moments, we have

ğ‘ (hğ‘“ ) = GP hğ‘“ ; hğœ‡, hğ¾ h ,

(. )

where we have used the same notational convention for the transpose. Further, ğ‘“, âˆ‡ğ‘“, and hğ‘“ will have a joint Gaussian process distribution given by augmenting ( . ) with the marginal in ( . ) and the cross-
covariance functions

cov[hğ‘“, ğ‘“ ] = hğ¾; cov[hğ‘“, âˆ‡ğ‘“ ] = hğ¾ âˆ‡.

observations

posterior mean

.. posterior % credible interval

ğ‘“

ğ‘‘ğ‘“ ğ‘‘ğ‘¥

Figure . : The joint posterior of the derivative of our example Gaussian process after adding an exact observation of the derivative at the indicated location. The dashed line in the lower plot corresponds to a derivative of zero.

We can continue further in this vein if needed; however, we rarely reason about derivatives of third-or-higher order in Bayesian optimization.
Other linear transformations
The joint distribution between a suitably smooth -distributed function and its gradient ( . ) is simply an in nite-dimensional analog of the general result that Gaussian random variables are jointly Gaussian distributed with arbitrary linear transformations ( . ), after noting that di erentiation is a linear operator. We can extend this result to reason about other linear transformations of -distributed functions.
â€™s original motivation for studying Bayesian numerical methods was quadrature, the numerical estimation of intractable integrals. It turns out that Gaussian processes are a rather convenient model for this task: if ğ‘ (ğ‘“ ) = GP (ğ‘“ ; ğœ‡, ğ¾) and we want to reason about the expectation
âˆ« ğ‘ = ğ‘“ (ğ‘¥) ğ‘ (ğ‘¥) dğ‘¥,
then (under mild conditions) we again have a joint Gaussian process distribution over ğ‘“ and ğ‘ . This enables both inference about ğ‘ and conditioning on noisy observations of integrals, such as a Monte Carlo estimate of an expectation. The former is the basis for Bayesian quadrature, an analog of Bayesian optimization bringing Bayesian experimental design to bear on numerical integration. , ,
.
The primary use of s in Bayesian optimization is to inform optimization decisions, which will be our focus for the majority of this book. Before continuing down this path, we pause to consider whether global
Draft as of January . Feedback welcome: https://bayesoptbook.com/

This is true in classical optimization as well!

.

( ). Bayesian Numerical Anal-

ysis. In: Statistical Decision Theory and Related

Topics .

This can be shown, for example, by considering the limiting distribution of Riemann sums.

.â€™

( ). Bayesâ€“Hermite quadrature.

Journal of Statistical Planning and Inference

( ): â€“

..

and .

Bayesian Monte Carlo. eur

( ).

mutual information and entropy search: Â§ . , p.

.

( ). Principles of Mathematical

Analysis. McGrawâ€“Hill. [theorem . ]

sample path continuity: Â§ . , p.

A centered Gaussian process has identaically zero mean function ğœ‡ â‰¡ 0.

.

and .

( ). Cube Root

Asymptotics. The Annals of Statistics ( ): â€“

. [lemma . ]

optimization of a -distributed function is a well-posed problem, in particular, whether the model guarantees the existence of a global maximum at all.
Consider a function ğ‘“ : X â†’ â„ with distribution GP (ğ‘“ ; ğœ‡, ğ¾), and consider the location and value of its global optimum, if one exists:

ğ‘¥âˆ— = arg max ğ‘“ (ğ‘¥);
ğ‘¥ âˆˆX

ğ‘“ âˆ— = max ğ‘“ (ğ‘¥) = ğ‘“ (ğ‘¥âˆ—).
ğ‘¥ âˆˆX

As ğ‘“ is unknown, these quantities are random variables. Many Bayesian optimization algorithms operate by reasoning about the distributions of (and uncertainties in) these quantities induced by our belief on ğ‘“.
There are two technical issues we must address. The rst is whether we can be certain that a globally optimal value ğ‘“ âˆ— exists when the objective function is random. If existence is not guaranteed, then its distribution is meaningless. The second issue is one of uniqueness: assuming the objective does attain a maximal value, can we be certain the optimum is unique? In general ğ‘¥âˆ— is a set-valued random variable, and thus its distribution might have support over arbitrary subsets of the domain, rendering it complicated to reason about. However, if we could ensure the uniqueness of ğ‘¥âˆ—, its distribution would have support on X rather than its power set, allowing more straightforward inference.
Both the existence of ğ‘“ âˆ— and uniqueness of ğ‘¥âˆ— are tacitly assumed throughout the Bayesian optimization literature when building algorithms based on distributions of these quantities, but these properties are not guaranteed for arbitrary Gaussian processes. However, we can ensure these properties hold almost surely under mild conditions.

Existence of global maxima
To begin, guaranteeing the existence of an optimal value is straightforward if we suppose the domain X is compact, a pervasive assumption in optimization. This is no coincidence! In this case, if ğ‘“ is continuous then it achieves a global optimum by the extreme value theorem. Thus sample path continuity of ğ‘“ and compactness of X is su cient to ensure that ğ‘“ âˆ— exists almost surely. Both conditions can be readily established: sample path continuity by following our previous discussion, and compactness of the domain by standard arguments (for example, ensuring that X âŠ‚ â„ğ‘‘ be closed and bounded).

Uniqueness of global maxima

We now turn to the question of uniqueness of ğ‘¥,âˆ— which obviously only becomes a meaningful question after presupposing that ğ‘“ âˆ— exists. Again,

this condition is easy to ensure almost surely under simple conditions

on the covariance function of a Gaussian process.

and

considered this issue and provided straightforward

conditions under which the uniqueness of ğ‘¥âˆ— is guaranteed for a centered

Gaussian process. , Namely, no two unique points in the domain can

..

have perfectly correlated function values, a natural condition that can be easily veri ed.

Theorem ( and

, ). Let X be a compact metric space.

Suppose ğ‘“ : X â†’ â„ has distribution GP (ğ‘“ ; ğœ‡ â‰¡ 0, ğ¾), and that ğ‘“ is sample

path continuous. If for all ğ‘¥, ğ‘¥ âˆˆ X with ğ‘¥ â‰  ğ‘¥ we have

var[ğœ™ âˆ’ ğœ™ | ğ‘¥, ğ‘¥ ] = ğ¾ (ğ‘¥, ğ‘¥) âˆ’ 2ğ¾ (ğ‘¥, ğ‘¥ ) + ğ¾ (ğ‘¥, ğ‘¥ ) â‰  0,

then ğ‘“ almost surely has a unique maximum on X.

provided slightly weaker conditions for uniqueness of the supremum, avoiding the requirement of sample path continuity.

Counterexamples
Although the above conditions for ensuring existence of ğ‘“ âˆ— and uniqueness of ğ‘¥âˆ— are fairly mild, it is easy to construct counterexamples.
Consider a function on the closed unit interval, which we note is compact: ğ‘“ : [0, 1] â†’ â„. We endow ğ‘“ with a â€œwhite noiseâ€ Gaussian process with
ğœ‡ (ğ‘¥) â‰¡ 0; ğ¾ (ğ‘¥, ğ‘¥ ) = [ğ‘¥ = ğ‘¥ ].
Now ğ‘“ almost surely does not have a maximum. Roughly, because the value of ğ‘“ at every point in the domain is independent of every other, there will almost always be a point with value exceeding any putative maximum. However, the conditions of sample path continuity were violated as the covariance is discontinuous at ğ‘¥ = ğ‘¥ .
We may also construct a Gaussian process that almost surely achieves a maximum that is not unique. Consider a random function ğ‘“ de ned on the (compact) interval [0, 4ğœ‹] de ned by the parametric model

ğ‘“ (ğ‘¥) = ğ›¼ cos ğ‘¥ + ğ›½ sin ğ‘¥,

where ğ›¼ and ğ›½ are independent standard normal random variables. Then ğ‘“ has a Gaussian process distribution with

ğœ‡ (ğ‘¥) â‰¡ 0; ğ¾ (ğ‘¥, ğ‘¥ ) = cos(ğ‘¥ âˆ’ ğ‘¥ ).

(. )

Here ğœ‡ is continuous and ğ¾ is HÃ¶lder continuous, and thus ğ‘“ is sample path continuous and almost surely achieves a global maximum. However, ğ‘“ is also periodic with period 2ğœ‹ with probability one and will thus almost surely achieve its maximum twice. Note that the covariance function does not satisfy the conditions outlined in the above theorem, as any input locations separated by 2ğœ‹ have perfectly correlated function values.

. Gaussian process inference is tractable when the observed values are jointly Gaussian distributed with the function of interest ( . ). However, this may not always hold for all relevant information we may receive.
Draft as of January . Feedback welcome: https://bayesoptbook.com/

Although unlikely to matter in practice,

and

allow X to be ğœ-compact and

show that the supremum (rather than the max-

imum) is unique under the same conditions.

..

( ). On the arg max of a Gaus-

sian process. Statistics & Probability Letters

( ): â€“ .

It turns out this naÃ¯ve model of white noise has horrible mathematical properties, but it is su cient for this counterexample.
Let ğ‘„ = â„š âˆ© [0, 1] = {ğ‘ğ‘– } be the rationals in the domain and let ğ‘“ âˆ— be a putative maximum. De ning ğœ™ğ‘– = ğ‘“ (ğ‘ğ‘– ), we must have ğœ™ğ‘– â‰¤ ğ‘“ âˆ— for every ğ‘–; call this event ğ´.
De ne the event ğ´ğ‘˜ by ğ‘“ âˆ— exceeding the rst ğ‘˜ elements of ğ‘„. From independence,
ğ‘˜
Pr(ğ´ğ‘˜ ) = Pr(ğœ™ğ‘– â‰¤ ğ‘“ âˆ—) = Î¦( ğ‘“ âˆ—)ğ‘˜,
ğ‘– =1
so Pr(ğ´ğ‘˜ ) â†’ 0 as ğ‘˜ â†’ âˆ. But {ğ´ğ‘˜ } ğ´, so Pr(ğ´) = 0, and ğ‘“ âˆ— is almost surely not the maximum.

Our counterexample without a unique maximum. Every sample achieves its maximum twice.
inference from jointly Gaussian distributed observations: Â§ . , p.

observations

objective

mean, Gaussian noise

% credible interval, Gaussian noise

Figure . : Regression with observations corrupted with heavy-tailed noise. The triangular marks indicate observations lying beyond the plotted range. Shown is the posterior distribution of an objective function (ground truth plotted in red) modeling the errors as Gaussian. The posterior is heavily a ected by the outliers.

ğ‘(ğ‘¦ | ğ‘¥,ğœ™)

ğ‘¦
ğœ™
A Student-ğ‘¡ error model (solid) with a Gaussian error model (dashed) for reference. The heavier tails of the Student-ğ‘¡ model can better explain large outliers.

..

et al. ( ). Robust Statistical Mod-

eling Using the ğ‘¡ Distribution. Journal of the

American Statistical Association ( ): â€“

.

di erentiability, derivative observations: Â§ . , p.

One obvious limitation is an incompatibility with naturally nonGaussian observations. A scenario particularly relevant to optimization is heavy-tailed noise. Consider the data shown in gure . , where some observations represent extreme outliers. These errors are poorly modeled as Gaussian, and attempting to infer the underlying objective function with the additive Gaussian noise model leads to over tting and poor predictive performance. A Student-ğ‘¡ error model with ğœˆ â‰ˆ 4 degrees of freedom provides a robust alternative:

ğ‘ (ğ‘¦ | ğ‘¥, ğœ™) = T (ğ‘¦; ğœ™, ğœğ‘›2, ğœˆ).

(. )

The heavier tails of this model can better explain large outliers; unfortunately, the non-Gaussian nature of this model also renders exact inference impossible. We will demonstrate how to overcome this impasse.
Constraints on an objective function, such as bounds on given function values, can also provide valuable information during optimization, but many natural constraints cannot be reduced to observations that can be handled in closed form. Several Bayesian optimization policies impose hypothetical constraints on the objective function when designing each observation, requiring inference from intractable constraints even when the observations themselves pose no di culties.
To see how constraints might arise in optimization, consider a Gaussian process belief on a one-dimensional objective ğ‘“, and suppose we wish to condition on ğ‘“ on having a local maximum at a given location ğ‘¥. Assuming the function is twice di erentiable, we can invoke the second-derivative test to encode this information in two constraints:

ğ‘“ (ğ‘¥) = 0; ğ‘“ (ğ‘¥) < 0.

(. )

We can condition a on the rst of these conditions by following our previous discussion. However, no is compatible with the second

true distribution samples

..
Figure . : The probability density function of an example distribution along with samples drawn independently from the distribution. In Monte Carlo approaches, the distribution is e ectively approximated by a mixture of Dirac delta distributions at the sample locations.

condition as ğ‘“ (ğ‘¥) would necessarily have a Gaussian distribution with unbounded support ( . ). We need some other means to proceed.

Non-Gaussian observations: general case
We can address both non-Gaussian observations and constraints with the following general case, which is exible enough to handle a large range of information. As in our discussion on exact inference, suppose there is some vector y sharing a joint Gaussian process distribution with a function of interest ğ‘“ ( . ):

ğ‘ (ğ‘“, y) = GP

ğ‘“ y

;

ğœ‡ m

,

ğ¾ ğœ…

ğœ… C

.

Suppose we receive some information about y in the form of information D inducing a non-Gaussian posterior on y. Here, it is convenient to adopt the language of factor graphs and write the resulting posterior as proportional to the prior weighted by a function ğ‘¡ (y) encoding the available information, which may factorize:

ğ‘ (y | D) âˆ ğ‘ (y) ğ‘¡ (y) = N (y; m, C) ğ‘¡ğ‘– (y).
ğ‘–

(. )

The functions {ğ‘¡ğ‘– } are called factors or local functions that may comprise

a likelihood augmented by any desired (hard or soft) constraints. The

term â€œlocal functionsâ€ arises because each factor often depends only on

a low-dimensional subspace of y, often a single entry.

The posterior on y ( . ) in turn induces a posterior on ğ‘“ : âˆ«

ğ‘ (ğ‘“ | D) = ğ‘ (ğ‘“ | y) ğ‘ (y | D) dy.

(. )

At rst glance, we may hope to resolve this posterior easily as ğ‘ (ğ‘“ | y) is
a Gaussian process ( . â€“ . ). Unfortunately, the non-Gaussian posterior on y usually renders the posterior on ğ‘“ intractable.

Monte Carlo sampling A Monte Carlo approach to approximating the ğ‘“ posterior ( . ) begins by drawing samples from the y posterior ( . ):
{yğ‘– }ğ‘ ğ‘–=1 âˆ¼ ğ‘ (y | D).
Draft as of January . Feedback welcome: https://bayesoptbook.com/

..

et al. ( ). Factor Graphs

and the Sumâ€“Product Algorithm. Trans-

actions on Information Theory ( ): â€“ .

factors, local functions, {ğ‘¡ğ‘– }
For example, when observations are conditionally independent given the corresponding function values, the likelihood factorizes into a product of one-dimensional factors ( . ):
ğ‘ (y | x, ğ“) = ğ‘ (ğ‘¦ğ‘– | ğ‘¥ğ‘–, ğœ™ğ‘– ).
ğ‘–

observations

objective

posterior mean

posterior % credible interval

Figure . : Regression with observations corrupted with heavy-tailed noise. The triangular marks indicate observations lying beyond the plotted range. Shown is the posterior distribution of an objective function (ground truth plotted in red) modeling the errors as Student-ğ‘¡ distributed with ğœˆ = 4 degrees of freedom. The posterior was approximated from 100 000 Monte Carlo samples. Comparing with the additive Gaussian noise model from gure . , this model e ectively ignores the outliers and the t is excellent.

Handbook of Markov Chain Monte Carlo ( ). Chapman & Hall.

.

et al. ( ). Elliptical slice sampling.

.

example: Student-ğ‘¡ observation model

We may generate these by appealing to one of numerous Markov chain Monte Carlo ( ) routines. One natural choice would be elliptical slice sampling, which is speci cally tailored for latent Gaussian models of this form. Samples from a one-dimensional toy example distribution are shown in gure . .
Given posterior samples of y, we may then approximate ( . ) via the standard Monte Carlo estimator

ğ‘(ğ‘“

| D) â‰ˆ

1 ğ‘ 

âˆ‘ğ‘ ï¸
ğ‘– =1

ğ‘(ğ‘“

| yğ‘– ) =

1 ğ‘ 

âˆ‘ğ‘ ï¸
ğ‘– =1

GP

(ğ‘“

;

ğœ‡Dğ‘–

,

ğ¾D ) .

(. )

This is a mixture of Gaussian processes, each of the form in ( . â€“ . ). The posterior mean functions depend on the corresponding y samples, whereas the posterior covariance functions are identical as there is no
dependence on the observed values. In this approximation, the marginal
belief about any function value is then a mixture of univariate Gaussians:

ğ‘ (ğœ™

|

ğ‘¥, D)

â‰ˆ

1 ğ‘ 

âˆ‘ğ‘ ï¸ N (ğœ™; ğœ‡ğ‘–, ğœ2);
ğ‘– =1

ğœ‡ğ‘– = ğœ‡Dğ‘– (ğ‘¥);

ğœ2 = ğ¾D (ğ‘¥, ğ‘¥). (. )

Although slightly more complex than the Gaussian marginals of a Gaus-

sian process, this is often convenient enough for most needs.

A Monte Carlo approximation to the posterior for the heavy-tailed

dataset from gure . is shown in gure . . The observations were

modeled as corrupted by Student-ğ‘¡ errors with ğœˆ = 4 degrees of freedom.

The posterior was approximated using a truly excessive number of sam-

ples (100 000, with a burn-in of 10 000) from the y posterior drawn using

elliptical slice sampling. The outliers in the data are ignored and the

predictive performance is excellent.

..

Gaussian approximate inference

An alternative to sampling is approximate inference, where we make a parametric approximation to the y posterior that yields a tractable posterior on ğ‘“. In particular, if the posterior ( . ) were actually normal,
it would induce a Gaussian process posterior on ğ‘“. This insight is the
basis for most approximation schemes.
In this vein, we proceed by rst â€“ somehow â€“ approximating the true posterior over y with a multivariate Gaussian distribution:

ğ‘ (y | D) â‰ˆ ğ‘(y | D) = N (y; mËœ , CËœ ).

(. )

We are free to design this approximation as we see t. There are several

general-purpose approaches available, distinguished by how they ap-

proach maximizing the delity of tting the true posterior ( . ). These

include the Laplace approximation, Gaussian expectation propagation,

and variational Bayesian inference. The rst two of these methods are

covered in appendix , and

and

provide an exten-

sive survey of these and other approaches in the context of Gaussian

process binary classi cation.

Regardless of the details of the approximation scheme, the high-level

result is the same â€“ the normal approximation ( . ) in turn induces an

approximate Gaussian process posterior on ğ‘“. To demonstrate this, we

consider the posterior on ğ‘“ that would arise from a direct observation of y ( . â€“ . ) and integrate against the approximate posterior ( . ):

âˆ«

ğ‘ (ğ‘“ | D) â‰ˆ ğ‘ (ğ‘“ | y) ğ‘(y | D) dy = GP (ğ‘“ ; ğœ‡D, ğ¾D),

(. )

where

ğœ‡D (ğ‘¥) = ğœ‡ (ğ‘¥) + ğœ… (ğ‘¥) Câˆ’1 (mËœ âˆ’ m);

ğ¾D (ğ‘¥, ğ‘¥ ) = ğ¾ (ğ‘¥, ğ‘¥ ) âˆ’ ğœ… (ğ‘¥) Câˆ’1 (C âˆ’ CËœ ) Câˆ’1ğœ… (ğ‘¥ ).

(. )

For most approximation schemes, the posterior covariance on ğ‘“ simpli es to a nicer, more familiar form. Most approximations to the y
posterior ( . ) yield an approximate posterior covariance of the form

CËœ = C âˆ’ C (C + N) âˆ’1C,

(. )

where N is positive de nite. Although this might appear mysterious, it is actually a natural form: it is the posterior covariance that would result from observing y corrupted by additive Gaussian noise with covariance N ( . ), except we are now free to design the noise covariance to maximize
the t. For approximations of this form ( . ), the approximate posterior covariance function on ğ‘“ simpli es to the more familiar

ğ¾D (ğ‘¥, ğ‘¥ ) = ğ¾ (ğ‘¥, ğ‘¥ ) âˆ’ ğœ… (ğ‘¥) (C + N)âˆ’1ğœ… (ğ‘¥ ).

(. )

To demonstrate the power of approximate inference, we return to our motivating scenario of conditioning a one-dimensional process on having a local maximum at an identi ed point ğ‘¥, which we can achieve by

Draft as of January . Feedback welcome: https://bayesoptbook.com/

Laplace approximation: Â§ . , p.
Gaussian expectation propagation: Â§ . p.

.

and . .

( ). App-

proximations for Binary Gaussian Process

Classi cation. Journal of Machine Learning Re-

search (Oct): â€“ .

example: conditioning on a local optimum

posterior mean

posterior % credible interval

posterior sample

ğ‘¥

ğ‘¥

ğ‘¥

ğ‘“ (ğ‘¥) = 0

ğ‘“ (ğ‘¥) < 0

Figure . : Approximately conditioning a Gaussian process to have a local maximum at the marked point ğ‘¥. We show each stage of the conditioning process with a sample drawn from the corresponding posterior. We begin with the unconstrained process (left), which we condition on the rst derivative being zero at ğ‘¥ using exact inference (middle). Finally we use Gaussian expectation propagation to approximately condition on the second derivative being negative at ğ‘¥.

derivative observations: Â§ . , p.

.

and . .

( ). Gener-

alized Linear Models. Chapman & Hall.

conditioning the rst derivative to be zero and constraining the second derivative to be negative at ğ‘¥ ( . ). We illustrate an approximation to the resulting posterior step-by-step in gure . , beginning with the example Gaussian process in the left-most panel. We rst condition the process on the rst derivative observation ğ‘“ (ğ‘¥) = 0 using exact inference; the result is shown in the middle panel. Both the updated posterior mean and the sample re ect this information; however, the sample displays a local minimum at ğ‘¥, as the second-derivative constraint has not yet been addressed.
To incorporate the second-derivative constraint, we begin with this updated and consider the second derivative â„ = ğ‘“ (ğ‘¥), which is Gaussian distributed prior to the constraint ( . ):
ğ‘ (â„) = N (â„; ğ‘š, ğ‘ 2).
The negativity constraint induces a posterior on â„ incorporating the factor [â„ < 0] ( . ); see gure . :
ğ‘ (â„ | D) âˆ ğ‘ (â„) [â„ < 0].
The result is a truncated normal posterior on â„. We may use Gaussian expectation propagation, which is especially convenient for handling bound constraints of this form, to produce a Gaussian approximation:
ğ‘ (â„ | D) â‰ˆ ğ‘(â„ | D) = N (â„; ğ‘šËœ , ğ‘ Ëœ2).
Incorporating the updated belief on â„ into the Gaussian process ( . ) yields the approximate posterior in the right-most panel of gure . . Although there is still some residual probability that the second derivative is positive at ğ‘¥ in the approximate posterior (approximately %; see gure . ), the belief re ects the desired information reasonably faithfully.

prior, ğ‘ (â„) constraint factor, [â„ < 0]

..
true posterior, ğ‘ (â„ | D) âˆ ğ‘ (â„) [â„ < 0] Gaussian approximation

0

0

Figure . : A demonstration of Gaussian expectation propagation. On the left we have a Gaussian belief on the second derivative, ğ‘ (â„). We wish to constrain this value to be negative, introducing a step-function factor encoding the constraint, [â„ < 0]. The resulting distribution is non-Gaussian (right), but we can approximate it with a
Gaussian, which induces an updated posterior on the function approximately incorporating the constraint.

Going beyond this example, we may use the approach outlined above to realize a general framework for Bayesian nonlinear regression by combining a prior on a latent function with an observation model appropriate for the task at hand, then approximating the posterior as desired. The convenience and modeling exibility o ered by Gaussian processes can easily justify any extra e ort required for approximating the posterior. This can be seen as a nonlinear extension of the well-known family of generalized linear models.
This approach is quite popular and been realized countless times. Notable examples include binary classi cation using a logistic or probit observation model, modeling point processes as a nonhomogenous Poisson process with unknown intensity, , and robust regression with heavy-tailed additive noise such as Laplace or Student-ğ‘¡ , distributed errors. With regard to the latter and our previous heavy-tailed noise example, a Laplace approximation to the posterior for the data in gures . â€“ . with the Student-ğ‘¡ observation model produces an approximate posterior in excellent agreement with the Monte Carlo approximation in gure . ; see gure . . The cost of approximate inference in this case was dramatically (several orders of magnitude) cheaper than Monte Carlo sampling.

.

Gaussian processes have been studied â€“ in one form or another â€“ for over

years. Although we have covered a lot of ground in this chapter, we

have only scratched the surface of an expansive body of literature. A good

entry point to that literature is

and

â€™s monograph,

which focuses on machine learning applications of Gaussian processes

but also covers their theoretical underpinnings and properties in depth.

A good companion to this work is the book of

and

, which

takes a deep dive into the properties and geometry of sample paths,

including statistical properties of their maxima.

Draft as of January . Feedback welcome: https://bayesoptbook.com/

.

and . .

( ). App-

proximations for Binary Gaussian Process

Classi cation. Journal of Machine Learning Re-

search (Oct): â€“ .

.

et al. ( ). Log Gaussian Cox Pro-

cesses. Scandinavian Journal of Statistics ( ):

â€“.

..

et al. ( ). Tractable Nonpara-

metric Bayesian Inference in Poisson Pro-

cesses with Gaussian Process Intensities.

.

. ( ). Gaussian Process Models for Robust Regression, Classi cation, and Reinforcement Learning. PhD thesis. Technische UniversitÃ¤t Darmstadt.[Â§ . ]

. . ( ). Monte Carlo Implementation
of Gaussian Process Models for Bayesian Regression and Classi cation. Technical report ( ). Department of Statistics, University of Toronto.

.

et al. ( ). Robust Gaussian Pro-

cess Regression with a Student-ğ‘¡ Likelihood.

Journal of Machine Learning Research ( ):

â€“.

identi ed an early application of s

by

for nonlinear regression:

.

( ). Bayesian Numerical Anal-

ysis. In: Statistical Decision Theory and Related

Topics .

.

( ). Calcul des probabilitÃ©s.

Gauthierâ€“Villars.

..

and . . .

( ).

Gaussian Processes for Machine Learning.

Press.

..

and . .

( ). Random

Fields and Geometry. Springerâ€“Verlag.

observations

objective

posterior mean

posterior % credible interval

Figure . : A Laplace approximation to the posterior from gure . .

inference from arbitrary joint Gaussian observations: Â§ . , p.
interpretation of posterior moments: Â§ . , p.
joint Gaussian processes: Â§ . , p.
extensions and other settings: chapter , p.
continuity: Â§ . , p. di erentiability: Â§ . , p.

Fortunately, the basic de nitions and properties covered in Â§ . and exact inference procedure covered in Â§ . already provide a su cient foundation for the majority of practical applications of Bayesian optimization. This material also provides su cient background knowledge for the majority of the remainder of the book. However, we wish to underscore the major results from this chapter at a high level.
â€¢ Gaussian processes extend the multivariate normal distribution to model functions on in nite domains. As in the nite-dimensional case, Gaussian processes are speci ed by their rst two moments â€“ a mean function and a positive-de nite covariance function â€“ which endow any nite set of function values with a multivariate normal distribution ( . â€“ . ).
â€¢ Conditioning a Gaussian process on function observations that are either exact or corrupted by additive Gaussian noise yields a Gaussian process posterior with updated moments re ecting the assumptions in the prior and the information in the observations ( . â€“ . ).
â€¢ In fact, we may condition a Gaussian process on the observation of any observations sharing a joint Gaussian distribution with the function of interest.
â€¢ In the case of exact inference, the posterior moments of a Gaussian process can be rewritten in terms of correlations among function values and ğ‘§-scores of the observed values in a manner that may be more intuitive than the standard formulas.
â€¢ We may extend Gaussian processes to jointly model multiple correlated functions via careful bookkeeping, a construction known as a joint Gaussian process. Joint s are widely used in optimization settings involving multiple objectives and/or cheaper surrogates for an expensive objective.
â€¢ Continuity and di erentiability of Gaussian process sample paths can be guaranteed under mild assumptions on the mean and covariance functions. When these functions are su ciently di erentiable, a distributed function shares a joint distribution with its gradient ( . ).

This joint distribution allows us to condition a Gaussian process on (potentially noisy) derivative observations.
â€¢ The existence and uniqueness of global maxima for Gaussian process sample paths can be guaranteed under mild assumptions on the mean and covariance functions. Establishing these properties ensures that the location ğ‘¥âˆ— and value ğ‘“ âˆ— of the global maximum are well-founded random variables, which will be critical for some optimization methods introduced later in the book.
â€¢ Inference from non-Gaussian observations and constraints is possible via Monte Carlo sampling or Gaussian approximate inference.
Looking forward, the focus of this chapter has been on theoretical rather than practical properties of Gaussian processes. A huge outstanding question is how to actually design a Gaussian process to model a given system. This will be our focus for the next two chapters. In the next chapter, we will explore model construction, and in the following chapter we will consider model assessment in light of available data.
Finally, we have not yet discussed any computational issues inherent to Gaussian process inference, including, most importantly, how the cost of computing the posterior grows with respect to the number of observations. We will discuss implementation details and scaling in a dedicated chapter later in the book.

..
derivative observations: Â§ . , p. existence and uniqueness of global maxima: Â§ . , p. In particular, policies grounded in information theory under the umbrella of â€œentropy search.â€ See Â§ . , p. for more. inference with non-Gaussian observations and constraints: Â§ . , p.
implementation and scaling of Gaussian process inference: chapter , p.

Draft as of January . Feedback welcome: https://bayesoptbook.com/

MODELING WITH GAUSSIAN PROCESSES

Bayesian optimization relies on a faithful model of the system of in-

terest to make well-informed decisions. In fact, even more so than the

details of the optimization policy, the delity of the underlying model of

the objective function is the most decisive factor determining optimiza-

tion performance. This has been long acknowledged, with

for

example commenting in his seminal work that:

The development of some system of a priori distributions suitable for di erent classes of the function ğ‘“ is probably the most important problem in the application of [the] Bayesian approach to. . . global optimization.

The importance of careful modeling has not waned in the intervening years, but our capacity for building sophisticated models has improved.
Recall our approach to modeling observations obtained during optimization combines a prior process for a (perhaps not directly observable) objective function ( . ) and an observation model linking the values of the objective to measured values ( . ). Both distributions must be speci-
ed before we can derive a posterior belief about the objective function ( . ) and predictive distribution for proposed observations ( . ), which together serve as the key enablers of Bayesian optimization policies.
In practice, the choice of observation model is often noncontroversial, and our running prototypes of exact observation and additive Gaussian noise su ce for many systems. The bulk of modeling e ort is thus spent crafting the prior process. Although specifying a Gaussian process is seemingly as simple as choosing a mean and covariance function, it can be di cult to intuit appropriate choices without a great deal of knowledge about the system of interest. As an alternative to prior knowledge, we may appeal to a data-driven approach, where we establish a space of candidate models and search through this space for those o ering the best explanation of available data. Almost all Gaussian process models used in practice are designed in this manner, and we will lay the groundwork for this approach in this chapter and the next.
As a Gaussian process is speci ed by its rst two moments, datadriven model design boils down to searching for the prior mean and covariance functions most harmonious with our observations. This can be a daunting task as the space of possibilities is limitless. However, we do not need to begin from scratch: there are mean and covariance functions available o -the-shelf for modeling a variety behavioral archetypes, and by systematically combining these components we may model functions with a rich variety of behavior. We will explore the world of possibilities in this chapter, while addressing details important to optimization.
Once we have established a space of candidate models, we will require some mechanism to di erentiate possible choices based on their merits, a process known as model assessment that we will explore at length in the next chapter. We will begin the present discussion by revisiting the topic

This material will be published by Cambridge University Press as Bayesian Optimization. This prepublication version is free to view and download for personal use only. Not for redistribution, resale, or use in derivative works. Â©Roman Garnett .

.

( ). On Bayesian Methods for

Seeking the Extremum. Optimization Tech-

niques Technical Conference.

Bayesian inference of the objective function: Â§ . , p.

However, we may not be certain about some details, such as the scale of observation noise, an issue we will address in the next chapter.

chapter : model assessment, selection, and averaging, p.

Figure . : The importance of the prior mean function in determining sample path behavior. The models in the rst two panels di er in their mean function but share the same covariance function. Sample path behavior is identical up to translation. The model in the third panel features the same mean function as the rst panel but a di erent covariance function. Samples exhibit dramatically di erent behavior.

of prior mean and covariance functions with an eye toward practical utility.

impact of prior mean on sample paths impact of prior mean on posterior mean

.
Recall the mean function of a Gaussian process speci es the expected value of an arbitrary function value ğœ™ = ğ‘“ (ğ‘¥):
ğœ‡ (ğ‘¥) = ğ”¼[ğœ™ | ğ‘¥].
Although this is obviously a fundamental concern, the choice of prior mean function has received relatively little consideration in the Bayesian optimization literature.
There are several reasons for this. To begin, it is actually the covariance function rather than the mean function that largely determines the behavior of sample paths. This should not be surprising: the mean function only a ects the marginal distribution of function values, whereas the covariance function can further modify the joint distribution of function values. To elaborate, consider an arbitrary Gaussian process GP (ğ‘“ ; ğœ‡, ğ¾). Its sample paths are distributed identically to those from the corresponding centered process ğ‘“ âˆ’ ğœ‡, after shifting pointwise by ğœ‡. Therefore the sample paths of any Gaussian process with the same covariance function are e ectively the same up to translation, and it is the covariance function determining their behavior otherwise; see the demonstration in gure . .
It is also important to understand the role of the prior mean function in the posterior process. Suppose we condition a Gaussian process GP (ğ‘“ ; ğœ‡, ğ¾) on the observation of a vector y with marginal distribution ( . ) and cross covariance function ( . )
ğ‘ (y) = N (y; m, C); ğœ… (ğ‘¥) = cov[y, ğœ™ | ğ‘¥].
The prior mean in uences the posterior process only through the posterior mean ( . ):
ğœ‡D (ğ‘¥) = ğœ‡ (ğ‘¥) + ğœ… (ğ‘¥) Câˆ’1 (y âˆ’ m).

extrapolation

interpolation

extrapolation

We can roughly understand the behavior of the posterior mean by identifying two regimes determined by the strength of correlation between a given function value and the observations. In â€œinterpolatoryâ€ regions, where function values have signi cant correlation with one-or-more observed value, the posterior mean is mostly determined by the data rather than the prior mean. On the other hand, in â€œextrapolatoryâ€ regions, where ğœ… (ğ‘¥) â‰ˆ 0, the data have little in uence and the posterior mean e ectively equals the prior mean. Figure . demonstrates this e ect.

Constant mean function
The primary impact of the prior mean on our predictions â€“ and on an optimization policy informed by these predictions â€“ is in the extrapolatory regime. However, extrapolation without strong prior knowledge can be a dangerous business. As a result, in Bayesian optimization, the prior mean is often taken to be a constant:

ğœ‡(ğ‘¥; ğ‘) â‰¡ ğ‘,

( .)

in order to avoid any unwanted bias on our decisions caused by spurious structure in the prior process. This simple choice is supported empirically by a study comparing optimization performance across a range of problems as a function of the choice of prior mean.
When adopting a constant mean, the value of the constant ğ‘ is usually treated as a parameter to be estimated or (approximately) marginalized, as we will discuss in the next chapter. However, we can actually do better in some cases. Consider a parametric Gaussian process prior with constant mean ( . ) and arbitrary covariance function:

ğ‘ (ğ‘“ | ğ‘) = GP (ğ‘“ ; ğœ‡ â‰¡ ğ‘, ğ¾),

and suppose we place a normal prior on ğ‘:

ğ‘ (ğ‘) = N (ğ‘; ğ‘, ğ‘2).

(.)

Then we can marginalize the unknown constant mean exactly to derive

the marginal Gaussian process

âˆ«

ğ‘ (ğ‘“ ) = ğ‘ (ğ‘“ | ğ‘) ğ‘ (ğ‘) dğ‘ = GP (ğ‘“ ; ğœ‡ â‰¡ ğ‘, ğ¾ + ğ‘2),

(.)

Draft as of January . Feedback welcome: https://bayesoptbook.com/

..
Figure . : The in uence of the prior mean on the posterior mean. We show two Gaussian process posteriors di ering only in their prior mean functions, shown as dashed lines. In the â€œinterpolatoryâ€ region between the observations, the posterior means are mostly determined by the data, but devolve to the respective prior means when extrapolating outside this region.
behavior in interpolatory and extrapolatory regions

.

et al. ( ). What do you Mean?

The Role of the Mean Function in Bayesian

Optimization.

.

marginalizing constant prior mean

model selection and averaging: Â§Â§ . â€“ . , p.

Noting that ğ‘ and ğ‘“ form a joint Gaussian process, we may perform inference as described in Â§ . , p. to reveal their joint posterior.

The basis functions can be arbitrarily complex, such as the output layer of a deep neural network:

.

et al. ( ). Scalable Bayesian Opti-

mization Using Deep Neural Networks.

.

basis functions, ğ weight vector, ğœ·

.â€™

( ). Curve Fitting and Optimal

Design for Prediction. Journal of the Royal Sta-

tistical Society Series B (Methodological) ( ):

â€“.

..

and . . .

( ).

Gaussian Processes for Machine Learning.

Press. [Â§ . ]

For an example of such modeling in physics, where the mean function was taken to be the output of a physically informed model, see:

..

et al. ( ). Physics makes

the di erence: Bayesian optimization and ac-

tive learning via augmented Gaussian process.

arXiv: 2108.10280 [physics.comp-ph].

This mean function was proposed in the con-
text of Bayesian optimization (with diagonal A) by

.

et al. ( ). Scalable Bayesian Opti-

mization Using Deep Neural Networks.

,

who also proposed appropriate priors for A and b. The mean was also proposed in the related context of Bayesian quadrature (see Â§ . , p. ) by

.

( ). Variational Bayesian Monte

Carlo. eur

.

where the uncertainty in the mean has been absorbed into the prior covariance function. We may now use this prior directly, avoiding any estimation of ğ‘. The unknown mean will be automatically marginalized in both the prior and posterior process, and we may additionally derive the posterior belief over ğ‘ given data if it is of interest.

Linear combination of basis functions
We may extend the above result to marginalize the weights of an arbitrary linear combination of basis functions under a normal prior, making this a particularly convenient class of mean functions. Namely, consider a parametric mean function of the form

ğœ‡(ğ‘¥; ğœ·) = ğœ· ğ (ğ‘¥),

(.)

where the vector-valued function ğ : X â†’ â„ğ‘› de nes the basis functions and ğœ· is a vector of weights.
Now consider a parametric Gaussian process prior with a mean function of this form ( . ) and arbitrary covariance function ğ¾. Placing a multivariate normal prior on ğœ·,

ğ‘ (ğœ·) = N (ğœ·; a, B),

(.)

and marginalizing yields the marginal prior, ,

ğ‘ (ğ‘“ ) = GP (ğ‘“ ;ğ‘š, ğ¶),

where

ğ‘š(ğ‘¥) = a ğ (ğ‘¥); ğ¶ (ğ‘¥, ğ‘¥ ) = ğ¾ (ğ‘¥, ğ‘¥ ) + ğ (ğ‘¥) Bğ (ğ‘¥ ). ( . )

We may recover the constant mean case above by taking ğœ“ (ğ‘¥) â‰¡ 1.

Other options

We stress that a constant or linear mean function is by no means necessary, and when a system is understood su ciently well to suggest a plausible alternative â€“ perhaps the output of a baseline predictive model â€“ it should be strongly considered. However, it is hard to provide general advice, as this modeling will be situation dependent.
One option that might be a reasonable choice in some optimization contexts is a concave quadratic mean:

ğœ‡ (x; A, b, ğ‘) = (x âˆ’ b) Aâˆ’1 (x âˆ’ b) + ğ‘,

(.)

where A â‰º 0. This mean encodes that values near b (according to the Mahalanobis distance ( . )) are expected to be higher than those farther away and could reasonably model an objective function expected to be â€œbowl-shapedâ€ to a rst approximation. The middle panel of gure . incorporates a mean of this form; note that the maxima of sample paths are of course not constrained to agree with that of the prior mean.

..

.
The prior covariance function determines the covariance between the function values corresponding to a pair of input locations ğ‘¥ and ğ‘¥ :

ğ¾ (ğ‘¥, ğ‘¥ ) = cov[ğœ™, ğœ™ | ğ‘¥, ğ‘¥ ].

(.)

The covariance function determines fundamental properties of sample path behavior, including continuity, di erentiability, and aspects of the global optima, as we have already seen. Perhaps more so than the mean function, careful design of the covariance function is critical to ensure
delity in modeling. We will devote considerable discussion to this topic, beginning with some important properties and moving on to useful examples and mechanisms for systematically modifying and composing multiple covariance functions together to model complex behavior.
After appropriate normalization, a covariance function ğ¾ may be loosely interpreted as a measure of similarity between points in the domain. Namely, given ğ‘¥, ğ‘¥ âˆˆ X, the correlation between the corresponding function values is

ğœŒ = corr[ğœ™, ğœ™ | ğ‘¥, ğ‘¥ ] = âˆšï¸ ğ¾ (ğ‘¥, ğ‘¥ )

,

(.)

ğ¾ (ğ‘¥, ğ‘¥) ğ¾ (ğ‘¥, ğ‘¥ )

and we may interpret the strength of this dependence as a measure of similarity between the input locations. This intuition can be useful, but some caveats are in order. To begin, note that correlation may be negative, which might be interpreted as indicating dis-similarity as the function values react to information with opposite sign.
Further, for a proposed covariance function ğ¾ to be admissible, it must satisfy two global consistency properties ensuring that the collection of random variables comprising ğ‘“ are able to satisfy the purported relationships. First, we can immediately deduce from its de nition ( . ) that ğ¾ must be symmetric in its inputs. Second, the covariance function must be positive semide nite; that is, given any nite set of points x âŠ‚ X, the Gram matrix ğ¾ (x, x) must have only nonnegative eigenvalues.
To illustrate how positive semide niteness ensures statistical validity, note that a direct consequence is that ğ¾ (ğ‘¥, ğ‘¥) = var[ğœ™ | ğ‘¥] â‰¥ 0, and thus marginal variance is always nonnegative. On a slightly less trivial level, consider a pair of points x = (ğ‘¥, ğ‘¥ ) and normalize the corresponding Gram matrix ğšº = ğ¾ (x, x) to yield the correlation matrix:

P = corr[ğ“ | x] =

1 ğœŒ

ğœŒ 1

,

where ğœŒ is given by ( . ). For this matrix to be valid, we must have ğœŒ âˆˆ [âˆ’1, 1]. This happens precisely when P is positive semide nite, as its eigenvalues are 1 Â± ğœŒ. Finally, noting that P is congruent to ğšº, we conclude the implied correlations are consistent if and only if ğšº is positive
semide nite. With more than two points, the positive semide niteness of ğ¾ ensures similar consistency at higher orders.

Draft as of January . Feedback welcome: https://bayesoptbook.com/

sample path continuity: Â§ . , p. sample path di erentiability: Â§ . , p. existence and uniqueness of global maxima: Â§ . , p.
correlation between function values, ğœŒ

symmetry and positive semide niteness
Symmetry guarantees the eigenvalues are real. consequences of positive semide niteness

We ğ‘†ğ‘–ğ‘–

=haâˆšvÎ£eğ‘–ğ‘–ğšº.

=

SPS,

where

S

is

diagonal

with

Figure . : Left: a sample from a stationary Gaussian process in two dimensions. The joint distribution of function values is translation- but not rotationinvariant, as the function tends to vary faster in some directions than others. Right: a sample from an isotropic process. The joint distribution of function values is both translation- and rotationinvariant.

stationary and anisotropic

stationary and isotropic

stationary covariance function, ğ¾ (ğ‘¥ âˆ’ ğ‘¥ )
Of course this de nition requires ğ‘¥ âˆ’ ğ‘¥ to be well de ned. This is trivial in Euclidean spaces; a fairly general treatment for more exotic spaces would assume an abelian group structure on X with binary operation + and inverse âˆ’ and de ne ğ‘¥ âˆ’ ğ‘¥ = ğ‘¥ + (âˆ’ğ‘¥ ).

isotropic covariance function, ğ¾ (ğ‘‘)

.

( ). Monotone Funktionen,

Stieltjessche Integrale und harmonische Anal-

yse. Mathematische Annalen : â€“ .

We do not quote the most general version of the theorem here; the result can be extended to complex-valued covariance functions on arbitrary locally compact abelian groups if necessary. It is remarkably universal.

Stationarity, isotropy, and Bochnerâ€™s theorem
Some covariance functions exhibit structure giving rise to certain computational bene ts. Namely, a covariance function ğ¾ (ğ‘¥, ğ‘¥ ) that only depends on the di erence ğ‘¥ âˆ’ ğ‘¥ is called stationary. When convenient, we will abuse notation and write a stationary covariance function in terms of a single input, writing ğ¾ (ğ‘¥ âˆ’ ğ‘¥ ) for ğ¾ (ğ‘¥, ğ‘¥ ) = ğ¾ (ğ‘¥ âˆ’ ğ‘¥, 0). If a
has a stationary covariance function and constant mean function ( . ), then the process itself is also called stationary. A consequence of stationarity is that the distribution of any set of function values is invariant under translation; that is, the function â€œacts the sameâ€ everywhere from a statistical viewpoint. The left panel of gure . shows a sample from a 2ğ‘‘ stationary , demonstrating this translation-invariant behavior.
Stationarity is a convenient assumption when modeling, as de ning the local behavior around a single point su ces to specify the global behavior of an entire function. Many common covariance functions have this property as a result. However, this may not always be a valid assumption in the context of optimization, as an objective function may for example exhibit markedly di erent behavior near the optimum than elsewhere. We will shortly see some general approaches for addressing nonstationarity when appropriate.
If X âŠ‚ â„ğ‘›, a covariance function ğ¾ (ğ‘¥, ğ‘¥ ) only depending on the Euclidean distance ğ‘‘ = |ğ‘¥ âˆ’ğ‘¥ | is called isotropic. Again, when convenient, we will notate such a covariance with ğ¾ (ğ‘‘). Isotropy is a more restrictive assumption than stationarity â€“ indeed it trivially implies stationarity â€“ as it implies the covariance is invariant to both translation and rotation, and thus the function has identical behavior in every direction from every point. An example sample from a 2ğ‘‘ isotropic is shown in the right panel of gure . . Many of the standard covariance functions we will de ne shortly will be isotropic on rst de nition, but we will again develop generic mechanisms to modify them in order to induce anisotropic behavior when desired.
â€™s theorem is an landmark result characterizing stationary covariance functions in terms of their Fourier transforms: ,

..

Theorem (

, ). A continuous function ğ¾ : â„ğ‘› â†’ â„ is positive

semide nite (that is, represents a stationary covariance function) if and

only if we have

âˆ«

ğ¾ (x) = exp(2ğœ‹ğ‘–x ğƒ ) dğœˆ,

where ğœˆ is a nite, positive Borel measure on â„ğ‘›. Further, this measure
is symmetric around the origin; that is, ğœˆ (ğ´) = ğœˆ (âˆ’ğ´) for any Borel set ğ´ âŠ‚ â„ğ‘›, where âˆ’ğ´ is the â€œnegationâ€ of ğ´: âˆ’ğ´ = {âˆ’ğ‘ | ğ‘ âˆˆ ğ´}.

To summarize,

â€™s theorem states that the Fourier transform of

any stationary covariance function on â„ğ‘› is proportional to a probability

measure and vice versa; the constant of proportionality is ğ¾ (0). The

measure ğœˆ corresponding to ğ¾ is called the spectral measure of ğ¾. When

a corresponding density function ğœ… exists, it is called the spectral density

of ğ¾ and forms a Fourier pair with ğ¾:

âˆ«

âˆ«

ğ¾ (x) = exp(2ğœ‹ğ‘–x ğƒ ) ğœ… (ğƒ ) dğƒ ; ğœ… (ğƒ ) = exp(âˆ’2ğœ‹ğ‘–x ğƒ ) ğ¾ (x) dx.

(. ) The symmetry of the spectral measure implies a similar symmetry in the spectral density: ğœ… (ğƒ ) = ğœ… (âˆ’ğƒ ) for all ğƒ âˆˆ â„ğ‘›.
â€™s theorem is surprisingly useful in practice, allowing us to approximate an arbitrary stationary covariance function by approximating (e.g., by modeling or sampling from) its spectral density. This is the basis of the spectral mixture covariance described in the next section, as well as the sparse spectrum approximation scheme, which facilitates the computation of some Bayesian optimization policies.

spectral measure, ğœˆ spectral density, ğœ…
symmetry of spectral density
sparse spectrum approximation: Â§ . , p.

.
It can be di cult to de ne a new covariance function for a given scenario de novo, as the positive-semide nite criterion can be nontrivial to guarantee for what might otherwise be an intuitive notion of similarity. In practice, it is common to instead construct covariance functions by combining and transforming established â€œbuilding blocksâ€ modeling various atomic behaviors while following rules guaranteeing the result will be valid. We describe several useful examples below.
Our presentation will depart from most in that several of the covariance functions below will initially be de ned without parameters that some readers may be expecting. We will shortly demonstrate how coupling these covariance functions with particular transformations of the function domain and output gives rise to common covariance function parameters such as characteristic length scales and output scales.

For a more complete survey, see

..

and . . .

( ).

Gaussian Processes for Machine Learning.

Press. [chapter ]

The MatÃ©rn family If there is one class of covariance functions to be familiar with, it is the MatÃ©rn family. This is a versatile family of covariance functions for modeling isotropic behavior on Euclidean domains X âŠ‚ â„ğ‘› of any
Draft as of January . Feedback welcome: https://bayesoptbook.com/

ğœˆ = 1/2, ( . )

ğœˆ = 3/2, ( . )

ğœˆ = 5/2, ( . )

Figure . : Samples from centered Gaussian processes with the MatÃ©rn covariance function with di erent values of the smoothness parameter ğœˆ. Sample paths with ğœˆ = 1/2 are continuous but not di erentiable; incrementing this
parameter by one unit increases the number of continuous derivatives by one.

sample path di erentiability: Â§ . , p. In theoretical contexts, general values for the smoothness parameter ğœˆ âˆˆ ğ‘…>0 are considered, but lead to unwieldy expressions ( . ).
exponential covariance function
Ornsteinâ€“Uhlenbeck ( ) process

Samples from a centered Gaussian process with squared exponential covariance ğ¾ .

..

( ). Interpolation of Spatial Data:

Some Theory for Kriging. Springerâ€“Verlag.

[Â§ . ]

desired degree of smoothness, in terms of the di erentiability of sample paths. The MatÃ©rn covariance ğ¾M(ğœˆ) depends on a parameter ğœˆ âˆˆ â„>0 determining this smoothness; sample paths from a centered Gaussian process with this covariance are ğœˆ âˆ’ 1 times continuously di erentiable. In practice ğœˆ is almost always taken to be a half-integer, in which case
the expression for the covariance assumes a simple form as a function of the Euclidean distance ğ‘‘ = |ğ‘¥ âˆ’ ğ‘¥ |.
To begin with the extremes, the case ğœˆ = 1/2 yields the so-called
exponential covariance:

ğ¾M1/2 (ğ‘¥, ğ‘¥ ) = exp(âˆ’ğ‘‘).

(. )

Sample paths from a centered Gaussian process with exponential covariance are continuous but nowhere di erentiable, which is perhaps too rough to be interesting in most optimization contexts. However, this covariance is often encountered in historical literature. In the onedimensional case X âŠ‚ â„, a Gaussian process with this covariance is known as a Ornsteinâ€“Uhlenbeck ( ) process and satis es a continuoustime Markov property that renders its posterior moments particularly convenient.
Taking the limit of increasing smoothness ğœˆ â†’ âˆ yields the squared exponential covariance from the previous chapter:

ğ¾

(ğ‘¥, ğ‘¥

)

=

exp

âˆ’

1 2

ğ‘‘

2

.

(. )

We will refer to the MatÃ©rn and the limiting case of the squared expo-

nential covariance functions together as the MatÃ©rn family. The squared

exponential covariance is without a doubt the most prevalent covari-

ance function in the statistical and machine learning literature. However,

it may not always be a good choice in practice. Sample paths from a

centered Gaussian process with squared exponential covariance are in-

nitely di erentiable, which has been ridiculed as an absurd assumption

for most physical processes.

does not mince words on this, start-

ing o a three-sentence â€œsummary of practical suggestionsâ€ with â€œuse

the MatÃ©rn modelâ€ and devoting signi cant e ort to discouraging the

use of the squared exponential in the context of geostatistics.

..

Between these extremes are the cases ğœˆ = 3/2 and ğœˆ = 5/2, which

respectively model once- and twice-di erentiable functions:

âˆš

âˆš

ğ¾M3/2 (ğ‘¥, ğ‘¥ ) = ğ¾M5/2 (ğ‘¥, ğ‘¥ ) =

1 + 3ğ‘‘ exp âˆ’ 3ğ‘‘ ;

1

+

âˆš 5ğ‘‘

+

5 3

ğ‘‘

2

âˆš exp âˆ’ 5ğ‘‘

.

(. ) (. )

Figure . illustrates samples from centered Gaussian processes with di erent values of the smoothness parameters ğœˆ. The ğœˆ = 5/2 case in particular has been singled out as a prudent o -the-shelf choice for Bayesian optimization when no better alternative is obvious.

.

et al. ( ). Practical Bayesian Op-

timization of Machine Learning Algorithms.

eur

.

The spectral mixture covariance

Covariance functions in the MatÃ©rn family express fairly simple corre-

lation structure, with the covariance dropping monotonically to zero

as the distance ğ‘‘ = |ğ‘¥ âˆ’ ğ‘¥ | increases. All di erences in sample path

behavior such as di erentiability, etc. are expressed entirely through

nuances in the tail behavior of the covariance functions; see the gure

in the margin.

The Fourier transforms of these covariances are also broadly com-

parable: all are proportional to unimodal distributions centered on the

origin. However,

â€™s theorem indicates that there is a vast world

of stationary covariance functions indexed by the entire space of sym-

metric spectral measures, which may have considerably more complex

structure. Several authors have sought to exploit this characterization to

build stationary covariance functions with virtually unlimited exibility.

A notable contribution in this direction is the spectral mixture covari-

ance function proposed by

and

. The idea is simple but

powerful: we parameterize a space of stationary covariance functions

by some suitable family of mixture distributions in the Fourier domain

representing their spectral density. The parameters of this spectral mix-

ture distribution specify a covariance function via the correspondence

in ( . ), and we can make the resulting family as rich as desired by

adjusting the number of components in the mixture.

and

proposed Gaussian mixtures for the spectral density, which are universal

approximators and have a convenient Fourier transform. We de ne a

Gaussian mixture spectral density ğœ… as

âˆ‘ï¸ ğ‘˜ (ğƒ ) = ğ‘¤ğ‘– N (ğƒ ; ğğ‘–, ğšºğ‘– );
ğ‘–

ğœ…(ğƒ) =

1 2

ğ‘˜ (ğƒ ) + ğ‘˜ (âˆ’ğƒ )

,

where the indirect construction via ğ‘˜ ensures the required symmetry. Note that the weights {ğ‘¤ğ‘– } must be positive but need not sum to unity. Taking the inverse Fourier transform ( . ), the corresponding covariance
function is

ğ¾

x, x âˆ‘ï¸

;

{ğ‘¤ğ‘–

},

{ğğ‘–

},

{ğšºğ‘–

}

=

ğ‘¤ğ‘– exp âˆ’2ğœ‹2 (x âˆ’ x ) ğšºğ‘– (x âˆ’ x ) cos 2ğœ‹ (x âˆ’ x ) ğğ‘– . ( . )

ğ‘–

Draft as of January . Feedback welcome: https://bayesoptbook.com/

ğ¾ (ğ‘¥, ğ‘¥ )

ğ¾
ğ¾M3/2 ğ¾M1/2

ğ‘‘
Some members of the MatÃ©rn family and the squared exponential covariance as a function of the distance between inputs. All decay to zero correlation as distance increases.

..

and . .

( ). Gaussian

Process Kernels for Pattern Discovery and Ex-

trapolation.

.

Samples from centered Gaussian processes with two realizations of a Gaussian spectral mixture covariance function, o ering a glimpse into the exibility of this class.

Inspecting this expression, we can see that every covariance function induced by a Gaussian mixture spectral density is in nitely di erentiable, and one might object to this choice on the grounds of overly smooth sample paths. This can be mitigated by using enough mixture components to induce su ciently complex structure in the covariance (on the order of âˆ¼5 is common). Another option would be to use a di erent family of spectral distributions; for example, a mixture of Cauchy distributions would induce a family of continuous but nondi erentiable covariance functions analogous to the exponential covariance ( . ), but this idea has not been explored.

Independence is usual but not necessary; an arbitrary joint prior would add a term of 2b x to ( . ), where b = cov[ğœ·, ğ›½ ].
linear basis functions: Â§ . , p.
Samples from a centered Gaussian process with linear covariance ğ¾ .

Linear covariance function
Another useful covariance function arises from a Bayesian realization of linear regression. Let the domain be Euclidean, X âŠ‚ â„,ğ‘› and consider the model
ğ‘“ (x) = ğ›½ + ğœ· x,
where we have abused notation slightly to distinguish the constant term from the remaining coe cients. Following our discussion on linear basis functions, if we take independent normal priors on ğ›½ and ğœ·:
ğ‘ (ğ›½) = N (ğ›½; ğ‘, ğ‘2); ğ‘ (ğœ·) = N (ğœ·; a, B),

we arrive at the so-called linear covariance: ğ¾ (x, x ; ğ‘, B) = ğ‘2 + x Bx.

(. )

Although this covariance is unlikely to be of any direct use in Bayesian optimization (linear programming is much simpler!), it can be a useful component of more complex composite covariance structures.

.
With the notable exception of the spectral mixture covariance, which can approximate any stationary covariance function, several of the covariances introduced in the last section are still too rigid to be useful.
In particular, consider any of the MatÃ©rn family ( . â€“ . ). Each of these covariances encodes several explicit and possibly dubious assumptions about the function of interest. To begin, each prescribes unit variance for every function value:

var[ğœ™ | ğ‘¥] = ğ¾ (ğ‘¥, ğ‘¥) = 1,

(. )

Although an important concept, there is no clear-cut de nition of characteristic length scale. It is simply a convenient separation distance for which correlation remains appreciable, but beyond which correlation begins to noticeably decay.

which is an arbitrary, possibly inappropriate choice of scale. Further, each

of these covariance functions xes an isotropic characteristic length scale

of correlation of approximately one unit: at a separation of |ğ‘¥ âˆ’ ğ‘¥ | = 1,

the correlation between the corresponding function values drops to

roughly

corr[ğœ™, ğœ™ | ğ‘¥, ğ‘¥ ] â‰ˆ 0.5,

(. )

..

prior mean

prior % credible interval

samples

scaling function, ğ‘ 1

Figure . : Scaling a stationary covariance by a nonconstant function (here, a smooth bump function of compact support) to yield a nonstationary covariance.

0

and this correlation continues to drop e ectively to zero at a separation of approximately ve units. Again, this choice of scale is arbitrary, and the assumption of isotropy is particularly restrictive.
In general, a Gaussian process encodes strong assumptions regarding the joint distribution of function values ( . ), which may not be compatible with a given function â€œout of the box.â€ However, we can often improve model t by appropriate transformations of the objective. In fact, linear transformations of function inputs and outputs are almost universally considered, although only implicitly by introducing parameters conveying the e ects of these transformations. We will show how both linear and nonlinear transformations of function input and output lead to more expressive models and give rise to common model parameters.

Scaling function outputs

We rst address the issue of scale in function output ( . ) by considering
the statistical e ects of arbitrary scaling. Consider a random function ğ‘“ : X â†’ â„ with covariance function ğ¾ and let ğ‘ : X â†’ â„ be a known scaling function. Then the pointwise product ğ‘ğ‘“ : ğ‘¥ â†¦â†’ ğ‘(ğ‘¥) ğ‘“ (ğ‘¥) has
covariance function

cov[ğ‘ğ‘“ | ğ‘] = ğ‘(ğ‘¥)ğ¾ (ğ‘¥, ğ‘¥ )ğ‘(ğ‘¥ ),

(. )

by the bilinearity of covariance. If the scaling function is constant, ğ‘ â‰¡ ğœ†,

then we have

cov[ğœ†ğ‘“ | ğœ†] = ğœ†2ğ¾ .

(. )

This simple result allows us to extend a â€œbaseâ€ covariance ğ¾ with xed scale, as in ( . ), to a parametric family with arbitrary scale:

ğ¾ (ğ‘¥, ğ‘¥ ; ğœ†) = ğœ†2ğ¾ (ğ‘¥, ğ‘¥ ).

Draft as of January . Feedback welcome: https://bayesoptbook.com/

For this result ğ‘“ need not have a distribution.

ğ¾
ğ‘‘ The squared exponential covariance ğ¾ scaled by a range of output scales ğœ† ( . ).
Sample paths from centered s with smaller (top) and larger (bottom) output scales.
ğ¾
ğ‘‘ The squared exponential covariance ğ¾ dilated by a range of length scales â„“ ( . ).
Sample paths from centered s with shorter (top) and longer (bottom) characteristic length scales.

In this context the parameter ğœ† is known as an output scale, or when the base covariance is stationary with ğ¾ (ğ‘¥, ğ‘¥) = 1, the signal variance, as it determines the variance of any function value: var[ğœ™ | ğ‘¥, ğœ†] = ğœ†2. The illustration in the margin shows the e ect of scaling the squared exponential covariance function by a series of increasing output scales.
We can also of course consider nonlinear transformations of the function output as well. This can be useful for modeling constraints â€“ such as nonegativity or boundedness â€“ that are not compatible with the Gaussian assumption. However, a nonlinear transformation of a Gaussian process is no longer Gaussian, so it often more convenient to model the transformed function after â€œremoving the constraint.â€
We may use the general form of this scaling result ( . ) to transform a stationary covariance into a nonstationary one, as any nonconstant scaling is su cient to break translation invariance. We show an example of such a transformation in gure . , where we have scaled a stationary covariance by a bump function to create a prior on smooth functions with compact support.

Transforming the domain and length scale parameters

We now address the issue of the scaling of correlation as a function of

distance ( . ) by introducing a powerful tool: transforming the domain

of the function of interest into a more convenient space for modeling.

Namely, suppose we wish to reason about a function ğ‘“ : X â†’ â„,

and let ğ‘” : X â†’ Z be a map from the domain to some arbitrary space

Z, which might also be X . If ğ¾Z is a covariance function on Z, then the

composition

ğ¾X (ğ‘¥, ğ‘¥ ) = ğ¾Z ğ‘”(ğ‘¥), ğ‘”(ğ‘¥ )

(. )

is trivially a covariance function on X. This allows us to de ne a covari-

ance for ğ‘“ indirectly by jointly designing a map ğ‘” to another space and

a corresponding covariance ğ¾Z (and mean ğœ‡Z ) on that space. This ap-

proach o ers a lot of exibility, as we are free to design these components

as we see t to impose any desired structure.

We will spend some time exploring this idea, beginning with the

relatively simple but immensely useful case of combining a linear transformation on a Euclidean domain X âŠ‚ â„ğ‘› with an isotropic covariance

on the output. Perhaps the simplest example is the dilation x â†¦â†’ x/â„“, which simply scales distance by â„“âˆ’. 1 Incorporating this transformation

into an isotropic base covariance ğ¾ (ğ‘‘) on X yields a parametric family

of dilated versions:

ğ¾ (ğ‘¥, ğ‘¥ ; â„“) = ğ¾ (ğ‘‘/â„“).

(. )

If the base covariance has a characteristic length scale of one unit, the length scale of the dilated version will be â„“; for this reason, this parameter is simply called the characteristic length scale of the parameterized covariance ( . ). Adjusting the length scale allows us to model functions with a range of â€œwiggliness,â€ where shorter length scale implies more wiggly behavior; see the margin for examples.

..
Figure . : Left: a sample from a centered Gaussian process in two dimensions with isotropic squared exponential covariance. Right: a sample from a centered Gaussian process with an squared exponential covariance. The length of the lines on each axis are proportional to the length scale along that axis.

Taking this one step further, we may consider dilating each axis by a separate factor:

ğ‘¥ğ‘– â†¦â†’ ğ‘¥ğ‘– /â„“ğ‘– ; x â†¦â†’ [diag â„“]âˆ’1x,

(. )

which induces the weighted Euclidean distance

ğ‘‘â„“

=

âˆšï¸„âˆ‘ï¸
ğ‘–

(ğ‘¥ğ‘–

âˆ’ â„“ğ‘–

ğ‘¥ğ‘–

)

2

.

(. )

Geometrically, the e ect of this map is to transform surfaces of equal dis-

tance around each point â€“ which represent curves of constant covariance

for an isotropic covariance â€“ from spheres into axis-aligned ellipsoids;

see the gure in the margin. Incorporating into an isotropic base covari-

ance ğ¾ (ğ‘‘) produces a parametric family of anisotropic covariances with

di erent characteristic length scales along each axis, corresponding to

the parameters â„“:

ğ¾ (ğ‘¥, ğ‘¥ ; â„“) = ğ¾ (ğ‘‘â„“ ).

(. )

When the length scale parameters are inferred from data, this construction is known as automatic relevance determination ( ). The motivation for the name is that if the function has only weak dependence on some mostly irrelevant dimension of the input, we could hope to infer a very long length scale for that dimension. The contribution to the weighted distance ( . ) for that dimension would then be e ectively nulli ed, and the resulting covariance would e ectively â€œignoreâ€ that dimension.
Figure . shows samples from 2ğ‘‘ centered Gaussian processes, comparing behavior with an isotropic covariance and an modi ed version that contracts the horizontal and expands the vertical axis (see curves of constant covariance in the margin). The result is anisotropic behavior with a longer characteristic length scale in the vertical direction than in the horizontal direction, but with the behavior of local features remaining aligned with the axes overall.
Finally, we may also consider an arbitrary linear transformation ğ‘” : x â†¦â†’ Ax, which induces the Mahalanobis distance ( . )

ğ‘‘A = |Ax âˆ’ Ax |.

Draft as of January . Feedback welcome: https://bayesoptbook.com/

Possible surfaces of equal covariance with the center when combining separate dilation of each axis with an isotropic covariance.
automatic relevance determination,
Surfaces of equal covariance with the center for the examples in gure . : the isotropic covariance in the left panel (red), and the covariance in the right panel (blue).

Possible surfaces of equal covariance with the center when combining an arbitrary linear transformation with an isotropic covariance.
high-dimensional domains: Â§ . , p.

We did see a periodic in the previous chapter ( . ); however, that model only had support on perfectly sinusoidal functions.

.. .

( ). Introduction to Gaus-

sian Processes. Neural Networks and Machine

Learning. Vol. . Springerâ€“Verlag. [Â§ . ]

The covariance on the circle is usually inherited from a covariance on â„2. The result
of composing with the squared exponential
covariance in particular is often called â€œtheâ€
periodic covariance, but we stress that any other covariance on â„2 could be used instead.

A sample path of a centered with MatÃ©rn covariance with ğœˆ = 5/2 ( . ) after applying
the periodic warping function ( . ).

.

et al. ( ). Input Warping for Bayes-

ian Optimization of Non-Sationary Functions.

.

As before, we may incorporate this map into an isotropic base covariance ğ¾ to realize a family of anisotropic covariance functions:

ğ¾ (ğ‘¥, ğ‘¥ ; A) = ğ¾ (ğ‘‘A).

(. )

Geometrically, an arbitrary linear map can transform surfaces of constant covariance from spheres into arbitrary ellipsoids; see the gure in the margin. The sample from the left-hand side of gure . was generated by composing an isotropic covariance with a map inducing both anisotropic scaling and rotation. The e ect of the underlying transformation can be seen in the shapes of local features, which are not aligned with the axes.
Due to the inherent number of parameters required to specify a general transformation, this construction is perhaps most useful when the map is to a much lower-dimensional space: â„ğ‘› â†’ â„,ğ‘˜ ğ‘˜ ğ‘›. This has been promoted as one strategy for modeling functions on highdimensional domains suspected of having hidden low-dimensional structure â€“ if this low-dimensional structure is along a linear subspace of the domain, we could capture it by an appropriately designed projection A. We will discuss this idea further in the next section.

Nonlinear warping

When using a covariance function with an inherent length scale, such as a MatÃ©rn or squared exponential covariance, some linear transformation of the domain is almost always considered, whether it be simple dilation ( . ), anisotropic scaling ( . ), or a general transformation ( . ). How-

ever, nonlinear transformations can also be useful for imposing structure on the domain, a process commonly referred to as warping.
To provide an example that may not often be useful in optimization but is illustrative nonetheless, suppose we wish to model a function ğ‘“ : â„ â†’ â„ that we believe to be smooth and periodic with period ğ‘. None of the covariance functions introduced thus far would be able to

induce the periodic correlations that this assumption would entail. A

construction due to

is to compose a map onto a circle of radius

ğ‘Ÿ = ğ‘/(2ğœ‹):

ğ‘¥ â†¦â†’

ğ‘Ÿ cos ğ‘¥ ğ‘Ÿ sin ğ‘¥

(. )

with a covariance function on that space re ecting any desired properties of ğ‘“. As this map identi es points separated by any multiple of the period, the corresponding function values are perfectly correlated, as

desired. A sample from a Gaussian process employing this construction with a MatÃ©rn covariance after warping is shown in the margin.

A compelling use of warping is to build nonstationary models by composing a nonlinear map with a stationary covariance, an idea

et al. explored in the context of Bayesian optimization. Many objective functions exhibit di erent behavior depending on the proximity to

the optimum, suggesting that nonstationary models may sometimes be

worth exploring.

et al. proposed a exible family of warping func-

tions for optimization problems with box-bounded constraints, where

prior mean

.. prior % credible interval

samples

1

Figure . : Am example of the beta warping method proposed

by

et al. We show three samples of the

stationary Gaussian process prior from gure .

(above) after applying a nonlinear warping through

a beta ( . ) with (ğ›¼, ğ›½) = (4, 4) (right). The

length scale is compressed in the center of the do-

main and expanded near the boundary.

0

0

1

we may take the domain to be the unit cube by scaling and translating as necessary: X = [0, 1]ğ‘›. The idea is to warp each coordinate of the input
via the cumulative distribution function of a beta distribution:

ğ‘¥ğ‘– â†¦â†’ ğ¼ (ğ‘¥ğ‘– ; ğ›¼ğ‘–, ğ›½ğ‘– ),

(. )

where (ğ›¼ğ‘–, ğ›½ğ‘– ) are shape parameters and ğ¼ is the regularized beta func-

tion. This represents a monotonic bijection on the unit interval that can

assume several shapes; see the marginal gure for examples. The map

may contract portions of the domain and expand others, e ectively de-

creasing and increasing the length scale in those regions. Finally, taking

ğ›¼ = ğ›½ = 1 recovers the identity map, allowing us to degrade gracefully

to the unwarped case if desired.

In gure . we combine a beta warping on a one-dimensional domain

with a stationary covariance on the output. The chosen warping shortens

the length scale near the center of the domain and extends it near the

boundary, which might be reasonable for an objective function expected

to exhibit the most â€œinterestingâ€ behavior on the interior of its domain.

A recent innovation is to use sophisticated arti cial neural networks

as warping maps for modeling functions of high-dimensional data with

complex structure. Notable examples of this approach include the fami-

lies of manifold Gaussian processes introduced by

et al. and

deep kernels introduced contemporaneously by

et al. Here the

warping function was taken to be an arbitrary neural network, the output

layer of which was fed into a suitable stationary covariance function.

This gives a highly parameterized covariance function where the pa-

rameters of the base covariance and the neural map become parameters

of the resulting model. In the context of Bayesian optimization, this

can be especially useful when there is su cient data to learn a useful

representation of the domain via unsupervised methods.

Draft as of January . Feedback welcome: https://bayesoptbook.com/

1

0 0
Some examples of beta ( . ).

1 warping functions

.

et al. ( ). Manifold Gaussian

Processes for Regression.

.

..

et al. ( ). Deep Kernel Learn-

ing.

.

neural representation learning: Â§ . , p.

ğ¾1

ğ¾2

ğ¾1 + ğ¾2

Figure . : Samples from centered Gaussian processes with di erent covariance functions: (left) a squared exponential covariance, (middle) a squared exponential covariance with smaller output scale and shorter length scale, and (right) the sum of the two. Samples from the process with the sum covariance show smooth variation on two di erent scales.

The assumption of the processes being centered is needed for the product result only; otherwise, there would be additional terms involving scaled versions of each individual covariance as in ( . ). The sum result does not depend on any assumptions regarding the mean functions.
A sample from a centered Gaussian process with an â€œalmost periodicâ€ covariance function.

Combining covariance functions
In addition to modifying covariance functions via scaling the output and/or transforming the domain, we may also combine multiple covariance functions together to model functions in uenced by multiple random processes.
Let ğ‘“, ğ‘” : X â†’ â„ be two centered, independent (not necessarily Gaussian) random functions with covariance functions ğ¾ğ‘“ and ğ¾ğ‘”, respectively. By the properties of covariance, the sum and pointwise product of these functions have covariance functions with the same structure:

cov[ğ‘“ + ğ‘”] = ğ¾ğ‘“ + ğ¾ğ‘”; cov[ğ‘“ ğ‘”] = ğ¾ğ‘“ ğ¾ğ‘”,

(. )

and thus covariance functions are closed under addition and pointwise multiplication. Combining this result with ( . ), we have that any polynomial of covariance functions with nonnegative coe cients forms a valid covariance. This enables us to construct in nite families of increasingly complex covariance functions from simple components.
We may use a sum of covariance functions to model a function with independent additive contributions, such as random behavior on several length scales. Precisely such a construction is illustrated in gure . . If the covariance functions are nonnegative and have roughly the same scale, the e ect of addition is roughly one of logical disjunction: the sum will assume nontrivial values whenever any one of its constituents does.
Meanwhile, a product of covariance functions can loosely be interpreted in terms of logical conjunction, with function values having appreciable covariance only when every individual covariance function does. A prototypical example of this e ect is a covariance function modeling functions that are â€œalmost periodic,â€ formed by the product of a bump-shaped isotropic covariance function such as a squared exponential ( . ) with a warped version modeling perfectly periodic functions ( . ). The former moderates the in uence of the latter by driving the correlation between function values to zero for inputs that are su ciently separated, regardless of their positions in the periodic cycle. We show a sample from such a covariance in the margin, where the length scale of the modulation term is three times the period.

..

.

Optimization on a high-dimensional domain can be challenging, as we

can succumb to the curse of dimensionality if we are not careful. As an

example, consider optimizing an objective function on the unit cube

[0, 1]ğ‘›. Suppose we model this function with an isotropic covariance

from the MatÃ©rn family, taking the length scale to be â„“ = 1/10 so that

ten length scales span the domain along each axis. This choice implies

that function values on the corners of the domain would be e ectively independent, as exp(âˆ’10) < 10âˆ’4 ( . ) and exp(âˆ’50) is smaller still

( . ). If we were to demand even a modicum of con dence in these

regions at termination, say by having a measurement within one length

scale of every corner, we would need 2ğ‘› observations! This exponential

growth in the number of observations required to cover the domain is

the tyrannical curse of dimensionality.

However, compelling objectives do not tend to have so many degrees

of freedom; if they did, we should perhaps give up on the idea of global

optimization altogether. Rather, many authors have noted a tendency

toward low intrinsic dimensionality in real-world problems: that is, most

of the variation in the objective is con ned to a low-dimensional sub-

space of the domain. This phenomenon has been noted for example in

hyperparameter optimization and optimizing the parameters of neural

networks.

and

suggested that â€œhiddenâ€ low-dimensional

structure is actually a universal requirement for success on any task:

There is a consensus in the high-dimensional data analysis community that the only reason any methods work in very high dimensions is that, in fact, the data are not truly high dimensional.

The global optimization community shares a similar consensus: typical high-dimensional objectives are not â€œtrulyâ€ high dimensional. This intuition presents us with an opportunity: if we could only identify inherent low-dimensional structure during optimization, we could sidestep the curse of dimensionality by restricting our search accordingly.
Several strategies are available for capturing low intrinsic dimension with Gaussian process models. The general approach closely follows our discussion from the previous section: we identify some appropriate mapping from the high-dimensional domain to a lower-dimensional space, then model the objective function after composing with this embedding ( . ). This is one realization of the general class of manifold Gaussian processes, where the sought-after manifold is low dimensional. Adopting this approach then raises the issue of identifying useful families of mappings that can suitably reduce dimension while preserving enough structure of the objective to keep optimization feasible.

Neural embeddings Given the success of deep learning in designing feature representations for complex, high-dimensional objects, neural embeddings â€“ as used in
Draft as of January . Feedback welcome: https://bayesoptbook.com/

curse of dimensionality
This is far from excessive: the domain for the marginal sampling examples in this chapter spans length scales and thereâ€™s just enough room for interesting behavior to emerge.

.

and .

( ). Random

Search for Hyper-Parameter Optimization.

Journal of Machine Learning Research : â€“

.

. et al. ( a). Measuring the Intrinsic Di-

mension of Objective Landscapes.

.

.

and . .

( ). Maximum

Likelihood Estimation of Intrinsic Dimension.

eur

.

.

et al. ( ). Manifold Gaussian

Processes for Regression.

.

Figure . : An objective function on a two-dimensional domain (left) with intrinsic dimension 1. The entire variation of the objective is determined on the one-dimensional linear subspace Z corresponding to the diagonal black line, which we can model in its inherent dimension (right).

..

et al. ( ). Deep Kernel Learn-

ing.

.

neural representation learning: Â§ . , p.

.

et al. ( ). Scalable Bayesian Opti-

mization Using Deep Neural Networks.

.

cost of Gaussian process inference: Â§ . , p.

.

and .

( ). Random

Search for Hyper-Parameter Optimization.

Journal of Machine Learning Research : â€“

.

.

and . . .

( ). Dis-

covering hidden features with Gaussian pro-

cess regression. eur

.

.

et al. ( b). Bayesian Optimization

in a Billion Dimensions via Random Embed-

dings. Journal of Arti cial Intelligence Research

:â€“

Z

Z
the family of deep kernels â€“ present a tantalizing option. Neural embeddings have shown some success in Bayesian optimization, where they can facilitate optimization over complex structured objects by providing a nice continuous latent space to work in.
et al. demonstrated excellent performance on hyperparameter tuning tasks by interpreting the output layer of a deep neural network as a set of custom nonlinear basis functions for Bayesian linear regression, as in ( . ). An advantage of this particular construction is that Gaussian process inference and prediction is accelerated dramatically by adopting the linear covariance ( . ) â€“ the cost of inference scales linearly with the number of observations, rather than cubically as in the general case.

Linear embeddings

Another line of attack is to search for a low-dimensional linear subspace
of the domain encompassing the relevant variation in inputs and model
the function after projection onto that space. For an objective ğ‘“ on a high-dimensional domain X âŠ‚ â„,ğ‘› we consider models of the form

ğ‘“ (x) = ğ‘”(Ax); A âˆˆ â„,ğ‘˜Ã—ğ‘›

(. )

where ğ‘” : â„ğ‘˜ â†’ â„ is a (ğ‘˜ ğ‘›)-dimensional surrogate for ğ‘“. The simplest such approach is automatic relevance determination
( . ), where we learn separate length scales along each dimension. Although the corresponding linear transformation ( . ) does not reduce dimension, axes with su ciently long length scales are e ectively eliminated, as they do not have strong in uence on the covariance. This can be e ective when some dimensions are likely to be irrelevant, but limits us to axis-aligned subspaces only.
A more exible option is to consider arbitrary linear transformations in the model ( . , . ), an idea that has seen signi cant attention for Gaussian process modeling in general and for Bayesian optimization in particular. Figure . illustrates a simple example where a onedimensional objective function is embedded in two dimensions in a nonaxis-aligned manner. Both axes would appear important for explaining the function when using , but a one-dimensional subspace su ces

..

Figure . : A sample from a in two dimensions with the

ğ‘¥2âˆ—

decomposition ğ‘“ (x) = ğ‘”1 (ğ‘¥1) + ğ‘”2 (ğ‘¥2). Here a

nominally two-dimensional function is actually

the sum of two one-dimensional components de-

ned along each axis with no interaction. The

maximum of the function is achieved at the point

corresponding to the maxima of the individual

components.

ğ‘¥1âˆ—

if chosen carefully. This approach o ers considerably more modeling

exibility than at the expense of a ğ‘˜-fold increase in the number

of parameters that must be speci ed. However, several algorithms have been proposed for e ciently identifying a suitable map A, , and

et al. demonstrated success in optimizing objectives in extremely high

dimension by simply searching along a random low-dimensional sub-

space. The authors also provided theoretical guarantees regarding the

recoverability of the global optimum with this approach, assuming the

hypothesis of low intrinsic dimensionality holds.

If more exibility is desired, we may represent an objective function

as a sum of contributions on multiple relevant linear subspaces:

âˆ‘ï¸ ğ‘“ (x) = ğ‘”ğ‘– (Ağ‘– x).

(. )

ğ‘–

This decomposition is similar in spirit to the classical family of gener-

alized additive models, where the linear maps can be arbitrary and of

variable dimension. If we assume the additive components in ( . ) are independent, each with Gaussian process prior GP (ğ‘”ğ‘– ; ğœ‡ğ‘–, ğ¾ğ‘– ), then the resulting model for ğ‘“ is a Gaussian process with additive moments ( . ):

âˆ‘ï¸ ğœ‡ (x) = ğœ‡ğ‘– (Ağ‘– x);
ğ‘–

âˆ‘ï¸ ğ¾ (x, x ) = ğ¾ğ‘– (Ağ‘– x, Ağ‘– x ).
ğ‘–

Several speci c schemes have been proposed for building such decompositions. One convenient approach is to partition the coordinates of the input into disjoint groups and add a contribution de ned on each subset. , Figure . shows an example, where a two-dimensional objective is the sum of independent axis-aligned components. We might use such a model when every feature of the input is likely to be relevant but only through interaction with a limited number of additional variables.

Draft as of January . Feedback welcome: https://bayesoptbook.com/

.

et al. ( ). High-Dimensional

Gaussian Process Bandits. eur

.

.

et al. ( ). Active Learning of

Linear Embeddings for Gaussian Processes.

.

.

and .

( ). General-

ized Additive Models. Statistical Science ( ):

â€“.

.

et al. ( ). High Dimensional

Bayesian Optimisation and Bandits via Addi-

tive Models.

.

..

et al. ( ). Discovering and Ex-

ploiting Additive Structure for Bayesian Opti-

mization.

.

.

et al. ( ). High-Dimensional

Bayesian Optimization via Additive Models

with Overlapping Groups.

.

.

and .

( ). E cient

High Dimensional Bayesian Optimization

with Additivity and Quadrature Fourier Fea-

tures. eur

.

..

et al. ( ). Decentralized High-

Dimensional Bayesian Optimization with Fac-

tor Graphs.

.

.

et al. ( ). Scaling Multidimen-

sional Gaussian Processes using Projected Ad-

ditive Approximations.

.

. . et al. ( ). High Dimensional Bayes-

ian Optimization via Restricted Projection Pur-

suit Models.

.

prior mean function: Â§ . , p.
impact on sample path behavior: gure . , p. and surrounding discussion
impact on extrapolation: gure . , p. and surrounding discussion

prior covariance function: Â§ . , p.

An advantage of a disjoint partition is that we may reduce optimization of the high-dimensional objective to separate optimization of each of its lower-dimensional components ( . ). Several other additive schemes have been proposed as well, including partitions with (perhaps sparsely) overlapping groups , , and decompositions of the general form ( . ) with arbitrary projection matrices. ,
.
Specifying a Gaussian process entails choosing a mean and covariance function for the function of interest. As we saw in the previous chapter, the structure of these functions has important implications regarding sample path behavior, and as we will see in the next chapter, important implications regarding its ability to explain a given set of data.
In practice, the design of a Gaussian process model is usually datadriven: we establish some space of candidate models to consider, then search this space for the models providing the best explanation of available data. In this chapter we o ered some guidance for the construction of models â€“ or parametric spaces of models â€“ as possible explanations of a given system. We will continue the discussion in the next chapter by taking up the question of assessing model quality in light of data. Below we summarize the important ideas arising in the present discussion.
â€¢ The mean function of a Gaussian process determines the expected value of function values. Although an important concern, the mean function can only a ect sample path behavior through pointwise translation, and most interesting properties of sample paths are determined by the covariance function instead.
â€¢ Nonetheless, the mean function has important implications for prediction, namely, in extrapolation. When making predictions in locations poorly explained by available data â€“ that is, locations where function value are not strongly correlated with any observation â€“ the prior mean function e ectively determines the posterior predictive mean.
â€¢ There are no restrictions on the mean function of a Gaussian process, and we are free to use any sensible choice in a given scenario. In practice, unless a better option is apparent, the mean function is usually taken to have some relatively simple parametric form, such as a constant ( . ) or a low-order polynomial ( . ). Such choices are both simple and unlikely to cause grossly undesirable extrapolatory behavior.
â€¢ When the mean function includes a linear combination of basis functions, we may exactly marginalize the coe cients under a multivariate normal prior ( . ). The result is a marginal Gaussian process where uncertainty in the linear terms of the mean is absorbed into the covariance function ( . ). As an important special case, we may marginalize the value of a constant mean ( . ) under a normal prior ( . ).
â€¢ The covariance function of a Gaussian process is critical to determining the behavior of its sample paths. To be valid, a covariance function must

be symmetric and positive semide nite. The latter condition can be di cult to guarantee for arbitrary â€œsimilarity measures,â€ but covariance functions are closed under several natural operations, allowing us to build complex covariance functions from simple building blocks.

â€¢ In particular, sums and pointwise products of covariance functions are valid covariance functions, and by extension any polynomial expression of covariance functions with positive coe cients.

â€¢ Many common covariance functions are invariant to translation of their

inputs, a property known as stationarity. An important result known

as

â€™s theorem provides a useful representation for the space of

stationary covariance functions: their Fourier transforms are symmetric,

nite measures, and vice versa. This result has important implications for

modeling and computation, as the Fourier representation can be much

easier to work with than the covariance function itself.

â€¢ Numerous useful covariance functions are available â€œo -the-shelf.â€ The family of MatÃ©rn covariances â€“ and its limiting case the squared exponential covariance â€“ can model functions with any desired degree of smoothness ( . â€“ . ). A notable special case is the MatÃ©rn covariance with ğœˆ = 5/2 ( . ), which has been promoted as a reasonable default.

â€¢ The spectral mixture covariance ( . ) appeals to

â€™s theorem to

provide a parametric family of covariance functions able to approximate

any stationary covariance.

â€¢ Covariance functions can be modi ed by arbitrary scaling of function outputs ( . ) and/or arbitrary transformation of function inputs ( . ). This ability allows us to create parametric families of covariance functions with tunable behavior.

â€¢ Considering arbitrary constant scaling of function outputs gives rise to parameters known as output scales ( . ).

â€¢ Considering arbitrary dilations of function inputs gives rise to parameters known as characteristic length scales ( . ). Taking the dilation to be anisotropic introduces a characteristic length scale for each input dimension, a construction known as automatic relevance determination ( ). With an covariance, setting a given dimensionâ€™s length scale very high e ectively â€œturns o â€ its in uence on the model.

â€¢ Nonlinear warping of function inputs is also possible. This enables us to easily build custom nonstarionary covariance functions by combining a nonlinear warping with a stationary base covariance.

â€¢ Optimization can be especially challenging in high dimensions due to the curse of dimensionality. However, if an objective function has intrinsic low-dimensional structure, we can avoid some of the challenges by nding a structure-preserving mapping to a lower-dimensional space and modeling the function on the â€œsmallerâ€ space. This idea has repeatedly proven successful, and several general-purpose constructions are available.

..
sums and products of covariance functions: Â§ . , p. stationarity: Â§ . , p.
â€™s theorem: Â§ . , p.
the MatÃ©rn family and squared exponential covariance: Â§ . , p.
spectral mixture covariance: Â§ . , p. scaling function outputs: Â§ . , p. transforming function inputs: Â§ . , p.
nonlinear warping: gure . , p. and surrounding discussion modeling functions on high-dimensional domains: Â§ . , p.

Draft as of January . Feedback welcome: https://bayesoptbook.com/

MODEL ASSESSMEN T, SELECTION, AND AVERAGING
The previous chapter o ered a glimpse into the exibility of Gaussian processes, which can evidently model functions with a wide range of behavior. However, a critical question remains: how we can identify which models are most appropriate in a given situation?
The di culty of this question is compounded by several factors. To begin, the number of possible choices is staggering. Any function can serve as a mean function for a Gaussian process, and we may construct arbitrary complex covariance functions through a variety of mechanisms. Even if we x the general form of the moment functions, introducing natural parameters such as output and length scales yields an in nite spectrum of possible models.
Further, many systems of interest act as â€œblack boxes,â€ about which we may have little prior knowledge. Before optimization, we may have only a vague notion of which models might be reasonable for a given objective function or how any parameters of these models should be set. We might even be uncertain about aspects of the observation process, such as the nature or precise scale of observation noise. Therefore, we may nd ourselves in the unfavorable position of having in nitely many possible models to choose from and no idea how to choose!
Acquiring data, however, provides a way out of this conundrum. After obtaining some observations of the system, we may determine which models are the most compatible with the data and thereby establish preferences over possible choices, a process known as model assessment. Model assessment is a surprisingly complex and nuanced subject â€“ even if we limit the scope to Bayesian methods â€“ and no method can rightfully be called â€œtheâ€ Bayesian approach. In this chapter we will present one convenient framework for model assessment via Bayesian inference over models, which are evaluated based on their ability to explain observed data and our prior beliefs.
We will begin our presentation by carefully de ning the models we will be assessing and discussing how we may build useful spaces of models for consideration. With Gaussian processes, these spaces will most often be built from what we will call model structures, comprising a parametric mean function, covariance function, and observation model; in the context of model assessment, the parameters of these model components are known as hyperparameters. We will then show how to perform Bayesian inference over the hyperparameters of a model structure from observations, resulting in a model posterior enabling model assessment and other tasks. We will later extend this process to multiple model structures and show how we can even automatically search for better model structures.
Central to this approach is a fundamental measure of model t known as the marginal likelihood of the data or model evidence. Gaussian process models are routinely selected by maximizing this score, which can produce excellent results when su cient data are available to unambiguously determine the best- tting model. However, model construction
This material will be published by Cambridge University Press as Bayesian Optimization. This prepublication version is free to view and download for personal use only. Not for redistribution, resale, or use in derivative works. Â©Roman Garnett .

prior mean function: Â§ . , p. prior covariance function: Â§ . , p.
output and length scales: Â§ . , p.

The interested reader can nd an overview of this rich subject in:

.

and .

( ). A survey of

Bayesian predictive methods for model assess-

ment, selection and comparison. Statistics Sur-

veys : â€“ .

models and model structures: Â§ . , p.

Bayesian inference over parametric model spaces: Â§ . , p.
multiple model structures: Â§ . , p. automating model structure search: Â§ . , p.
marginal likelihood, model evidence: Â§ . , p. model selection via inference: Â§ . , p.

,

,

model averaging: Â§ . , p.

in the context of Bayesian optimization is unusual as the expense of gathering observations relegates us to the realm of small data. E ective modeling with small datasets requires careful consideration of model uncertainty: models explaining the data equally well may disagree drastically in their predictions, and committing to a single model may yield biased predictions with poorly calibrated uncertainty â€“ and disappointing optimization performance as a result. Model averaging is one solution that has proven e ective in Bayesian optimization, where the predictions of multiple models are combined in the interest of robustness.

model, ğ‘ (y | x)
model induced by prior process and observation model

.

In model assessment, we seek to evaluate a space of models according to their ability to explain a set of observations D = (x, y). Before taking up

this problem in earnest, let us establish exactly what we mean by â€œmodelâ€

in this context, which is a model for the given observations, rather than

of a latent function alone as was our focus in the previous chapter.

For this discussion we will de ne a model to be a prior probability distribution over the measured values y that would result from observing

at a set of locations x: ğ‘ (y | x). In the overarching approach we have

adopted for this book, a model is speci ed indirectly via a prior process

on a latent function ğ‘“ and an observation model linking this function to

the observed values:

ğ‘ (ğ‘“ ), ğ‘ (ğ‘¦ | ğ‘¥, ğœ™) .

( .)

Given explicit choices for these components, we may form the desired dis-

tribution by marginalizing the latent function values ğ“ = ğ‘“ (x) through

the observation model: âˆ«

ğ‘ (y | x) = ğ‘ (y | x, ğ“) ğ‘ (ğ“ | x) dğ“.

(.)

All models we will consider below will be of this composite form ( . ), but the assessment framework we will describe will accommodate arbitrary models.

Although de ning a space of candidate models may seem natural and innocuous, this is actually a major point of contention between di erent approaches to Bayesian model assessment. If we subscribe to the maxim â€œall models are wrong,â€ we might conclude that the true model will never be contained in any space we de ne, no matter how expansive. However, some are likely â€œmore wrongâ€ than others, and we can still reasonably establish preferences over the given space.
model structure

Spaces of candidate models
To proceed, we must establish some space of candidate models we wish to consider as possible explanations of the observed data. Although this space can in principle be arbitrary, with Gaussian process models it is convenient to consider parametric collections of models de ned by parametric forms for the observation model and the prior mean and covariance functions of the latent function. We invested signi cant e ort in the last chapter laying the groundwork to enable this approach: a running theme was the introduction of exible parametric mean and covariance functions that can assume a wide range of di erent shapes â€“ perfect building blocks for expressive model spaces.
We will call a particular combination of observation model, prior mean function ğœ‡, and prior covariance function ğ¾ a model structure.

Corresponding to each model structure is a natural model space formed by exhaustively traversing the joint parameter space:

M = ğ‘ (ğ‘“ | ğœ½ ), ğ‘ (ğ‘¦ | ğ‘¥, ğœ™, ğœ½ ) | ğœ½ âˆˆ Î˜ ,

(.)

where

ğ‘ (ğ‘“ | ğœ½ ) = GP ğ‘“ ; ğœ‡(ğ‘¥; ğœ½ ), ğ¾ (ğ‘¥, ğ‘¥ ; ğœ½ ) .

We have indexed the space by a vector ğœ½ , the entries of which jointly specify any necessary parameters from their joint range Î˜. The entries of ğœ½ are known as hyperparameters of the model structure, as they parameterize the prior distribution for the observations, ğ‘ (y | x, ğœ½ ) ( . ).
In many cases we may be happy with a single suitably exible model structure for the data, in which case we can proceed with the corresponding space ( . ) as the set of candidate models. We may also consider multiple model structures for the data by taking a discrete union of such spaces, an idea we will return to later in this chapter.

Example
Let us momentarily take a step back from abstraction and create an explicit model space for optimization on the interval X = [ğ‘, ğ‘]. Suppose our initial beliefs are that the objective will exhibit stationary behavior with a constant trend near zero, and that our observations will be corrupted by additive noise with unknown signal-to-noise ratio.
For the observation model, we take homoskedastic additive Gaussian noise, a reasonable choice when there is no obvious alternative:

ğ‘ (ğ‘¦ | ğœ™, ğœğ‘›) = N (ğ‘¦; ğœ™, ğœğ‘›2),

(.)

and leave the scale of the observation noise ğœğ‘› as a parameter. Turning to the prior process, we assume a constant mean function ( . ) with a zero-mean normal prior on the unknown constant:

ğœ‡ (ğ‘¥; ğ‘) â‰¡ ğ‘; ğ‘ (ğ‘) = N (ğ‘; 0, ğ‘2),

and select the MatÃ©rn covariance function with ğœˆ = 5/2 ( . ) with unknown output scale ğœ† ( . ) and unknown length scale â„“ ( . ):

ğ¾ (ğ‘¥, ğ‘¥ ; ğœ†, â„“) = ğœ†2ğ¾M5/2 (ğ‘‘/â„“).

Following our discussion in the last chapter, we may eliminate one of the parameters above by marginalizing the unknown constant mean under its assumed prior, leaving us with the identically zero mean function and an additive contribution to the covariance function ( . ):

ğœ‡ (ğ‘¥) â‰¡ 0; ğ¾ (ğ‘¥, ğ‘¥ ; ğœ†, â„“) = ğ‘2 + ğœ†2ğ¾M5/2 (ğ‘‘/â„“).

(.)

This, combined with ( . ), completes the speci cation of a model structure with three hyperparameters: ğœ½ = [ğœğ‘›, ğœ†, â„“]. Figure . illustrates

Draft as of January . Feedback welcome: https://bayesoptbook.com/

..
model space, M
vector of hyperparameters, ğœ½ range of hyperparameter values, Î˜
multiple model structures: Â§ . , p.
The interval can be arbitrary; our discussion will be purely qualitative. observation model: additive Gaussian noise with unknown scale
prior mean function: constant mean with unknown value
prior covariance function: MatÃ©rn ğœˆ = 5/2 with unknown output and length scales
eliminating mean parameter via marginalization: Â§ . , p. We would ideally marginalize the other parameters as well, but it would not result in a Gaussian process, as we will discuss shortly.

,

,

Figure . : Samples from our example

ğœğ‘›

model space for a range of the

hyperparameters: ğœğ‘›, the observation noise scale, and â„“,

the characteristic length scale.

The output scale ğœ† is xed

for each example. Each exam-

ple demonstrates a sample of

the latent function and obser-

vations resulting from mea-

surements at a xed set of

locations x. Elements of the

model space can model func-

tions with short- or long-scale

correlations that are observed

with a range of delity from

virtually exact observation to

extreme noise.

â„“

samples from the joint prior over the objective function and the observed values y that would result from measurements at locations x ( . ) for a range of these hyperparameters. Even this simple model space is quite
exible, o ering degrees of freedom for the variation in the objective
function and the precision of our measurements.

.

.

and .

( ). A survey

of Bayesian predictive methods for model as-

sessment, selection and comparison. Statistics

Surveys : â€“ .

As it is most likely that no model among the candidates actually generated the data, some authors have suggested that any choice of prior is dubious. If this bothers the reader, it can help to frame the inference as being over the model â€œclosest to the truthâ€ rather than over the â€œtrue modelâ€ itself.
model prior, ğ‘ (ğœ½ )

Given a space of candidate models, we now turn to the question of assessing the quality of these models in light of data. There are multiple paths forward, but Bayesian inference o ers one e ective solution. By accepting that we can never be absolutely certain regarding which model is the most faithful representation of a given system, we can â€“ as with anything unknown in the Bayesian approach â€“ treat that â€œbest modelâ€ as a random variable to be inferred from data and prior beliefs.
We will limit this initial discussion to parametric model spaces built from a single model structure ( . ), which will simplify notation and allow us to con ate models and their corresponding hyperparameters ğœ½ as convenient. We will consider more complex spaces comprising multiple alternative model structures presently.
Model prior
We rst endow the model space with a prior encoding which models are more plausible a priori, ğ‘ (ğœ½ ). For convenience, it is common to design the model hyperparameters such that the uninformative (and possibly improper) â€œuniform priorâ€

ğ‘ (ğœ½) âˆ 1

(.)

..
Figure . : The dataset for our model assessment example, generated using a hidden model from the space on the facing page.

may be used, in which case the model prior may not be explicitly acknowledged at all. However, it can be helpful to express at least weakly informative prior beliefs â€“ especially when working with small datasets â€“ as it can o er gentle regularization away from patently absurd choices. This should be possible for most hyperparameters in practice. For example, when modeling a physical system, it would be unlikely that interaction length scales of say one nanometer and one kilometer would be equally plausible a priori; we might capture this intuition with a wide prior on the logarithm of the length scale.

Model posterior
Given a set of observations D = (x, y), we may appeal to Bayesâ€™ theorem to derive the posterior distribution over the candidate models:

ğ‘ (ğœ½ | D) âˆ ğ‘ (ğœ½ ) ğ‘ (y | x, ğœ½ ).

(.)

The model posterior provides support to the models most consistent with our prior beliefs and the observed data. Consistency with the data is encapsulated by the ğ‘ (y | x, ğœ½ ) term, the prior over observations evaluated on the actual data. This value is known as the model evidence or the marginal likelihood of the data, as it serves as a likelihood in Bayesâ€™ theorem ( . ) and, in our class of latent function models, is computed by marginalizing the latent function values at the observed locations ( . ).

Marginal likelihood and Bayesian Occamâ€™s razor

Model assessment becomes trivial in light of the model posterior if we

simply establish preferences over models according to their posterior

probability. When using the uniform model prior ( . ) (perhaps implic-

itly), the model posterior is proportional to the marginal likelihood alone,

which can be then used directly for model assessment.

It is commonly argued that the model evidence encodes automatic

penalization for model complexity, a phenomenon known as Bayesian

Occamâ€™s razor.

outlines a simple argument for this e ect by

noting that a model ğ‘ (y | x) must integrate to unity over all possible

measurements y. Thus if a â€œsimplerâ€ model wishes to become more

â€œcomplexâ€ by putting support over a wider range of possible observations,

it can only do so by reducing the support for the datasets that are already

well explained; see the illustration in the margin.

The marginal likelihood of a given dataset can be conveniently com-

puted in closed form for Gaussian process models with additive Gaussian

Draft as of January . Feedback welcome: https://bayesoptbook.com/

model posterior, ğ‘ (ğœ½ | D)

Recall that this distribution is precisely what a model de nes: Â§ . , p. .
model evidence, marginal likelihood, ğ‘ (y | x, ğœ½ )

.. .

( ). Information Theory, In-

ference, and Learning Algorithms. Cambridge

University Press. [chapter ]

y

S

A cartoon of the Bayesian Occamâ€™s razor

e ect due to

. Interpreting models

as s over measurements y, the â€œsimpleâ€

model in red explains datasets in S well, but

not elsewhere. The â€œcomplexâ€ model in blue

explains datasets outside S better, but in S

worse; the probability density must be lower

there to explain a broader range of data.

,

,

Figure . : The posterior distribution over the model space from gure . (the range of the axes are compatible with that gure) conditioned on the dataset in gure . . The output scale is xed (to its true value) for the purposes of illustration. Signi cant uncertainty remains in the exact values of the hyperparameters, but the model posterior favors models featuring either short length scales with low noise or long length scales with high noise. The points marked â€“ are referenced in gure . ; the point marked âˆ— is the ( gure . ).

marginal likelihood for Gaussian process models with additive Gaussian noise
interpretation of terms
The dataset was realized using a moderate length scale ( length scales spanning the domain) and a small amount of additive noise, shown below. But this is impossible to know from inspection of the data alone, and many alternative explanations are just as plausible according to the model posterior!

ğœğ‘›
2 3

âˆ—

1

0

â„“

noise or exact observation. In this case, we have ( . ): ğ‘ (y | x, ğœ½ ) = N (y; ğ, ğšº + N),

where ğ and ğšº are the prior mean and covariance of the latent objective function values ğ“ ( . ), and N is the observation noise covariance matrix (the zero matrix for exact observation) â€“ all of which may depend on
ğœ½ . As this value can be exceptionally small and have high dynamic
range, the logarithm of the marginal likelihood is usually preferred for
computational purposes ( . â€“ . ):

log ğ‘ (y | x, ğœ½ ) =

âˆ’

1 2

(y âˆ’ ğ)

(ğšº + N)âˆ’1 (y âˆ’ ğ) + log |ğšº + N| + ğ‘› log 2ğœ‹

.

(.)

The rst term of this expression is the sum of the squared Mahalanobis
norms ( . ) of the observations under the prior and represents a measure
of data t. The second term serves as a complexity penalty: the volume of any con dence ellipsoid under the prior is proportional to |ğšº + N|, and thus this term scales according to the volume of the modelâ€™s support
in observation space. The third term simply ensures normalization.

Return to example
Let us return to our example scenario and model space. We invite the reader to consider the hypothetical set of observations in gure . from our example system of interest and contemplate which models from our space of candidates in gure . might be the most compatible with these observations.
We illustrate the model posterior given this data in gure . , where, in the interest of visualization, we have xed the covariance output

..

posterior mean posterior % credible interval, ğ‘¦

posterior % credible interval, ğœ™

Figure . : Posterior distributions given

the observed data correspond-

1

ing to the three settings

of the model hyperparam-

eters marked in gure . .

Although remarkably di er-

ent in their interpretations,

each model represents is an

2

equally plausible explanation

in the model posterior. Model

favors near-exact observa-

tions with a short length

scale, and models â€“ favor

large observation noise with

a range of length scales. 3

scale to its true value and set the range of the axes to be compatible with the samples from gure . . The model prior was designed to be weakly informative regarding the expected order of magnitude of the hyperparameters by taking independent, wide Gaussian priors on the logarithm of the observation noise and covariance length scale.
The rst observation we can make regarding the model posterior is that it is remarkably broad, with many settings of the model hyperparameters remaining plausible after observing the data. However, the model posterior does express a preference for models with either low noise and short length scale or high noise combined with a range of compatible length scales. Figure . provides examples of objective function and observation posteriors corresponding to the hyperparameters indicated in gure . . Although each is equally plausible in the posterior, their explanations of the data are diverse.

.

Winnowing down a space of candidate models to a single model for use

in inference and prediction is known as model selection. Model selection

becomes straightforward if we agree to rank candidates according to the

model posterior, as we may then select the maximum a posteriori ( )

( . ) model:

ğœ½Ë† = arg max ğ‘ (ğœ½ ) ğ‘ (y | x, ğœ½ ).

(.)

ğœ½

When the model prior is at ( . ), the model corresponds to the maximum likelihood estimate ( ) of the model hyperparameters. Fig-

Draft as of January . Feedback welcome: https://bayesoptbook.com/

Both parameters are nonnegative, so the prior has support on the entire parameter range.
The posterior probability density of these points is approximately % of the maximum.
If we only wish to nd the maximum, there is no bene t to normalizing the posterior.

,

,

Figure . : The predictions of the maximum a posteriori ( ) model from the example data in gure . .

acceleration via gradient-based optimization
gradient of log marginal likelihood with respect to ğœ½ : Â§ . , p.

ure . shows the predictions made by the model for our running example; in this case, the hyperparameters are in fact a reasonable match to the parameters used to generate the example dataset.
When the model space is de ned over a continuous space of hyperparameters, computation of the model can be signi cantly accelerated via gradient-based optimization. Here it is advisable to work in the log domain, where the objective becomes the unnormalized log posterior:

log ğ‘ (ğœ½ ) + log ğ‘ (y | x, ğœ½ ).

(. )

The log marginal likelihood is given in ( . ), noting that ğ, ğšº, and N are all implicitly functions of the hyperparameters ğœ½ . This objective ( . ) is di erentiable with respect to ğœ½ assuming the Gaussian process prior moments, the noise covariance, and the model prior are as well, in which case we may appeal to o -the-shelf gradient methods for solving ( . ). However, a word of warning is in order: the model posterior is not guaranteed to be concave and may have multiple local maxima, so multistart optimization is prudent.

.

model-marginal objective posterior, ğ‘ ( ğ‘“ | D)
model-marginal predictive distribution, ğ‘ (ğ‘¦ | ğ‘¥, D)

Reliance on a single model is questionable when the model posterior is

not well determined by the data. For example, in our running example,

a diverse range of models are consistent with the data ( gures . â€“ . ).

Committing to a single model in this case may systematically bias our

predictions and underestimate predictive uncertainty â€“ note how the

diversity in predictions from gure . is lost in the model ( . ).

An alternative is to marginalize the model with respect to the model

posterior, a process known as model averaging:

âˆ«

ğ‘ (ğ‘“ | D) = ğ‘ (ğ‘“ | D, ğœ½ ) ğ‘ (ğœ½ | D) dğœ½ ;

(. )

âˆ¬

ğ‘ (ğ‘¦ | ğ‘¥, D) = ğ‘ (ğ‘¦ | ğ‘¥, ğœ™, ğœ½ ) ğ‘ (ğœ™ | ğ‘¥, D, ğœ½ ) ğ‘ (ğœ½ | D) dğœ™ dğœ½, ( . )

Although it may be unusual to consider the choice of model a â€œnuisance!â€

where we have marginalized the hyperparameters of both the objective and observation models. Model averaging is more consistent with the ideal Bayesian convention of marginalizing nuisance parameters when possible and promises robustness to model misspeci cation, at least over the chosen model space.
Unfortunately, neither of these model-marginal distributions ( . â€“ . ) can be computed exactly for Gaussian process models except in

posterior mean

posterior % credible interval, ğ‘¦

.. posterior % credible interval, ğœ™

Figure . : A Monte Carlo estimate to the model-marginal predictive distribution ( . ) for our example sceneario using samples drawn from the model posterior in gure . ( . â€“ . ); see illustration in margin. Samples
from the objective function posterior display a variety of behavior due to being associated with di erent hyperparameters.

some special cases, so we must resort to approximation if we wish

to pursue this approach. In fact, maximum a posteriori estimation can

be interpreted as one rather crude approximation scheme where the

model posterior is replaced by a Dirac delta distribution at the

hyperparameters:

ğ‘ (ğœ½ | D) â‰ˆ ğ›¿ (ğœ½ âˆ’ ğœ½Ë†).

This can be defensible when the dataset is large compared to the number of hyperparameters, in which case the model posterior is often unimodal with little residual uncertainty. However, large datasets are the exception rather than the rule in Bayesian optimization, and more sophisticated approximations can pay o when model uncertainty is signi cant.

Monte Carlo approximation
Monte Carlo approximation is one straightforward path forward. Drawing a set of hyperparameter samples from the model posterior,

{ğœ½ğ‘– }ğ‘ ğ‘–=1 âˆ¼ ğ‘ (ğœ½ | D),

(. )

yields the following simple Monte Carlo estimates:

ğ‘(ğ‘“

| D) â‰ˆ

1 ğ‘ 

âˆ‘ğ‘ ï¸
ğ‘– =1

GP

ğ‘“ ; ğœ‡D (ğœ½ğ‘– ), ğ¾D (ğœ½ğ‘– )

;

ğ‘ (ğ‘¦

|

ğ‘¥, D)

â‰ˆ

1 ğ‘ 

âˆ‘ğ‘ ï¸ âˆ«
ğ‘– =1

ğ‘ (ğ‘¦

|

ğ‘¥, ğœ™, ğœ½ğ‘–)

ğ‘ (ğœ™

|

ğ‘¥, D, ğœ½ğ‘– )

dğœ™ .

(. ) (. )

The objective function posterior is approximated by a mixture of Gaussian processes corresponding to the sampled hyperparameters, and the posterior predictive distribution for observations is then derived by integrating a Gaussian mixture ( . ) against the observation model.
Any Markov chain Monte Carlo procedure could be used to generate the hyperparameter samples ( . ); a variation on Hamiltonian Monte

Draft as of January . Feedback welcome: https://bayesoptbook.com/

A notable example is marginalizing the coe cients of a linear prior mean against a Gaussian prior: Â§ . , p. .
ğœğ‘›
â„“ The hyperparameter samples used to produce gure . .

,

,

posterior mean

posterior % credible interval, ğ‘¦

posterior % credible interval, ğœ™

Figure . : An approximation to the model-marginal posterior ( . ) using the central composite design approach proposed by et al. A total of nine hyperparameter samples are used for the approximation, illustrated in the margin below.

..

and .

( ). The

No-U-turn Sampler: Adaptively Setting Path

Lengths in Hamiltonian Monte Carlo. Journal

of Machine Learning Research ( ): â€“ .

Carlo ( ) such as the no -turn sampler ( ) would be a reasonable choice when the gradient of the log posterior ( . ) is available, as it can exploit this information to accelerate mixing.
Figure . demonstrates a Monte Carlo approximation to the modelmarginal posterior ( . â€“ . ) for our running example. Comparing with the approximation in gure . , the predictive uncertainty of both objective function values and observations has increased considerably due to accounting for model uncertainty in the predictive distributions.

Laplace approximation: Â§ . , p.

. et al. ( ). Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations.
Journal of the Royal Statistical Society Series B (Methodological) ( ): â€“ .

. . . and . .

( ). On the Ex-

perimental Attainment of Optimum Condi-

tions. Journal of the Royal Statistical Society

Series B (Methodological) ( ): â€“ .

Deterministic approximation schemes

The downside of Monte Carlo approximation is relatively ine cient use of the hyperparameter samples â€“ the price of random sampling rather than careful design. This ine ciency in turn leads to an increased computational burden for inference and prediction from having to derive a posterior for each sample. Several more e cient (but less accurate) alternative approximations for hyperparameter marginalization have also been proposed. A common simplifying tactic taken by these cheaper procedures is to approximate the hyperparameter posterior with a multivariate normal via a Laplace approximation:

ğ‘ (ğœ½ | D) â‰ˆ N (ğœ½ ; ğœ½Ë†, C),

(. )

where ğœ½Ë† is the ( . ). Integrating this approximation into ( . ) gives

âˆ«

ğ‘ (ğ‘“ | D) â‰ˆ GP ğ‘“ ; ğœ‡D (ğœ½ ), ğ¾D (ğœ½ ) N (ğœ½ ; ğœ½Ë†, C) dğœ½ .

(. )

Unfortunately this integral remains intractable due to the nonlinear dependence of the posterior moments on the hyperparameters, but reducing to this common form allows us to derive deterministic approximations against a single assumed posterior.
et al. introduced several approximation schemes representing di erent tradeo s between e ciency and delity. Notable among these is a simple, sample-e cient procedure grounded in classical experimental design. Here a central composite design in hyperparameter

posterior mean

posterior % credible interval, ğ‘¦

.. posterior % credible interval, ğœ™

Figure . : The approximation to the model-marginal posterior ( . ) for our running example using the approach proposed

by

et al.

space is transformed to agree with the moments of ( . ), then used as nodes in a numerical quadrature approximation to ( . ). The resulting approximation again takes the form of a (now weighted) mixture of Gaussian processes ( . ): the model augmented by a small number of additional models designed to re ect the important variation in the hyperparameter posterior. The number of hyperparamater samples required by this scheme grows relatively slowly with the dimension of the hyperparameter space: less than for |ğœ½ | â‰¤ 8 and less than for |ğœ½ | â‰¤ 21. The nine samples required for our running example are shown in the marginal gure. Figure . shows the resulting approximate posterior; comparing with the gold-standard Monte Carlo approximation from gure . , the agreement is excellent.
An even more lightweight approximation was proposed by et al., which despite its crudeness is arguably still preferable to estimation and can be used as a drop-in replacement. This approach again relies on a Laplace approximation to the hyperparameter posterior ( . â€“ . ). The key observation is that under the admittedly strong assumption that the posterior mean were in fact linear in ğœ½ and the posterior covariance independent of ğœ½, we could resolve ( . ) in closed form. We proceed by taking the best linear approximation to the posterior mean around the :

ğœ‡D (ğ‘¥; ğœ½ ) â‰ˆ ğœ‡D (ğ‘¥; ğœ½Ë† ) + g(ğ‘¥) (ğœ½ âˆ’ ğœ½Ë† );

g (ğ‘¥ )

=

ğœ•

ğœ‡D (ğ‘¥ ğœ•ğœ½

;

ğœ½

)

(ğœ½Ë†

),

and assuming the posterior covariance is universal: ğ¾D (ğœ½ ) â‰ˆ ğ¾D (ğœ½Ë†). The result is a single Gaussian process approximation to the posterior:

ğ‘ (ğ‘“ | D) â‰ˆ GP (ğ‘“ ; ğœ‡Ë†D, ğ¾Ë†D),

(. )

where

ğœ‡Ë†D (ğ‘¥) = ğœ‡D (ğ‘¥; ğœ½Ë† ); ğ¾Ë†D (ğ‘¥, ğ‘¥ ) = ğ¾D (ğ‘¥, ğ‘¥ ; ğœ½Ë† ) + g(ğ‘¥) Cg(ğ‘¥ ).

Draft as of January . Feedback welcome: https://bayesoptbook.com/

ğœğ‘›

â„“
A Laplace approximation to the model posterior (performed in the log domain) and hyperparameter settings corresponding to the central composite design proposed by et al. The samples do a good job covering the support of the true posterior.

..

and . .

( ). Very

Large Fractional Factorial and Central Com-

posite Designs. Transactions on Modeling

and Computer Simulation ( ): â€“ .

..

et al. ( ). Active Learning of

Model Evidence Using Bayesian Quadrature.

eur

.

This is analogous to the linearization step in the extended Kalman lter, whereas the central composite design approach is closer to the unscented Kalman lter in pushing samples through the nonlinear transformation.

,

,

uncertainty in additive noise scale ğœğ‘›
This term vanishes if the hyperparameters are completely determined by the data, in which case the approximation regresses gracefully to the estimate. In general we have
ğ‘ (ğ‘¦ | ğ‘¥, D) = âˆ¬ ğ‘ (ğ‘¦ | ğ‘¥, ğœ™, ğœğ‘›) ğ‘ (ğœ™, ğœğ‘› | ğ‘¥, D) dğœ™ dğœğ‘›,
and we have resolved the integral on ğœ™ using the single- approximation.

This is the model with covariance in ated by a term determined by the dependence of the posterior mean on the hyperparameters, g, and the uncertainty in the hyperparameters, C.
et al. did not address how to account for uncertainty in observation model parameters when approximating ğ‘ (ğ‘¦ | ğ‘¥, D), but we can derive a natural approach for independent additive Gaussian noise with unknown scale ğœğ‘›. Given ğ‘¥, let ğ‘ (ğœ™ | ğ‘¥, D) â‰ˆ N (ğœ™; ğœ‡, ğœ2) as in ( . ). We must approximate
âˆ« ğ‘ (ğ‘¦ | ğ‘¥, D) â‰ˆ N (ğ‘¦; ğœ‡, ğœ2 + ğœğ‘›2) ğ‘ (ğœğ‘› | ğ‘¥, D) dğœğ‘›.
A moment-matched approximation ğ‘ (ğ‘¦ | ğ‘¥, D) â‰ˆ N (ğ‘¦; ğ‘š, ğ‘ 2) is possible by appealing to the law of total variance:
ğ‘š = ğ”¼[ğ‘¦ | ğ‘¥, D] â‰ˆ ğœ‡; ğ‘ 2 = var[ğ‘¦ | ğ‘¥, D] â‰ˆ ğœ2 + ğ”¼[ğœğ‘›2 | ğ‘¥, D].
If the noise scale is parameterized by its logarithm, then the Laplace approximation ( . ) in particular yields
ğ‘ (log ğœğ‘› | ğ‘¥, D) â‰ˆ N (log ğœğ‘›; log ğœË†ğ‘›, ğ‘ 2); ğ”¼[ğœğ‘›2 | ğ‘¥, D] â‰ˆ ğœË†ğ‘›2 exp(2ğ‘ 2).
Thus we predict with the estimate ğœË†ğ‘› in ated by a factor commensurate with the residual uncertainty in the noise contribution.
Figure . shows the resulting approximation for our running example. Although not perfect, the predictive uncertainty in the observations is more faithful than the model from gure . , which severely underestimates the scale of observation noise in the posterior.

model structure index, M model structure prior, Pr(M)

.

We have now covered model inference, selection, and averaging with a single parametric model space ( . ). With a bit of extra bookkeeping, we may extend this framework to handle multiple model structures comprising di erent combinations of parametric prior moments and observation models.
To begin, we may build a space of candidate models by taking a discrete union of parametric spaces as in ( . ), with one built from each desired model structure: {Mğ‘– }. It is natural to index this space by (ğœ½, M), where ğœ½ is understood to be a vector of hyperparameters associated with the speci ed model structure; the size and interpretation of this vector may di er across structures. All that remains is to derive our previous results while managing this compound structureâ€“hyperparameter index.
We may de ne a model prior over this compound space by combining a prior over the chosen model structures with priors over the hyperparameters of each:

ğ‘ (ğœ½, M) = Pr(M) ğ‘ (ğœ½ | M).

(. )

Given data, the model posterior has a similar form as before ( . ):

ğ‘ (ğœ½, M | D) = Pr(M | D) ğ‘ (ğœ½ | D, M).

(. )

..
Figure . : The objective and dataset for our multiple-model example.

The structure-conditional hyperparameter posterior ğ‘ (ğœ½ | D, M) is as in ( . ) and may be reasoned about following our previous discussion. The model structure posterior is then given by

Pr(M | D) âˆ Pr(M) ğ‘ (y | x, M); âˆ«
ğ‘ (y | x, M) = ğ‘ (y | x, ğœ½, M) ğ‘ (ğœ½ | M) dğœ½ .

(. ) (. )

The expression in ( . ) is the normalizing constant of the structure-

conditional hyperparameter posterior ( . ), which we could ignore when

there was only a single model structure. This integral is in general

intractable, but several approximations are feasible. One e ective choice

is the Laplace approximation ( . ), which provides an approximation

to the integral as a side e ect ( . ). The classical Bayesian information

criterion ( ) may be seen as an approximation to this approximation.

Model selection may now be pursued by maximizing the model

posterior over the model space as before, although we may no longer

appeal to gradient methods as the model space is not continuous with

multiple model structures. A simple approach would be to nd the

hyperparameters for each of the model structures separately, then use

these points to approximate ( . ) for each structure via the Laplace

approximation or . This would be su cient to estimate ( . â€“ . )

and maximize over the models.

Turning to model averaging, the model-marginal posterior to the

objective function is:

âˆ‘ï¸ ğ‘ (ğ‘“ | D) = Pr(Mğ‘– | D) ğ‘ (ğ‘“ | D, Mğ‘– ).

(. )

ğ‘–

The structure-conditional, hyperparameter-marginal distribution on each space ğ‘ (ğ‘“ | D, M) is as before ( . ) and may be approximated following our previous discussion. These are now combined in a mixture distribution weighted by the model structure posterior ( . ).

Multiple structure example
We now present an example of model inference, selection, and averaging over multiple model structures using the dataset in gure . . The data were sampled from a Gaussian process with linear prior mean (a linear trend with positive slope is evident) and MatÃ©rn ğœˆ = 3/2 prior covariance ( . ), with a small amount of additive Gaussian noise. We also show a sample from the objective function posterior corresponding to the true model generating the data for reference.
Draft as of January . Feedback welcome: https://bayesoptbook.com/

model structure posterior, Pr(M | D)

Laplace approximation: Â§ . , p.

.

and .

( ). Informa-

tion Criteria and Statistical Modeling. Springerâ€“

Verlag. [chapter ]

model selection

model averaging

The data are used as a demo in the code released with:

..

and . . .

( ).

Gaussian Processes for Machine Learning.

Press.

,

,

Ã—

Figure . : Sample paths from our example model structures.

+

Ã—

initial model structure: p.
Ã— + Ã—
approximation to model structure posterior

We build a model space comprising several model structures by augmenting our previous space with structures incorporating additional covariance functions. The treatment of the prior mean (unknown constant marginalized against a Gaussian prior) and observation model (additive Gaussian noise with unknown scale) will remain the same for all. The model structures re ect a variety of hypotheses positing potential linear or quadratic behavior:
: the MatÃ©rn ğœˆ = 5/2 covariance ( . ) from our previous example;
: the linear covariance ( . ), where the the prior on the slope is vague and centered at zero and the prior on the intercept agrees with the model;
: the product of two linear covariances designed as above, modeling a latent quadratic function with unknown coe cients;
: the sum of a MatÃ©rn ğœˆ = 5/2 and linear covariance designed as in the corresponding individual model structures; and
: the product of a MatÃ©rn ğœˆ = 5/2 and linear covariance designed as in the corresponding individual model structures.

Objective function samples from models in each of these structures are shown in gure . . Among these, the model structure closest to the truth is arguably + .
Following the above discussion, we nd the hyperparameters for each of these model structures separately and use a Laplace approximation ( . ) to approximate the hyperparameter posterior on each space, along with the normalizing constant ( . ). Normalizing over the structures provides an approximate model structure posterior:

Pr( Pr( + Pr( Ã—

| D) â‰ˆ 10.8%; | D) â‰ˆ 71.8%; | D) â‰ˆ 17.0%,

with the remaining model structures ( and Ã— ) sharing the re-

..

posterior mean posterior % credible interval, ğœ™

objective

Figure . : An approximation to the model-marginal posterior ( . ) for our multiple-model example. The posterior on each model structure is approximated separately as a mixture of Gaussian processes following et al. (see gure . ); these are then combined by weighting by an approximation of the model structure posterior ( . ). We show the result with three superimposed, transparent credible intervals, which are shaded with respect to their weight in contributing to the nal approximation.

maining 0.4%. The + model structure is the clear winner, and there is strong evidence that the purely polynomial models are insu cient for explaining the data alone.
Figure . illustrates an approximation to the model-marginal posterior ( . ), approximated by applying et al.â€™s central composite design approach to each of the model structures, then combining these into a Gaussian process mixture by weighting by the approximate model structure posterior. The highly asymmetric credible intervals re ect the diversity in explanations for the data o ered by the chosen model structures, and the combined model makes reasonable predictions of our example objective function sampled from the true model.
For this example, averaging over the model structure has important implications regarding the behavior of the resulting optimization policy. Figure . illustrates a common acquisition function built from the o the-shelf model, as well as from the structure-marginal model. The former chooses to exploit near what it believes is a local optimum, but the latter has a strong belief in an underlying linear trend and chooses to explore the right-hand side of the domain instead. For our example objective function sample, this would in fact reveal the global optimum with the next observation.
.
We now have a comprehensive framework for reasoning about model uncertainty, including methods for model assessment, selection, and averaging across one or multiple model structures. However, it is still not clear how we should determine which model structures to consider for a given system. This is critical as our model inference procedure requires
Draft as of January . Feedback welcome: https://bayesoptbook.com/

approximation to marginal predictive distribution
averaging over a space of Gaussian processes in policy computation: Â§ . , p. speci cally, expected improvement: Â§ . p.

,

,

acquisition function

next observation location

Figure . : Optimization policies built from the

model (left) and the structure-marginal posterior (right). The

model chooses to exploit near the local optimum, but the structure-marginal model is aware of the underlying

linear trend and chooses to explore the right-hand side as a result.

.

et al. ( ). Structure Discovery

in Nonparametric Regression through Com-

positional Kernel Search.

.

addition and multiplication of covariance functions: Â§ . , p.

base kernels, B

the space of candidate models to be prede ned; equivalently, the model prior ( . ) is implicitly set to zero for every model outside this space. Ideally, we would simply enumerate every possible model structure and average over all of them, but even a naÃ¯ve approximation of this ideal would entail overwhelming computational e ort.
However, the set of model structures we consider for a given dataset can be adaptively tailored as we gather data. One powerful idea is to appeal to metaheuristics such as local search: by establishing a suitable space of candidate model structures, we can dynamically explore this space for the best explanations of available data.

Spaces of candidate model structures

To enable this approach, we must rst establish a su ciently rich space

of candidate model structures.

et al. proposed one convenient

mechanism for de ning such a space via a simple productive grammar.

The idea is to appeal to the closure of covariance functions under ad-

dition and pointwise multiplication to systematically build up families

of increasingly complex models from simple components. We begin by

choosing a set of so-called base kernels, B, modeling relatively simple

behavior, then extend this set to an in nite family of compositions via

the following context-free grammar:

ğ¾ â†’ğµ ğ¾ â†’ğ¾ +ğ¾ ğ¾ â†’ ğ¾ğ¾ ğ¾ â†’ (ğ¾).

The symbol ğµ in the rst rule represents any desired base kernel. The ve model structures considered in our multiple-structure example above in

..

fact represent ve members of the language generated by this grammar with the base kernels B = {ğ¾M5/2, ğ¾ } or simply B = { , }. The grammar however also generates arbitrarily more complicated expres-
sions such as

+( + ) ( + )+ .

(. )

We are free to design the base kernels to capture any potential atomic behavior in the objective function. For example, if the domain is high dimensional and we suspect that the objective may depend only on sparse interactions of mostly independent variables, we might design the base kernels to model variations in single variables at a time, then rely on the grammar to generate an array of possible interaction structures.
Other spaces of model structures have also been proposed for automated structure search. With an eye toward high-dimensional domains,
et al. for example considered spaces of additive model structures indexed by every possible partition of the input variables. This is an expressive class of model structures, but the number of partitions grows so rapidly that exhaustive search is not feasible.

Samples from objective function models incorporating the example covariance structure ( . ).
additive decompositions, Â§ . , p.

..

et al. ( ). Discovering and Ex-

ploiting Additive Structure for Bayesian Opti-

mization.

.

Searching over model structures

Once a space of candidate model structures has been established, we

may develop a search procedure seeking the most promising structures

to explain a given dataset. Several approaches have been proposed for

this search with a range of complexity, all of which frame the problem in

terms of optimizing some gure of merit over the space. Although any

score could be used in this context, a natural choice is an approximation

to the (unnormalized) model structure posterior ( . ) such as the Laplace

approximation or the Bayesian information criterion, and every method

described below uses one of these two scores.

et al. suggested traversing their kernel grammar via

greedy search â€“ here, we rst evaluate the base kernels, then subject the

productive rules to the best among them to generate similar structures to

search next. We continue in this manner as desired, alternating between

evaluating the newly proposed structures, then using the grammar to

expand around the best-seen structure to generate new proposals. This

simple procedure is easy to implement and o ers a strong baseline.

et al. re ned this approach by replacing greedy search

with Bayesian optimization over the space of model structures. As in

the

et al. procedure, the authors pose the problem in terms of

maximizing a score over model structures: a Laplace approximation of

the (log) unnormalized structure posterior ( . ). This objective function

was then modeled using a Gaussian process, which informed a sequen-

tial Bayesian optimization procedure seeking to e ectively manage the

explorationâ€“exploitation tradeo in the space of candidate structures.

The Gaussian process in model space requires a covariance function over

model structures, and the authors proposed an exotic â€œkernel kernelâ€

evaluating the similarity of proposed structures in terms of the overlap

Draft as of January . Feedback welcome: https://bayesoptbook.com/

.

et al. ( ). Bayesian optimiza-

tion for automated model selection. eur

.

,

,

between their hyperparameter-marginal priors for the given dataset. The resulting optimization procedure was found to rapidly locate promising models across a range of regression tasks.

.

and .

( ). Automat-

ing Bayesian optimization with Bayesian opti-

mization. eur

.

..

et al. ( ). Discovering and Ex-

ploiting Additive Structure for Bayesian Opti-

mization.

.

End-to-end automation

Follow-on work demonstrated a completely automated Bayesian opti-

mization system built on this structure search procedure avoiding any

manual modeling at all. The key idea was to dynamically maintain a

set of plausible model structures throughout optimization. Predictions

are made via model averaging over this set, o ering robustness to model

misspeci cation when computing the outer optimization policy. Every

time a new observation is obtained, the set of model structures is then

updated via a continual Bayesian optimization in model space given the

new data. This interleaving of Bayesian optimization in data space and

model space o ered promising performance.

Finally,

et al. o ered an alternative to optimization over

model structures by constructing a Markov chain Monte Carlo routine

to sample model structures from their posterior ( . ). The proposed

sampler was a realization of the Metropolisâ€“Hastings algorithm with a

custom proposal distribution making minor modi cations to the incum-

bent structure. In the case of the additive decompositions considered

in that work, this step consisted of applying random atomic operations

such as merging or splitting components of the existing decomposition.

Despite the absolutely enormous number of possible additive decomposi-

tions, this

routine was able to quickly locate promising structures,

and averaging over the sampled structures for prediction resulted in

superior optimization performance as well.

.

et al. ( ). Practical Bayesian Op-

timization of Machine Learning Algorithms.

eur

.

models and model structures: Â§ . , p.

.
We have presented a convenient framework for model assessment, selection, and averaging grounded in Bayesian inference; this is the predominant approach with Gaussian process models. In the context of Bayesian optimization, perhaps the most important development was the notion of model averaging, which has proven bene cial to empirical performance and has become standard practice.
â€¢ Model assessment entails deriving preferences over a space of candidate models of a given system in light of available data.
â€¢ In its purest form, a model in this context is a prior distribution over observed values y arising from observations at a given set of locations x, ğ‘ (y | x). A convenient mechanism for specifying a model is via a prior process for a latent function, ğ‘ (ğ‘“ ), and an observation model conditioned on this function, ğ‘ (ğ‘¦ | ğ‘¥, ğœ™) ( . â€“ . ).
â€¢ With Gaussian process models, it is convenient to work with combinations of parametric forms for the prior mean function, prior covariance

