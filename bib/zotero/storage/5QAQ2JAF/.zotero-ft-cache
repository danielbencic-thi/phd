IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathZoom.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Journals & Magazines > IEEE Transactions on Intellig... > Volume: 22 Issue: 7
ReViewNet: A Fast and Resource Optimized Network for Enabling Safe Autonomous Driving in Hazy Weather Conditions
Publisher: IEEE
Cite This
PDF
Aryan Mehra ; Murari Mandal ; Pratik Narang ; Vinay Chamola
All Authors
8
Paper
Citations
917
Full
Text Views

    Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Related Work
    III.
    Proposed Method
    IV.
    Experiments and Analysis
    V.
    Conclusion

Authors
Figures
References
Citations
Keywords
Metrics
Abstract:
Adverse weather conditions such as fog, haze, snow, mist and glare create visibility problems for applications of autonomous vehicles. To ensure safe and smooth operations in frequent bad weather scenarios, image dehazing is crucial to any vehicular motion and navigation task on road or air. Moreover, the commonly deployed mobile systems are resource constrained in nature. Therefore, it is important to ensure memory, compute and run-time efficiency of dehazing algorithms. In this manuscript we propose ReViewNet, a fast, lightweight and robust dehazing system suitable for autonomous vehicles. The network uses components like spatial feature pooling, quadruple color-cue, multi-look architecture and multi-weighted loss to effectively dehaze images captured by cameras of autonomous vehicles. The effectiveness of the proposed model is analyzed by exhaustive quantitative evaluation on five benchmark datasets demonstrating its supremacy over other existing state-of-the-art methods. Further, a component-wise ablation and loss weight ratio analysis demonstrates the contribution of each and every component of the network. We also show the qualitative analysis with special use cases and visual responses on distinctive vehicular vision instances, establishing the effectiveness of the proposed method in numerous hazy weather conditions for autonomous vehicular applications.
Published in: IEEE Transactions on Intelligent Transportation Systems ( Volume: 22 , Issue: 7 , July 2021 )
Page(s): 4256 - 4266
Date of Publication: 14 August 2020
ISSN Information:
INSPEC Accession Number: 21049754
DOI: 10.1109/TITS.2020.3013099
Publisher: IEEE
Funding Agency:
SECTION I.
Introduction

Visibility for autonomous vehicular systems powered with perception based sensors for navigation and surveillance is severely hindered by adverse weather condition. For autonomous vehicles, the low-level visual perception functions such as object detection [1] – [2] [3] , segmentation [4] – [5] [6] , object tracking [7] , [8] require clear image representation of the street scenes. Similarly, for accurate analysis of the surveillance videos, good quality image/frames are desired. As some of the most common bad-weather conditions, fog, mist, and haze drastically degrade the visual quality of images. Such visibility degradation has negative impacts on the performance of other vision-based systems as mentioned above. More importantly, effective dehazing is one of the primary tasks to avoid accidents in driver-less vehicular applications [9] on land [10] , water or air [11] . The dehazing methods also need to be fast and close to real-time since they have to extended to real-time video applications, which necessitates the need for resource efficiency. While deep learning has been used for image processing in general [12] , [13] , the onset of deep learning in vehicular technologies is ever increasing and efficient as well [14] .

The haze in the atmosphere creates whitening effect which occludes and deforms both the foreground and background. Distant haze further reduces the visibility by the accumulated veiling effect. In addition to the deterioration in color, contrast and texture features, the degradation in hazy images also increases non-linearly with change in the distance between the camera lens and the scene, making accurate dehazing a very challenging task. The effect of fog/haze is significant in street scenes resulting in degradation of high-level perception functions of autonomous vehicles and surveillance systems [15] .

In most of the dehazing algorithms, the physical scattering model is frequently used to represent image formation. In this model, the image is formulated based on the properties of light transmission through the air. The earlier learning-based dehazing methods in the literature have used the physical scattering model for dehazing. The network usually learns one or more of the components of the scattering model. However, the accuracy of the estimated atmospheric light and transmission map greatly influence the quality of the dehazed image. The disjoint optimization of transmission map or atmospheric light may hamper the overall dehazing performance.

In this paper, we formulate the image dehazing problem as an end-to-end image-to-image mapping task, free from the intermediate computation of transmission map without relying on the physical scattering model. We propose a fast, lightweight network, ReViewNet, for dehazing in autonomous vehicles. To the best of our knowledge, only few papers [16] , [17] have adopted such intermediate-computation free approach using the Pix-to-Pix and Cycle-GAN architectures respectively. However, re-purposing image-to-image translation GANs for dehazing can be very difficult to optimize and not necessarily produce the optimal results, as can be verified in the experimental results comparison in Section IV . Moreover, GANs tend to be heavier in computation at train and test time, having indirectly encompassed two networks within them. The proposed ReViewNet generates more realistic haze-free images in terms of color and details in comparison to existing state-of-the-art approaches, with lesser trainable parameters and faster runtime. The main contributions of this work can be summarized as follows:

    We propose a fast and lightweight end-to-end model ReViewNet for image dehazing. The model is free from any intermediate component computation for the physical scattering model and, thus, learns the most optimal mapping between the hazy and haze-free image while being easily extendable to real time applications that process multiple frames per second.

    The ReViewNet is designed to have multiple looks at different stages in the network for dehazing. We use different loss weights ratio for the first and second look. We introduce a new bottleneck parallel spatial cleaning module. Moreover, we feed the multi-cue color space (RGB, HSV, YCrCb and LAB) to the network for robust haze removal.

    ReViewNet is a fast and highly resource-efficient (5 MB model size, 399,670 trainable parameters) network and can perform image dehazing in a highly resource-constrained environment with high speed (CPU speed – 0.28 seconds per frame, GPU speed – 0.025 seconds per frame) for real-time applications.

    ReViewNet significantly outperforms the existing state-of-the-art methods in terms of PSNR and SSIM in HazeRD, D-Hazy and the more recent RESIDE-Standard (SOTS), RESIDE- β (SOTS) and RESIDE- β (HSTS) datasets.

SECTION II.
Related Work
A. Prior Based Approaches

The Dark Channel Prior (DCP) [18] approach by He et al. estimates the transmission map and soft matts the response, while Berman et al. [19] employed Non local priors (NLD) to model the hazy image with lines in the RGB space. Further, Ancuti et al. [20] used a multi-fusion algorithm to propose a night-time dehazing approach. Authors such as [19] , [21] have also estimated the atmospheric light to remove the haze. These approaches are limited by their dependence on the priors, and this causes the dehazing approach to fail in case of complicated image structures. While the results produced by such approaches often have unrealistic color distortion and contrast, their performance is also limited by the accuracy of the assumptions they make.
B. Learning Based Approaches

Learning based approaches for dehazing are generally trained to directly learn the atmospheric light, transmission map, or both. Li et al. [22] proposed AOD-Net which learns a CNN-based mapping function for the reformulated physical scattering model, while Cai et al. [23] proposed DehazeNet which estimates the intermediate transmission map which is used to generate the haze-free image. The approach of Liu et al. [24] learned haze-relevant priors with an iteration algorithm using deep CNNs. The estimation of transmission maps or atmospheric light (or both) for image dehazing has been performed by several CNN-based architectures proposed in literature [25] – [26] [27] [28] [29] .

The success of GANs in image-to-image translation tasks has also attracted its use in image dehazing. Zhu et al. [30] proposed DehazeGAN which utilizes differential programming to re-formulate the atmospheric scattering model. More recently, CD-Net [31] and RI-GAN by Dudhane et al. [32] re-purposed the Cycle-GAN architecture to learn the transmission map. Some researchers [16] , [17] , [33] have proposed GAN based architectures which argue in favor of direct image-to-image mapping over intermediate transmission map estimation. The estimation of intermediate transmission map or atmosphere light through CNN or GAN based methods increases the training cost and result generation time. Furthermore, such architecture often fail on dense haze conditions.
SECTION III.
Proposed Method

ReViewNet is unique in design and functioning, and has four pivotal contributions which make it the lightest learning-based dehazing solution – the use of quadruple color space, the double look architecture, the different loss weights ratio for the first and second look, and the use of bottleneck parallel spatial cleaning. The use of multiple color spaces provides the network a wholesome input feature vector, which enables a faster convergence. Fig. 1 shows the network architecture in greater detail.
Fig. 1. - ReViewNet Network Architecture: Inter and intra-carry connections with 4 path spatial pooling bottleneck.
Fig. 1.

ReViewNet Network Architecture: Inter and intra-carry connections with 4 path spatial pooling bottleneck.

Show All

The following sections explain each component of the network and their contribution towards the performance achieved.
A. Quadruple-Color Space

Most of the current work in the field of dehazing uses a single color space, mostly restricted to RGB or HSV. While RGB is the most common, YCrCb is also an absolute color space lying in the family of 3 dimensional vector color spaces. HSV aligns more closely with how the humans perceive the colors around them. The change in the amount of color perceived in Lab color space is same as the numerical change in the values, which is the inspiration behind it’s inception. The proposed method leverages information from above mentioned 4 color spaces, namely RGB, HSV, YCrCb and Lab, thus mapping the 12 channeled input to an RGB output. The inter-conversion between these color spaces is simply mathematical, requiring no additional storage for learnt parameters while pre-processing. Let an image matrix of height m and width n , consisting of 3 channels be represented as I 3 . Then the quadruple-color space matrix I 12 consisting of 12 channels be represented as in Eq. 1 .
I 12 = [ R m , n , 1 , G m , n , 1 , B m , n , 1 , H m , n , 1 , S m , n , 1 , V m , n , 1 , Y m , n , 1 , C b m , n , 1 , C r m , n , 1 , L m , n , 1 , a m , n , 1 , b m , n , 1 ] (1)
View Source Right-click on figure for MathML and additional features. \begin{align*} I_{12}=&[R_{m,n,1},G_{m,n,1},B_{m,n,1}, H_{m,n,1}, \\&S_{m,n,1}, V_{m,n,1},Y_{m,n,1},Cb_{m,n,1}, \\&Cr_{m,n,1},L_{m,n,1},a_{m,n,1},b_{m,n,1}] \tag{1}\end{align*} where R, G, B, H, S, V, Y, Cb, Cr, L, a, b, denote the red, green, blue, hue, saturation, value, luma, blue-difference, red-difference, lightness, chromaticity coordinates between red to green axis, chromaticity coordinates between yellow to blue axis, respectively. The RGB space is the most universal image depiction color space and has special importance because the output of the network is also RGB. Research has conclusively shown that the HSV color space is beneficial for dehazing [34] , [35] . The YCrCb color space contributes to better luminance and color contrast to the dehazed output, as analyzed by Tufail et al. [36] and Bianco et al. [37] . The YCrCb channel contributes to a lesser mean squared error as compared to the RGB channel, making it a suitable choice specifically for image dehazing and enhancement tasks. It is intuitive that the L channel of the Lab color space is pivotal to the dehazing task because haze primarily affects the lightness of the pixels, thus affecting clarity. Wang et al. [38] and [39] use the CIELAB color space modelling and remove the haze further by processing the image using simple linear iterative clustering to generate super-pixels of the image. All these advantages are clear from the performance boost that we witness by incorporating quadruple-color-space into the model, as explained in the ablation section and Table XI -F.

TABLE I Description of the Training and Testing Datasets (Number of Images Contributed in Each Category)
Table I- Description of the Training and Testing Datasets (Number of Images Contributed in Each Category)
TABLE II Comparative Results Over Reside-Standard Sots [41] Indoor Dataset
Table II- Comparative Results Over Reside-Standard Sots [41] Indoor Dataset
TABLE III Comparative Results Over Reside- β Sots [41] Outdoor Dataset
Table III- Comparative Results Over Reside- $\beta$ Sots [41] Outdoor Dataset
TABLE IV Comparative Results Over HazeRD [52] Dataset
Table IV- Comparative Results Over HazeRD [52] Dataset

B. Multi-Look Architecture

Most prior work on image dehazing incorporate a single encoder-decoder based networks or GANs that are heavy in terms of the number of parameters and require deepening of the networks for an increase in accuracy. Moreover, a single look does not give an opportunity to create customised losses and quantitatively perceive the improvement taking place in the image as it passes through the network. It is imperative to understand that the the multi-look architecture used by ReViewNet does not add on to the number of parameters because the second look or pass is not added after a static first look. Instead, keeping the number of parameters and size of the architecture fixed, the entire network is split into two looks – making the entire structure more efficient for image enhancement and light enough to be incorporated in autonomous vehicles.

The proposed ReViewNet consist of two set of encoder-decoder modules. We denote them as the first look ( R V N e t f ) and second look ( R V N e t s ) modules, respectively. For an input tensor I x , the R V N e t f is computed using Eq. 2 .
R V N e t f = D e M f ( E n M f ( I x ) ) (2)
View Source Right-click on figure for MathML and additional features. \begin{equation*} RVNet_{f} = DeM_{f}(EnM_{f}(I_{x})) \tag{2}\end{equation*} where the first look encoder ( E n M f ) and decoder ( D e M f ) modules are computed as shown in Fig. 1 . For the tensor I x of size P × P with the number of channels denoted as c h , the response of the convolutional layer (conv) with a kernel function f ( ⋅ ) and size h × h is computed by Eq. 3 .
c o n v ( I x ) = ∑ j = 1 c h f k ( h ) × I n j | d k = 1 (3)
View Source Right-click on figure for MathML and additional features. \begin{equation*} conv(I_{x}) = \sum ^{ch}_{j=1} f^{k}(h)\times I^{n}_{j} |^{d}_{k=1} \tag{3}\end{equation*} where n ∈ [ 1 , P ] and d is the filter depth. Eq. 4 shows the constituent operations in the E n M f ( I x ) module.
E n M f ( I x ) = C f 3 ( C f 2 ( C f 1 ( I x ) ) ) (4)
View Source Right-click on figure for MathML and additional features. \begin{equation*} EnM_{f}(I_{x}) = Cf_{3}(Cf_{2}(Cf_{1}(I_{x}))) \tag{4}\end{equation*}

Each C f i for i ∈ [ 1 , 2 , 3 ] is composed of two convolution (conv) operations and one maxpool (mp) operation.
C f i = m p ( c o n v ( c o n v ( F ) ) ) (5)
View Source Right-click on figure for MathML and additional features. \begin{equation*} Cf_{i} = mp(conv(conv(F))) \tag{5}\end{equation*} where F denotes the input feature maps. Similarly, the D e M f ( ⋅ ) module is composed of the following set of operations.
D e M f ( E n M f ) = u p ( c o n v ( D f 3 ( D f 2 ( D f 1 ( E n M f ) ) ) ) ) (6)
View Source Right-click on figure for MathML and additional features. \begin{equation*} DeM_{f}(EnM_{f}) = up(conv(Df_{3}(Df_{2}(Df_{1}(EnM_{f}))))) \tag{6}\end{equation*}

The D f i for i ∈ [ 1 , 2 , 3 ] is composed of a transpose convolution ( c o n v T ) and an upsample (up) operation.
D f i = u p ( c o n v T ( F ) ) (7)
View Source Right-click on figure for MathML and additional features. \begin{equation*} Df_{i} = up(conv_{T}(F)) \tag{7}\end{equation*}

The convolutional layers in the encoder and decoder modules have internal skip connections to accelerate the learning process and enable the low-level features to propagate without progressively degrading through the network and even circumvent the bottleneck. It also helps in improving the colour and edge feature extraction process in the network.

While a primary glance is essential for learning the dehazing task, the second pass acts as a revision with certain insights of its own. The second pass also has over six carry connections from the first look. They act as an inter-network feed forward system, ensuring that the second look definitely learns something more than the first. The features thus circumvent the bottleneck and take the context straight to the output [40] . The second look module R V N e t s is computed using Eq. 8 .
R V N e t s = D e M s ( S P B ( E n M s ( [ I x , R V N e t f ] ) ) ) (8)
View Source Right-click on figure for MathML and additional features. \begin{equation*} RVNet_{s} = DeM_{s}(SPB(EnM_{s}([I_{x},RVNet_{f}]))) \tag{8}\end{equation*}

The second look encoder ( E n M s ) is composed of convolutional blocks along with the carry responses from the first look encoder ( E n M f ) as shown in Eq. 9 .
E n M s = C s 3 ( C s 2 ( C s 1 ( [ I x , R V N e t f ] ) ) ) (9)
View Source Right-click on figure for MathML and additional features. \begin{equation*} EnM_{s} = Cs_{3}(Cs_{2}(Cs_{1}([I_{x},RVNet_{f}]))) \tag{9}\end{equation*} where the C s i is computed as
C s i = { m p ( c o n v ( c o n v ( [ F , C f i − 1 ] ) ) ) , m p ( c o n v ( c o n v ( [ I x , R V N e t f ] ) ) ) , if  i ∈ [ 2 , 3 ] if  i = 1 (10)
View Source Right-click on figure for MathML and additional features. \begin{align*} Cs_{i} = \begin{cases} \displaystyle mp(conv(conv([F,Cf_{i-1}]))),& \text {if } i\in [{2,3}]\\ \displaystyle mp(conv(conv([I_{x},RVNet_{f}]))),& \text {if } i=1 \end{cases}\quad \tag{10}\end{align*}

The encoded features are cleansed with a spatial pooling block (SPB). More details about the SPB is discussed in the next subsection. The response of SPB block is decoded through D e M s module as given in Eq. 11 .
D e M f = c o n v t ( c o n v ( c o n v ( [ D s 3 ( D s 2 ( D s 1 ( S P B ) ) ) , D f 3 ] ) ) (11)
View Source Right-click on figure for MathML and additional features. \begin{align*} DeM_{f} = conv_{t}(conv(conv([Ds_{3}(Ds_{2}(Ds_{1}(SPB))),Df_{3}])) \\ \tag{11}\end{align*}

The D s i is computed as in Eq. 12 . The carry branches from the first look decoder and second look encoder are also fed to these decoder blocks.
D s i = { u p ( c o n v T ( [ F , D f i − 1 , C s 4 − i ] ) ) , u p ( c o n v T ( S P B ) ) , if  i ∈ [ 2 , 3 ] if  i = 1 (12)
View Source Right-click on figure for MathML and additional features. \begin{align*} Ds_{i} = \begin{cases} \displaystyle up(conv_{T}([F,Df_{i-1},Cs_{4-i}])),& \text {if } i\in [{2,3}]\\ \displaystyle up(conv_{T}(SPB)), &\text {if } i=1 \end{cases}\quad \tag{12}\end{align*}

Spatial Pooling Block: The second look has an additional parallel pooling layer that leverages the spatial dimensions of the feature vector after the encoding process as shown in Fig. 1 . The feature vector is pooled via global pooling layers to extract coarse features, average pooling to extract medium level features and maxpooling to extract fine features. This bottleneck also uses different spatial strides in these pooling layers to facilitate a similar intuition. Such spatial pooling helps shallow networks to extract features that would otherwise build up in deeper layers. The SPB is computed as given in Eq. 13 and Eq. 14 .
S P B = Q = Q 1 = Q 2 = Q 3 = Q 4 = [ Q , Q 1 , Q 2 , Q 3 , Q 4 ] [ E n M s , C s 3 ] u p ( c o n v ( m p ( Q ) ) ) u p ( c o n v ( a p 4 ( Q ) ) ) u p ( c o n v ( a p 8 ( Q ) ) ) u p ( c o n v ( g p ( Q ) ) ) (13) (14)
View Source Right-click on figure for MathML and additional features. \begin{align*} SPB=&[Q,Q1,Q2,Q3,Q4] \tag{13}\\ Q=&[EnM_{s},Cs_{3}] \\ Q1=&up(conv(mp(Q))) \\ Q2=&up(conv(ap_{4}(Q))) \\ Q3=&up(conv(ap_{8}(Q))) \\ Q4=&up(conv(gp(Q)))\tag{14}\end{align*} where a p , g p , m p denote the average, global and max pooling respectively. The a p 4 and a p 8 downsample the input feature maps by the factor of 4 and 8, respectively. The Q 1 , Q 2 , Q 3 , Q 4 are encoded in parallel and concatenated with Q before passing on to the next layer.

The spatial pooling aspect is added in the second and not the first look because the second look builds upon the output of the first, and as demonstrated in the ablation section and Table XI -F, giving spatial advantage to the second look yields better results.
C. Different Loss Weights

The network is capable of producing two outputs, referred as the auxiliary and the main output, produced at the end of the first and second look respectively. The network is thus trained like a multi-output image-to-image mapping architecture and the two MSE losses are combined by multiplying them with suitable scaling factors. These scaling factors or loss weights can be treated as the importance given to the two looks. Thus if L 1 is the MSE loss for the auxiliary output and L 2 is the MSE loss for the main output, the net loss or principle target function for the network is given as
L o s s n e t = W 1 L 1 + W 2 L 2 (15)
View Source Right-click on figure for MathML and additional features. \begin{equation*} Loss_{net} = W_{1} L_{1} + W_{2} L_{2} \tag{15}\end{equation*} where W 1 is the scaling factor of the first look and W 2 is the scaling factor for the second look. In order to analyse the principle behind introducing a second look and understand the importance of the two looks, the scaling factors are varied across five different combinations, as discussed further in Section IV-D .

SECTION IV.
Experiments and Analysis

The model is trained and tested on a total of 5 different datasets. The details of the train-test split are highlighted in Table I along with descriptions in the datasets section below. For quantitative evaluation, structural similarity index (SSIM) and peak signal to noise ratio (PSNR) metrics are used, which are the most frequently adopted metrics for image dehazing algorithms. These metrics enable comparison of the proposed method to a wide range of existing solutions. Training takes place separately for indoor and outdoor models, primarily because autonomous vehicles for the outdoors differ significantly from indoor applications of the same.
A. Datasets

We use 5 datasets in this work – RESIDE-standard indoor (13,990 image pairs), RESIDE- β outdoor (72,135 image pairs), RESIDE HSTS (10 image pairs) [41] , HazeRD (75 image pairs) [52] and D-Hazy (1,499 image pairs) [53] . We choose RESIDE dataset for the training because, apart from being one of the largest publicly available datasets for dehazing, it benchmarks nine representative state of the art dehazing methods by providing full reference evaluation metrics like PSNR and SSIM for the synthetic objective testing set (SOTS). For robustness, the indoor models are also trained on the D-Hazy [53] dataset. Further, the train-test split is highlighted in the Table I .
B. Quantitative Analysis

We report the average PSNR and SSIM of all stated networks and the proposed method. Since the proposed method outperforms all the existing state-of-the-art on all mentioned datasets, we also report the percentage increase it brings on every existing method. Table II to Table VI clearly demonstrate the supremacy of the proposed method on PSNR and SSIM as compared to other benchmarks that exist on these datasets. As an example, Table III demonstrates how ReViewNet outperforms the existing methods with improvements ranging from 8.08 to 33.56 percent in PSNR on MADN and DCP methods respectively. There is a 2.70 percent improvement over Deep DCP method on the HSTS dataset in Table V on SSIM and a remarkable 29.80 percent over BCCR method.
TABLE V Comparative Results Over HSTS Dataset [41]
Table V- Comparative Results Over HSTS Dataset [41]
TABLE VI Comparative Results Over D-HAZY [53] Dataset
Table VI- Comparative Results Over D-HAZY [53] Dataset
TABLE VII Loss Weight Analysis for Different Weight Ratios for the First and Second Look
Table VII- Loss Weight Analysis for Different Weight Ratios for the First and Second Look

C. Qualitative Analysis for Safe Autonomous Driving

We present a detailed qualitative analysis which visually depicts the efficacy of ReViewNet for enabling safe autonomous driving in hazy weather conditions. A visual comparison with existing approaches also establishes the superior performance of ReViewNet. We delineate several use-cases related to the autonomous vehicular context in hazy weather conditions. ReViewNet is able to perform effective dehazing in a large variety of driving scenarios and consistently produces haze-free images with clear color and contrast details. These results further strengthen the application of dehazed output of ReViewNet for Vision-based vehicular applications like object detection or semantic segmentation.
1) Multiple Objects and Contextual Clarity:

Real life situations will require the autonomous vehicles to occasionally enter busy streets full of two-wheelers, pedestrians and dynamic road surfaces as depicted in Fig. 2 . The highlighted ares of the image depict how ReViewNet elegantly dehazes the image with clarity in context of the pedestrian, vehicles and distant objects. While the other deep learning based solutions like AODNet and DCPDN fail to provide complete dehazing, mathematical models like DCP, NLD and FVR introduce an unusual color contrast, hindering object detection or road segmentation tasks performed by an autonomous vehicle.
Fig. 2. - Qualitative comparison: Multiple objects in a scene. Magnify for minor details.
Fig. 2.

Qualitative comparison: Multiple objects in a scene. Magnify for minor details .

Show All

2) Road Sign and Structural Navigation Integrity:

Road signs and intersections like T-points are extremely important in the context of autonomous vision based driving applications. Fig. 3 shows how the parking sign is most clearly visible in the dehazed image produced by ReViewNet, the output being closest to the available ground truth as well. Similarly the distant T-point is most clearly dehazed by the proposed method as compared to other methods demonstrated in Fig. 3 . For networks with extremely low runtime for real time applications, it is rare to witness such structural integrity and nuance.
Fig. 3. - Qualitative comparison: Roadside signs and distant T-Point dead-end enhancement. Magnify for minor details.
Fig. 3.

Qualitative comparison: Roadside signs and distant T-Point dead-end enhancement. Magnify for minor details .

Show All

3) Short Distance Obstacles and Urban Dynamic Traffic Situations:

It is important for the image to maintain a color contrast similar to the actual truth and have clearly visible road surfaces for navigation. ReViewNet effectively uses its spatial feature extraction and skip connections to obtain the dehazed output as close to the ground truth as possible, as depicted in Fig. 4 . Similarly Fig. 5 shows the same effect on a real dynamic traffic image taken from a traffic camera.
Fig. 4. - Qualitative comparison: Short distance and color contrast quality in dehazing outputs. Magnify for minor details.
Fig. 4.

Qualitative comparison: Short distance and color contrast quality in dehazing outputs. Magnify for minor details .

Show All
Fig. 5. - Qualitative comparison: Urban traffic and dynamic scenarios. Magnify for minor details.
Fig. 5.

Qualitative comparison: Urban traffic and dynamic scenarios. Magnify for minor details .

Show All

4) Atmospheric Light and Dispersion:

Fig. 6 depicts how non-learning based methods like DCP, BCCR, NLD and FVR cannot sometimes deal with atmospheric dispersion and instead form a circumvented image about the source of light in haze. While the learning based methods like AODNet and DCPDN do overcome that issue, they fail to dehaze the image completely. ReViewNet, on the other hand, distinctively dehazes the aerial view with minimal distortion in structure and color contrast.
Fig. 6. - Qualitative comparison: Comparison of dehazing in atmospheric dispersion. Magnify for minor details.
Fig. 6.

Qualitative comparison: Comparison of dehazing in atmospheric dispersion. Magnify for minor details .

Show All

5) Specific Use Cases for Autonomous Transportation Safety and Surveillance:

In this section, we discuss several use cases for the utility of the proposed ReViewNet. Fig. 7 highlights over 32 examples of hazy and our dehazed outputs, divided into 5 categories – traffic camera view, autonomous driving road navigation, road navigation and pedestrian prone crossings, railroad and waterway use cases and aerial (drone-based) building and rooftop applications.
Fig. 7. - Vehicular vision specific qualitative use cases. Magnify for minor details.
Fig. 7.

Vehicular vision specific qualitative use cases. Magnify for minor details .

Show All

The proposed system is a useful preprocessor for several vision based traffic analysis tasks such as object detection, lane detection, and segmentation. The first five rows in Fig. 7 highlight the effectiveness of dehazing in the prominence of zebra crossings and turnings of the road that are essential for automated decisions required in autonomous vehicles, such road lane segmentation and lane changes. The photos of pedestrian-prone areas clearly show how, in some cases, object which were not clearly visible in the hazy image are made visible in the dehazed output, which in turn improves object detection. The dehazed objects do not merge with the haze anymore, and they become more prominent and natural, despite the lightweight and fast architecture of the model. These objects range from parking lot signs to number plates. The sixth row in Fig. 7 extend to applications beyond roads to railroad and water bodies. Sample images in the last two rows in Fig. 7 demonstrate that aerial imagery, that is simulated from the perspective of delivery drones, is effectively dehazed despite the adverse solar glare present in the images. Detecting rooftops better can act as a big advantage to aerial vehicular technology for adverse weather conditions. During our experimentation, the only limitation that was observed was on images with artificial dense haze combined with mushy or fuzzy green backgrounds. ReViewNet could not fully dehaze them. As a future prospect, it is planned to overcome this difficulty.
D. Loss Weight Ratio Analysis and Explainable Revision Improvement

We now answer the mathematical basis for varying the loss ratio of the first and second look. As stated before, the main impact of the multi-look architecture is seen in the reduction of running time and lightweight architecture of the model. We now experimentally demonstrate that given the same weight (50-50 ratio) for both the looks, the second look does in fact learn more than the first through Table VIII . It depicts how the main output is consistently better than the first on all four datasets in the table on both PSNR and SSIM, thus reiterating the fact and assumption that the second look improves upon the first like a “revision” or “review”.
TABLE VIII Comparison Between Equally Weighted First (Auxiliary) and Second (Main) Outputs
Table VIII- Comparison Between Equally Weighted First (Auxiliary) and Second (Main) Outputs

To decide the weights of the first and second look, we create five trials that have ratio of losses for the first and second look as depicted by the Table VII . The table depicts the final output (second look) of the five trials. The Fig. 8 shows the graphical plots of these values for SSIM and PSNR. We can clearly see that though the second look is more important than the first, the sharp distinctive peak at a weight ratio of 0.4 for the first versus 0.6 for the second look shows an optimal configuration. After this, the graphs takes a dip again. The values at 0.3/0.7 weight are still better than the 0.6/0.4 and 0.7/0.3, demonstrating that the second look is learning much more than the first. Thus, context specific novelty of the experimentation is observed. We successfully establish the technique of dividing the network into two parts, boosting performance without increase in parameters with unique dehazing-specific yet computationally simple components for image enhancement.
Fig. 8. - Graphs depicting loss weight ratio to decide $W1$ and $W2$ . (a) PSNR values for all the datasets at different $W1/W2$ ratios, (b) SSIM values for all the datasets at different $W1/W2$ ratios.
Fig. 8.

Graphs depicting loss weight ratio to decide W 1 and W 2 . (a) PSNR values for all the datasets at different W 1 / W 2 ratios, (b) SSIM values for all the datasets at different W 1 / W 2 ratios.

Show All

E. Running Time Analysis

For all vehicular vision applications, dehazing will predominantly be used as a pre-processing step and hence it necessitates that it be fast as well as computationally efficient. We present a comparison of the state of the art learning based and mathematical methods in terms of their running time on CPU and GPU environments. We see from Table IX and X that the proposed ReViewNet is the fastest strategy among all methods. AODNet [22] is famous for being one of the fastest networks, and ReViewNet is almost three times faster in computation, without any compromise in PSNR or SSIM. The running time shown here includes the multi color-cue spacial modelling and inter-conversion as well. This will give a tremendous boost to the top level vision tasks on aerial drones and autonomous cars and boats, making the model most suitable for typical autonomous vehicle video frame rates up-to the range of 40 frames per second.
TABLE IX Average per Image CPU Running Time (in seconds) Comparison of Various Methods Over RESIDE SOTS Images
Table IX- Average per Image CPU Running Time (in seconds) Comparison of Various Methods Over RESIDE SOTS Images
TABLE X Average per Image GPU Running Time (in seconds) Comparison of Various Methods
Table X- Average per Image GPU Running Time (in seconds) Comparison of Various Methods
TABLE XI Ablation Analysis. t1 Is Without the Multi-Color Space, t2 Has Spatial Pooling in the First Look and t3 is a Single Look Architecture. The Rest of the Parameters Are Constant
Table XI- Ablation Analysis. t1 Is Without the Multi-Color Space, t2 Has Spatial Pooling in the First Look and t3 is a Single Look Architecture. The Rest of the Parameters Are Constant

F. Ablation and Novelty Analysis

The ablation analysis in Table XI -F shows the rationale behind several design choices by comparing performance on two of the largest datasets in the experiment. Comparison of ReViewNet with t1 shows the quantitative contribution of multi-color space, while comparison with t2 shows the increase in accuracy when spatial pooling is used in the second look rather than the first. An explanation for this is also hinted in the fact that since the second look uses the output of the first along with the original image as well, giving it the spatial component gives the network an upper hand. The performance increase by dividing the network into two parts is seen by comparison with t3, showing how ReViewNet is better than a simple encoder decoder with spatial pooling. Hence, the novelty of the network is observed in its use of multi-color spaces, multi-look architecture, loss weighting, spatial pooling, skip connections and computation efficient dehazing without dependency on physical phenomenon. It is the harmonious synchronisation of all these aspects that makes the network unique and, more importantly, context-specific for dehazing in real-time autonomous driving applications. This harmony is also demonstrated in the fact that the network outperforms all state of the art in the field, in both PSNR ans SSIM metrics. Moreover, we experimentally establish a precedent that image enhancement tasks will see an increase in performance by dividing the network into two parts, keeping the training parameters same.
SECTION V.
Conclusion

This work presents a fast and resource-efficient network, ReViewNet, for image dehazing which is suitable for real time applications in autonomous driving. The ReViewNet architecture looks twice over the hazy image and the network is optimized with a hybrid weighted loss. Ablation analysis of components and experimentation on ratios of the two different losses are conducted to determine the optimal weight ratio. We demonstrate sample qualitative results over 32 different scenarios for specific use-cases of vehicular vision. This work adds to the state-of-the-art by demonstrating conclusive proof that using context specific components and features of a deep learning network can result in faster (0.025 seconds per frame on a GPU) and more accurate image enhancement modules. This further leads better preprocessing and higher performance in Vision-based tasks for vehicular technologies such as object detection and road segmentation, thus enabling safer autonomous driving. An exhaustive experimental analysis on five benchmark haze datasets demonstrates that the proposed ReViewNet network significantly outperforms all the existing state-of-the-art CNN and GAN based methods in quantitative, qualitative, and computation speed evaluations.

Authors
Figures
References
Citations
Keywords
Metrics
More Like This
Assessment of Chlorophyll Content Based on Image Color Analysis, Comparison with SPAD-502

2010 2nd International Conference on Information Engineering and Computer Science

Published: 2010
Simultaneous object tracking and depth estimation using color shifting property of a multiple color-filter aperture camera

2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)

Published: 2011
Show More
References
1. L. Zhou, W. Min, D. Lin, Q. Han and R. Liu, "Detecting motion blurred vehicle logo in IoV using filter-DeblurGAN and VL-YOLO", IEEE Trans. Veh. Technol. , vol. 69, no. 4, pp. 3604-3614, Apr. 2020.
Show in Context View Article Full Text: PDF (5647) Google Scholar
2. M. Mandal, M. Shah, P. Meena and S. K. Vipparthi, "SSSDET: Simple short and shallow network for resource efficient vehicle detection in aerial scenes", Proc. IEEE Int. Conf. Image Process. (ICIP) , pp. 3098-3102, Sep. 2019.
Show in Context View Article Full Text: PDF (499) Google Scholar
3. M. Mandal, M. Shah, P. Meena, S. Devi and S. K. Vipparthi, "AVDNet: A small-sized vehicle detection network for aerial visual data", IEEE Geosci. Remote Sens. Lett. , vol. 17, no. 3, pp. 494-498, Mar. 2020.
Show in Context View Article Full Text: PDF (2775) Google Scholar
4. M. Mandal, M. Chaudhary, S. K. Vipparthi, S. Murala, A. B. Gonde and S. K. Nagar, "ANTIC: Antithetic isomeric cluster patterns for medical image retrieval and change detection", IET Comput. Vis. , vol. 13, no. 1, pp. 31-43, Feb. 2019.
Show in Context CrossRef Google Scholar
5. M. Mandal, P. Saxena, S. K. Vipparthi and S. Murala, "CANDID: Robust change dynamics and deterministic update policy for dynamic background subtraction", Proc. 24th Int. Conf. Pattern Recognit. (ICPR) , pp. 2468-2473, Aug. 2018.
Show in Context View Article Full Text: PDF (791) Google Scholar
6. M. Mandal, V. Dhar, A. Mishra and S. K. Vipparthi, "3DFR: A swift 3D feature reductionist framework for scene independent change detection", IEEE Signal Process. Lett. , vol. 26, no. 12, pp. 1882-1886, Dec. 2019.
Show in Context View Article Full Text: PDF (1345) Google Scholar
7. P.-H. Chiu, P.-H. Tseng and K.-T. Feng, "Interactive mobile augmented reality system for image and hand motion tracking", IEEE Trans. Veh. Technol. , vol. 67, no. 10, pp. 9995-10009, Oct. 2018.
Show in Context View Article Full Text: PDF (3056) Google Scholar
8. M. Mandal, L. K. Kumar, M. S. Saran and S. K. Vipparthi, "MotionRec: A unified deep framework for moving object recognition", Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV) , pp. 2734-2743, Mar. 2020.
Show in Context View Article Full Text: PDF (1292) Google Scholar
9. A. Jolfaei and K. Kant, "Privacy and security of connected vehicles in intelligent transportation system", Proc. 49th Annu. IEEE/IFIP Int. Conf. Dependable Syst. Netw.-Supplemental Volume (DSN-S) , pp. 9-10, Jun. 2019.
Show in Context View Article Full Text: PDF (97) Google Scholar
10. M. Shafiq, Z. Tian, A. K. Bashir, A. Jolfaei and X. Yu, "Data mining and machine learning methods for sustainable smart cities traffic classification: A survey", Sustain. Cities Soc. , vol. 60, Sep. 2020.
Show in Context CrossRef Google Scholar
11. S. Kouroshnezhad, A. Peiravi, M. S. Haghighi and A. Jolfaei, "An energy-aware drone trajectory planning scheme for terrestrial sensors localization", Comput. Commun. , vol. 154, pp. 542-550, Mar. 2020.
Show in Context CrossRef Google Scholar
12. G. Bansal, V. Chamola, P. Narang, S. Kumar and S. Raman, "Deep3DSCan: Deep residual network and morphological descriptor based framework for lung cancer classification and 3D segmentation", IET Image Process. , vol. 14, no. 7, pp. 1240-1247, May 2020.
Show in Context CrossRef Google Scholar
13. A. Mehra, N. Jain and H. S. Srivastava, "A novel approach to use semantic segmentation based deep learning networks to classify multi-temporal SAR data", Geocarto Int. , pp. 1-16, 2020.
Show in Context CrossRef Google Scholar
14. V. Hassija, V. Gupta, S. Garg and V. Chamola, "Traffic jam probability estimation based on blockchain and deep neural networks", IEEE Trans. Intell. Transp. Syst. , Jun. 2020.
Show in Context View Article Full Text: PDF (1968) Google Scholar
15. S. Huang, B. Chen and Y. Cheng, "An efficient visibility enhancement algorithm for road scenes captured by intelligent transportation systems", IEEE Trans. Intell. Transp. Syst. , vol. 15, no. 5, pp. 2321-2332, Oct. 2014.
Show in Context View Article Full Text: PDF (1525) Google Scholar
16. Y. Qu, Y. Chen, J. Huang and Y. Xie, "Enhanced Pix2pix dehazing network", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 8160-8168, Jun. 2019.
Show in Context View Article Full Text: PDF (2100) Google Scholar
17. D. Engin, A. Genc and H. K. Ekenel, "Cycle-dehaze: Enhanced CycleGAN for single image dehazing", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , pp. 825-833, Jun. 2018.
Show in Context View Article Full Text: PDF (1364) Google Scholar
18. K. He, J. Sun and X. Tang, "Single image haze removal using dark channel prior", IEEE Trans. Pattern Anal. Mach. Intell. , vol. 33, no. 12, pp. 2341-2353, Dec. 2011.
Show in Context View Article Full Text: PDF (6817) Google Scholar
19. D. Berman, T. Treibitz and S. Avidan, "Non-local image dehazing", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 1674-1682, Jun. 2016.
Show in Context View Article Full Text: PDF (6175) Google Scholar
20. C. Ancuti, C. O. Ancuti, C. De Vleeschouwer and A. C. Bovik, "Night-time dehazing by fusion", Proc. IEEE Int. Conf. Image Process. (ICIP) , pp. 2256-2260, Sep. 2016.
Show in Context View Article Full Text: PDF (842) Google Scholar
21. M. Sulami, I. Glatzer, R. Fattal and M. Werman, "Automatic recovery of the atmospheric light in hazy images", Proc. IEEE Int. Conf. Comput. Photogr. (ICCP) , pp. 1-11, May 2014.
Show in Context View Article Full Text: PDF (14596) Google Scholar
22. B. Li, X. Peng, Z. Wang, J. Xu and D. Feng, "AOD-Net: All-in-one dehazing network", Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , pp. 4770-4778, Oct. 2017.
Show in Context View Article Full Text: PDF (5912) Google Scholar
23. B. Cai, X. Xu, K. Jia, C. Qing and D. Tao, "DehazeNet: An end-to-end system for single image haze removal", IEEE Trans. Image Process. , vol. 25, no. 11, pp. 5187-5198, Nov. 2016.
Show in Context View Article Full Text: PDF (5831) Google Scholar
24. Y. Liu, J. Pan, J. Ren and Z. Su, "Learning deep priors for image dehazing", Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV) , pp. 2492-2500, Oct. 2019.
Show in Context View Article Full Text: PDF (562) Google Scholar
25. Y.-T. Peng, Z. Lu, F.-C. Cheng, Y. Zheng and S.-C. Huang, "Image haze removal using airlight white correction local light filter and aerial perspective prior", IEEE Trans. Circuits Syst. Video Technol. , vol. 30, no. 5, pp. 1385-1395, May 2020.
Show in Context View Article Full Text: PDF (34215) Google Scholar
26. S. Chen, Y. Chen, Y. Qu, J. Huang and M. Hong, "Multi-scale adaptive dehazing network", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , pp. 1-9, Jun. 2019.
Show in Context View Article Full Text: PDF (1802) Google Scholar
27. W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao and M.-H. Yang, "Single image dehazing via multi-scale convolutional neural networks", Proc. Eur. Conf. Comput. Vis. , pp. 154-169, 2016.
Show in Context CrossRef Google Scholar
28. W. Ren et al., "Gated fusion network for single image dehazing", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , pp. 3253-3261, Jun. 2018.
Show in Context View Article Full Text: PDF (3344) Google Scholar
29. C. O. Ancuti et al., "NTIRE 2020 challenge on NonHomogeneous dehazing", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , pp. 490-491, Jun. 2020.
Show in Context View Article Full Text: PDF (4448) Google Scholar
30. H. Zhu, X. Peng, V. Chandrasekhar, L. Li and J.-H. Lim, "DehazeGAN: When image dehazing meets differential programming", Proc. IJCAI , pp. 1234-1240, Jul. 2018.
Show in Context CrossRef Google Scholar
31. A. Dudhane and S. Murala, "CDNet: Single image de-hazing using unpaired adversarial training", Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV) , pp. 1147-1155, Jan. 2019.
Show in Context View Article Full Text: PDF (1167) Google Scholar
32. A. Dudhane, H. S. Aulakh and S. Murala, "RI-GAN: An end-to-end network for single image haze removal", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , pp. 1-10, Jun. 2019.
Show in Context View Article Full Text: PDF (1116) Google Scholar
33. A. Mehta, H. Sinha, P. Narang and M. Mandal, "HIDeGan: A hyperspectral-guided image dehazing GAN", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , pp. 212-213, Jun. 2020.
Show in Context View Article Full Text: PDF (1514) Google Scholar
34. Y. Wan and Q. Chen, "Joint image dehazing and contrast enhancement using the HSV color space", Proc. Vis. Commun. Image Process. (VCIP) , pp. 1-4, Dec. 2015.
Show in Context View Article Full Text: PDF (398) Google Scholar
35. T. Zhang, H.-M. Hu and B. Li, "A naturalness preserved fast dehazing algorithm using HSV color space", IEEE Access , vol. 6, pp. 10644-10649, 2018.
Show in Context View Article Full Text: PDF (3667) Google Scholar
36. Z. Tufail, K. Khurshid, A. Salman, I. F. Nizami, K. Khurshid and B. Jeon, "Improved dark channel prior for image defogging using RGB and YCbCr color space", IEEE Access , vol. 6, pp. 32576-32587, 2018.
Show in Context View Article Full Text: PDF (5179) Google Scholar
37. S. Bianco, L. Celona, F. Piccoli and R. Schettini, "High-resolution single image dehazing using encoder-decoder architecture", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , Jun. 2019.
Show in Context Google Scholar
38. P. Wang, Q. Fan, Y. Zhang, F. Bao and C. Zhang, "A novel dehazing method for color fidelity and contrast enhancement on mobile devices", IEEE Trans. Consum. Electron. , vol. 65, no. 1, pp. 47-56, Feb. 2019.
Show in Context View Article Full Text: PDF (2631) Google Scholar
39. M. Yang, J. Liu and Z. Li, "Superpixel-based single nighttime image haze removal", IEEE Trans. Multimedia , vol. 20, no. 11, pp. 3008-3018, Nov. 2018.
Show in Context View Article Full Text: PDF (7403) Google Scholar
40. P. Isola, J.-Y. Zhu, T. Zhou and A. A. Efros, "Image-to-image translation with conditional adversarial networks", arXiv:1611.07004 , 2016, [online] Available: https://arxiv.org/abs/1611.07004.
Show in Context Google Scholar
41. B. Li et al., "Benchmarking single-image dehazing and beyond", IEEE Trans. Image Process. , vol. 28, no. 1, pp. 492-505, Jan. 2019.
Show in Context View Article Full Text: PDF (7303) Google Scholar
42. J.-Y. Zhu, T. Park, P. Isola and A. A. Efros, "Unpaired image-to-image translation using cycle-consistent adversarial networks", Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , pp. 2223-2232, Oct. 2017.
View Article Full Text: PDF (4554) Google Scholar
43. J.-P. Tarel and N. Hautiere, "Fast visibility restoration from a single color or gray level image", Proc. IEEE 12th Int. Conf. Comput. Vis. , pp. 2201-2208, Sep. 2009.
View Article Full Text: PDF (4768) Google Scholar
44. G. Meng, Y. Wang, J. Duan, S. Xiang and C. Pan, "Efficient image dehazing with boundary constraint and contextual regularization", Proc. IEEE Int. Conf. Comput. Vis. , pp. 617-624, Dec. 2013.
View Article Full Text: PDF (1416) Google Scholar
45. A. Dudhane and S. Murala, " C 2 MSNet: A novel approach for single image haze removal ", Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV) , pp. 1397-1404, Mar. 2018.
Google Scholar
46. P. Isola, J.-Y. Zhu, T. Zhou and A. A. Efros, "Image-to-image translation with conditional adversarial networks", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 1125-1134, Jul. 2017.
View Article Full Text: PDF (2033) Google Scholar
47. X. Yang, Z. Xu and J. Luo, "Towards perceptual image dehazing by physics-based disentanglement and adversarial training", Proc. 32nd AAAI Conf. Artif. Intell. , pp. 1-8, 2018.
Google Scholar
48. Q. Zhu, J. Mai and L. Shao, "A fast single image haze removal algorithm using color attenuation prior", IEEE Trans. Image Process. , vol. 24, no. 11, pp. 3522-3533, Nov. 2015.
View Article Full Text: PDF (2996) Google Scholar
49. C. Chen, M. N. Do and J. Wang, "Robust image and video dehazing with visual artifact suppression via gradient residual minimization", Proc. Eur. Conf. Comput. Vis. , pp. 576-591, 2016.
CrossRef Google Scholar
50. A. Dudhane and S. Murala, "RYF-Net: Deep fusion network for single image haze removal", IEEE Trans. Image Process. , vol. 29, pp. 628-640, 2020.
View Article Full Text: PDF (5268) Google Scholar
51. H. Zhang and V. M. Patel, "Densely connected pyramid dehazing network", Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , pp. 3194-3203, Jun. 2018.
View Article Full Text: PDF (3258) Google Scholar
52. Y. Zhang, L. Ding and G. Sharma, "HazeRD: An outdoor scene dataset and benchmark for single image dehazing", Proc. IEEE Int. Conf. Image Process. (ICIP) , pp. 3205-3209, Sep. 2017.
Show in Context View Article Full Text: PDF (21311) Google Scholar
53. C. Ancuti, C. O. Ancuti and C. De Vleeschouwer, "D-HAZY: A dataset to evaluate quantitatively dehazing algorithms", Proc. IEEE Int. Conf. Image Process. (ICIP) , pp. 2226-2230, Sep. 2016.
Show in Context View Article Full Text: PDF (1036) Google Scholar
54. A. Golts, D. Freedman and M. Elad, "Unsupervised single image dehazing using dark channel prior loss", arXiv:1812.07051 , 2018, [online] Available: http://arxiv.org/abs/1812.07051.
Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
