IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathMenu.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Conferences > 2021 IEEE International Intel...
Let it Snow: On the Synthesis of Adverse Weather Image Data
Publisher: IEEE
Cite This
PDF
  << Results   
Thomas Rothmeier ; Werner Huber
All Authors
View Document
118
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Related Work
    III.
    Unpaired Image-To-Image Translation for Adverse Weather Image Generation
    IV.
    Experiments
    V.
    Conclusion

Authors
Figures
References
Keywords
Metrics
More Like This

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: Camera systems of automated vehicles capture images from the surrounding environment and process these datastreams with algorithms to detect and classify objects. A lot o... View more
Metadata
Abstract:
Camera systems of automated vehicles capture images from the surrounding environment and process these datastreams with algorithms to detect and classify objects. A lot of research has been devoted to improve object detection algorithms in order to provide highly accurate detection results in real time. However, these algorithms show a strong drop in performance as soon as they are exposed to adverse weather. Poor weather conditions such as rain, fog or snow lead to a reduction in visibility and thus objects are more difficult to recognize or not visible at all. This leads to a high degree of uncertainty for an automotive camera system. To enable automated driving, camera systems must be able to cope with adverse weather and the associated high uncertainty. Including more weather image data when training the algorithms can improve object detection in bad visibility conditions. However, weather image data is difficult to collect in reality and thus only available to a limited extent. In this work, we evaluate the possibility of using Generative Adversarial Networks to create synthetic weather image data. For this purpose, we compare the generated images of different network architectures trained on a diverse weather dataset collected from Flickr. The resulting data is evaluated qualitatively and quantitatively with respect to its realism and suggests that our approach is capable of generating realistic weather images.
Published in: 2021 IEEE International Intelligent Transportation Systems Conference (ITSC)
Date of Conference: 19-22 Sept. 2021
Date Added to IEEE Xplore : 25 October 2021
ISBN Information:
INSPEC Accession Number: 21259476
DOI: 10.1109/ITSC48978.2021.9565008
Publisher: IEEE
Conference Location: Indianapolis, IN, USA
Contents
SECTION I.
Introduction

Automated vehicles need to perceive and analyse their surrounding environment permanently with high frame rates to even detect subtle environmental changes. For this purpose, they are equipped with camera sensors and corresponding computer vision algorithms in addition to other sensors. While camera sensor technology and algorithms are constantly improving, they are mostly designed for clear weather images and videos. They do not take into account adverse weather like rain, fog and snow [21] . However, it is not possible for an automated vehicle to simply “turn off’ the bad weather. When exposed to these bad visibility conditions the performance of object detectors show a large decrease [24] . Thus, computer vision system for automated vehicles need to work reliably also in adverse weather conditions.

Atmospheric conditions such as rain, fog and snow are induced by small to microscopic particles in the air that scatter and absorb light rays and thus alter the appearance of a scene severely. The resulting images show often drastically degraded visibility, contrast and color fidelity, which makes it difficult to recognize and distinguish objects. This affects not only human drivers, but also the computer vision system of an automated vehicle. Lots of effort was put into image dehazing [8] , [22] , [33] and deraining [4] , [34] , [37] to improve scene visibility. Yet, restoring the original scene from a single image remains a challenging task.
Fig. 1. - Results of the weather domain adaptation models for different environmental conditions using cyclegan.
Fig. 1.

Results of the weather domain adaptation models for different environmental conditions using cyclegan.

Show All

During the last few years, remarkable progress has been made in clear weather object detection and semantic scene understanding. These advances can be attributed to the use of AI algorithms in computer vision, mainly based on convolutional neural networks (CNNs). In general, large annotated datasets serve as training input for neural networks. Existing training datasets have mostly been recorded under clear weather conditions, which is why there are significant drops in the performance of the algorithms in adverse weather conditions. The lack of training data for adverse weather conditions can be attributed to the difficulties involved in recording it, as weather is not controllable and reproducible in reality. Further, recordings of critical traffic scenarios are already a rarity in clear weather, but not available in various weather conditions.

To address this problem, we turn away from the traditional approach of collecting data in real world. We also avoid using synthetic data captured in a fully virtual environment, as the gap between reality and simulation is often too large and models naively trained on synthetic data do not typically generalize to real images. Arising from the fact that large scale annotated datasets with clear weather images are available, we propose a method to enhance existing clear weather images with synthetic weather effects (see Fig. 1 ). For this purpose we use Generative Adversarial Networks (GANs) in an unsupervised setting where no paired input images are needed. Paired input images for identical scenes (e.g. clear and snow-covered scene) are seldom available in nature and thus very difficult to collect. Our primary contribution is to train and evaluate different neural network architectures on the weather domain adaptation task and compare their outcomes qualitatively and quantitatively. Our domain adaptation models focus especially on visible influences on the scene like snow-covered roads, wet roads or the appearance of fog. We do not consider the influences of snowfall and raindrops. We also contribute by collecting adverse weather datasets online and at the CARISSMA Institute in Ingolstadt.
SECTION II.
Related Work

Generative Adversarial Networks (GANs ) have shown impressive results regarding image generation [5] , [13] . More recent work applies the idea of GANs to tasks like image inpainting [23] , text-to-image translation [35] , super resolution [32] and video synthesis [30] , [31] . The key to GANs success is an adversarial loss that forces the generated image to be indistinguishable from a real photo. A generative model tries to generate images that resemble real images, while a discriminative model distinguishes between real and synthetic ones. This loss has shown to be particularly useful in image generation tasks. We utilize several GAN based architectures for generating synthetic weather images.

Image-to-Image Translation aims to learn the mapping between a source image domain and a target image domain. Therefore a dataset of input-output examples is used to learn a translation function based on CNNs. In this work we use the pix2pix [12] framework, which employs a conditional GAN [20] to learn a mapping from source to target domain. However, paired input-output examples are needed for this approach, which makes it often difficult to apply to real world samples.

Unpaired Image-to-Image Translation is an approach to find a relationship between two data domains without the need for tuples of corresponding images. More recently, several approaches tackling the unsupervised setting have been proposed. CoGAN [19] achieves a common representation across domains by enforcing a weight-sharing constraint. CycleGAN [39] , DiscoGAN [14] and UNIT [18] leverage a cycle consistency loss to strengthen semantic consistency between source and target image. To enforce more diversity in the output space MUNIT [10] and DRIT [15] assume that the image representation can be decomposed into a content code and a style code. In this work we utilize several state-of-the-art network architectures for unpaired image-to-image translation to generate adverse weather image data and compare the results against each other.

Synthetic Visual Data is data that was not recorded in reality, but generated by simulation methods. The enormous progress in the field of computer vision can be attributed to large scale annotated image datasets [2] , [3] . At present, however, it is impossible to collect and annotate data for every new problem that requires training data. Often it is simply too difficult and time consuming to acquire visual grasping data. Thus, learning with synthetic data is gaining more and more attention recently, as synthetic data is cheap and large amounts of data can be easily collected in no time. In [1] , [27] , [28] synthetic datasets are utilized to successfully improve real world robotics and object detection tasks. Yet, models trained purely on synthetic data often fail to generalize to real world scenarios. In this work we evaluate existing GAN architectures for synthetic weather image generation and examine their usefulness in reducing the reality gap between real and generated images.

Weather Effect Simulation is a method to enhance existing data with the influences of weather related phenomena. Sakaridis et al. proposed a method to enhance existing image data with fog by applying an optical model for fog [25] . A GAN based approach to weather simulation was shown in [17] with focus on photographic style transfer. A physics based approach for simulating rain effects for camera, radar and lidar was introduced in [7] . Volk et al. generated training data with rain variations by simulating rainstreaks with brightness reduction and raindrops on the windshield to improve on object detection [29] . In this work, on the other hand, the focus is on the visible influences of the weather on the environment, such as snow-covered or wet roads.
SECTION III.
Unpaired Image-To-Image Translation for Adverse Weather Image Generation

As mentioned at the beginning, collecting images of adverse weather scenes in reality is often difficult, because the associated weather conditions cannot be reproduced at the push of a button. To tackle this problem we propose an approach to enhance existing images of clear weather scenes with adverse weather effects and let them look as though they were recorded in reality. First, we will look into challenges of the weather domain adaptation task and then give a short overview of unpaired image-to-image translation algorithms.
A. Weather Domain Adaptation

Referring to [26] environmental conditions can be divided into four categories: weather (wind, rain, snow), weather-induced roadway conditions (standing water, snow-covered road), particulate matter (fog, smoke) and illumination (lighting, daytime). Weather and weather-related effects occur in many forms and lead to visible changes in the scene. To realistically simulate weather effects, it is not enough to simply add raindrops or snowflakes to the image. A winter day, for example, leads to snow deposits on roads, vehicles and buildings. A heavy rainfall, on the other hand, leaves water on the road, which can lead to wet roads and reflective surfaces. Camera sensor perform in general very poorly in such environmental conditions. Furthermore, changes in environmental conditions lead to changes in vegetation. E.g. in a summer to winter change trees lose their foliage and plants wither. Also the appearance of the sky often changes due to weather changes. Fog is considered easiest, as it does not lead to weather-induced roadway conditions. It is currently impossible to map these complex processes of weather change manually. However, to be sufficient as a dataset for object recognition training, the generated data must correspond to reality to a high degree. Therefore, in this work we try to learn the correlations on existing data and transfer them to new data.

To understand the full breadth of the problem, it is indispensable to remember that a scene can take on any number of weather manifestations. For example, one can distinguish between light and heavy snow or rain, a snowy road but no snowfall, and so on. For an algorithm for weather domain adaptation, it is therefore important to be able to capture weather in its various forms and intensities. Optimally, this would be possible through a targeted parameterisation by a human. Furthermore, weather effects can be mixed, for example a snowy road with the presence of fog. Adding weather effects to existing images is a challenging task due to the wide variability of weather and the specific content-aware changes that need to be made to the source image. In this work we focus on generating realistic weather images for rain, fog and snow. We do not delve deeper into parameterisation or the mixing of weather effects, as this would go beyond the scope of this paper.
B. Unpaired Image-To-Image Translation

The general goal of unpaired image-to-image translation is to learn a mapping between two domains X and Y given a set of training samples { x i } N i = 1 where x i ∈ X and { y j } M j = 1 where y j ∈ Y . The mapping function can have different underlying assumptions.

In case of CycleGAN [39] two mappings G : X → Y and F : Y → X are defined, where G and F are neural networks. Additionally two adversarial discriminators D X and D Y are defined, where D X tries to distinguish between real images { x } and fake images { F ( y ) } . The same applies for D Y vice versa. An adversarial loss [5] is used as objective in order to match the distribution of synthesized images to the data distribution in the target domain. Moreover a cycle consistency loss ensures semantic consistency between input image and generated images.

The authors of UNIT [18] assume that a pair of images ( x 1 ,   y 1 ) can be mapped to the same latent code z in a shared-latent space Z . Based on this code z, both images can be reconstructed. Therefore two encoding functions E 1 and E 2 that map images to latent codes z and two generator functions G 1 and G 2 that map latent codes to images are defined. A function that maps from X to Y can then be represented by G 2 ( E 1 ( x 1 ) ) and from Y to X by G 1 ( E 2 ( x 2 ) ) . Similar to CycleGAN an adversarial loss and a cycle consistency constraint are defined as necessary objectives.

The MUNIT framework [10] extends this thought and proposes a partially shared latent space. This means that an image x i consists of a shared latent code z shared between two domains and an additional style latent code s i ∈ S i that is specific to each domain. The separation of content and style in the MUNIT framework enables many-to-many mappings.

While sharing similar ideas of loss functions the underlying implementation and architecture for each network is different. For a more in-depth explanation of the concepts, we refer the reader to the respective publications. In the following chapter we aim to utilize the proposed GAN architectures to generate images with realistic weather effects using images recorded in clear weather conditions as input.
SECTION IV.
Experiments

We collected different datasets in order to use them as training data for a variety of GANs. The trained GANs are capable of translating an input image into another weather domain. The results are evaluated using image quality metrics and human judgement.
A. Evaluation Metrics

Peak Signal-to-Noise ratio (PSNR ) is a widely used image quality metric that serves as a performance indicator for subjective image quality. A high PSNR indicates good image quality [11] .

Structural Similarity Index (SSIM ) is a measurement tool that is based on luminance, structure and contrast of an image to better suit the human visual system. A high value suggests high similarity [38] .

Frechet Inception Distance (FID ) was first introduced in [9] and is a metric that calculates the distance between feature vectors for real and generated images. It summarizes how similar two datasets are in terms of statistics based on computer vision features that are calculated by an inception v3 model. A lower scores correlates with higher image quality.

Learned Perceptual Image Patch Similarity (LPIPS ) measures the distance between two images using feature vectors of deep neural networks. A higher value indicates more diversity between two images, whereas a lower value indicates more similarity [36] .

Human Judgement Human judgement is the gold standard in evaluating image quality as evaluation metrics often lack expressiveness. Human qualitative judgment in this work is conducted by the authors.
B. Datasets
Flickr Weather Image Dataset

Weather image datasets are hardly available in the public domain. This is largely due to the lack of reproducability of weather image data in reality. To conduct our experiment, we collected adverse weather image data from flickr and sorted it by weather domains. The weather domains considered are clear, fog, snow and rain (mainly wet roads due to the difficulty of collecting images with visible rainstreaks). The collected data set mainly shows traffic scenarios and is divided into images with objects (e.g. cars, trucks, persons) and without objects (only road and environment visible). This results in a total of 6 unpaired data sets: clear-snow, clear-snow-objs, clear-fog, clear-fog-objs, clear-wet, clear-wet-objs . Table I gives an overview of the amount of images used. Since the data was collected on flickr with images from all over the planet, it has a high diversity.
Carissma Weather Image Dataset

This set of image data was recorded in the weather facility of CARISSMA Research Institute in Ingolstadt. The institute has the possibility to reproducibly simulate rain and fog under controlled environmental conditions over a length of 50m in a closed indoor laboratory. We recorded a car attrap that was placed on a platform which moves away from the camera sensor over a distance of 50m in different environmental conditions (clear, fog, rain). Using the recorded data two unpaired datasets were created: clear-fog-indoor, clear-rain-indoor . As it is possible to record similar images with and without the influence of weather effects in the indoor laboratory we also created two datasets with paired images where we matched corresponding images: clear-fog-indoor-paired, clear-rain-indoor-paired . 295 images were collected for the fog-indoor dataset and 281 images for the rain-indoor dataset.

All images in the flickr and CARISSMA datasets are center cropped and scaled to a size of 256x256 pixels.
Table I Overview of the respective amount of images collected for each dataset.
Table I- Overview of the respective amount of images collected for each dataset.

C. Baselines
Network Details

Many GAN network architectures have recently appeared that share similar ideas. Not all of them have been evaluated, but for the sake of completeness, they are listed in parentheses to the respective similar architectures. For our evaluation we use CycleGAN [39] (DiscoGAN [14] ), UNIT [18] and MUNIT [10] (DRIT [16] ), all of which learn mappings between two domains in an unsupervised manner. For the paired dataset we additionaly evaluate the pix2pix network [12] .
Training Details

We trained each CycleGAN and pix2pix network for 200 epochs on the respective datasets with a batch size of 1. The learning rate is decayed to zero linearly after 100 epochs. UNIT and MUNIT were trained for 300k iterations on the flickr dataset and 100k iterations on the CARISSMA weather image dataset with a batch size of 1. In case of CycleGAN we also varied the identity loss between 0.0, 0.1 and 1.0. The identity loss is used in the original paper to preserve the color of input images. The original code provided on GitHub by the respective authors is used.
D. Qualitative Evaluation
Flickr Weather Image Dataset

For the qualitative evaluation of the trained models we use images unseen by the algorithm and transform them into the respective weather domains. Fig. 2 shows the results of the models trained on the flickr weather image dataset. Looking at the results obtained, we found that our trained models are capable of generating realistic images for each weather domain. We have noticed that UNIT and MUNIT have problems in maintaining structures with the necessary sharpness and generate in general blurrier results. CycleGAN on the other hand generates images that are visually more appealing and often look as though they were sampled from the target domain. Looking at all transformations, fog seems to be the simplest domain. This may be explained by the fact that less content-aware changes need to be performed in the fog domain. In the snow domain, the added snow looks sometimes artificial and misses 3-dimensional structure. For CycleGAN(0.0) we often observed that the model simply inverses parts of the image in the snow domain. In the case of wet roads, we believe it is the most difficult domain, as reflections and mirror effects have to be added to the image. Yet the results are better than initially expected and the trained models often add a reflective surface to the road. For CycleGAN with identity values of 0.0 and 0.1, we often found that cars in the target domain were colored differently than in the source domain. This behavior can be prevented by setting the identity loss to 1.0. It can be said that the CycleGAN(1.0) generally provides the best quality results from a human visual perspective. Moreover, we found that some domain adapatation was harder than the other and did not deliver appropriate results. Fig. 3 provides an overview of some poor results.
Carissma Weather Image Dataset

Fig. 4 shows the results of the trained models on the CARISSMA weather image dataset. Here we see that UNIT and MUNIT also have difficulties to generate sharp results. The CycleGAN results tend to be closer towards the reference image in terms of image similarity. As we were able to collect paired data in this scenario we also evaluated a pix2pix model on the dataset. It can be observed, that the vehicle in the image is blurred and has a lower image quality compared to CycleGAN. This is the result of paired images that are not completely identical in terms of their semantics. This fact underlines the problem of matching images with different environmental conditions. Image quality is generally good on this dataset, partly because it has very little diversity compared to the flickr weather dataset and thus is easier to learn for a model. In addition we also compared the generated results with physics-based fog and rain models developed in [6] . Regarding realism of simulated weather effects our trained models show higher perceptual realism compared to the physics-based models.
E. Quantitative Evaluation

The results of the quantitative evaluation for the flickr weather image dataset are shown in Table II . The FID is calculated between the respective real and synthetized image data (e.g. fog ↔ synthetic-fog, snow ↔ synthetic-snow). CycleGAN(1.0) achieves the best results on almost all datasets except for the clear-wet dataset. In general all CycleGAN models have a low FID score indicating good image quality.
Fig. 2. - Qualitative evaluation of the gan models trained on the flickr weather image dataset. An input image is given to the trained gan model and transformed into the respective weather domain. The cyclegan model is evaluated with varying identity losses. Unit and munit tend to produce blurry results while cyclegan generates visually more appealing images in the respective weather domains.
Fig. 2.

Qualitative evaluation of the gan models trained on the flickr weather image dataset. An input image is given to the trained gan model and transformed into the respective weather domain. The cyclegan model is evaluated with varying identity losses. Unit and munit tend to produce blurry results while cyclegan generates visually more appealing images in the respective weather domains.

Show All
Table II Fid calculated between real and synthetic images on the flickr weather dataset.
Table II- Fid calculated between real and synthetic images on the flickr weather dataset.

For the CARISSMA weather dataset we calculated the PSNR, SSIM, LPIPS and FID. These metrics are calculated between 9 images that were synthesized from clear weather images and 9 real images from the respective weather domain. For the fog-indoor dataset the CycleGAN(1.0) model scores best for each metric (see Table III ). For the rain-indoor dataset UNIT scores best for SSIM, whereas PSNR, LPIPS and FID are best for CycleGAN(0.1) and Cycle-GAN(1.0) (see Table IV ). The image quality metrics largely correlate with the qualitative evaluation. Yet, it should be noted that PSNR and SSIM as image quality metrics may not be suitable for assessing synthetic data. In addition, the significance of all metrics applied may be reduced by the small number of test images.
Table III Different image quality metrics calculated between real and synthetic fog images on the carissma weather image dataset.
Table III- Different image quality metrics calculated between real and synthetic fog images on the carissma weather image dataset.

SECTION V.
Conclusion

In this paper we have demonstrated that GANs trained on adverse weather datasets can be utilized to generate realistically looking weather effects for the domains fog, rain and snow. We have compared different network architectures qualitatively and quantitatively and found that CycleGAN delivers the best results when trained on the proposed datasets. We also found that some transformations were easier than others. Scaling the images to a size of 256x256 for training may be problematic, as the loss of information due to image compression affects the quality of the simulated weather effects (e.g. texture of snow is often missing). We believe that a larger and higher quality training dataset will contribute significantly to the quality of the generated images. Yet, the usefulness of the data generated this way must be evaluated in a next step and applied to the training of object detection algorithms. To use this approach for training object detection algorithms for automated driving functions, it is important to establish a way to get more control over the generated results and more consistent results overall. Nevertheless, this work shows that it is possible to generate realistic weather image data with the help of neural networks and thus contributes towards safe automated driving in bad visibility conditions.
Fig. 3. - Some examples from the experiments where the weather domain adaptation showed poor results. From top-left to bottom-right: 1) no fog was added to the scenery, 2) instead of a wet surface we get a dark surface and glowing objects, 3) the image appears to be grayscaled, 4) no fog was added to the scenery and the visible vehicles front light appears to be a rear light.
Fig. 3.

Some examples from the experiments where the weather domain adaptation showed poor results. From top-left to bottom-right: 1) no fog was added to the scenery, 2) instead of a wet surface we get a dark surface and glowing objects, 3) the image appears to be grayscaled, 4) no fog was added to the scenery and the visible vehicles front light appears to be a rear light.

Show All
Table IV Different image quality metrics calculated between real and synthetic rain images on the carissma weather image dataset.
Table IV- Different image quality metrics calculated between real and synthetic rain images on the carissma weather image dataset.

Fig. 4. - Qualitative evaluation of the gan models trained on the carissma weather image dataset. An image is given to the gan as input and then transformed into the respective weather domain. For this task we evaluated unit, munit, pix2pix and cyclegan with different identity losses. In addition, we visually compare our approach with a physics-based approach towards weather simulation.
Fig. 4.

Qualitative evaluation of the gan models trained on the carissma weather image dataset. An image is given to the gan as input and then transformed into the respective weather domain. For this task we evaluated unit, munit, pix2pix and cyclegan with different identity losses. In addition, we visually compare our approach with a physics-based approach towards weather simulation.

Show All

ACKNOWLEDGMENT

This work is supported and published within the AI-SEE project. AI-SEE is a co-labelled PENTA and EURIPIDES project endorsed by EUREKA. National funding authorities: Austrian Research Promotion Agency (FFG), Business Finland, Federal Ministry of Education and Research (BMBF), Canadian National Research Council Industrial Research Assistance Program (NRC-IRAP).

Authors
Figures
References
Keywords
Metrics
   Back to Results   
More Like This
Effective object detection from traffic camera videos

2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)

Published: 2017
Independent Moving Object Detection Based on a Vehicle Mounted Binocular Camera

IEEE Sensors Journal

Published: 2021
Show More
References
1. Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan and Dilip Krishnan, "Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks", 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 95-104, July 2017.
Show in Context View Article Full Text: PDF (769) Google Scholar
2. J. Deng, W. Dong, R. Socher, L. Li, Kai Li and Li Fei-Fei, "ImageNet: A large-scale hierarchical image database", 2009 IEEE Conference on Computer Vision and Pattern Recognition , pp. 248-255, June 2009.
Show in Context View Article Full Text: PDF (3548) Google Scholar
3. Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn and Andrew Zisserman, "The Pascal Visual Object Classes (VOC) Challenge" in International Journal of Computer Vision, Springer US, vol. 88, no. 2, pp. 303-338, June 2010.
Show in Context CrossRef Google Scholar
4. K. Garg and S. K. Nayar, "Detection and removal of rain from videos", Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2004. CVPR 2004. , vol. 1, June 2004.
Show in Context View Article Full Text: PDF (1274) Google Scholar
5. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, et al., Generative Adversarial Networks, June 2014.
Show in Context Google Scholar
6. Sinan Hasirlioglu, A Novel Method for Simulation-based Testing and Validation of Automotive Surround Sensors under Adverse Weather Conditions / submitted by Sinan Hasirlioglu , 2020.
Show in Context Google Scholar
7. Sinan Hasirlioglu and Andreas Riener, "A General Approach for Simulating Rain Effects on Sensor Data in Real and Virtual Environments", IEEE Transactions on Intelligent Vehicles , pp. 1-1, 2019.
Show in Context View Article Full Text: PDF (2248) Google Scholar
8. K. He, J. Sun and X. Tang, "Single Image Haze Removal Using Dark Channel Prior", IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 33, no. 12, pp. 2341-2353, December 2011.
Show in Context View Article Full Text: PDF (6817) Google Scholar
9. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler and Sepp Hochreiter, GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium, January 2018.
Show in Context Google Scholar
10. Xun Huang, Ming-Yu Liu, Serge Belongie and Jan Kautz, "Multimodal Unsupervised Image-to-Image Translation" in Computer Vision - ECCV 2018, Cham:Springer International Publishing, vol. 11207, pp. 179-196, 2018.
Show in Context Google Scholar
11. Quan Huynh-Thu and Mohammed Ghanbari, The accuracy of PSNR in predicting video quality for different video scenes and frame rates, pp. 14.
Show in Context Google Scholar
12. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou and Alexei A. Efros, Image-to-Image Translation with Conditional Adversarial Networks, November 2016.
Show in Context Google Scholar
13. Tero Karras, Timo Aila, Samuli Laine and Jaakko Lehtinen, Progressive Growing of GANs for Improved Quality Stability and Variation, October 2017.
Show in Context Google Scholar
14. Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee and Jiwon Kim, Learning to Discover Cross-Domain Relations with Generative Adversarial Networks, May 2017.
Show in Context Google Scholar
15. Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh and Ming-Hsuan Yang, "Diverse Image-to-Image Translation via Disentangled Representations", Computer Vision - ECCV 2018 , vol. 11205, pp. 36-52, 2018.
Show in Context CrossRef Google Scholar
16. Hsin-Ying Lee, Hung-Yu Tseng, Qi Mao, Jia-Bin Huang, Yu-Ding Lu, Maneesh Singh, et al., DRIT++: Diverse Image-to-Image Translation via Disentangled Representations, December 2019.
Show in Context Google Scholar
17. Xuelong Li, Kai Kou and Bin Zhao, Weather GAN: Multi-Domain Weather Translation Using Generative Adversarial Networks, March 2021.
Show in Context Google Scholar
18. Ming-Yu Liu, Thomas Breuel and Jan Kautz, Unsupervised Image-to-Image Translation Networks, July 2018.
Show in Context Google Scholar
19. Ming-Yu Liu and Oncel Tuzel, Coupled Generative Adversarial Networks, June 2016.
Show in Context Google Scholar
20. Mehdi Mirza and Simon Osindero, Conditional Generative Adversarial Nets, November 2014.
Show in Context Google Scholar
21. Srinivasa G. Narasimhan and Shree K. Nayar, "Vision and the atmosphere" in ACM SIGGRAPH ASIA 2008 courses on - SIGGRAPH Asia ‘08, Singapore:ACM Press, pp. 1-22, 2008.
Show in Context CrossRef Google Scholar
22. Ko Nishino, Louis Kratz and Stephen Lombardi, "Bayesian Defogging", International Journal of Computer Vision , vol. 98, no. 3, pp. 263-278, July 2012.
Show in Context CrossRef Google Scholar
23. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell and Alexei A. Efros, Context Encoders: Feature Learning by Inpainting, November 2016.
Show in Context View Article Full Text: PDF (1363) Google Scholar
24. Thomas Rothmeier and Werner Huber, "Performance Evaluation of Object Detection Algorithms Under Adverse Weather Conditions" in Intelligent Transport Systems From Research and Development to the Market Uptake, Cham:Springer International Publishing, vol. 364, pp. 211-222, 2021.
Show in Context CrossRef Google Scholar
25. Christos Sakaridis, Dengxin Dai and Luc Van Gool, "Semantic Foggy Scene Understanding with Synthetic Data", International Journal of Computer Vision , vol. 126, no. 9, pp. 973-992, September 2018.
Show in Context CrossRef Google Scholar
26. Eric Thorn, Shawn Kimmel and Michelle Chaka, A framework for automated driving system testable cases and scenarios, 2018.
Show in Context Google Scholar
27. J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world", 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pp. 23-30, September 2017.
Show in Context View Article Full Text: PDF (3135) Google Scholar
28. Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, et al., Training Deep Networks With Synthetic Data: Bridging the Reality Gap by Domain Randomization, pp. 969-977, 2018.
Show in Context Google Scholar
29. Georg Volk, Stefan Muller, Alexander Von Bernuth, Dennis Hospach and Oliver Bringmann, "Towards Robust CNN-based Object Detection through Augmentation with Synthetic Rain Variations", 2019 IEEE Intelligent Transportation Systems Conference (ITSC) , pp. 285-292, October 2019.
Show in Context View Article Full Text: PDF (1467) Google Scholar
30. Carl Vondrick, Hamed Pirsiavash and Antonio Torralba, Generating Videos with Scene Dynamics, September 2016.
Show in Context Google Scholar
31. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, et al., Video-to-Video Synthesis, August 2018.
Show in Context Google Scholar
32. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz and Bryan Catanzaro, High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs, 2017.
Show in Context Google Scholar
33. Y. Wang and C. Fan, "Single Image Defogging by Multiscale Depth Fusion" in IEEE Transactions on Image Processing, IEEE Transactions on Image Processing, vol. 23, no. 11, pp. 4826-4837, November 2014.
Show in Context View Article Full Text: PDF (3387) Google Scholar
34. Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo and Shuicheng Yan, Deep Joint Rain Detection and Removal From a Single Image, pp. 10.
Show in Context Google Scholar
35. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, et al., StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks, August 2017.
Show in Context View Article Full Text: PDF (1986) Google Scholar
36. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman and Oliver Wang, The Unreasonable Effectiveness of Deep Features as a Perceptual Metric, April 2018.
Show in Context View Article Full Text: PDF (1782) Google Scholar
37. X. Zhang, H. Li, Y. Qi, W. K. Leow and T. K. Ng, "Rain Removal in Video by Combining Temporal and Chromatic Properties", 2006 IEEE International Conference on Multimedia and Expo , pp. 461-464, July 2006.
Show in Context View Article Full Text: PDF (618) Google Scholar
38. Zhou Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, "Image quality assessment: from error visibility to structural similarity", IEEE Transactions on Image Processing , vol. 13, no. 4, pp. 600-612, April 2004.
Show in Context View Article Full Text: PDF (1718) Google Scholar
39. Jun-Yan Zhu, Taesung Park, Phillip Isola and Alexei A. Efros, Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, November 2018.
Show in Context Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
