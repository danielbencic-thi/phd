IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Journals & Magazines > IEEE Access > Volume: 8
Path Planning for UAV Ground Target Tracking via Deep Reinforcement Learning
Publisher: IEEE
Cite This
PDF
Bohao Li ; Yunjie Wu
All Authors
22
Paper
Citations
4246
Full
Text Views
Open Access
Comment(s)

    Alerts

Under a Creative Commons License
Abstract
Document Sections

    I.
    Introduction
    II.
    Background
    III.
    Proposed Method
    IV.
    Experiments
    V.
    Conclusion and Prospect

Authors
Figures
References
Citations
Keywords
Metrics
Abstract:
In this paper, we focus on the study of UAV ground target tracking under obstacle environments using deep reinforcement learning, and an improved deep deterministic policy gradient (DDPG) algorithm is presented. A reward function based on line of sight and artificial potential field is constructed to guide the behavior of UAV to achieve target tracking, and a penalty term of action makes the trajectory smooth. In order to improve the exploration ability, multiple UAVs, which controlled by the same policy network, are used to perform tasks in each episode. Taking into account that the history observations have a great degree of correlation with the policy, long short-term memory networks are used to approximate the state of environments, which improve the approximation accuracy and the efficiency of data utilization. The simulation results show that the propose method can make the UAV keep target tracking and obstacle avoidance effectively.
Published in: IEEE Access ( Volume: 8 )
Page(s): 29064 - 29074
Date of Publication: 05 February 2020
Electronic ISSN: 2169-3536
INSPEC Accession Number: 19362120
DOI: 10.1109/ACCESS.2020.2971780
Publisher: IEEE
Funding Agency:
The improved DRL framework for UAV target tracking.
The improved DRL framework for UAV target tracking.
Hide Full Abstract
CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https :// creativecommons . org / licenses / by / 4 . 0 / to obtain full-text articles and stipulations in the API documentation.
SECTION I.
Introduction

Unmanned aerial vehicles (UAVs) have the advantages of safety, low cost and high maneuverability. They are widely used in military or civil fields such as reconnaissance, strike, rescue and early warning, etc. One of the typical application of UAV [1] is target tracking and obstacle avoidance. High autonomy on-line trajectory planning of UAV for target tracking and obstacle avoidance in unknown working environment arosed great attentions [2] – [3] [4] .

With the rapid development of artificial intelligence technology in recent years, deep reinforcement learning(DRL) plays an important role in more and more fields for its excellent environmental awareness and decision control performance [5] . Reinforcement learning can directly map the environment state to the control signal, which provides a dynamic planning solution for UAV trajectory planning. Flight environment is usually local or completely unknown during on-line path planning. How to react to the dynamic environment using incomplete information is a key issue in UAV on-line path planning. Reinforcement learning has advantages of strong robustness , independence on environment model and prior knowledge , solves the on-line path planning problem by trial and error [6] .

Q-learning [7] is an effective tool in reinforcement learning , which is widely used and followed by many improved algorithms such as SARSA [8] , double Q-learning [9] , and the first DRL algorithm DQN (Deep Q Network) [10] . There for Q-learning has been applied into UAV path planning [11] . Zhang et al. [12] , uses geometric method to calculate value matrix containing threat information. In this method , it combines greedy strategy with Boltzmann strategy to improve exploration. Yijing et al. [13] proposes an adaptive random exploration Q-learning method which designs the learning, escape and action modules of UAV respectively. Yan et al. [14] initializes Q matrix with environment and target, and adds the function of avoiding repetitive actions in the initial stage of training. Most Q-learning path planning methods are restricted in grid like environments for Q-learning and its improves can be used only in discrete space. Policy gradient [15] – [16] [17] [18] and actor-critic [19] – [20] [21] can be used to deal with path planing problems in continuous action space. Zhaowei et al. [22] proposes an actor-critic algorithm based on RBF neural network to achieve UAV avoidance. Zhu et al. [23] designs an end-to-end DRL model to control UAV indoor target searching with images as input. Deep deterministic policy gradient (DDPG) [27] is a DRL algorithm which combines DQN with actor-critic and can be operated in continuous action space. Wang et al. [24] transforms the UAV path planning problem into a partially observable Markov decision process, builds \${a}\$ recurrent deterministic policy gradient framework to navigation in large-scale and complex environments. You et al. [25] proposes a DDPG algorithm based on a generative model and variational Bayesian estimation to search target in cognitive electronic warfare.

Complex, dynamic and partially observable environments are major challenges for UAV target tracking [26] . To overcome these difficulties , we improve DDPG in terms of reward function and data. A good reward function is an excellent description of the relationship between environment and mission objectives, which improves the ability and generalization of the algorithm. Considering the impact of dynamic environment, a reward for DDPG is consisted of line of sight(LOS) [28] , [29] and artificial potential field [30] . The quality and utilization efficiency of data are the key to the success of model training [31] . The quality of data is closely related to the exploration which is limited by the scope of action. A large scope of actions is beneficial for exploration. But UAV is hard to accomplish actions that if vary too much due to the dynamics limitation. We add penalty term of action to the reward function. The action range of UAV will decrease with the convergence of the algorithm correspondingly . Mean while the exploration can be improved by exploring starts [32] . Multiple UAVs with different initial conditions, which controlled by the same model, are used to perform tasks in each episode. The utilization efficiency of data can be improved through the networks and input states [33] . LSTM [34] – [35] [36] is a kind of recurrent neural networks with sequence data as input. A part of information of data is transmitted to the next moment in the form of memory and participates in the training of input-output data pairs. That is to say, the training results at the current moment are determined by both the current training data and the historical training data. In partially observable environments, the combination of LSTM and historical sequence of observations can approximate the value network and actor network better [37] . The improved DDPG algorithm is trained in a virtual simulation environment, and the well-trained algorithm can be used for online target tracking and obstacle avoidance in new dynamic environments .

The main contributions of this paper are as follows:

    A DRL model for target tracking and obstacle avoidance is developed.

    Critic network and actor network based on LSTM are designed and well trained.

    A virtual simulation environment for target tracking and obstacle avoidance is constructed. The results verify that the algorithm has satisfactory performance and favorable generalization.

The structure of this paper is as follows: Section II , the background is introduced. Including the DDPG algorithm, the ground target tracking environment, the kinetic models of UAV, observation space and action space. A new method s presented in Section III with the improvement in learning framework, reward function and networks. Section IV gives the the network structure, the simulation parameters and results, and the efficiency of the improved method is proved. The summary and prospect is given in Section five.
SECTION II.
Background

In this section we give an introduction to the background knowledge of DRL ground target tracking and obstacle avoidance , including a DRL algorithm – DDPG and the environment used for tracking.
A. DDPG

An agent-environment interaction process with time discreteness can be represented as a trajectory: \begin{equation*}S_{0}, A_{0}, R_{1}, S_{1}, A_{1},R_{2},S_{2}\ldots\end{equation*} View Source \begin{equation*}S_{0}, A_{0}, R_{1}, S_{1}, A_{1},R_{2},S_{2}\ldots\end{equation*} where \$S_{t}\$ , \$A_{t}\$ , and \$R_{t}\$ represented the state, action , and reward at step \$t\$ respectively . If the trajectory satisfies the Markov property, that is, the values of \$R_{t+1}\$ and \$S_{t+1}\$ have well defined discrete probability distributions dependent only on the preceding state \$S_{t}\$ and action \$R_{t}\$ , we can call this interaction process as a Markov Decision Process (MDP) [38] . The agent tries to select actions so that the sum of the discounted rewards is maximized over the future . The sum of the discounted rewards after step \$t\$ can be defined as the return \$G_{t}\$ whose normal form is: \begin{equation*} G_{t} = R_{t}+\gamma R_{t+1} + \gamma ^{2} R_{t+2} +\ldots =\sum _{\tau =0}^{+\infty }\gamma ^{\tau } R_{t+\tau } \tag{1}\end{equation*} View Source \begin{equation*} G_{t} = R_{t}+\gamma R_{t+1} + \gamma ^{2} R_{t+2} +\ldots =\sum _{\tau =0}^{+\infty }\gamma ^{\tau } R_{t+\tau } \tag{1}\end{equation*} where \$\gamma \in [{0,1}]\$ is a parameter, called the discount rate, is used to determines the current value of future rewards.

A policy is a map from state to action, which determines the behavior of agents and it is a probability distribution of states in generally. Deterministic policy gradient algorithm outputs specific behavior rather than probability. A deterministic policy \$\pi \$ can be defined as a function: \begin{equation*} a = \pi (s;\theta) \tag{2}\end{equation*} View Source \begin{equation*} a = \pi (s;\theta) \tag{2}\end{equation*} where \$\theta \$ refers to the parameters of the function. Our task is to find a set of parameters \$\theta \$ make \$\pi \$ optimum or make \$E_{\pi }[G_{0}]\$ maximum, where \$E_{\pi }[\cdot]\$ denotes the expected value of a random variable given that the agent follows policy \$\pi \$ . In order to find the parameters, we defines two functions, the state-value function \$v_{\pi }(s)\$ and the action-value function \$q_{\pi }(s,a)\$ . \$v_{\pi }(s)\$ represents the value of state s under the policy \$\pi \$ , is the expected return when starting in \$s\$ and following \$\pi \$ thereafter, which can be expressed by formula: \begin{equation*} v_\pi (s_{t}) = E_\pi [G_{t}|S_{t}=s_{t}] \tag{3}\end{equation*} View Source \begin{equation*} v_\pi (s_{t}) = E_\pi [G_{t}|S_{t}=s_{t}] \tag{3}\end{equation*} \$q_{\pi }(s,a)\$ represents the value of taking action \$a\$ in state \$s\$ under a policy \$\pi \$ , is the expected return starting from \$s\$ , taking the action \$a\$ , and thereafter following policy \$\pi \$ , which can be expressed by formula: \begin{equation*} q_\pi (s_{t},a_{t}) = E_\pi [G_{t}|S_{t}=s_{t},A_{t}=a_{t}]. \tag{4}\end{equation*} View Source \begin{equation*} q_\pi (s_{t},a_{t}) = E_\pi [G_{t}|S_{t}=s_{t},A_{t}=a_{t}]. \tag{4}\end{equation*} The relationship between state and action value function could be described by Bellman equation [39] , and the Bellman equation for deterministic policy is defined as: \begin{align*} v_\pi (s)\,\,=&q_\pi (s,a;\eta) \tag{5}\\ q_\pi (s,a)=&r(s,a)+ \gamma \sum _{s'} p(s'|s,\pi (\theta)) v_\pi (s') \tag{6}\end{align*} View Source \begin{align*} v_\pi (s)\,\,=&q_\pi (s,a;\eta) \tag{5}\\ q_\pi (s,a)=&r(s,a)+ \gamma \sum _{s'} p(s'|s,\pi (\theta)) v_\pi (s') \tag{6}\end{align*} where \$\eta \$ is the parameter of value function. There are two steps for policy optimization, the first step is value update. Use temporal difference Learning to update the value functions, in each step: \begin{equation*} y = R_{t+1} + \gamma q_\pi (S_{t+1},A_{t+1};\eta)) \tag{7}\end{equation*} View Source \begin{equation*} y = R_{t+1} + \gamma q_\pi (S_{t+1},A_{t+1};\eta)) \tag{7}\end{equation*} and the parameter \$\eta \$ can be updated by : \begin{equation*} \eta \gets \eta + \alpha [y - q_\pi (S_{t},A_{t};\eta)]\nabla _\eta q_\pi (S_{t},A_{t};\eta) \tag{8}\end{equation*} View Source \begin{equation*} \eta \gets \eta + \alpha [y - q_\pi (S_{t},A_{t};\eta)]\nabla _\eta q_\pi (S_{t},A_{t};\eta) \tag{8}\end{equation*} The second step is policy improvement. Taking the gradient of Bellman equation and calculating its expectations about \$S_{t}\$ , we have the recurrence formula: \begin{align*}E[\nabla v_{\pi (\theta)}(S_{t})] = &E[\nabla \pi (S;\theta)[\nabla _{a}q(S,a)]_{a=\pi (S_{t};\theta)}] \\&\qquad \qquad \qquad{{{ +\, \gamma E[\nabla v_{\pi (\theta)}(S_{t+1})] } }} \tag{9}\end{align*} View Source \begin{align*}E[\nabla v_{\pi (\theta)}(S_{t})] = &E[\nabla \pi (S;\theta)[\nabla _{a}q(S,a)]_{a=\pi (S_{t};\theta)}] \\&\qquad \qquad \qquad{{{ +\, \gamma E[\nabla v_{\pi (\theta)}(S_{t+1})] } }} \tag{9}\end{align*} According to the formula (3) we have \begin{equation*} \nabla E_{\pi }[G_{0}] = E[\nabla v_{\pi }(S_{0})] \tag{10}\end{equation*} View Source \begin{equation*} \nabla E_{\pi }[G_{0}] = E[\nabla v_{\pi }(S_{0})] \tag{10}\end{equation*} Using (9) and (10) , we calculate the policy gradient: \begin{equation*} \nabla E_{\pi }[G_{0}] = \sum _{t=0}^{+\infty } E[\gamma ^{t} \nabla \pi (S;\theta)[\nabla _{a}q(S,a)]_{a=\pi (S_{t};\theta)}]\qquad \tag{11}\end{equation*} View Source \begin{equation*} \nabla E_{\pi }[G_{0}] = \sum _{t=0}^{+\infty } E[\gamma ^{t} \nabla \pi (S;\theta)[\nabla _{a}q(S,a)]_{a=\pi (S_{t};\theta)}]\qquad \tag{11}\end{equation*} In each time step, we update parameters \$\theta \$ according to: \begin{equation*} \theta \gets \theta + \beta \gamma ^{t} \nabla _\theta \pi (S_{t};\theta)[\nabla _{a} q(S_{t},a;\eta)]_{a=\pi (S_{t};\theta)}. \tag{12}\end{equation*} View Source \begin{equation*} \theta \gets \theta + \beta \gamma ^{t} \nabla _\theta \pi (S_{t};\theta)[\nabla _{a} q(S_{t},a;\eta)]_{a=\pi (S_{t};\theta)}. \tag{12}\end{equation*} The principle of DDPG is the same as that of the deterministic policy gradient. The main difference is that the technology of experience replay and target network are used in DDPG whose detailes are given in algorithm 1 .
Algorithm 1 Deep Deterministic Policy Graidient Algorithm

Initialize policy network \$\pi(s)\$ and value network \$q(s,a)\$ with parameters \$\theta\$ and \$\eta\$ ;

Initialize target policy network \$\pi'(s)\$ and target value network \$q'(s,a)\$ with parameters \$\theta' \gets \theta\$ and \$\eta' \gets \eta\$ ;

Initialize the learning rate of target network \$\varepsilon\$ , batch size \$N\$ , replay memory \$R\$ ;

Take action action according to the state \$a_{t} = \pi(s_{t})\$ ;

Executes the action \$a_{t}\$ , receives the reward \$r_{t+1}\$ , acquire new state \$s_{t+1}\$ ;

Save \$\{s_{t}, a_{t}, r_{t+1}, s_{t+1}\}\$ to the memory;

Sample a batch size of \$N\$ datas \$\{(s_{i}, a_{i}, r_{i+1}, s_{i+1})\}_{i=1}^{N}\$ from memory randomly;

Update the policy network and value network: \begin{align*} y_{i}=&r_{i+1} + \gamma q(s_{i+1},\pi (s_{i+1};\theta '); \eta ')\\ \eta\gets&\eta + \alpha \frac {1}{N} \sum _{i} [y_{i} - q(s_{i},a_{i};\eta)]\nabla _\eta q(s_{i},a_{i};\eta)\\ \theta\gets&\theta + \beta \frac {1}{N}\sum _{i}\nabla _{\theta } \pi (h_{i};\theta)[\nabla _{a}q(h_{i},a;\eta)]_{a=\pi (h_{i};\theta)}\end{align*} View Source \begin{align*} y_{i}=&r_{i+1} + \gamma q(s_{i+1},\pi (s_{i+1};\theta '); \eta ')\\ \eta\gets&\eta + \alpha \frac {1}{N} \sum _{i} [y_{i} - q(s_{i},a_{i};\eta)]\nabla _\eta q(s_{i},a_{i};\eta)\\ \theta\gets&\theta + \beta \frac {1}{N}\sum _{i}\nabla _{\theta } \pi (h_{i};\theta)[\nabla _{a}q(h_{i},a;\eta)]_{a=\pi (h_{i};\theta)}\end{align*}

Update the target networks \begin{align*}\eta '\gets&\varepsilon \eta + (1-\varepsilon)\eta '\\ \theta '\gets&\varepsilon \theta _{c} + (1-\varepsilon)\theta '\end{align*} View Source \begin{align*}\eta '\gets&\varepsilon \eta + (1-\varepsilon)\eta '\\ \theta '\gets&\varepsilon \theta _{c} + (1-\varepsilon)\theta '\end{align*}

Back to step 2 untill to the maximum number of training.

B. Environments

The target tracking and obstacle avoidance environment can be illustrated in FIGURE 1 . The location of the target changes in the environment with time steps. The distance and direction of target can be obtained by UAV. Sensors are equipped in the front of the UAV which can measure the distance of obstacles in the range of \$d_{max}\$ ahead. Obstacles can be shaped as arbitrary polygons or circles. When the distance received by the sensor less than a certain threshold, it means that the UAV collision with obstacles and the task fails. When the distance between UAV and target is less than a certain threshold, it means the tracking task is being performed.
FIGURE 1.

Target tracking and obstacle avoidance environment. \$d_{0}\$ denotes the distance between UAV and target; \$d_{1} \sim d_{9}\$ represent the distances returned by the distance sensors; \$\chi \$ denotes the orientation of target; \$\psi \$ denotes the orientation of the UAV velocity.

Show All

We use matplotlib - python to build a complete environment model. The whole environment is confined to a two-dimensional plane space of 500 meters in length and width with boundaries around it. Several stationary polygons are randomly placed in the boundaries as obstacles, and target at rest or in uniform motion. More informations about the environments are shown in Section IV .
C. Observation and Action Space

The observations we can get from the environment include the position and velocity of UAVs, the distance and direction of target, obstacle distance in directions of the sensors. Considering the dynamic environment and generalization, we abandon the position of UAV, retain the speed \$v\$ and direction \$\psi \$ . We use the relative azimuth \$[\chi, d_{0}]\$ to indicate the relationship between UAV and target. We use the distance measured by sensors indicates the relationship between UAV and obstacles. The observation space: \$o=[\psi,\chi,d_{0},d_{1},d_{2},\ldots,d_{n}]\$ , where \$n\$ indicates the number of sensors, \$\psi,\chi \in [-\pi, \pi]\$ , \$d_{0} \in [0, +\infty]\$ , \$d_{1} \sim d_{n} \in [0, d_{max}]\$ .

UAV can be regarded as a point when planning the trajectory in large scale environments, its three-dimensional continuous-time motion model is given as follows: \begin{equation*} \left [{\begin{matrix} \dot {x}\\ \dot {y}\\ \dot {z}\\ \dot {v}\\ \dot {\psi }\\ \dot {\gamma }\\ \end{matrix}}\right] = \left [{\begin{matrix} v cos \psi cos \gamma \\ v sin \psi cos \gamma \\ v sin \gamma \\ u_{\dot {v}}\\ u_{\dot {\psi }}\\ u_{\dot {\gamma }} \end{matrix}}\right] \tag{13}\end{equation*} View Source \begin{equation*} \left [{\begin{matrix} \dot {x}\\ \dot {y}\\ \dot {z}\\ \dot {v}\\ \dot {\psi }\\ \dot {\gamma }\\ \end{matrix}}\right] = \left [{\begin{matrix} v cos \psi cos \gamma \\ v sin \psi cos \gamma \\ v sin \gamma \\ u_{\dot {v}}\\ u_{\dot {\psi }}\\ u_{\dot {\gamma }} \end{matrix}}\right] \tag{13}\end{equation*} where [ \$x\$ , \$y\$ , \$z\$ ] is the coordinates of UAV in three-dimensional space. [ \$v\$ , \$\psi \$ , \$\gamma \$ ] indicate the speed, yaw angular, pitch angular, respectively.[ \$u_{\dot {\psi }}\$ , \$u_{\dot {\gamma }}\$ , \$u_{\dot {v}}\$ ] is the control command . For simplify, we limit the trajectory planning problem into a two-dimensional continuous state environment and set the velocity to a constant value . The simplified dynamic discrete mode1 in time is presented as follows: \begin{equation*} \left [{\begin{matrix} x(t+1)\\ y(t+1)\\ \psi (t+1) \end{matrix}}\right] = \left [{\begin{matrix} x(t) + v cos \psi (t+1)\\ y(t) + v sin \psi (t+1)\\ \psi (t) + a(t) \end{matrix}}\right] \tag{14}\end{equation*} View Source \begin{equation*} \left [{\begin{matrix} x(t+1)\\ y(t+1)\\ \psi (t+1) \end{matrix}}\right] = \left [{\begin{matrix} x(t) + v cos \psi (t+1)\\ y(t) + v sin \psi (t+1)\\ \psi (t) + a(t) \end{matrix}}\right] \tag{14}\end{equation*} The task for target tracking is to drive the UAV reach the target position in the shortest time and keeps it within bounds without colliding with any obstacles.

The action \$a\$ refers to the variation of \$\psi \$ , while the the control quantity of true UAVs usually expressed as acceleration [40] . In order to guarantee that the action can be followed, the mechanical system between acceleration and action is considered in subsequent part. In the horizontal direction, the force on a moving UAV satisifices the relationship: \begin{equation*} F = \frac {mV^{2}}{\rho }\tag{15}\end{equation*} View Source \begin{equation*} F = \frac {mV^{2}}{\rho }\tag{15}\end{equation*} and the normal acceleration \$a'\$ is shown as: \begin{equation*} a' = \frac {F}{m} = \frac {V^{2}}{\rho }.\tag{16}\end{equation*} View Source \begin{equation*} a' = \frac {F}{m} = \frac {V^{2}}{\rho }.\tag{16}\end{equation*} where \$\rho \$ is the curvature radius of the path, which can be shown in FIGURE 2 . \$AB\$ and \$BC\$ have the same length which equals the distance traveled by UAV within half sampling period. According to the geometric relation, we have the curvature radius as: \begin{equation*} \rho = 0.5 V \Delta t cot {\frac {a}{2}}\tag{17}\end{equation*} View Source \begin{equation*} \rho = 0.5 V \Delta t cot {\frac {a}{2}}\tag{17}\end{equation*} where \$\Delta t\$ is the sampling time-interval. Thus we have the relationship between acceleration and action: \begin{equation*} a' =\frac {2V tan {\frac {a}{2}}}{ \Delta t } \approx \frac {V a}{\Delta t}.\tag{18}\end{equation*} View Source \begin{equation*} a' =\frac {2V tan {\frac {a}{2}}}{ \Delta t } \approx \frac {V a}{\Delta t}.\tag{18}\end{equation*} The relationship can be used to determine the range of action, or it can be used to check whether the selected action range is reasonable. If the acceleration range is covered by the mobility of real UAV, the generated trajectories can be followed by UAV.
FIGURE 2.

Action and curvature radius. \$a\$ denotes the action; \$\rho \$ represent the curvature radius; the broken line \$ABC\$ denotes the generated trajectory in one sampling period; the arc \$AC\$ denotes the real trajectory in one sampling period.

Show All

SECTION III.
Proposed Method

We improve the DDPG in framework, reward function and networks in this section.
A. Learning Framework

DRL framework for UAV target tracking and obstacle avoidance includes three modules:

    Environment description module. The function of this module is to sense target informations, describe the threats faced by UAVs and extract relevant features.

    DRL Control module. DRL control is the corner of the framework, which receives the environment description signal, estimates the state value of environment and modifies the control policy continuously.

    UAV module. UAV adjusts the attitude and position by means of the control signal , then affects the received environment description.

Compared with traditional DRL, our framework which is shown in FIGURE 3 is different in two aspects. The first is that feature extraction functions are placed in the DRL control module, the feature extraction network and the actor network can be trained simultaneously. The second is that multiple UAVs with the same policy interact with environments, which is helpful for UAVs to discover more unknown environments. The exploration ability and convergence speed are improved simultaneously . Due to the improvement in exploration, the problem of local optimization has been also alleviated.
FIGURE 3.

The improved DRL framework for UAV target tracking. Feature extraction module connected with Actor-Critic modules directly, which is convenient for joint training. Multi-UAVs explore environments cooperatively, which speed up the convergence and alleviate the local optimization.

Show All

B. Reward Function

Reinforcement learning uses reward to estimate the expected return and gets the optimal strategy. The setting of reward function is closely related to the quality of training results. A simpler method is to set sparse rewards based on results, that is to say, each episode only gives positive or negative rewards according to whether the mission can be fulfilled or not . This method has the advantage of strong applicability and can be used in various environmental models, while the disadvantage is that the convergence speed is slow for the update of network only at the end of each episode, and the algorithm is easy to get stuck at locally optimal value for random exploration.

In order to improve the efficiency and practicability , a non-sparse reward is designed to guide UAV tracking and obstacle avoidance in the special application environment, which consists of LOS reward, distance reward, terminal reward and action penalty. LOS is the line between UAV and target, and the LOS reward is designed as: \begin{equation*} r_{LOS} = \lambda \left({\frac {\pi }{2} - |\chi - \psi |}\right) \tag{19}\end{equation*} View Source \begin{equation*} r_{LOS} = \lambda \left({\frac {\pi }{2} - |\chi - \psi |}\right) \tag{19}\end{equation*} where \$\lambda \$ is a positive constant, \$|\chi - \psi |\$ is the LOS angle in velocity coordinate system. The smaller the LOS angle is, the faster UAV approach to the target. Once the LOS larger than \$\frac {\pi }{2}\$ , UAV will fly away from the target. The physical significance of \$r_{LOS}\$ is that no matter where the UAV is, it can get a higher reward as long as flies towards the target. \$r_{LOS}\$ is the most important reward to guide the UAV fly to the target.

The obstacle reward is designed as: \begin{align*}r_{obstacle} =&\sigma \Biggl\{\left[{\sum _{i=1}^{n}\left({\frac {1}{d_{i}} - \frac {1}{d_{max}}}\right)cos{\frac {i \pi }{n-1}}}\right]^{2} \\&\qquad \qquad \,\,\, +\,\left[{\sum _{i=1}^{n}\left({\frac {1}{d_{i}} - \frac {1}{d_{max}}}\right)sin{\frac {i \pi }{n-1}}}\right]^{2} \Biggr\} \tag{20}\end{align*} View Source \begin{align*}r_{obstacle} =&\sigma \Biggl\{\left[{\sum _{i=1}^{n}\left({\frac {1}{d_{i}} - \frac {1}{d_{max}}}\right)cos{\frac {i \pi }{n-1}}}\right]^{2} \\&\qquad \qquad \,\,\, +\,\left[{\sum _{i=1}^{n}\left({\frac {1}{d_{i}} - \frac {1}{d_{max}}}\right)sin{\frac {i \pi }{n-1}}}\right]^{2} \Biggr\} \tag{20}\end{align*} where \$\sigma \$ is a negative constant. The reward is transformed from artificial potential field, which represents the overlap value of obstacle repulsive fields in sensor directions.

The terminal reward relate to the success of the mission, which is designed as: \begin{equation*} r_{terminal} = \begin{cases} -k & if~d_{i} \leq d_{min1},\quad i\in [1], [n] \\ k & if~d_{0} \leq d_{min2}\\ 0 & else \end{cases} \tag{21}\end{equation*} View Source \begin{equation*} r_{terminal} = \begin{cases} -k & if~d_{i} \leq d_{min1},\quad i\in [1], [n] \\ k & if~d_{0} \leq d_{min2}\\ 0 & else \end{cases} \tag{21}\end{equation*} where \$k\$ is a positive constant and \$d_{min1}\$ is the threshold of obstacle avoidance, \$d_{min2}\$ is the threshold of target tracking. \$r_{terminal}\$ is a sparse reward whose physical significance is that no matter what the direction of the UAV is, it can get a higher reward as long as its distance from the target is less than a threshold, and it will get a higher penalty as long as its distance from the obstacles is less than another threshold. If any of these two conditions are triggered, the UAV will be re-initialized. The main function of \$r_{terminal}\$ is to guide UAV avoid obstacles and hover around the target.

In order to make trajectories smoother, an action penalty is given as: \begin{equation*} r_{action} = \alpha |\Delta a| \tag{22}\end{equation*} View Source \begin{equation*} r_{action} = \alpha |\Delta a| \tag{22}\end{equation*} where \$\alpha \$ is a negtive constant, \$\Delta a\$ indicates the change of actions in adjacent time.

To summarize, we give the final reward function: \begin{equation*} r = r_{LOS} + r_{obstacle} + r_{terminal} + r_{action} \tag{23}\end{equation*} View Source \begin{equation*} r = r_{LOS} + r_{obstacle} + r_{terminal} + r_{action} \tag{23}\end{equation*} In previous studies, the sparse reward \$r_{terminal}\$ is a common reward function. However, we found that the model trained by \$r_{terminal}\$ has a high probability to fall into local optimum. The UAV controlled by the local optimum model only consider the obstacle avoidance while ignoring the target. The local optimization problem has been alleviated when using multi-UAVs to explore environment cooperatively, while the training results are still unstable. In our study, we design the reward \$r_{LOS}\$ , UAVs will head to the target to get more rewards. The application of \$r_{LOS}\$ reduces the meaningless patrols and circling, improves the training successful ratio greatly, and alleviate the local optimization problem further.
C. Actor Recurrent Critic

The tasks for ground target tracking are performed in dynamic and partially observable environments where the observations are quite different from the states. We receive the observation and corresponding action at a certain time, but the reward may comes later, which can be illustrated by the function: \begin{equation*} r(o_{t},a_{t}) = \sum _{i=1}^{T} \kappa _{t}^{t-T+i} r'(o_{t-T+i}, a_{t-T+i})\tag{24}\end{equation*} View Source \begin{equation*} r(o_{t},a_{t}) = \sum _{i=1}^{T} \kappa _{t}^{t-T+i} r'(o_{t-T+i}, a_{t-T+i})\tag{24}\end{equation*} where \$r(o_{t},a_{t})\$ indicates the reward received in time step \$t\$ , and \$r'(o_{t-T+i}, a_{t-T+i})\$ indicates the reward produced in time step \$t-T+i\$ . Define \$\kappa _{t}^{t-T+i} r'(o_{t-T+i}, a_{t-T+i})\$ as a part of reward which is produced in time step \$t-T+i\$ while received in time step \$t\$ , where \$\kappa _{t}^{t-T+i} > 0\$ is a discount factor and \$\sum _{i}^{T} \kappa _{t}^{t-T+i} = 1\$ . For the function (20) , we have the conclusion that the reward we detected in each time step is a function about history observations which can be represented as a trajectory: \begin{equation*} O_{0}, A_{0}, O_{1}, A_{1},O_{2},\ldots,S_{t}\end{equation*} View Source \begin{equation*} O_{0}, A_{0}, O_{1}, A_{1},O_{2},\ldots,S_{t}\end{equation*} In order to speed up the computation, we set a maximum history length \$T\$ , use the historical observations from \$t-T\$ to \$t\$ to train the network in each time \$t\$ . For the actions could get from the adjacent observations, it can be omitted . We have the history as: \begin{equation*} h_{t} = o_{t-T}, o_{t-t+1},\ldots,o_{t} \tag{25}\end{equation*} View Source \begin{equation*} h_{t} = o_{t-T}, o_{t-t+1},\ldots,o_{t} \tag{25}\end{equation*}

DDPG updates the value network with the detected reward in each step, so the value network can be seen as a function about \$h_{t}\$ . We have already known in equation (3) that value function is a function about state, so that we can simulate the state by history. Recurrent networks could synthesize the historical observations, have a better representation of states. LSTM is a kind of excellent recurrent neural networks, which consists of keep gate, write gate and read gate, it has a great ability of controlling historical information to participates in training. Using the LSTM network to simulate the state from observation history, We have the state as: \begin{equation*} s_{t} = f(h_{t}; \omega) \tag{26}\end{equation*} View Source \begin{equation*} s_{t} = f(h_{t}; \omega) \tag{26}\end{equation*} where \$f(\cdot)\$ is determined by the LSTM network, \$\omega \$ represents the parameters of LSTM.

There are two kinds of networks in DDPG. Actor network is used to adjust the parameters of policy, determine the best action in a specific state. Critic network is used to evaluate the value of current action. We improve the framework of DDPG and name the new structure as ARC(Actor - Recurrent - Critic Network). The main structure of the ARC network is shown in FIGURE 4 . The actor network and the critic network are consisted of Dense network, which computes the continuous actions and value functions respectively. LSTM and actor combine together to make up the policy network, the value network is consisted of LSTM and critic. Policy and value networks share the same structure and parameters of LSTM. The policy is defined as: \begin{equation*} a = \pi (h_{t}; \omega, \theta) \tag{27}\end{equation*} View Source \begin{equation*} a = \pi (h_{t}; \omega, \theta) \tag{27}\end{equation*} where \$\theta \$ represents the parameters of actor network. The value is defined as: \begin{equation*} q = Q(h_{t}, a_{t}; \omega, \eta) \tag{28}\end{equation*} View Source \begin{equation*} q = Q(h_{t}, a_{t}; \omega, \eta) \tag{28}\end{equation*} where \$\eta \$ represents the parameters of critic network. In each time step, we update parameters according to (29)–(32) . \begin{align*} y=&r_{t+1} + \gamma Q(h_{t+1},a_{t+1}) \tag{29}\\ \eta\gets&\eta + \alpha [y - Q(H_{t},A_{t})]\nabla _\eta Q(h_{t},a_{t};\omega,\eta) \tag{30}\\ \omega\gets&\omega + \alpha [y - Q(h_{t},a_{t})]\nabla _{\omega } Q(h_{t},a_{t};\omega,\eta) \tag{31}\\ \theta\gets&\theta + \beta \nabla _{\theta } \pi (h_{t};\omega,\theta)[\nabla _{a}q(h_{t},a;\omega,\eta)]_{a=\pi (h_{t};\omega,\theta)} \qquad ~~ \tag{32}\end{align*} View Source \begin{align*} y=&r_{t+1} + \gamma Q(h_{t+1},a_{t+1}) \tag{29}\\ \eta\gets&\eta + \alpha [y - Q(H_{t},A_{t})]\nabla _\eta Q(h_{t},a_{t};\omega,\eta) \tag{30}\\ \omega\gets&\omega + \alpha [y - Q(h_{t},a_{t})]\nabla _{\omega } Q(h_{t},a_{t};\omega,\eta) \tag{31}\\ \theta\gets&\theta + \beta \nabla _{\theta } \pi (h_{t};\omega,\theta)[\nabla _{a}q(h_{t},a;\omega,\eta)]_{a=\pi (h_{t};\omega,\theta)} \qquad ~~ \tag{32}\end{align*} Algorithm 2 provides the overall steps of our ARC algorithm.
Algorithm 2 Actor - Recurrent - Critic Algorithm

Initialize policy network \$\pi (h)\$ and value network \$q(h,a)\$ with parameters \$\omega \$ , \$\theta \$ and \$\eta \$ ;

Initialize target policy network \$\pi '(h)\$ and target value network \$q'(h,a)\$ with parameters \$\omega ' \gets \omega \$ , \$\theta ' \gets \theta \$ and \$\eta ' \gets \eta \$ ;

Initialize the learning rate of target network \$\varepsilon \$ , batch size \$N\$ , maximum history length \$T\$ , replay memory \$R\$ ;

Take action action according to the historical observation \$a_{t} = \pi (h_{t})\$ ;

Executes the action \$a_{t}\$ , receives the reward \$r_{t+1}\$ , acquire new observation \$o_{t+1}\$ ;

Update the history \$h_{t+1} = o_{t-T+1}, o_{t-t+2},\ldots,o_{t+1}\$ and save \$\{h_{t}, a_{t}, r_{t+1}, h_{t+1}\}\$ to the memory;

Sample a batch size of \$N\$ datas \$\{(h_{i}, a_{i}, r_{i+1}, h_{i+1})\}_{i=1}^{N}\$ from memory randomly;

Update the policy network and value network: \begin{align*} y_{i}=&r_{i+1} + \gamma q(h_{i+1},\pi (h_{i+1};\omega ',\theta ');\omega ', \eta ')\\[-3pt] \eta\gets&\eta + \alpha \frac {1}{N} \sum _{i} [y_{i} - q(h_{i},a_{i};\omega,\eta)]\nabla _\eta q(h_{i},a_{i};\omega,\eta)\\[-3pt] \omega\gets&\omega + \alpha \frac {1}{N}\sum _{i}[y_{i} - q(h_{i},a_{i};\omega,\eta)]\nabla _{\omega } q(h_{i},a_{i};\omega,\eta)\\ \theta\gets&\theta + \beta \frac {1}{N}\sum _{i}\nabla _{\theta } \pi (h_{i};\omega,\theta)[\nabla _{a}q(h_{i},a;\omega,\eta)]_{a=\pi (h_{i};\omega,\theta)}\end{align*} View Source \begin{align*} y_{i}=&r_{i+1} + \gamma q(h_{i+1},\pi (h_{i+1};\omega ',\theta ');\omega ', \eta ')\\[-3pt] \eta\gets&\eta + \alpha \frac {1}{N} \sum _{i} [y_{i} - q(h_{i},a_{i};\omega,\eta)]\nabla _\eta q(h_{i},a_{i};\omega,\eta)\\[-3pt] \omega\gets&\omega + \alpha \frac {1}{N}\sum _{i}[y_{i} - q(h_{i},a_{i};\omega,\eta)]\nabla _{\omega } q(h_{i},a_{i};\omega,\eta)\\ \theta\gets&\theta + \beta \frac {1}{N}\sum _{i}\nabla _{\theta } \pi (h_{i};\omega,\theta)[\nabla _{a}q(h_{i},a;\omega,\eta)]_{a=\pi (h_{i};\omega,\theta)}\end{align*}

Update the target networks \begin{align*} \eta '\gets&\varepsilon \eta + (1-\varepsilon)\eta '\\[-3pt] \omega\gets&\varepsilon \omega + (1-\varepsilon)\omega '\\[-3pt] \theta '\gets&\varepsilon \theta _{c} + (1-\varepsilon)\theta '\end{align*} View Source \begin{align*} \eta '\gets&\varepsilon \eta + (1-\varepsilon)\eta '\\[-3pt] \omega\gets&\varepsilon \omega + (1-\varepsilon)\omega '\\[-3pt] \theta '\gets&\varepsilon \theta _{c} + (1-\varepsilon)\theta '\end{align*}

Back to step 2 untill to the maximum number of training.
FIGURE 4.

Actor - Recurrent - Critic(ARC) networks architecture. The LSTM network simulate the state; The actor network selects actions; The critic network applies to estimate state-action values.

Show All

SECTION IV.
Experiments

In this section, simulation experiment is carried out based on TensorFlow2.0 - python, the network and hyper parameters required by the experiment are given, and the experimental results are analyzed.
A. Experimental Settings

There are two kinds of network structures in DDPG – the value and the policy, which are shown in FIGURE 5 . The layer of LSTM is used to extract feature informations from history observations, those two networks share the same LSTM and update it synchronously. In our environments, the speed is set to \$V = 3m/s\$ , the action range of UAVs set as \$[-\pi /20, \pi /20]\$ and the sampling time-interval \$\Delta t = 1s\$ . According to equation(18) , we have the normal acceleration \$a'\$ with small range [−0.47, 0.47] m/s 2 which makes the generated trajectories easier to follow. The observations of UAV are normalized to [0,1] and the action signal is normalized to [−1,1]. The reward is instantiated as: \$\lambda = 1, \sigma = 0.1, k = 100, \alpha = 0.01\$ . The maximum history length is set to 5. The capacity of memory is set to 4000. The batch size is set to 32. RMSprop optimizer [41] is employed to learn the network parameters with a learning rate of \$10^{-3}\$ . The discount factor is \$\gamma = 0.9\$ and the soft target update rate is \$\varepsilon = 0.01\$ . The exploration noise is set to \$Var(-0.2, 0.2)\$ . The number of train episodes is n = 300, the maximum steps in each episode is m = 500.
FIGURE 5.

Policy and value networks of ARC. The number of trainable params of policy network is 47,241, and the number of trainable params of value network is 47,441. These two networks share the same LSTM structure and parameters.

Show All

B. Simulation and Performance Analysis

In this subsection, the training process is given. We observe the number of steps and rewards in each episode, statistic and analysis their changes from the initial to the convergence. FIGURE 6 shows the simulation results, where (a) represents the number of training steps in each episode. In the first 40 episodes, the training steps of UAV are less than 100, which indicates the termination condition is triggered, i.e. the UAV will collision with obstacles. Then the number of training steps began to increase, creep up to 100 , or even arrive at 500, which indicates that the UAV learned how to avoid obstacles. After about 70th episode, the number of training steps began to decrease and stable between 100 and 200 finally , which indicates that the UAV learned better policy and could approach the target successfully.
FIGURE 6.

Simulation results of ARC. The x-axis represent the number of episodes, and the y-axis represent the steps, reward and average reward in each episode respectively. Approach the target or colliding with an obstacle indicates the end of an episode, and each episode have the most time steps 500.

Show All

The purpose of DRL is to improve the cumulative reward through continuous learning, so as to obtain the maximum cumulative reward. Therefore, the higher the reward is, the better the traning effect is . The cumulative reward of each episode is shown in FIGURE 6 (b) . By comparison, we find that the trend of cumulative reward is consistent with the obtained analysis results. The average reward for each step in each episode reflects the effect of the training process. As shown in FIGURE 6 (c) , the average reward increase gradually . After about 100th episode, the average reward reaches the top and stays stable. Due to the exploration noise and random initial state, UAV collides with obstacles in a small number of episodes results in low average reward.

In order to prove the reliability of the improved method further, traditional DDPG with the same reward function and hyper parameters is used for comparison, and the results are shown in FIGURE 7 . The simulation shows that the number of training steps began to convergence after about 130th episode, which is slower than the improved method. Due to inadequate exploration, there are large fluctuations in the last 20 episodes and the reward drops considerably. It can be found that both the stability and the convergence speed are enhanced significantly in the proposed method .
FIGURE 7.

Simulation results of DDPG. The x-axis represent the number of episodes, and the y-axis represent the steps, reward and average reward in each episode respectively. Approach the target or colliding with an obstacle indicates the end of an episode, and each episode have the most time steps 500.

Show All

C. Experiment Result

This subsection shows the effect of target tracking by trajectories and the normalized distance between UAV and target. FIGURE 8 shows the trajectories in different environments, and FIGURE 9 shows their normalized tracking distance correspondingly .
FIGURE 8.

Some examples of the simulated complete environments. Obstacles in different types of environments are distinguished from shapes, sizes and numbers.

Show All
FIGURE 9.

Some examples of the simulated complete environments. Obstacles in different types of environments are distinguished from shapes, sizes and numbers.

Show All

Environment 1 shows the tracking results for stationary target in a barrier-free environment. At first the UAV approaches the target quickly, and then circles around the target to keep continuously tracking and observing within the range of maneuverability. Environment 2 shows the tracking results for stationary target in the case of simple obstacle interference. UAV can avoid the obstacle successfully and approach the target quickly . In the final phase, UAV also flights around the target. Environment 3 shows the tracking results for stationary target in a environment with complex obstacles. Due to the dynamic constraints and interference of obstacles , the UAV cannot fly around the target. However, we can find that, the UAV can still complete the task of continuous observation and tracking by keeping a certain distance from the target on the premise of guaranteeing its own safety.

Environment 4–6 shows the tracking results for moving target. The moving speed is set to be 0.8m/s which is less than the speed of UAV. Therefore, the UAV has the ability to fuifil the tracking task under the correct guidance. Environment 4 shows the tracking results in a barrier-free environment. The results show that after approaches the target, the UAV always hovers within a certain range of the target and can observe and track the target stably. Environment 5 shows the tracking results for moving targets in a environment with a simple obstacle . The UAV approaches the target quickly and keeps tracking in the initial stage. When the obstacle is encountered, after a short adjustment to ensure flight safety, UAV flies to the target again and maintain tracking. Environment 6 shows the tracking results for the moving target in the environment with complex obstacles . Due to the density of obstacles and the constraint of flying ability , the UAV cannot maintain a stable observation distance to the target, but it can still fly to the target on the premise of avoiding obstacles.

In order to prove the effectiveness and practicability further. Two environments shown in FIGURE 10 are selected to repeat the experiment. The target is set to stationary in the center, and the initial state of UAV is random in each episode . For target tracking problem in the environment 7 which has sparse obstacles in it, compared with the traditional DDPG algorithm, the success rate of the improved algorithm is increased from 70.0% to 91.8%. For target tracking problem in the environment 8 which has dense obstacles in it, the success rate is increased from 13.6% to 67.5%.
FIGURE 10.

Environments used for repeat the experiment. Environments with stationary target in the center. Obstacles in different types of environments are distinguished from shapes, sizes and numbers.

Show All

SECTION V.
Conclusion and Prospect

In this paper, we improve the DDPG algorithm make it more suitable for UAV target tracking. The simulation results show that the training process is more stable and the convergence is faster. In the verification process, we observe that the UAV could generate the collision free trajectory for target tracking. Besides, the failure rate decreases significantly compared with traditional DDPG.

Despite the better results, there are still areas for improvement . The following scheme is proposed for future work.

    State space. The visual based DRL method can extract the obstacle information directly from the depth images collected by the camera, but it can’t detect the target position information. One solution is that the relative position information of the target is calculated and fused with the images. Then the expanded high-level image information can be used as the input of DRL.

    Reward function. The design of reward function is crucial to the training effect of DRL. In order to obtain satisfactory results, we have compared more than 10 kinds of reward functions. More effective method for reward function definition will be the focus of the follow-up research.

    Combination with rule-based methods. Although the target tracking method based on DRL can ensure convergence, it lacks security and practicability. The rule-based path planning algorithms are usually more stable and effective. Therefore, the combination of DRL with rule-based methods will be a promising research direction, which can not only deal with complex and changeable environment, but also enhance the stability and efficiency.

Authors
Figures
References
Citations
Keywords
Metrics
More Like This
An Improved Real-Time Path Planning Method Based on Dragonfly Algorithm for Heterogeneous Multi-Robot System

IEEE Access

Published: 2020
Path Planning Based on Ant Colony Algorithm and Distributed Local Navigation for Multi-Robot Systems

2006 International Conference on Mechatronics and Automation

Published: 2006
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
