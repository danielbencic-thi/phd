IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathZoom.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Conferences > 2021 IEEE/RSJ International C...
Autonomous Flights in Dynamic Environments with Onboard Vision
Publisher: IEEE
Cite This
PDF
Yingjian Wang ; Jialin Ji ; Qianhao Wang ; Chao Xu ; Fei Gao
All Authors
1
Paper
Citation
189
Full
Text Views

    Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Related Work
    III.
    Dynamic Environment Perception
    IV.
    Dynamic Planning
    V.
    Experiments and Benchmarks

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
Media
Footnotes
Abstract:
In this paper, we introduce a complete system for autonomous flight of quadrotors in dynamic environments with onboard sensing. Extended from existing work, we develop an occlusion-aware dynamic perception method based on depth images, which classifies obstacles as dynamic and static. For representing generic dynamic environment, we model dynamic objects with moving ellipsoids and fuse static ones into an occupancy grid map. To achieve dynamic avoidance, we design a planning method composed of modified kinodynamic path searching and gradient-based optimization. The method leverages manually constructed gradients without maintaining a signed distance field (SDF), making the planning procedure finished in milliseconds. We integrate the above methods into a customized quadrotor system and thoroughly test it in real-world experiments, verifying its effective collision avoidance in dynamic environments.
Published in: 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Date of Conference: 27 Sept.-1 Oct. 2021
Date Added to IEEE Xplore : 16 December 2021
ISBN Information:
ISSN Information:
INSPEC Accession Number: 21487383
DOI: 10.1109/IROS51168.2021.9636117
Publisher: IEEE
Conference Location: Prague, Czech Republic
Funding Agency:
SECTION I.
Introduction

Thanks to the maturity of autonomous navigation technology, aerial robots, especially quadrotors, have made impressive progress in the last decade. However, it is still challenging to let quadrotors operate autonomously in dynamic environments due to trifold reasons. Firstly, the perception of dynamic obstacles is hard to satisfy the efficiency and accuracy requirements at the same time, making the quadrotor fragile in a complex dynamic environment. Moreover, to make a quadrotor fly smoothly among moving obstacles, the onboard planner must fully utilize the obstacle position and velocity estimation while retaining high efficiency. Finally, the system integration requires robustness from all modules, including localization, perception, and planning, under limited onboard sensing and computing resources. These challenges render autonomous flights in an unknown dynamic environment hard, making robotic community always lacks a convincing systematic solution.

In this paper, we bridge this research gap by introducing a complete system composed of an accurate dynamic environment perception module and an accompanied trajectory generation method. For dynamic environment perception, we classify the obstacles into static ones and dynamic ones by clustering, tracking, and voting the point cloud data from a depth sensor, inspired by [1] . To address the issue of occlusion, we propose an occlusion-aware Kalman filter to estimate the motion states of dynamic objects, including position, velocity, size, and associated uncertainty, making the classification more accurate and robust. We then fuse the motionless points into an occupancy grid map and model the dynamic ones as several moving ellipsoids for the subsequent trajectory generation.
Fig. 1. - Experimental results of autonomous flight in a dynamic environment with onboard sensing. Top left: a flying UAV in front of a walking person. Top right: onboard grayscale image. Bottom left: depth image. Bottom right: the optimized B-spline trajectory (blue points and red curve) between the detected moving person(red ellipsoids with a red line indicating the velocity) and the static grids.
Fig. 1.

Experimental results of autonomous flight in a dynamic environment with onboard sensing. Top left: a flying UAV in front of a walking person. Top right: onboard grayscale image. Bottom left: depth image. Bottom right: the optimized B-spline trajectory (blue points and red curve) between the detected moving person(red ellipsoids with a red line indicating the velocity) and the static grids.

Show All

Considering the static and the dynamic obstacles simultaneously, we propose a hierarchical trajectory generation method. Firstly, we design a kinodynamic path searching method to search for a safe, feasible, and minimum-time initial path by dynamic safety check. After parameterizing the initial path into a B-spline curve, we refine the curve with a gradient-based optimization. Extending our previous work [2] , we obtain gradients by manually constructed penalties without maintaining an SDF, which dramatically lowers the computational burden.

Compared to existing state-of-the-art works [2] , [3] , our proposed method can generate safe trajectories in more generic environments composed of cluttered static and dynamic obstacles. We perform comprehensive tests in simulation and real world to validate the robustness of our method. Summarizing our contributions as follows:

    We develop an occlusion-aware dynamic environment perception method based on [1] , which efficiently yet accurately classifies the obstacles into static ones and dynamic ones. Moreover, we represent both classes of obstacles as two different data structures to handle generic complex environments.

    We propose a hierarchical trajectory generation method composed of a kinodynamic searching method and a gradient-based optimization. We leverage the environment representation mentioned above to construct gradients manually, which avoids the overhead of maintaining an SDF, making the whole procedure finished in milliseconds.

    We integrate our proposed methods into a fully autonomous quadrotor system and release our software for the community’s reference 1 .

SECTION II.
Related Work
A. Dynamic Perception

There are mainly two categories of methods for dynamic perception. One is to use imagery data. In [4] , authors adopt the Frame-Difference (FD) method [5] to detect moving objects, which requires that UAVs hover steady. However, frequent hovering will greatly affect the smoothness of flight in dynamic avoidance missions. Besides, some work utilize outliers of the feature tracking module in feature-based vision system to detect dynamic objects [6] [7] – [8] . However, it requires dense feature points, which is hard to satisfy in generic environments. Extended from optical flow, scene flow method [9] , [10] computes 3D velocity of each pixel. This technique, however, is unable to run in real-time on a computationally-constrained platform. Another approach is using detector [11] , or segmentation networks [12] , which behaves well in detection of predefined classes such as pedestrians or cars [13] , [14] , but cannot handle generic cases.

Apart from using imagery data, point-cloud-based methods aim to estimate the motion of objects leveraging 3D information. Kinect Fusion [15] implements Iterative Closest Point (ICP) [16] to deal with moving objects. However, this approach would result in high computational overhead. Some work [17] , [18] use U-map [19] to detect obstacles which considers that points with similar depth belong to the same object. This method divides the point cloud into individual objects without dynamic classification, which does not represent the dynamic environment accurately. Our approach builds upon and extends a dynamic obstacle detection and tracking algorithm [1] , which tracks clusters and classifies them as dynamic or static. Varying from them, we conduct tracking with occlusion-awareness and replace their human detector with our proposed re-free strategy to address the issue of erroneous occupancy caused by temporary standstills.
B. Dynamic Planning

Most planning methods in dynamic environments inherit the ones in static environments and just design some techniques to handle the moving obstacles. Reactive methods, including velocity obstacles (VO) [20] [21] – [22] , inevitable collision states (ICS) [23] , [24] and artificial potential field (APF) [25] , leverage the information of current moving obstacles and only compute one-step actions, which makes them quite short-sighted due to the lack of long horizon. Seder et.al [26] first adopt a focused D* to find a reference path without considering obstacle motions and then generates admissible local trajectories around the reference path using a dynamic window algorithm. In [27] , the RRT and forward SR sets are utilized to find the reference path and avoid moving obstacles, respectively.

In recent studies, Gao et.al [28] use semi-definite relaxation on nonconvex Quadratical Constraint Quadratic Programming (QCQP) to optimize a piecewise polynomial trajectory in dynamic environments. However, this method and other global planners [29] , [30] , assume that locations and velocities of all obstacles are known, which cannot work in autonomous drone platform. In [3] , the authors obtain objects’ necessary state information by onboard detection method and utilize chance-constrained nonlinear model predictive control for obstacle avoidance in dynamic environments. However, it regards all obstacles as ellipsoids, which makes it cannot handle complex static obstacles. Beyond that, nonlinear optimization is prone to fall into a local minimum without a proper initial guess.
SECTION III.
Dynamic Environment Perception

In this section, we describe a dynamic perception module built upon [1] . Given a point cloud generated from depth image at time τ ∈ ℝ, we filter it to reduce sensor noise and lower computational overhead, and then cluster the filtered point cloud into individual objects per frame using DBSCAN [31] , resulting in a set of m clusters C τ = { c 1 τ , c 2 τ , … , c m τ } . To estimate moving agents’ necessary states, we then conduct occlusion-aware tracking and dynamic classification.
A. Occlusion-aware Tracking

We denote a list of cluster sets over k frames separated by Δ t as C ~ t , k = { C t − k ⋅ Δ t , … , C t − Δ t , C t } . Given C ~ t , k , we use an occlusion-aware tracking algorithm extended from [1] to obtain their tracking history H i t = { c ∗ t − n ⋅ Δ t , … , c ∗ t − Δ t , c i t } which include indices of the same cluster.

As shown in Fig. 2 , for each cluster at time t 0 , we will apply a Kalman filter to it initially with a conservative motion model, similar to the work of Azim et al. [32] , resulting in a list of Kalman filters K t 0 = { κ 1 t 0 , κ 2 t 0 , … , κ l t 0 } . Then, in every later frame at time t i , we firstly propagate all Kalman filters in K t i − 1 forward and then associate centroids of clusters in the current frame to the forward propagated positions by k-Nearest Neighbor searching [33] . If the association of cluster c j t 0 successes, the cluster will inherit its corresponding tracking history. Otherwise, we suppose the unassociated cluster is a newly appeared object and create a new Kalman filter for it. To address the short-time occlusion issue, we mark the unassociated clusters lost and keep their Kalman filters running until they are tracked or lost for a threshold time t d , as shown in the right figure of Fig. 3 .

Moreover, it is essential that we attempt to associate current clusters to forward propagated Kalman filters rather than clusters in the previous frame like [1] . As shown in the right figure of Fig. 3 , when there are several moving agents, the association without considering motion information may be incorrect due to the movements of dynamic obstacles. Compared with it, we track clusters with forward propagated Kalman filters, efficiently avoiding plenty of incorrect associations.
Fig. 2. - Non-connected KFs in the previous frame will keep propagating until their corresponding cluster disappears for specific frames (i.e., the white ellipsoids in t1 and tk−1), while non-connected clusters in the current frame will be marked as new objects and begin their tracking (frame tk−1).
Fig. 2.

Non-connected KFs in the previous frame will keep propagating until their corresponding cluster disappears for specific frames (i.e., the white ellipsoids in t 1 and t k−1 ), while non-connected clusters in the current frame will be marked as new objects and begin their tracking (frame t k−1 ).

Show All
B. Classification as Dynamic or Static

We then classify the generated cluster as either static or dynamic. Inspired by [1] , we obtain the motion of clusters by comparing point clouds from two frames being δ seconds apart. Firstly, we build kdTree with the dense, non-filtered point cloud from the reference frame at time t − δ. Then we measure the global nearest neighbor distance d i,j of each point p i,j belonging to the cluster c i t in the current frame at time t by the kdTree built before. Then velocity of each point is calculated by v = d i , j δ and the points with v > v d vote for dynamic. After voting, a cluster will be considered dynamic obstacle if the absolute or relative amount of votes for being dynamic surpass respective thresholds l a b s d y n or l r e l d y n .

To guarantee that each point must be observed in both current frame and reference frame, there are two cases that should be noted to improve performance of classification. One is that when the FOV changes between two frames, we only allow points which appear in the overlapped area to vote. Another is the occlusion caused by dynamic objects. To address the issue, firstly we associate current cluster c i t to its corresponding cluster c j t − δ by the tracking history H t and project their 3D centroids onto the image plane. Self-occlusion is supposed to happen if d e p t h [ q j t − δ ] < d e p t h [ q j t ] and all points of c i t can vote, where q denotes the projected 2D point of 3D point p. Otherwise, for points p t ∈ ℝ 3 belonging to other clusters at time t , we firstly reproject them onto the image plane at time t-δ ,
[ q ^ t − δ 1 ] = T − 1 t − δ [ p t 1 ] , [ q t − δ 1 ] = K q ^ t − δ , (1) (2)
View Source Right-click on figure for MathML and additional features. \begin{align*} & \left[ {\begin{array}{c} {{{\hat q}_{t - \delta }}} \\ 1 \end{array}} \right] = T_{t - \delta }^{ - 1}\left[ {\begin{array}{c} {{p_t}} \\ 1 \end{array}} \right],\tag{1} \\ & \left[ {\begin{array}{c} {{q_{t - \delta }}} \\ 1 \end{array}} \right] = K{\hat q_{t - \delta }},\tag{2}\end{align*} where T t−δ is the transformation matrix of camera pose at time t − δ , K is the intrinsic matrix of the camera and q ^ t − δ is 3D point in the camera frame.

We then suppose point p t is occluded if | q ^ t − δ | Z − d e p t h [ q t − δ ] > ε , where |•| Z denotes the z-coordinate of a 3D point and the depth [ q t−δ ] and ϵ is set related to the velocity threshold v d to reduce sensor noise. Occluded points will be excluded from vote.

Finally, to improve the robustness of perception, it is essential to check the consistency of classification results over a time horizon t h . Namely, a cluster is judged as dynamic or static finally only if its classification results in all frames over t h keep the same.
Fig. 3. - The left figure shows our tracking algorithm could address the issue of occlusion. The right figure shows our method could avoid incorrect association, which would happen in the association without propagation.
Fig. 3.

The left figure shows our tracking algorithm could address the issue of occlusion. The right figure shows our method could avoid incorrect association, which would happen in the association without propagation.

Show All
Fig. 4. - Our previous work [2] search a collision-free path around static obstacles to generate gradients between control points and anchor points. However, these gradients may conflict with gradients from dynamic obstacle and make trajectory into local minimum. Thus, a path searching method considering dynamic objects is required.
Fig. 4.

Our previous work [2] search a collision-free path around static obstacles to generate gradients between control points and anchor points. However, these gradients may conflict with gradients from dynamic obstacle and make trajectory into local minimum. Thus, a path searching method considering dynamic objects is required.

Show All
C. Dynamic Environment Representation

In order to allow for autonomous flight of UAV in a dynamic environment, we represent the obstacles respectively according to their dynamic or not.

For dynamic agents, we model them as moving 3D ellipsoids with velocity v o :
( p − p o ) T Θ o ( p − p o ) = 1 , (3)
View Source Right-click on figure for MathML and additional features. \begin{equation*}{\left( {{\mathbf{p}} - {{\mathbf{p}}^o}} \right)^T}{\Theta ^o}\left( {{\mathbf{p}} - {{\mathbf{p}}^o}} \right) = 1,\tag{3}\end{equation*} where Θ o = R T d i a g ( 1 ( l x + r ) 2 ) , ( 1 ( l y + r ) 2 ) , ( 1 ( l z + r ) 2 ) R . p o ,l x ,l y ,l z ,r are estimated position and axis-length of ellipsoid and inflate radius.

For classified static point cloud, we fuse it into occupancy 3D grid map, which is efficient and easy to use for trajectory planning. Significantly, we introduce Re-Free strategy to eliminate the erroneously occupied space caused by the temporary standstill of moving objects and address the issue of time-lag in [1] caused by δ in classification. In detail, we fuse all points in current frame into the occupancy grid map initially. Furthermore, only if an agent is considered dynamic, we free all related occupancy grids according to its tracking history. In this way, the quadrotor may treat moving objects as static obstacles in the beginning and re-frame them after classification. Sufficient experiments prove that this implementation could significantly improve the robustness and safety of UAVs with limited FOV in obstacle avoidance missions. We finally represent the dynamic environment with moving ellipsoids and occupancy grids.
SECTION IV.
Dynamic Planning

In this section, we describe an ESDF-free gradient-based dynamic planning method based on our previous work [2] . In [2] , anchor points are constructed manually to generate gradients by searching a collision-free path around each colliding segment of the initial trajectory. However, this procedure cannot handle dynamic obstacles, which makes it get stuck into a local minimum easily, shown in Fig. 4 . In fact, most gradient-based planning methods like [3] would face the same problem if the initial trajectory clamps between two near obstacles.
Fig. 5. - An illustration of the dynamic obstacle-avoiding kinodynamic path searcher. The red curve indicates the motion primitives (full line means safe and dotted line means unsafe), while the yellow curve represents the analytic expansion scheme which speeds up searching [34]. Importantly, we predict the motion of dynamic obstacle and check that if the state at moment τ collides with predicted ellipsoid at the same time.
Fig. 5.

An illustration of the dynamic obstacle-avoiding kinodynamic path searcher. The red curve indicates the motion primitives (full line means safe and dotted line means unsafe), while the yellow curve represents the analytic expansion scheme which speeds up searching [34] . Importantly, we predict the motion of dynamic obstacle and check that if the state at moment τ collides with predicted ellipsoid at the same time.

Show All

To solve this problem, we obtain a dynamic obstacles-avoiding initial trajectory for subsequent optimization by a dynamic obstacle avoiding kinodynamic searching method.
A. Dynamic Obstacles-avoiding Kinodynamic Searching

Similar to [34] , our method generates motion primitives by sampling the control inputs u i and durations T i for each node. We define the cost of a trajectory as
J ( T ) = ∫ T 0 ∥ u ( τ ) ∥ 2 d τ + ρ T , (4)
View Source Right-click on figure for MathML and additional features. \begin{equation*}J\left( T \right) = \int_0^T {{{\left\| {u\left( \tau \right)} \right\|}^2}d\tau + \rho T} ,\tag{4}\end{equation*} where T is the total duration and ρ is a tunable weight. Heuristic cost is defined as J ( T h ), where T h is obtained by solving a closed form Optimal Boundary Value Problem.

Differently, the valid check of each motion primitive is modified by considering the collision check of the dynamic obstacles represented as ellipsoids, as shown in Fig. 5 . If the collision condition between the motion primitive at time τ and the predicted ellipsoid at the same time is satisfied, this motion primitive will be considered unsafe. The minimum distance between a point and an ellipsoid can not perform in closed form [35] . Thus, the collision condition is
C i j : = { ∥ ∥ p i − p o j ∥ ∥ Θ o ≤ 1 } , (5)
View Source Right-click on figure for MathML and additional features. \begin{equation*}{C_{ij}}: = \left\{ {{{\left\| {{{\mathbf{p}}_{\mathbf{i}}} - {\mathbf{p}}_{\mathbf{j}}^{\mathbf{o}}} \right\|}_{{\Theta ^o}}} \leq 1} \right\},\tag{5}\end{equation*} where p o j and Θ o are the estimated position and ellipsoid matrix in III-C. After searching for an initial trajectory that lies near the optimum, our subsequent B-spline based optimization will find that optimum.

B. B-Spline Trajectory Optimization

Based on our previous work [2] , the trajectory is parameterized by a uniform B-spline curve Φ, which is a piecewise polynomial uniquely determined by its degree p b , a knot span Δ t , and N c control points Q i ∈ ℝ 3 .

According to the property that the k th derivative of a B-spline is still a B-spline with degree p p,k = p b − k , the velocity, acceleration and jerk control points V i , A i and J i are calculated by
V i = Q i + 1 − Q i Δ t , A i = V i + 1 − V i Δ t , J i = A i + 1 − A i Δ t . (6)
View Source Right-click on figure for MathML and additional features. \begin{equation*}{{\mathbf{V}}_i} = \frac{{{{\mathbf{Q}}_{i + 1}} - {{\mathbf{Q}}_i}}}{{\Delta t}},{{\mathbf{A}}_i} = \frac{{{{\mathbf{V}}_{i + 1}} - {{\mathbf{V}}_i}}}{{\Delta t}},{{\mathbf{J}}_i} = \frac{{{{\mathbf{A}}_{i + 1}} - {{\mathbf{A}}_i}}}{{\Delta t}}.\tag{6}\end{equation*}

Fig. 6. - An illustration of a initial curve (dotted line) and the corresponding trajectory optimized by dynamic collision penalty (full line). If a control point collides with predicted ellipsoids, the gradient will push it out of moving obstacles.
Fig. 6.

An illustration of a initial curve (dotted line) and the corresponding trajectory optimized by dynamic collision penalty (full line). If a control point collides with predicted ellipsoids, the gradient will push it out of moving obstacles.

Show All

Then the optimization problem is then formulated as:
min Q J = J s + λ f J f + λ c J c + λ d J d , (7)
View Source Right-click on figure for MathML and additional features. \begin{equation*}\mathop {\min }\limits_{\mathbf{Q}} J = {J_s} + {\lambda _f}{J_f} + {\lambda _c}{J_c} + {\lambda _d}{J_d},\tag{7}\end{equation*} where J s is the smoothness cost defined by
J s = ∑ i = 1 N c − 2 ∥ A i ∥ 2 + ∑ i = 1 N c − 3 ∥ J i ∥ 2 , (8)
View Source Right-click on figure for MathML and additional features. \begin{equation*}{J_s} = \sum\limits_{i = 1}^{{N_c} - 2} {{{\left\| {{{\mathbf{A}}_i}} \right\|}^2} + \sum\limits_{i = 1}^{{N_c} - 3} {{{\left\| {{{\mathbf{J}}_i}} \right\|}^2}} } ,\tag{8}\end{equation*}

J f , J c , J d are penalty terms for the constraints of feasibility, avoiding the static and dynamic obstacles. λ f , λ c and λ d are weights for each penalty terms.

Thanks to the convex-hull property of B-spline, dynamic feasibility of the whole trajectory can be guaranteed sufficiently if we constrain all the velocity and acceleration control points:
J f = ∑ i = 1 N c − 1 g ( ∥ V i ∥ 2 − v 2 m ) + ∑ i = 1 N c − 3 g ( ∥ A i ∥ 2 − a 2 m ) , (9)
View Source Right-click on figure for MathML and additional features. \begin{equation*}{J_f} = \sum\limits_{i = 1}^{{N_c} - 1} {g\left( {{{\left\| {{{\mathbf{V}}_i}} \right\|}^2} - v_m^2} \right) + \sum\limits_{i = 1}^{{N_c} - 3} {g\left( {{{\left\| {{{\mathbf{A}}_i}} \right\|}^2} - a_m^2} \right)} } ,\tag{9}\end{equation*} where g ( x ) = max{ x ,0} 3 is a penalty function and v m ,a m are the limits on velocity and acceleration.

For static obstacles, we follow the method of our previous work [2] . We firstly search for a collision-free path around static obstacles. Then fixed anchor points and corresponding vectors are generated to determine the static collision penalty and gradients, shown in Fig. 4 . The penalty is
J c = ∑ i = 1 N c ∑ j = 1 N i g ( ( Q i − p i j ) ⋅ v i j ) , (10)
View Source Right-click on figure for MathML and additional features. \begin{equation*}{J_c} = \sum\limits_{i = 1}^{{N_c}} {\sum\limits_{j = 1}^{{N_i}} {g\left( {\left( {{{\mathbf{Q}}_i} - {{\mathbf{p}}_{ij}}} \right) \cdot {{\mathbf{v}}_{ij}}} \right)} } ,\tag{10}\end{equation*} where { p ij , v ij } is a pair of anchor point and vector, and N i is the number of { p ij , v ij } pairs recorded by the single control point Q i .

Fig. 7. - A result of optimization considering both static and dynamic obstacles in simulation. The blue line with blue dots is the path searched by dynamic obstacle-avoiding kinodynamic searching and the red line is the optimized trajectory with control points (cyan points). The yellow points are invalid motion primitives which collide with predicted moving obstacles.
Fig. 7.

A result of optimization considering both static and dynamic obstacles in simulation. The blue line with blue dots is the path searched by dynamic obstacle-avoiding kinodynamic searching and the red line is the optimized trajectory with control points (cyan points). The yellow points are invalid motion primitives which collide with predicted moving obstacles.

Show All

For dynamic obstacles,
J d = ∑ i = 1 N c ∑ j = 1 N d g ( 1 − x T i j Θ o x i j ) , (11)
View Source Right-click on figure for MathML and additional features. \begin{equation*}{J_d} = \sum\limits_{i = 1}^{{N_c}} {\sum\limits_{j = 1}^{{N_d}} {g\left( {1 - {\mathbf{x}}_{ij}^T{\Theta ^o}{{\mathbf{x}}_{ij}}} \right)} } ,\tag{11}\end{equation*} where x i j = Q i − p o j − i ⋅ Δ t ⋅ v o j , and { p o j , v o j , Θ o j } which represents each ellipsoid is obtained from III-C. A initial curve and its corresponding trajectory optimized by dynamic collision penalty are as shown in Fig. 6 .

We reparameterize the searching result obtained from IV-A to B-spline as the initial guess and utilize the L-BFGS [2] to minimize the cost function (7) . A simulated result trajectory of the optimization influenced by both static and dynamic obstacles is shown as Fig. 7 .
SECTION V.
Experiments and Benchmarks
1) Implementation Details

In dynamic perception module, we set the time span ξ = 0.3 s and the dynamic velocity threshold v nn = 0.2 m/s. For a cluster, the absolute and relative threshold of vote for being dynamic are set as l a b s d y n = 100 and l r e l d y n = 0.8 , and the time horizon is set as t h = 0.3 s to improve the robustness of classification.

In dynamic planning module, we set the order of B-spline as p b = 3. The time span Δ t of B-spline alters around 100ms, which is determined by the path length searched by our modified kinodynamic A* and the number of control points N c = 20. We use the same L-BFGS solver as [2] whose complexity is linear on the same relative tolerance. To further enforce safety, a collision checker for both static and dynamic obstacles with enough clearance is performed. The optimization stops only if no collision is detected. In real-world experiments, we adopt a similar quadrotor platform of [36] but use Intel RealSense D455 differently, which provides broader FOV.
Fig. 8. - The RGB-D images and the classification result. As the left figure shown, the moving person and left boxes are classified as dynamic and static and marked with red or blue points respectively, while the yellow point cloud is from the reference frame.
Fig. 8.

The RGB-D images and the classification result. As the left figure shown, the moving person and left boxes are classified as dynamic and static and marked with red or blue points respectively, while the yellow point cloud is from the reference frame.

Show All
2) Real-World Experiments

We present several experiments in unknown cluttered environments with limited camera FOV. In the first experiment, the drone flies through static objects, avoiding a man walking toward it simultaneously. As shown in Fig. 9 , the moving person is modeled as an ellipsoid with velocity, and static obstacles are presented as 3D occupancy grids. The person’s speed is around 1.6 m/s and the speed limit of UAV is set as 1.5 m/s. A collision is unavoidable if there is no dynamic perception and prediction. The optimized trajectory also meets the performance in the simulation like Fig. 7 .

Another experiment requires the UAV to navigate from a start point to an endpoint while avoiding a man walking across the flight course with higher speed and plenty of static obstacles. Fig. 10 shows a series of optimized trajectories in a dynamic environment and a snapshot of the flying UAV and the walking person. With a limited maximum perception range of depth sensor around 4m, the UAV classifies obstacles as dynamic or static, estimates the velocity, and builds a 3D grid map below 30ms, indicating that our system can run efficiently in real-time. The person walks around 2m/s, while the UAV achieves a speed of 2.5m/s in the cluster environment during the experiment.

In addition, we also conduct plenty of experiments to especially test our proposed system’s robustness. More details are available in the attached video.
3) Benchmark Comparisons

In this section, we both compare the dynamic perception module and dynamic planning module.

Firstly, we compare our dynamic perception module with [3] . We carry on both simulated and real-world experiments. In simulated tests, a 40×10×3 m environment is generated with static obstacles and moving agents. In real-world experiments, we record a dataset of which one human walks back and forth in a cluster environment. The experiment’s snapshot is shown in Fig. 8 . We then use both perception methods and compare results with ground truth measurements. The method adopted in [3] treats each obstacle as individual ellipsoids without dynamic classification, which results in that the positions and velocities of static obstacles will be frequently misestimated due to the sudden FOV change and occlusion caused by dynamic agents.
Fig. 9. - Top: the optimized trajectory between a moving person and static obstacles in narrow space. Bottom: the experiment scenario in which the static obstacles are masked with yellow and the dynamic one is masked with red.
Fig. 9.

Top: the optimized trajectory between a moving person and static obstacles in narrow space. Bottom: the experiment scenario in which the static obstacles are masked with yellow and the dynamic one is masked with red.

Show All

In each comparison above, we calculate the average estimation errors of position and velocity comparing with the ground truth measurements, and the results are shown in Tab.I . It can be observed that our method could handle more generic environments and perform more accurately.

Afterwards, for dynamic planning, we compare our work with [37] . In [37] , authors assume that locations and velocities of all obstacles are known and do not classify them as dynamic or static. Therefore, we only compare the dynamic planning module in a fully dynamic environment without static obstacles. Each planner runs for twenty times of different obstacle velocities and densities from the same start points to endpoints. In each simulated experiment, obstacles move with constant velocities and reverse when they arrive at the boundary of 20×20×3 m map. The average performance statistics and the computation time are shown in Table.II
Fig. 10. - A sequence of optimized trajectories and corresponding snapshot during the experiment. An UAV is required to fly from a start point to a goal, avoiding an obliquely walking person and a plenty of obstacles. Top: estimated position and velocity of the moving person and optimized trajectory. Bottom: Snapshot of the experiment.
Fig. 10.

A sequence of optimized trajectories and corresponding snapshot during the experiment. An UAV is required to fly from a start point to a goal, avoiding an obliquely walking person and a plenty of obstacles. Top: estimated position and velocity of the moving person and optimized trajectory. Bottom: Snapshot of the experiment.

Show All
TABLE I Dynamic Perception Comparison
Table I- Dynamic Perception Comparison

From Table.II , we conclude that the performance of our generated trajectory is comparable to [37] . However, our proposed planner needs a much lower computation budget. This is because the time complexity of our method is O ( N c ), since one control point only affects nearby segments thanks to the local support property of B-spline, while [37] plans the pose of a quadrotor with nonlinear model predictive control, which considers more constrains. Moreover, our method would not fall into a local minimum, while [37] would.
TABLE II Dynamic Planning Comparison
Table II- Dynamic Planning Comparison
SECTION VI.
Conclusion

In this paper, we introduce a robust vision-based system with onboard sensing for UAV in the dynamic environment and implement it on a computational power-limited quadrotor platform. Firstly we adopt an accurate yet efficient dynamic perception module, which classify obstacles as dynamic or static, obtain necessary state information of moving obstacles, and build a high-quality static occupancy grid map for navigation. Then, our proposed gradient-based planning framework generates local collision-free trajectories without maintaining ESDF by a carefully designed optimization in less than 7ms. Sufficient real-world experiments and benchmark comparisons validate that our system is effective, robust, and highly lightweight in unknown cluster environment.

Authors
Figures
References
Citations
Keywords
Metrics
Media
Footnotes
   Back to Results   
More Like This
Dynamic Collision Avoidance Path Planning for Mobile Robot Based on Multi-sensor Data Fusion by Support Vector Machine

2007 International Conference on Mechatronics and Automation

Published: 2007
Mobile robot vision odometer based on point- line features and graph optimization

2018 Chinese Control And Decision Conference (CCDC)

Published: 2018
Show More
References
1. T. Eppenberger, G. Cesari, M. Dymczyk, R. Siegwart and R. Dubé, Leveraging stereo-camera data for real-time dynamic obstacle detection and tracking , 2020.
Show in Context Google Scholar
2. X. Zhou, Z. Wang, H. Ye, C. Xu and F. Gao, "Ego-planner: An esdf-free gradient-based local planner for quadrotors", IEEE Robotics and Automation Letters , 2020.
Show in Context Google Scholar
3. J. Lin, H. Zhu and J. Alonso-Mora, "Robust vision-based obstacle avoidance for micro aerial vehicles in dynamic environments", 2020 IEEE International Conference on Robotics and Automation (ICRA). , pp. 2682-2688, 2020.
Show in Context View Article Full Text: PDF (2839) Google Scholar
4. H. Cheng, L. Lin, Z. Zheng, Y. Guan and Z. Liu, "An autonomous vision-based target tracking system for rotorcraft unmanned aerial vehicles", 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). , pp. 1732-1738, 2017.
Show in Context View Article Full Text: PDF (673) Google Scholar
5. C. Zhan, X. Duan, S. Xu, Z. Song and M. Luo, "An improved moving object detection algorithm based on frame difference and edge detection", Fourth international conference on image and graphics (ICIG 2007) , pp. 519-523, 2007.
Show in Context View Article Full Text: PDF (1308) Google Scholar
6. B. Xu, W. Li, D. Tzoumanikas, M. Bloesch, A. Davison and S. Leutenegger, "Mid-fusion: Octree-based object-level multi-instance dynamic slam", 2019 International Conference on Robotics and Automation (ICRA) , pp. 5231-5237, 2019.
Show in Context View Article Full Text: PDF (3589) Google Scholar
7. K. Berker Logoglu, H. Lezki, M. Kerim Yucel, A. Ozturk, A. Kucukkomurler, B. Karagoz, et al., "Feature-based efficient moving object detection for low-altitude aerial platforms", Proceedings of the IEEE International Conference on Computer Vision Workshops , pp. 2119-2128, 2017.
Show in Context Google Scholar
8. I. A. Bârsan, P. Liu, M. Pollefeys and A. Geiger, "Robust dense mapping for large-scale dynamic environments", 2018 IEEE International Conference on Robotics and Automation (ICRA). , pp. 7510-7517, 2018.
Show in Context View Article Full Text: PDF (1524) Google Scholar
9. W. Wu, Z. Wang, Z. Li, W. Liu and L. Fuxin, Pointpwc-net: A coarse-to-fine network for supervised and self-supervised scene flow estimation on 3d point clouds , 2019.
Show in Context Google Scholar
10. S. L. Francis, S. G. Anavatti and M. Garratt, "Detection of obstacles in the path planning module using differential scene flow technique", 2015 International Conference on Advanced Mechatronics Intelligent Manufacture and Industrial Automation (ICAMIMIA) , pp. 53-57, 2015.
Show in Context View Article Full Text: PDF (8245) Google Scholar
11. J. Redmon and A. Farhadi, Yolov3: An incremental improvement , 2018.
Show in Context Google Scholar
12. K. He, G. Gkioxari, P. Dollár and R. Girshick, "Mask r-cnn", Proceedings of the IEEE international conference on computer vision , pp. 2961-2969, 2017.
Show in Context View Article Full Text: PDF (3278) Google Scholar
13. S. Zhang, R. Benenson, M. Omran, J. Hosang and B. Schiele, "Towards reaching human performance in pedestrian detection", IEEE transactions on pattern analysis and machine intelligence , vol. 40, no. 4, pp. 973-986, 2017.
Show in Context View Article Full Text: PDF (2766) Google Scholar
14. O. H. Jafari, D. Mitzel and B. Leibe, "Real-time rgb-d based people detection and tracking for mobile robots and head-worn cameras", 2014 IEEE international conference on robotics and automation (ICRA). , pp. 5636-5643, 2014.
Show in Context View Article Full Text: PDF (2002) Google Scholar
15. R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, et al., "Kinectfusion: Real-time dense surface mapping and tracking", 2011 10th IEEE international symposium on mixed and augmented reality. , pp. 127-136, 2011.
Show in Context View Article Full Text: PDF (2769) Google Scholar
16. S. Bouaziz, A. Tagliasacchi and M. Pauly, "Sparse iterative closest point", Computer graphics forum , vol. 32, no. 5, pp. 113-123, 2013, [online] Available: .
Show in Context CrossRef Google Scholar
17. H. Oleynikova, D. Honegger and M. Pollefeys, "Reactive avoidance using embedded stereo vision for mav flight", 2015 IEEE International Conference on Robotics and Automation (ICRA). , pp. 50-56, 2015.
Show in Context View Article Full Text: PDF (847) Google Scholar
18. P. Skulimowski, M. Owczarek, A. Radecki, M. Bujacz, D. Rzeszotarski and P. Strumillo, "Interactive sonification of u-depth images in a navigation aid for the visually impaired", Journal on Multimodal User Interfaces , vol. 13, no. 3, pp. 219-230, 2019.
Show in Context CrossRef Google Scholar
19. F. Arnell and L. Petersson, "Fast object segmentation from a moving camera", IEEE Proceedings. Intelligent Vehicles Symposium 2005. , pp. 136-141, 2005.
Show in Context View Article Full Text: PDF (1069) Google Scholar
20. P. Fiorini and Z. Shiller, "Motion planning in dynamic environments using velocity obstacles", The International Journal of Robotics Research , vol. 17, no. 7, pp. 760-772, 1998.
Show in Context CrossRef Google Scholar
21. D. Wilkie, J. Van Den Berg and D. Manocha, "Generalized velocity obstacles", 2009 IEEE/RSJ International Conference on Intelligent Robots and Systems. , pp. 5573-5578, 2009.
Show in Context View Article Full Text: PDF (1455) Google Scholar
22. J. Van Den Berg, J. Snape, S. J. Guy and D. Manocha, "Reciprocal collision avoidance with acceleration-velocity obstacles", 2011 IEEE International Conference on Robotics and Automation. , pp. 3475-3482, 2011.
Show in Context View Article Full Text: PDF (997) Google Scholar
23. T. Fraichard and H. Asama, "Inevitable collision statesa step towards safer robots?", Advanced Robotics , vol. 18, no. 10, pp. 1001-1024, 2004.
Show in Context CrossRef Google Scholar
24. D. Althoff, J. J. Kuffner, D. Wollherr and M. Buss, "Safety assessment of robot trajectories for navigation in uncertain and dynamic environments", Autonomous Robots , vol. 32, no. 3, pp. 285-302, 2012.
Show in Context CrossRef Google Scholar
25. N. Malone, H.-T. Chiang, K. Lesser, M. Oishi and L. Tapia, "Hybrid dynamic moving obstacle avoidance using a stochastic reachable set-based potential field", IEEE Transactions on Robotics , vol. 33, no. 5, pp. 1124-1138, 2017.
Show in Context View Article Full Text: PDF (1228) Google Scholar
26. M. Seder and I. Petrovic, "Dynamic window based approach to mobile robot motion control in the presence of moving obstacles", Proceedings 2007 IEEE International Conference on Robotics and Automation , pp. 1986-1991, 2007.
Show in Context View Article Full Text: PDF (869) Google Scholar
27. H.-T. L. Chiang, B. HomChaudhuri, A. P. Vinod, M. Oishi and L. Tapia, "Dynamic risk tolerance: Motion planning by balancing short-term and long-term stochastic dynamic predictions", 2017 IEEE International Conference on Robotics and Automation (ICRA). , pp. 3762-3769, 2017.
Show in Context View Article Full Text: PDF (746) Google Scholar
28. F. Gao and S. Shen, "Quadrotor trajectory generation in dynamic environments using semi-definite relaxation on nonconvex qcqp", 2017 IEEE International Conference on Robotics and Automation (ICRA). , pp. 6354-6361, 2017.
Show in Context View Article Full Text: PDF (1583) Google Scholar
29. C. Cao, P. Trautman and S. Iba, "Dynamic channel: A planning framework for crowd navigation", 2019 International Conference on Robotics and Automation (ICRA) , pp. 5551-5557, 2019.
Show in Context View Article Full Text: PDF (5599) Google Scholar
30. D. Zhu, T. Zhou, J. Lin, Y. Fang and M. Q.-H. Meng, Online state-time trajectory planning using timed-esdf in highly dynamic environments , 2020.
Show in Context Google Scholar
31. K. Khan, S. U. Rehman, K. Aziz, S. Fong and S. Sarasvady, "Dbscan: Past present and future", The Fifth International Conference on the Applications of Digital Information and Web Technologies (ICADIWT 2014) , pp. 232-238, 2014.
Show in Context CrossRef Google Scholar
32. A. Azim and O. Aycard, "Detection classification and tracking of moving objects in a 3d environment", 2012 IEEE Intelligent Vehicles Symposium. , pp. 802-807, 2012.
Show in Context View Article Full Text: PDF (1761) Google Scholar
33. Y. Tian, L. Deng and Q. Li, "A knn match based tracking-learning-detection method with adjustment of surveyed areas", 2017 13th International Conference on Computational Intelligence and Security (CIS) , pp. 447-451, 2017.
Show in Context View Article Full Text: PDF (2227) Google Scholar
34. B. Zhou, F. Gao, L. Wang, C. Liu and S. Shen, "Robust and efficient quadrotor trajectory generation for fast autonomous flight", IEEE Robotics and Automation Letters , vol. 4, no. 4, pp. 3529-3536, 2019.
Show in Context View Article Full Text: PDF (3656) Google Scholar
35. A. Y. Uteshev and M. V. Goncharova, "Point-to-ellipse and point-to-ellipsoid distance equation analysis", Journal of computational and applied mathematics , vol. 328, pp. 232-251, 2018.
Show in Context CrossRef Google Scholar
36. F. Gao, L. Wang, B. Zhou, X. Zhou, J. Pan and S. Shen, "Teach-repeat-replan: A complete and robust system for aggressive flight in complex environments", IEEE Transactions on Robotics , vol. 36, no. 5, pp. 1526-1545, 2020.
Show in Context View Article Full Text: PDF (12873) Google Scholar
37. H. Zhu and J. Alonso-Mora, "Chance-constrained collision avoidance for mavs in dynamic environments", IEEE Robotics and Automation Letters , vol. 4, no. 2, pp. 776-783, 2019.
Show in Context View Article Full Text: PDF (1304) Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
