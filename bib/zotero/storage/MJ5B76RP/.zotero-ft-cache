IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Typesetting math: 86%

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Conferences > 2021 IEEE/RSJ International C...
Trajectory Generation in New Environments from Past Experiences
Publisher: IEEE
Cite This
PDF
Weiming Zhi ; Tin Lai ; Lionel Ott ; Fabio Ramos
All Authors
77
Full
Text Views

    Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Related Work
    III.
    Methodology
    IV.
    Experimental Results and Discussion
    V.
    Conclusion

Authors
Figures
References
Keywords
Metrics
Abstract:
Being able to safely operate for extended periods of time in dynamic environments is a critical capability for autonomous systems. This generally involves the prediction and understanding of motion patterns of dynamic entities, such as vehicles and people, in the surroundings. Many motion prediction methods in the literature implicitly account for environmental factors by learning on observed motion in a fixed environment, and are designed to make predictions in the same environment. In this paper, we address the problem of generating likely motion trajectories for novel environments, represented as occupancy grid maps, where motion has not been observed. We introduce the Occupancy-Conditional Trajectory Network (OTNet) framework, capable of transferring the previously observed motion patterns in known environments to new environments. OTNet provides a functional representation for motion trajectories and utilises neural networks to learn occupancy-conditional distributions over the function parameters. We empirically demonstrate our method’s ability to generate complex multi-modal trajectory patterns in both simulated and real-world environments.
Published in: 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Date of Conference: 27 Sept.-1 Oct. 2021
Date Added to IEEE Xplore : 16 December 2021
ISBN Information:
ISSN Information:
INSPEC Accession Number: 21487424
DOI: 10.1109/IROS51168.2021.9636231
Publisher: IEEE
Conference Location: Prague, Czech Republic
SECTION I.
Introduction

Understanding movement trends in dynamic environments is critical for autonomous agents, such as service robots and delivery vehicles, to achieve long-term autonomy. This need is highlighted by the increasing interest in developing mobile robots capable of coexisting and interacting safely and helpfully with humans. Anticipating likely motion trajectories allows autonomous agents to anticipate future movements of other agents and thus navigate safely as well as plan socially compliant motions by imitating observed trajectories.

Many methods predicting motion extrapolate partially observed trajectories, without incorporating knowledge of the environment. Example of such methods include constant acceleration [1] , filtering methods [2] , and auto-regressive models [3] . Advances in machine learning have lead to the development of methods which learn the motion behaviours from a dataset of trajectories. By training on motion data collected in the same environment, some learning-based methods can learn the general flow of movement [4] [5] – [6] , which implicitly accounts for environmental geometry. However, such methods are typically environment-specific, requiring predictions to be made in the same environment as that the training data was collected.

Through experience, humans have developed the ability to anticipate movement patterns based on the layout of the environment. We hypothesise that the structure of environments contains information about how objects move within the environment. By considering 2D environment floor plans, given as occupancy grid maps, it is possible to transfer motion patterns from training environments to new environments where no motion observations have been made. We propose a probabilistic generative model, Occupancy-Conditional Trajectory Network (OTNet), capable of generating motion trajectories for new unseen environments by generalising trajectories from previously observed environments, as shown in Figure 1 . We empirically demonstrate that our model is capable of generating motion trajectories in simulated and real-world environments by motion behaviour observed in other environments.
Fig. 1: - We use OTNet to transfer motion patterns from maps where motion has been observed to new environments where motion has not been observed, by conditioning on the occupancy map of the new environment.
Fig. 1:

We use OTNet to transfer motion patterns from maps where motion has been observed to new environments where motion has not been observed, by conditioning on the occupancy map of the new environment.

Show All

OTNet is a generative model which combines ideas from kernel methods and neural networks, with the following desirable properties:

    It generalises motion patterns observed in previous environments, to generate motion trajectories in a new environment, where no motion has been observed;

    It effectively models the probabilistic and multi-modal nature of motion generation. Individual trajectories can be generated from the model by sampling from it;

    It generates individual trajectories as continuous functions, allowing trajectories to be queried at arbitrary resolution. We have the ability to specify fixed start-points for generated trajectories.

SECTION II.
Related Work
A. Motion Trajectory Prediction

Estimating likely motion trajectories has been studied for a long time. Early simple methods to predict motion are often dynamics-based methods which extrapolate based on the laws of physics, such as constant velocity and constant acceleration models [1] , with more complex Dynamics-based methods are utilised in [7] , [8] . These models typically requires making a priori assumptions about agent motions, leaving little room to learn from observations. Other attempts at modelling motion trajectories include building dynamic occupancy grid maps based on occupancy data over time [9] [10] – [11] , inverse motion planning methods [12] , and hidden Markov methods [13] . Recent developments in machine learning have led to an interest in data-driven models [14] , which make fewer assumptions but rely heavily on observed data. A class of learning-based methods focus on learning a map of motion in a specific environment to implicitly capture map-aware motion patterns in the environment [4] [5] – [6] , [15] .
B. Trajectory Generation

Neural network based generative models have been used in motion prediction, where they typically generate complete trajectories conditioned on an incomplete one. Generative adversarial networks (GANs) [16] are a popular class of generative models that for trajectory generation [14] , [17] . Conditional Variational Autoencoders (CVAE) [18] are another class of generative models [19] , [20] that have been utilised in the same problems. Similar to this work, ideas for transferring motion patterns in specific transport scenarios are considered in [21] . Methods using neural networks to generate trajectories based on observed trajectories have also gained attention in the motion planning community. A recent work, Motion Planning Network (MPNet) [22] , aims to generate trajectories by learning from a dataset of optimal trajectories in various simulated environments.
SECTION III.
Methodology
A. Problem Formulation

This paper addresses the problem of generating motion trajectories in new environments, by transferring trajectories data observed in other training environments. Figure 1 gives a high level idea of the problem at hand: We have observed data in the form of motion trajectories and maps for training (left), and provided an occupancy map of a novel environment (upper right), we wish to generate likely motion patterns (lower right).

We assume to have a training dataset consisting of occupancy grid maps of diverse environments and a collection of trajectories observed within each map. We denote the dataset as \mathcal{D} = \left\{ {{\mathcal{M}_n},\left\{ {{\xi _p}} \right\}_{p = 1}^{{P_n}}} \right\}_{n = 1}^N , where there are N maps and corresponding sets of observed trajectories, {\mathcal{M}_n} is the n th occupancy map, and \left\{ {{\xi _p}} \right\}_{p = 1}^{{P_n}} is the set of P n trajectories collected in the corresponding environment. Note that the number of trajectories in each environment may differ.

We now have a new map, {\mathcal{M}^{\ast}} , where no trajectory observations have been made, and seek to generate new likely motion trajectories on {\mathcal{M}^{\ast}} , based on patterns we see in our training dataset. Key requirements of a solution to the problem include: (1) the ability to generate trajectories are diverse enough to capture multiple trajectory patterns; (2) the ability to generate trajectories that start from a specified coordinate.
Fig. 2: - Process of learning model to generate p(w|ϕ)
Fig. 2:

Process of learning model to generate p( w |ϕ )

Show All
Fig. 3: - Process of generating trajectories
Fig. 3:

Process of generating trajectories

Show All
B. Overview of OTNet

On a high level, our method encodes maps as similarities between themselves and a pre-defined collection of maps, and represents trajectories as parameters of a function. A mixture density network [23] is then used to predict distributions over function parameters, conditional on an encoded map.

The training process is illustrated in Figure 2 , and can be summarised as:

    Construct feature vectors of similarities, ϕ , from each training map. Intuitively, we similarly shaped floor-plans to have similar motion patterns. Our map encoding explicitly introduces the notion of map similarity to the learning process. Details in Section III-C .

    Represent trajectory data as vectors of weight parameters, w , of a function. This allows us to operate on on trajectory data sequences which contain differing numbers of waypoints. Each trajectory can typically be represented with much fewer parameters than waypoint coordinates. Details in Section III-D .

    We use a mixture density network (MDN) [23] to learn the distribution over weight parameters conditioned on the map feature vectors, p ( w | ϕ ). Details in Section III-E .

A brief overview of the generative process is illustrated in Figure 3 .

After the MDN has been trained, we construct a feature vector ϕ * of a new map {\mathcal{M}^{\ast}} , and query the MDN to obtain p ( w | ϕ * ). Vectors of w can be sampled from p ( w | ϕ * ), and each sample w can be used to generate a new trajectory. As there are no explicit constraints in the MDN to prevent trajectories from overlapping with occupied regions, and we can efficient sample check for collisions, we accept collision-free trajectories to output. If we are only interested in trajectories that start at a certain point, we can generate trajectories conditional on a specified start-point.
C. Encoding of Environmental Occupancy

We represent occupancy maps as a vector of similarities between the given environment and a representative database of other maps. Intuitively, we can think of this as operating in the space of maps , where we pin-point the map of interest by its relation with other maps, where maps similar to one another are closer together in the space of map. We expect similar maps to have similar motion trajectory patterns. Related ideas have been explored in the context of pseudo-inputs for sparse Gaussian process [24] .

The Hausdorff distance is a widely used distance measure to shapes and images [25] , and can be efficiently computed in linear time. The Hausdorff distance measures the distance between two finite sets of points, and allows us to make comparisons between our maps. We place the Hausdorff distance into a distance substitute kernel [26] , to obtain our similarity function.

Given two sets of points A = { a 1 , a 2 , … , a n } and B = { b 1 , b 2 , … , b m }, and in general n and m are not required to be equal, the one-sided Hausdorff distance between the two sets is defined as: \begin{equation*}{\hat \delta _H}(A,B) = \mathop {\max }\limits_{a \in A} \mathop {\min }\limits_{b \in B} \left\| {a - b} \right\|.\tag{1}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}{\hat \delta _H}(A,B) = \mathop {\max }\limits_{a \in A} \mathop {\min }\limits_{b \in B} \left\| {a - b} \right\|.\tag{1}\end{equation*}

The one-sided Hausdorff distance is not symmetric, we enforce symmetry by taking the average of {\hat \delta _H}(A,B) and {\hat \delta _H}(B,A) , i.e.: \begin{equation*}{\delta _H}(A,B) = \frac{1}{2}\left( {{{\hat \delta }_H}(A,B) + {{\hat \delta }_H}(B,A)} \right).\tag{2}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}{\delta _H}(A,B) = \frac{1}{2}\left( {{{\hat \delta }_H}(A,B) + {{\hat \delta }_H}(B,A)} \right).\tag{2}\end{equation*}

We can then define a similarity function between two sets A and B , analogous to a distance substitute kernel described in [26] , as: \begin{equation*}{S_H}(A,B) = \exp \left\{ { - \frac{{{\delta _H}{{\left( {A,B} \right)}^2}}}{{2{\ell _H}}}} \right\},\tag{3}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}{S_H}(A,B) = \exp \left\{ { - \frac{{{\delta _H}{{\left( {A,B} \right)}^2}}}{{2{\ell _H}}}} \right\},\tag{3}\end{equation*} where ℓ H is a length scale hyper-parameter.

We extract occupied edge points of binary grid maps and evaluate the similarity function between each map in the representative database. We can select a set of maps within our training data to be in the representative database, or when the number of maps is low, we can consider all the training maps to be in the database. A simple method of selecting the set of examples to be in a representative database are described in [4] . If we have N training maps and M representative maps in the database, feature vector for the n th map, ϕ n , is: \begin{equation*}\left[ {\begin{array}{c} {{\phi _1}} \\ \vdots \\ {{\phi _N}} \end{array}} \right] = \left[ {\begin{array}{ccc} {{S_H}\left( {{\mathcal{M}_1},{\mathcal{M}_1}} \right)}& \ldots &{{S_H}\left( {{\mathcal{M}_1},{\mathcal{M}_M}} \right)} \\ \vdots & \ddots & \vdots \\ {{S_H}\left( {{\mathcal{M}_N},{\mathcal{M}_1}} \right),}&{ \ldots ,}&{{S_H}\left( {{\mathcal{M}_N},{\mathcal{M}_M}} \right)} \end{array}} \right].\tag{4}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}\left[ {\begin{array}{c} {{\phi _1}} \\ \vdots \\ {{\phi _N}} \end{array}} \right] = \left[ {\begin{array}{ccc} {{S_H}\left( {{\mathcal{M}_1},{\mathcal{M}_1}} \right)}& \ldots &{{S_H}\left( {{\mathcal{M}_1},{\mathcal{M}_M}} \right)} \\ \vdots & \ddots & \vdots \\ {{S_H}\left( {{\mathcal{M}_N},{\mathcal{M}_1}} \right),}&{ \ldots ,}&{{S_H}\left( {{\mathcal{M}_N},{\mathcal{M}_M}} \right)} \end{array}} \right].\tag{4}\end{equation*}

For every map M in our dataset, there is a corresponding vector of similarities ϕ ∈ ℝ M . The m th element in the feature vector ϕ n denotes the similarity between the n th occupancy map in the training dataset, and the m th occupancy map in the database.
D. Continuous Representation of Trajectories

This section introduces the representation of trajectories in learning. Recorded trajectory data typically takes the form a sequence of discrete waypoints coordinates, whereas our generated trajectories are continuous functions. The continuous function representation allows for querying at arbitrary resolution without additional interpolation. We make the distinction between discrete and continuous trajectories:

    Discrete trajectories are represented by an arbitrary-length sequence of waypoint coordinates. We denote a discrete trajectory, ξ , with time steps 1 … T as, {\mathbf{\xi }} = \left\{ {\left( {{x_t},{y_t}} \right)} \right\}_{t = 1}^T , where ( x t , y t ) are x,y-coordinates of the dynamic object at time t.

    Continuous trajectories are smooth continuous functions that map from [0, 1] to coordinates. We define a continuous trajectory, Ξ, as Ξ( τ ) = ( x, y ), where τ ∈ [0, 1], is a normalised time parameter.

We embed discrete trajectories, which are potentially of varying length, as fixed length vectors, allowing us to operate on trajectories of arbitrary length. The vectors correspond to weights of fixed basis functions which reconstructs a continuous trajectory that best fits the discrete trajectory.

We define a normalised timestep parameter τ ∈ [0, 1]. A continuous trajectory can be modelled function, Ξ( τ ) = [ x ( τ ), y ( τ )], which maps τ to the x and y coordinates of the trajectory. We model x ( τ ) and y ( τ ) as weighted sums of fixed radial basis functions centred on evenly spaced τ values. Suppose we have a discrete trajectory {\mathbf{\xi }} = \left\{ {\left( {{x_t},{y_t}} \right)} \right\}_{t = 1}^T , the weights that best fit a given discrete trajectory can be found by solving a pair of Kernel Ridge Regression (KRR) problems, defined as: \begin{align*} & \arg \mathop {\min }\limits_{{{\mathbf{w}}_x}} \sum\limits_{t = 1}^T {{{\left( {{x_t} - {\mathbf{w}}_x^T{\mathbf{k}}\left( {{\tau _t}} \right)} \right)}^2}} + \lambda {\left\| {{{\mathbf{w}}_x}} \right\|^2},\tag{5} \\ & \arg \mathop {\min }\limits_{{{\mathbf{w}}_y}} \sum\limits_{t = 1}^T {{{\left( {{y_t} - {\mathbf{w}}_y^T{\mathbf{k}}\left( {{\tau _t}} \right)} \right)}^2}} + \lambda {\left\| {{{\mathbf{w}}_y}} \right\|^2},\tag{6}\end{align*}
View Source Right-click on figure for MathML and additional features. \begin{align*} & \arg \mathop {\min }\limits_{{{\mathbf{w}}_x}} \sum\limits_{t = 1}^T {{{\left( {{x_t} - {\mathbf{w}}_x^T{\mathbf{k}}\left( {{\tau _t}} \right)} \right)}^2}} + \lambda {\left\| {{{\mathbf{w}}_x}} \right\|^2},\tag{5} \\ & \arg \mathop {\min }\limits_{{{\mathbf{w}}_y}} \sum\limits_{t = 1}^T {{{\left( {{y_t} - {\mathbf{w}}_y^T{\mathbf{k}}\left( {{\tau _t}} \right)} \right)}^2}} + \lambda {\left\| {{{\mathbf{w}}_y}} \right\|^2},\tag{6}\end{align*} where {\tau _t} = \frac{t}{T} is the normalised time parameter, λ is the ridge regularisation parameter, k ( τ t ) contains the radial basis function values evaluated at τ t , obtained by: \begin{equation*}{\mathbf{k}}(\tau ) = k(\tau ,\hat \tau ) = {\left[ {k\left( {\tau ,{{\hat \tau }_1}} \right),k\left( {\tau ,{{\hat \tau }_2}} \right), \ldots ,k\left( {\tau ,{{\hat \tau }_M}} \right)} \right]^T},\tag{7}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}{\mathbf{k}}(\tau ) = k(\tau ,\hat \tau ) = {\left[ {k\left( {\tau ,{{\hat \tau }_1}} \right),k\left( {\tau ,{{\hat \tau }_2}} \right), \ldots ,k\left( {\tau ,{{\hat \tau }_M}} \right)} \right]^T},\tag{7}\end{equation*} where \hat \tau = \left[ {{{\hat \tau }_1},{{\hat \tau }_2}, \ldots ,{{\hat \tau }_M}} \right] is a vector of τ values where the stationary radial basis functions are centred. In this work, we use the squared exponential basis function, as it is smooth and the default in many kernel based methods. Hence, our basis function is defined by \begin{equation*}{\mathbf{k}}(\tau ) = {\left[ {\frac{{ - {{\left\| {\tau - {{\hat \tau }_1}} \right\|}^2}}}{{2{\ell _b}}},\frac{{ - {{\left\| {\tau - {{\hat \tau }_2}} \right\|}^2}}}{{2{\ell _b}}}, \ldots ,\frac{{ - {{\left\| {\tau - {{\hat \tau }_M}} \right\|}^2}}}{{2{\ell _b}}}} \right]^T},\tag{8}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}{\mathbf{k}}(\tau ) = {\left[ {\frac{{ - {{\left\| {\tau - {{\hat \tau }_1}} \right\|}^2}}}{{2{\ell _b}}},\frac{{ - {{\left\| {\tau - {{\hat \tau }_2}} \right\|}^2}}}{{2{\ell _b}}}, \ldots ,\frac{{ - {{\left\| {\tau - {{\hat \tau }_M}} \right\|}^2}}}{{2{\ell _b}}}} \right]^T},\tag{8}\end{equation*} where ℓ b is the length scale hyper-parameter of the isquared exponential function, and ║ • ║ denotes the L2-norm.

After evaluating Equation (8) to obtain k ( τ t ) for each τ t considered, we can solve the KRR Equation (5) , by computing: \begin{align*} & {{\mathbf{w}}_x} = {\left( {\lambda {\mathbf{I}} + \sum\limits_{t = 1}^T {{\mathbf{k}}{{\left( {{\tau _t}} \right)}^T}{\mathbf{k}}\left( {{\tau _t}} \right)} } \right)^{ - 1}}\left( {\sum\limits_{t = 1}^T {{x_t}{\mathbf{k}}\left( {{\tau _t}} \right)} } \right),\tag{9} \\ & {{\mathbf{w}}_y} = {\left( {\lambda {\mathbf{I}} + \sum\limits_{t = 1}^T {{\mathbf{k}}{{\left( {{\tau _t}} \right)}^T}{\mathbf{k}}\left( {{\tau _t}} \right)} } \right)^{ - 1}}\left( {\sum\limits_{t = 1}^T {{{\mathbf{y}}_t}{\mathbf{k}}\left( {{\tau _t}} \right)} } \right),\tag{10}\end{align*}
View Source Right-click on figure for MathML and additional features. \begin{align*} & {{\mathbf{w}}_x} = {\left( {\lambda {\mathbf{I}} + \sum\limits_{t = 1}^T {{\mathbf{k}}{{\left( {{\tau _t}} \right)}^T}{\mathbf{k}}\left( {{\tau _t}} \right)} } \right)^{ - 1}}\left( {\sum\limits_{t = 1}^T {{x_t}{\mathbf{k}}\left( {{\tau _t}} \right)} } \right),\tag{9} \\ & {{\mathbf{w}}_y} = {\left( {\lambda {\mathbf{I}} + \sum\limits_{t = 1}^T {{\mathbf{k}}{{\left( {{\tau _t}} \right)}^T}{\mathbf{k}}\left( {{\tau _t}} \right)} } \right)^{ - 1}}\left( {\sum\limits_{t = 1}^T {{{\mathbf{y}}_t}{\mathbf{k}}\left( {{\tau _t}} \right)} } \right),\tag{10}\end{align*} where I is an identity matrix. We denote the concatenation of w x and w y as {\mathbf{w}} = {\left[ {{{\mathbf{w}}_x},{{\mathbf{w}}_y}} \right]^T} \in {\mathbb{R}^{2M}} , where M is the number of basis functions.

Every discrete trajectory {\mathbf{\xi }} = \left\{ {\left( {{x_t},{y_t}} \right)} \right\}_{t = 1}^T can be converted to a corresponding vector of weight parameters w ∈ ℝ 2 M . Typically there are significantly fewer weight parameters than waypoint coordinates required to represent a trajectory. We can then query the trajectory coordinates at τ * by evaluating x\left( {{\tau ^{\ast}}} \right) = {\mathbf{w}}_x^T{\mathbf{k}}\left( {{\tau ^{\ast}}} \right) and y\left( {{\tau ^{\ast}}} \right) = {\mathbf{w}}_y^T{\mathbf{k}}\left( {{\tau ^{\ast}}} \right) .
E. Learning a Mixture of Stochastic Processes

Recall that from Section III-C , each occupancy representation is encoded as a vector of similarities ϕ , and from Section III-D , all the trajectories observed in the map are embedded as a collection of fixed length weight vectors, { w 1 , w 2 , … , w p }, where each w represents a trajectory. We aim to train a neural network to connect map representations and trajectory representations, and learn p ( w | ϕ ).

There may exist many distinct groupings of trajectories in each environment. The distribution of trajectories is expected to be multi-modal, and we need to use a model capable of predicting multi-modal conditional distributions over the weights, p ( w | ϕ ). Mixture density networks (MDN) [23] are a class of neural networks capable of representing conditional distributions. We slightly modify the classical MDN described in [23] to learn a mixture of vectors of conditional distributions, corresponding to the conditional distribution for each element in w. We model the conditional distribution p ( w | ϕ ) as a mixture of Q vectors of distributions, which we call mixture components. We make the mean-field Gaussian assumption [27] on the weights giving, \begin{equation*}p({\mathbf{w}}\mid \phi ) = \sum\limits_{q = 1}^Q {{\alpha _q}{p_q}({\mathbf{w}}\mid \phi )} = \sum\limits_{q = 1}^Q {{\alpha _q}\mathcal{N}\left( {{\mathbf{w}}\mid {{\mathbf{\mu }}_q},{\Sigma _q}} \right)} ,\tag{11}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}p({\mathbf{w}}\mid \phi ) = \sum\limits_{q = 1}^Q {{\alpha _q}{p_q}({\mathbf{w}}\mid \phi )} = \sum\limits_{q = 1}^Q {{\alpha _q}\mathcal{N}\left( {{\mathbf{w}}\mid {{\mathbf{\mu }}_q},{\Sigma _q}} \right)} ,\tag{11}\end{equation*} where p q ( w | ϕ ) denotes the q th component, and α q is the associated component weight. From the mean-field Gaussian assumption, each mixture component has mean vector, {{\mathbf{\mu }}_q} = {\left[ {{\mu _{q,1}},{\mu _{q,2}}, \ldots ,{\mu _{q,2M}}} \right]^T} , and diagonal covariance, \operatorname{diag} \left( {{\Sigma _q}} \right) = {\mathbf{\sigma }}_q^2 = {\left[ {\sigma _{q,1}^2,\sigma _{q,2}^2, \ldots ,\sigma _{q,2M}^2} \right]^T} . We can write each component of the conditional distribution as: \begin{equation*}{p_q}({\mathbf{w}}\mid \phi ) = \prod\limits_{m = 1}^{2M} {\frac{1}{{\sqrt {2\pi \sigma _{q,m}^2} }}\exp \left\{ { - \frac{{{{\left( {{w_m} - {\mu _{q,m}}} \right)}^2}}}{{2\sigma _{q,m}^2}}} \right\}} ,\tag{12}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}{p_q}({\mathbf{w}}\mid \phi ) = \prod\limits_{m = 1}^{2M} {\frac{1}{{\sqrt {2\pi \sigma _{q,m}^2} }}\exp \left\{ { - \frac{{{{\left( {{w_m} - {\mu _{q,m}}} \right)}^2}}}{{2\sigma _{q,m}^2}}} \right\}} ,\tag{12}\end{equation*} giving us the negative log-likelihood loss function over N maps, and P n trajectories observed in the environment corresponding to the n th map in the dataset as: \begin{equation*}\mathcal{L}({\mathbf{\theta }}) = - \log \left[ {\prod\limits_{n = 1}^N {\prod\limits_{p = 1}^{{P_n}} {\sum\limits_{q = 1}^Q {{\alpha _q}{p_q}({\mathbf{w}}\mid \phi )} } } } \right],\tag{13}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}\mathcal{L}({\mathbf{\theta }}) = - \log \left[ {\prod\limits_{n = 1}^N {\prod\limits_{p = 1}^{{P_n}} {\sum\limits_{q = 1}^Q {{\alpha _q}{p_q}({\mathbf{w}}\mid \phi )} } } } \right],\tag{13}\end{equation*} where we denote the set of parameters to optimise as \left\{ {{\alpha _q},{{\mathbf{\mu }}_q},{\Sigma _q}} \right\}_{q = 1}^Q . Using a neural network to minimise the loss function defined in Equation (13) , we can learn a model that maps from the feature vector of similarities ϕ to the parameters required to construct p ( w | ϕ ). The neural network is relatively simple with a sequence of fully-connected layers, the details of the architecture and parameters are shown in Figure 4 .

The standard MDN constraints are applied using the activation functions highlighted in [23] . This includes:

    \sum\nolimits_{q = 1}^Q {{\alpha _q} = 1} such that component weights sum up to one, by applying the softmax activation function on associated network outputs;

    σ q,m ≥ 0, by applying an exponential activation function on associated network outputs.

Fig. 4: - The neural network architecture to learn conditional distributions over trajectory weight parameters p(w|ϕ). The input is ϕ and the outputs are µ, σ, and α. Hidden layers with n hidden units are denoted by h1 hn. ReLu, exponential, and softmax activation functions, along with, dropout, batch normalisation layers are also used.
Fig. 4:

The neural network architecture to learn conditional distributions over trajectory weight parameters p ( w | ϕ ). The input is ϕ and the outputs are µ , σ , and α. Hidden layers with n hidden units are denoted by h 1 h n . ReLu, exponential, and softmax activation functions, along with, dropout, batch normalisation layers are also used.

Show All

As a distribution is estimated for each of the elements in weight vector, w , the predicted p ( w | ϕ ) results in a mixture of discrete processes, where each realisation is a vector of w. k denotes the basis functions outlined in Section III-D .
F. Trajectory Generation and Conditioning

After we complete the training of our MDN model, we can generate trajectories in environments with no observed trajectories. We generate the feature vector of similarities, ϕ * , from the map of interest, {\mathcal{M}^{\ast}} , and into the MDN. We obtain parameters, that define conditional distributions over w. Realisations of w can be sampled randomly from the predicted p ( w | ϕ * ), and a possible continuous trajectory, Ξ, can be found by evaluating {\mathbf{\Xi }}(\tau ) = [x(\tau ),y(\tau )] = \left[ {{\mathbf{w}}_x^\prime {\mathbf{k}}(\tau ),{\mathbf{w}}_y^\prime {\mathbf{k}}(\tau )} \right] , where k ( τ ) gives the basis function evaluations given in Section III-D , and {\left[ {{\mathbf{w}}_x^\prime ,{\mathbf{w}}_u^\prime } \right]^T} is a realisation of w. As there are no explicit constraints in the MDN to prevent the generation of trajectories which overlap with occupied regions, we apply collision checking. The trajectories can be generated and checked very efficiently, as it involves randomly sampling a mixture of Gaussian distributions and checking a map.

In order to predict how agents observed at a known position move, we are often interested in generating likely trajectories which begin at a certain start-point. To achieve this, let us consider the distribution of trajectories x, y -coordinates. Intuitively, rather than the distribution on weight parameters w , we want to consider the joint distribution between trajectory coordinates. We evaluate the continuous trajectories at a set of L times of interest, τ = [ τ 1 , τ 2 … τ L ], \begin{equation*}p({\mathbf{\Xi }}(\tau )) = p\left( {\left[ {\begin{array}{l} {{\mathbf{w}}_x^TK(\tau )} \\ {{\mathbf{w}}_y^TK(\tau )} \end{array}} \right]} \right) = \sum\limits_{q = 1}^Q {{\alpha _q}\left[ {\begin{array}{l} {\mathcal{N}\left( {{\mathbf{\hat \mu }}_q^x,\hat \Sigma _q^x} \right)} \\ {\mathcal{N}\left( {\widehat {\mathbf{\mu }}_q^y,\hat \Sigma _q^y} \right)} \end{array}} \right]} ,\tag{14}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}p({\mathbf{\Xi }}(\tau )) = p\left( {\left[ {\begin{array}{l} {{\mathbf{w}}_x^TK(\tau )} \\ {{\mathbf{w}}_y^TK(\tau )} \end{array}} \right]} \right) = \sum\limits_{q = 1}^Q {{\alpha _q}\left[ {\begin{array}{l} {\mathcal{N}\left( {{\mathbf{\hat \mu }}_q^x,\hat \Sigma _q^x} \right)} \\ {\mathcal{N}\left( {\widehat {\mathbf{\mu }}_q^y,\hat \Sigma _q^y} \right)} \end{array}} \right]} ,\tag{14}\end{equation*} where K ( τ ) = [ k ( τ 1 ), k ( τ 2 ), … , k ( τ L )] T is a matrix containing basis function evaluations at L time points of interest τ . We can now find mean and covariance parameters \left( {\hat \mu _q^x,\hat \mu _q^y,\hat \Sigma _q^x,\hat \Sigma _q^y} \right) for the trajectory coordinates at times of interest as, \begin{align*} & {\mathbf{\hat \mu }}_q^x = \mu _q^{xT}K(\tau ),\quad \hat \Sigma _q^x = K(\tau )\Sigma _q^xK{(\tau )^T},\tag{15} \\ & {\mathbf{\hat \mu }}_q^y = {\mathbf{\mu }}_q^{yT}K(\tau ),\quad \hat \Sigma _q^y = K(\tau )\Sigma _q^yK{\left( \tau \right)^T},\tag{16}\end{align*}
View Source Right-click on figure for MathML and additional features. \begin{align*} & {\mathbf{\hat \mu }}_q^x = \mu _q^{xT}K(\tau ),\quad \hat \Sigma _q^x = K(\tau )\Sigma _q^xK{(\tau )^T},\tag{15} \\ & {\mathbf{\hat \mu }}_q^y = {\mathbf{\mu }}_q^{yT}K(\tau ),\quad \hat \Sigma _q^y = K(\tau )\Sigma _q^yK{\left( \tau \right)^T},\tag{16}\end{align*}

The covariances of the q th component between the elements in weight parameter vectors w x and w y are denoted as \Sigma _q^x and \Sigma _q^y respectively. The component covariance Σ q between all the weights w = [ w x , w y ] and \Sigma _q^x,\Sigma _q^y are \operatorname{diag} \left( {{\Sigma _q}} \right) = \left[ {\operatorname{diag} \left( {\Sigma _q^x} \right),\operatorname{diag} \left( {\Sigma _q^y} \right)} \right] . Note that although Σ q is diagonal, the covariance of Gaussian components between times of interest \hat \Sigma _q^x,\hat \Sigma _q^y are typically dense.

For any component, we can then enforce locations that the trajectories must pass through ( x * , y * ) at τ i , by finding the distributions of coordinates the times of interest conditioned on the fixed point. Consider when we wish to condition on the start point, i.e. τ 1 = ( x * , y * ), a single component in the mixture can be written as: \begin{equation*}{p_q}\left( {{\mathbf{\Xi }}\left( {{\tau _{1:L}}} \right)\mid {\mathbf{\Xi }}\left( {{\tau _1}} \right) = \left[ {{x^{\ast}},{y^{\ast}}} \right]} \right) = {\alpha _q}\left[ {\begin{array}{l} {\mathcal{N}\left( {{{{\mathbf{\bar \mu }}}^x},{{\bar \Sigma }^x}} \right)} \\ {\mathcal{N}\left( {{{{\mathbf{\bar \mu }}}^y},{{\bar \Sigma }^y}} \right)} \end{array}} \right].\tag{17}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}{p_q}\left( {{\mathbf{\Xi }}\left( {{\tau _{1:L}}} \right)\mid {\mathbf{\Xi }}\left( {{\tau _1}} \right) = \left[ {{x^{\ast}},{y^{\ast}}} \right]} \right) = {\alpha _q}\left[ {\begin{array}{l} {\mathcal{N}\left( {{{{\mathbf{\bar \mu }}}^x},{{\bar \Sigma }^x}} \right)} \\ {\mathcal{N}\left( {{{{\mathbf{\bar \mu }}}^y},{{\bar \Sigma }^y}} \right)} \end{array}} \right].\tag{17}\end{equation*}

If we consider elements in mean vectors {{\mathbf{\hat \mu }}^x},{{\mathbf{\hat \mu }}^y} and covariance matrices {\hat \Sigma ^x}{\hat \Sigma ^y} of the unconditioned distribution as: \begin{equation*}{{\mathbf{\hat \mu }}^z} = \left[ {\begin{array}{c} {{\mathbf{\hat \mu }}_1^z} \\ {{\mathbf{\hat \mu }}_{2:L}^z} \end{array}} \right],\quad {\hat \Sigma ^z} = \left[ {\begin{array}{cc} {\hat \Sigma _{1,1}^z}&{\hat \Sigma _{1,2:L}^zT} \\ {\hat \Sigma _{1,2:L}^z}&{\hat \Sigma _{2:L,2:L}^z} \end{array}} \right],\quad {\text{ for }}z = \{ x,y\} ,\tag{18}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}{{\mathbf{\hat \mu }}^z} = \left[ {\begin{array}{c} {{\mathbf{\hat \mu }}_1^z} \\ {{\mathbf{\hat \mu }}_{2:L}^z} \end{array}} \right],\quad {\hat \Sigma ^z} = \left[ {\begin{array}{cc} {\hat \Sigma _{1,1}^z}&{\hat \Sigma _{1,2:L}^zT} \\ {\hat \Sigma _{1,2:L}^z}&{\hat \Sigma _{2:L,2:L}^z} \end{array}} \right],\quad {\text{ for }}z = \{ x,y\} ,\tag{18}\end{equation*} then for z = { x, y }, we can express the parameters of the conditional distribution as [27] : \begin{align*} & {{\mathbf{\bar \mu }}^{\mathbf{z}}} = {\mathbf{\hat \mu }}_{2:L}^z + \hat \Sigma _{1,2:L}^z\hat \Sigma _{1,1}^z - 1\left( {{z^{\ast}} - {{{\mathbf{\hat \mu }}}_1}} \right)\tag{19} \\ & {\bar \Sigma ^z} = \hat \Sigma _{2:L,2:L}^z - \hat \Sigma _{1,2:L}^z\hat \Sigma {_{1,1}^{z{ - 1}}}\hat {\Sigma _{{1,2:L}^{z T}}}.\tag{20}\end{align*}
View Source Right-click on figure for MathML and additional features. \begin{align*} & {{\mathbf{\bar \mu }}^{\mathbf{z}}} = {\mathbf{\hat \mu }}_{2:L}^z + \hat \Sigma _{1,2:L}^z\hat \Sigma _{1,1}^z - 1\left( {{z^{\ast}} - {{{\mathbf{\hat \mu }}}_1}} \right)\tag{19} \\ & {\bar \Sigma ^z} = \hat \Sigma _{2:L,2:L}^z - \hat \Sigma _{1,2:L}^z\hat \Sigma {_{1,1}^{z{ - 1}}}\hat {\Sigma _{{1,2:L}^{z T}}}.\tag{20}\end{align*}

We can condition each mixture component to start at a certain point, or employ strategies to only condition selected mixture components, such as the nearest component to the conditioned coordinate, and only generate trajectories belonging to the conditioned mixture component.
SECTION IV.
Experimental Results and Discussion
A. Dataset and Metrics

Training an OTNet requires a dataset containing occupancy maps of multiple environments along with observed trajectories in each environment. To the best of our knowledge, there exists no real-world dataset of sufficient size with different occupancy maps and trajectories observed in each of the different environments. Therefore, we conduct our experiments with our simulated dataset, Occ-Traj120 [28] . This dataset contains 120 binary occupancy grid maps of indoor environments with rooms and corridors, as well as simulated motion trajectories.
Fig. 5: - Occupancy maps in the Occ-Traj 120 dataset, along with associated trajectories. We aim to generalise trajectories to unseen maps.
Fig. 5:

Occupancy maps in the Occ-Traj 120 dataset, along with associated trajectories. We aim to generalise trajectories to unseen maps.

Show All
Fig. 6: - Generated likely motion trajectories (in red) in new environments. End points are indicated by scatter points. OT-Net captures the probabilistic, multi-modal nature of motion trajectories. The ground truth trajectories (in blue) are hidden during training.
Fig. 6:

Generated likely motion trajectories (in red) in new environments. End points are indicated by scatter points. OT-Net captures the probabilistic, multi-modal nature of motion trajectories. The ground truth trajectories (in blue) are hidden during training.

Show All

We aim to utilise our method to learn transfer the motion to new environments. The dataset is split with a 80-20 ratio for training and testing respectively. Our aim is to transfer the trajectory behaviour, from a training subset to unseen test environments.

We evaluate the generated trajectories against a test set with hidden ground truth trajectories. Continuous trajectories outputted are discretised for evaluation by querying at uniform intervals. Due to the probabilistic and multi-modal nature of our output, the metric used is minimum trajectory distance (MTD), and is defined by: MTD = {\min _{i = 1, \ldots ,P}}\mathcal{D}\left( {{\xi _{gen}},{\xi _i}} \right) , where P denotes the number of trajectories observed in the environment, with i indexing each trajectory, and \mathcal{D}\left( {{\xi _{gen}},{\xi _i}} \right) is a distance measure of trajectory distance between the generated ξ gen and a ground truth trajectory ξ i . In our evaluations the Euclidean Hausdoff distance and discrete Frechét distance are considered. These trajectory distances are commonly used in distance-based trajectory clustering to quantify the dissimilarity between trajectories, and a review of these distances can be found in [29] . We also wish to evaluate the quality of the uncertainty captured. To this end, we calculate the average negative log-likelihood (ANLL) between the predicted distribution at 100 uniform time-steps and each ground truth trajectory. When only samples of generated trajectories are available, we fit Gaussian distributions over world-space coordinates at uniform time-coordinates. ANLL can take into account of probabilistic multi-modal distributions, and a relative lower ANLL indicates better performance.
B. Experimental Setup

We compare our proposed method, OTNet, with neural-network based generative models, a learning-based motion reconstruction method, and a k-nearest neighbour (k-NN) based method. The details of these models are as follows:
1) OTNet:

We train OTNet with the length-scale hyper-parameters ℓ H = 50 and ℓ b = 5 for 20 epochs. The number of bases are set to be M = 15, and the number of mixture components, Q = 4. These hyper-parameters are hand-picked and fairly not sensitive, with values in the same ball-park giving similar results. Cross-validation could be applied for further improvements. As the number of map examples in the training set is relatively small at 96, we select all our training maps to be in our database of maps, when encoding maps of interest as feature vectors. In each of our experiments, trajectories can be generated efficient during test time. On a standard desktop machine, predicting the distribution of trajectories can be done in under one second, negligible time is required to randomly sample the distribution to generate trajectories.
2) Generative network models:

Our proposed method is generative, we evaluate two popular generative models: GANs [16] CVAEs [18] , trained for 300 epochs to generate trajectory parameters w. The discriminator uses convolutional layers, and the generator samples a 100-dimensional latent {\mathbf{\hat z}} , concatenated with the map for conditioning. Five dense layers are used to output w. The hyperparameters of the CVAE model, are identical to the GAN model.
3) Learning-based motion reconstruction:

We evaluate the performance of the learning-based MPNet [22] , which conditions on the environment and imitates training trajectories. We pre-train on MPNet’s public 2D dataset with further training with the Occ-Traj120 dataset for 300 epochs. MPNet requires trajectory start and end coordinates, during evaluation we provide ground truth mean start and end points of groups of trajectories. To guarantee valid trajectories, we use the hybrid MPNet-RRT variant suggested by the authors. We emphasise that MPNet can only generate a single prediction, and cannot generate distributions of trajectories.
4) Nearest-neighbour:

We also consider a baseline k-NN based approach. Using the symmetric Hausdoff distance, we find the nearest map training map, and transfer the trajectories directly to the queried map. As we cannot average trajectories, we use k = 1. Note storage of not only all maps, but also trajectories are required.
C. Comparisons to Other Generative Models
TABLE I: Performance of OTNet and comparisons models. Lower values are better. OTNet outperforms the comparisons, although the performance of k-NNs are strong, k-NNs lack the ability to generate new trajectories that do not exist in the provided dataset.
Table I:- Performance of OTNet and comparisons models. Lower values are better. OTNet outperforms the comparisons, although the performance of k-NNs are strong, k-NNs lack the ability to generate new trajectories that do not exist in the provided dataset.

The performance results of our experiments are tabulated in Table I , we see that OTNet outperform the other generative models compared. The ANLL for CVAE and GANs suffer from a loss of precision due to an extremely low likelihood, resulting in taking log of a near 0 value, the ANLL will be much higher than OTNet. The ANLL for MPNet-RRT is also not applicable, as it generates single trajectories. The map representations are of too high dimensions to directly train neural networks with a limited dataset. In particular, the encoding of each occupancy map as a feature vector of similarities, ϕ , allows for flexible representations even when the number of maps in the dataset is small. Comparatively, other neural network-based models which attempt to directly extract map features using convolutional neural networks are unable to successfully condition on the occupancy maps. The nearest neighbour model, which transfers the trajectories of the nearest map, performs surprising well, comparable to the OTNet on the simulated on the Haussdoff and Frechét measures. This is due to OTNet often containing several relatively similar-looking maps. However, OTNet is able to take into consideration several maps, and weigh each by their similarity with the map of interest, while the nearest neighbour only considers the closest map. We also note that the single nearest neighbour approach often results in relative over-confidence of biased predictions, hence the higher ANLL relative to OTNet. We note that we can directly obtain closed form equations of trajectory distributions from OTNet, allowing us to repeatedly generate new trajectories. Whereas, MPNet-RRT produces single predictions. Although using k-NN can give trajectories with competitive performance, it cannot generate new trajectories, whereas OTNet is capable of generating trajectories that have not been observed in the original dataset. Examples of trajectories generated by OTNet are shown in red in Figure 6 , along with ground truth trajectories (blue).
D. Diverse Trajectories and Conditioning Trajectory Start Points

We also want to further investigate whether generated trajectories capture different groups of motion, and whether we can specify start-points for generated trajectories. Figure 8 shows generated trajectories in an unseen test environment, along with plots of the trajectory coordinates wrt the normalised time parameter, τ. The test environment is an indoor environment of a corridor with a connected room. Predicted trajectories are coloured in red, and hidden ground truth trajectories are under-laid in blue. The end coordinates of each trajectory have an attached marker. We clearly observe that the trajectories generated can belong to different groupings – one group starts from inside the room and exit into the corridor, while the other starts in the corridor and end in the room. The multi-modality of the distributions of trajectories learned is even more clear when observing the plots of coordinates against τ. If we observe the y-coordinates, y ( τ ), we clearly see two groupings of functions, one of which is at around x (0) = 15 and around y (1) = 32, the other grouping has values around x (0) = 32 and y (1) = 15.

In the same environment, we attempt to condition the generated trajectories to different start-points. The generated trajectories (top), as well as trajectory coordinates x ( τ ) (middle) and y ( τ ) (bottom) of the trajectories, are shown in Figure 9 . The generated trajectories are in red, with red marker endpoints, and black start-points. In the left and centre plots, we condition the trajectories’ start point on two coordinates in the room, ( x (0) = 5, y (0) = 20) (left) and ( x (0) = 5, y (0) = 5) (centre), as well conditioning on a start-point in the corridor, ( x (0) = 2, y (0) = 35). We see that in all three cases, OTNet was able to generate reasonable trajectories, which follow the environment structure, with each starting at their designated start coordinates.
Fig. 7: - After training on a simulated dataset, we condition on sections of real world occupancy data, from the Intel-Labs dataset [30], to generate motion trajectories (top two rows, green). The start point of the trajectories are fixed, with the end-points illustrated with magenta markers. We can compare this to trajectories from a baseline nearest neighbour model (bottom row, blue), which transfers trajectories from the most similar training map, and is unable to adapt to the new environments.
Fig. 7:

After training on a simulated dataset, we condition on sections of real world occupancy data, from the Intel-Labs dataset [30] , to generate motion trajectories (top two rows, green). The start point of the trajectories are fixed, with the end-points illustrated with magenta markers. We can compare this to trajectories from a baseline nearest neighbour model (bottom row, blue), which transfers trajectories from the most similar training map, and is unable to adapt to the new environments.

Show All
Fig. 8: - (Left) Generated trajectories (red) overlaid on the corridor and room test map, with markers indicating the endpoints. The hidden ground truth trajectories are under-laid in blue. We see two different groups of trajectories: one starts from inside the room and end in the corridor, the other moves in the opposite direction. (Right) Plots of coordinates wrt τ, x(τ) and y(τ) (top, bottom respectively), corresponding to the trajectories generated on the left plot. The ability to generate distinct groups of trajectories is clear.
Fig. 8:

(Left) Generated trajectories (red) overlaid on the corridor and room test map, with markers indicating the endpoints. The hidden ground truth trajectories are under-laid in blue. We see two different groups of trajectories: one starts from inside the room and end in the corridor, the other moves in the opposite direction. (Right) Plots of coordinates wrt τ , x ( τ ) and y ( τ ) (top, bottom respectively), corresponding to the trajectories generated on the left plot. The ability to generate distinct groups of trajectories is clear.

Show All
Fig. 9: - We generate trajectories (in red, with red markers as endpoints) conditional on the start points (black marker). We condition on two start points inside the room (left, centre), and one point in the corridor (right). The corresponding plots of x, y-coordinates wrt to τ, x(τ) and y(τ) (top, bottom respectively). We see that the trajectories can be conditioned on a variety of start points.
Fig. 9:

We generate trajectories (in red, with red markers as endpoints) conditional on the start points (black marker). We condition on two start points inside the room (left, centre), and one point in the corridor (right). The corresponding plots of x , y -coordinates wrt to τ , x ( τ ) and y ( τ ) (top, bottom respectively). We see that the trajectories can be conditioned on a variety of start points.

Show All
E. Transferring Trajectories in Simulated Environments to Real-world Data

We also investigate the transfer of trajectories from simulated maps to real-world environments. We select three different regions in the Intel-lab dataset [30] , and transfer trajectory patterns trained on the simulated Occ-Traj dataset. Figure 7 shows trajectories (top two rows, green with magenta end-points) generated on three different sub-maps, and conditioned on two different starting locations. We see that OT-Net is able to generate multi-modal distributions of trajectories, with distinct groupings, and largely conforms to the environmental geometry, even though the real-world occupancy differs from the simulated training environments significantly. We visually compare trajectories generated by OTNet to those by the 1-nearest neighbour approach (bottom row, blue with magenta end-points), which transfers the trajectories from the most similar map to the queried map. Qualitatively, we evaluate the trajectories generated by OTNet to conform better to the real-world environment compared to the nearest-neighbour trajectories. The 1-nearest neighbour approach can only draw from the most similar map. We note that the 1-nearest neighbour approach performs well when transferring to the simulated dataset, as it could often find a relatively similar map that has been observed. Whereas, the real-world occupancy map is not very similar to any single map we have experienced, but OTNet can generalise trajectories from a combination of maps which resembles the test environment closer. Hence, OT-Net is more capable of generalising to unseen environments. The nearest neighbour approach is also unable to immediately generate similar new trajectories beyond those included in the data, nor is it possible to condition predicted trajectories on a start-point. Both generating similar new trajectories, and conditioning on points can be achieved with OTNet.
SECTION V.
Conclusion

We present a novel generative model, OTNet, capable of producing likely motion trajectories in new environments where no motion has been observed. We generalise observed motion trajectories in alternative training environments. The OTNet encodes maps as a feature vector of similarities, and embeds observed trajectories as function parameters. A neural network is used to learn conditional distributions over the parameters. Realisations of the vectors can then be sampled from the conditional distribution, and used to reconstruct generated trajectories. We empirically show the strong performance of OTNet against popular generative methods. Future improvements on OTNet include incorporating temporal changes in trajectory patterns into the framework.

Authors
Figures
References
Keywords
Metrics
   Back to Results   
More Like This
Application of Artificial Neural Network Based on Q-learning for Mobile Robot Path Planning

2006 IEEE International Conference on Information Acquisition

Published: 2006
Mobile Robot Path Planning in Dynamic Environments Through Globally Guided Reinforcement Learning

IEEE Robotics and Automation Letters

Published: 2020
Show More
References
1. X. Rong Li and V. P. Jilkov, "Survey of maneuvering target tracking. part i. dynamic models", IEEE Transactions on Aerospace and Electronic Systems , 2003.
Show in Context Google Scholar
2. Y. Zhang, Z. Zhai, X. Nie, C. Ma and F. Zuo, "An extended self-adaptive kalman filtering object motion prediction model", International Conference on Intelligent Information Hiding and Multimedia Signal Processing , 2008.
Show in Context View Article Full Text: PDF (228) Google Scholar
3. A. Agarwal and B. Triggs, "Tracking articulated motion using a mixture of autoregressive models", ECCV , 2004.
Show in Context CrossRef Google Scholar
4. W. Zhi, R. Senanayake, L. Ott and F. Ramos, "Spatiotemporal learning of directional uncertainty in urban environments with kernel recurrent mixture density networks", IEEE Robotics and Automation Letters , 2019.
Show in Context View Article Full Text: PDF (2402) Google Scholar
5. T. P. Kucner, M. Magnusson, E. Schaffernicht, V. H. Bennetts and A. J. Lilienthal, "Enabling flow awareness for mobile robots in partially observable environments", IEEE Robotics and Automation Letters , 2017.
Show in Context View Article Full Text: PDF (1967) Google Scholar
6. R. Senanayake and F. Ramos, "Directional grid maps: modeling multi-modal angular uncertainty in dynamic environments", IEEE/RSJ International Conference on Intelligent Robots and Systems , 2018.
Show in Context Google Scholar
7. S. Zernetsch, S. Kohnen, M. Goldhammer, K. Doll and B. Sick, "Trajectory prediction of cyclists using a physical model and an artificial neural network", IEEE Intelligent Vehicles Symposium (IV) , 2016.
Show in Context View Article Full Text: PDF (746) Google Scholar
8. J. F. Kooij, F. Flohr, E. A. Pool and D. M. Gavrila, "Context-based path prediction for targets with switching dynamics", Int. J. Comput. Vision , 2019.
Show in Context CrossRef Google Scholar
9. D. Arbuckle, A. Howard and M. Mataric, "Temporal occupancy grids: a method for classifying the spatio-temporal properties of the environment", IEEE/RSJ International Conference on Intelligent Robots and Systems , 2002.
Show in Context View Article Full Text: PDF (461) Google Scholar
10. N. C. Mitsou and C. Tzafestas, "Temporal occupancy grid for mobile robot dynamic environment mapping", 2007.
Show in Context View Article Full Text: PDF (1271) Google Scholar
11. G. Tanzmeister, J. Thomas, D. Wollherr and M. Buss, "Grid-based mapping and tracking in dynamic environments using a uniform evidential environment representation", IEEE International Conference on Robotics and Automation , 2014.
Show in Context View Article Full Text: PDF (1185) Google Scholar
12. B. D. Ziebart, N. D. Ratliff, G. Gallagher, C. Mertz, K. M. Peterson, J. A. Bagnell, et al., "Planning-based prediction for pedestrians", IEEE/RSJ International Conference on Intelligent Robots and Systems , 2009.
Show in Context View Article Full Text: PDF (1580) Google Scholar
13. D. Vasquez, T. Fraichard and C. Laugier, "Growing hidden markov models: An incremental tool for learning and predicting human and vehicle motion", The International Journal of Robotics Research , 2009.
Show in Context CrossRef Google Scholar
14. A. Gupta, J. E. Johnson, F. Li, S. Savarese and A. Alahi, "Social gan: Socially acceptable trajectories with generative adversarial networks", IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2018.
Show in Context View Article Full Text: PDF (1213) Google Scholar
15. S. M. Mellado, G. Cielniak, T. Krajník and T. Duckett, "Modelling and predicting rhythmic flow patterns in dynamic environments", TAROS , 2018.
Show in Context Google Scholar
16. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, et al., "Generative adversarial nets", Advances in Neural Information Processing Systems , 2014.
Show in Context Google Scholar
17. J. Li, H. Ma and M. Tomizuka, "Interaction-aware multi-agent tracking and probabilistic behavior prediction via adversarial learning", International Conference on Robotics and Automation (ICRA) , 2019.
Show in Context View Article Full Text: PDF (1602) Google Scholar
18. K. Sohn, H. Lee and X. Yan, "Learning structured output representation using deep conditional generative models", Advances in neural information processing systems , pp. 3483-3491, 2015.
Show in Context Google Scholar
19. B. Ivanovic, E. Schmerling, K. Leung and M. Pavone, "Generative modeling of multimodal multi-human behavior", IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2018.
Show in Context View Article Full Text: PDF (2825) Google Scholar
20. B. Ivanovic and M. Pavone, "The trajectron: Probabilistic multi-agent trajectory modeling with dynamic spatiotemporal graphs", The IEEE International Conference on Computer Vision (ICCV) , 2019.
Show in Context View Article Full Text: PDF (2219) Google Scholar
21. N. Jaipuria, G. Habibi and J. P. How, "A transferable pedestrian motion prediction model for intersections with different geometries", 2018.
Show in Context Google Scholar
22. A. H. Qureshi, A. Simeonov, M. J. Bency and M. C. Yip, "Motion planning networks", International Conference on Robotics and Automation (ICRA) , 2019.
Show in Context View Article Full Text: PDF (280) Google Scholar
23. C. M. Bishop, "Mixture density networks", tech. rep. Aston University , 1994.
Show in Context Google Scholar
24. E. Snelson and Z. Ghahramani, "Sparse gaussian processes using pseudo-inputs", Advances in Neural Information Processing Systems , 2006.
Show in Context Google Scholar
25. D. P. Huttenlocher, G. A. Klanderman and W. J. Rucklidge, "Comparing images using the hausdorff distance", IEEE Transactions on Pattern Analysis and Machine Intelligence , 1993.
Show in Context View Article Full Text: PDF (1501) Google Scholar
26. B. Haasdonk and C. Bahlmann, "Learning with distance substitution kernels", DAGM-Symposium Lecture Notes in Computer Science , 2004.
Show in Context CrossRef Google Scholar
27. C. M. Bishop, Pattern Recognition and Machine Learning. , 2006.
Show in Context Google Scholar
28. T. Lai, W. Zhi and F. Ramos, "Occ-traj120: Occupancy maps with associated trajectories", CoRR , 2019.
Show in Context Google Scholar
29. P. C. Besse, B. Guillouet, J. Loubes and F. Royer, "Review and perspective for distance-based clustering of vehicle trajectories", IEEE Transactions on Intelligent Transportation Systems , 2016.
Show in Context View Article Full Text: PDF (2012) Google Scholar
30. D. Hähnel, W. Burgard, D. Fox and S. Thrun, "An efficient fastslam algorithm for generating maps of large-scale cyclic environments from raw laser range measurements", IEEE/RSJ International Conference on Intelligent Robots and Systems , 2003.
Show in Context Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
