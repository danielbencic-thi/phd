2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Macau, China, November 4-8, 2019

FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality
Winter Guerra, Ezra Tal, Varun Murali, Gilhyun Ryou and Sertac Karaman

Abstract— FlightGoggles is a photorealistic sensor simulator for perception-driven robotic vehicles. The key contributions of FlightGoggles are twofold. First, FlightGoggles provides photorealistic exteroceptive sensor simulation using graphics assets generated with photogrammetry. Second, it provides the ability to combine (i) synthetic exteroceptive measurements generated in silico in real time and (ii) vehicle dynamics and proprioceptive measurements generated in motio by vehicle(s) in ﬂight in a motion-capture facility. FlightGoggles is capable of simulating a virtual-reality environment around autonomous vehicle(s) in ﬂight. While a vehicle is in ﬂight in the FlightGoggles virtual reality environment, exteroceptive sensors are rendered synthetically in real time while all complex dynamics are generated organically through natural interactions of the vehicle. The FlightGoggles framework allows for researchers to accelerate development by circumventing the need to estimate complex and hard-to-model interactions such as aerodynamics, motor mechanics, battery electrochemistry, and behavior of other agents. The ability to perform vehicle-in-the-loop experiments with photorealistic exteroceptive sensor simulation facilitates novel research directions involving, e.g., fast and agile autonomous ﬂight in obstacle-rich environments, safe human interaction, and ﬂexible sensor selection. FlightGoggles has been utilized as the main test for selecting nine teams that will advance in the AlphaPilot autonomous drone racing challenge. We survey approaches and results from the top AlphaPilot teams, which may be of independent interest. FlightGoggles is distributed as open-source software along with the photorealistic graphics assets for several simulation environments, under the MIT license at http://flightgoggles.mit.edu.
I. INTRODUCTION
Simulation systems have long been an integral part of the development of robotic vehicles. They allow engineers to identify errors early on in the development process, and allow researchers to rapidly prototype and demonstrate their ideas. Despite their success in accelerating development, many researchers view results generated in simulation systems with skepticism, as all simulation systems are abstractions of reality and will disagree with reality at some scale. This skepticism towards results generated exclusively in simulation studies is exempliﬁed by Rodney Brooks’ well-known quote from 1993: “[experiment] simulations are doomed to succeed ... [because] simulations cannot be made sufﬁciently realistic” [1].
Despite the skepticism towards simulation results, several trends have emerged in recent years that have driven the
All authors are with the Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of Technology. {winterg, eatal, mvarun, ghryou, sertac}@mit.edu
This work was supported in part by the Ofﬁce of Naval Research (ONR) YIP program, the Army Research Laboratory (ARL) DCIST project, and the MIT Lincoln Laboratory.

Fig. 1: FlightGoggles renderings of the Abandoned Factory environment, designed for autonomous drone racing. Note the size of the environment and the high level of detail.
research community to develop better simulation systems out of necessity. A major driving trend towards realistic simulators stems from the emergence of data-driven algorithmic methods in robotics, for instance, based on machine learning methods that require extensive data. Simulation systems provide not only massive amounts of data but also the labels required for training algorithms. For example, simulation may provide an efﬁcient and safe environment for reinforcement learning [2], [3]. This driving trend has posed a critical need to develop better, more realistic simulation systems.
Several enabling trends have also recently emerged that allow for better, more-realistic simulation systems to be developed. The ﬁrst enabling trend is the development of new computing resources that enable realistic rendering. The rapid evolution of game engine technology, particularly 3D graphics rendering engines, has made available advanced features such as improved material characteristics, real-time reﬂections, volumetric lighting, and advanced illumination through deferred and raytraced rendering pipelines. Particularly, the maturation of off-the-shelf software packages such as Unreal Engine [4] and Unity [5], makes them suitable for high-ﬁdelity rendering in applications beyond video games, such as robotics simulation. Simultaneously, next-generation graphics processors simply pack more transistors, and the transistors are better organized for rendering purposes, e.g., for real-time ray tracing. In addition, they incorporate computation cores that utilize machine learning, for instance, trained with pictures of real environments to generate realistic renderings. This trend is an opportunity to utilize better software and hardware to realize realistic sensor

978-1-7281-4004-9/19/$31.00 ©2019 IEEE

6941

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:44:52 UTC from IEEE Xplore. Restrictions apply.

simulations. The second enabling trend stems from the proliferation of motion capture facilities for robotics research, enabling precise tracking of robotic vehicles and humans through various technologies, such as infrared cameras, laser tracking, and ultra-wide band radio. These facilities provide the opportunity to incorporate real motion and behavior of vehicles and humans into the simulation in real time. This trend provides the potential to combine the efﬁciency, safety, and ﬂexibility of simulation with real-world physics and agent behavior.
Traditionally, simulation systems embody “models” of the vehicles and the environment, which are used to emulate what the vehicles sense, how they move, and how their environment adapts. In this paper, we present two concepts that use “data” to drive realistic simulations. First, we heavily utilize photogrammetry to realistically simulate exteroceptive sensors. For this purpose, we photograph real-world objects, and reconstruct them in the simulation environment. Almost all objects in our simulation environment are, in fact, renderings of real-world objects. This approach allows realistic renderings, as shown in Fig. 1. Second, we utilize a novel virtual-reality system to realistically embed inertial sensors, vehicles dynamics, and human behavior into the simulation environment. Instead of modeling these effects, we place vehicles and human actors in motion-capture facilities. We acquire the pose of the vehicles and the conﬁguration of the human actors in real time, and create their avatars in the simulation environment. For each autonomous vehicle, its proprioceptive measurements are acquired using on-board sensors, e.g., inertial measurement units and odometers; while exteroceptive sensors are rendered photorealistically in real time. In addition, the human behavior observed by the vehicles is generated by humans reacting to the simulation. In other words, vehicles embedded in the FlightGoggles simulation system experience real dynamics, real inertial sensing, real human behavior, and synthetic exteroceptive sensor measurements rendered photorealistically effectively by transforming photographs of real-world objects.
The combination of real physics and data-driven exteroceptive sensor simulation that FlightGoggles provides is not achieved in traditional simulation systems. Such systems are typically built around a physics engine that simulates vehicles and the environment based on a “model”, most commonly a system of ordinary or partial differential equations [6]. While these models may accurately exemplify the behavior of a general vehicle or actor, this is not sufﬁcient to ensure that simulation results transfer to the real world. Complicated aspects of vehicle dynamics, e.g., vibrations and unsteady aerodynamics, and of human behavior may signiﬁcantly affect results, but can be very challenging to accurately capture in a physics model. In order to generate exteroceptive sensor data, robotics simulators employ a graphics rendering engine in conjunction with the physics engine. A popular example is Gazebo [7], which lets users select various underlying engines. It is often used in combination with the Robot Operating System (ROS) to enable hardware-in-theloop simulation. However, Gazebo is generally not capable

of photorealistic rendering. Speciﬁcally, for unmanned aerial vehicles simulation, two popular simulators that are built on Gazebo are the Hector Quadrotor package [8] and RotorS [9]. Both simulators include vehicle dynamics and exteroceptive sensor models, but lack the capability to render photorealistic camera streams. AirSim, on the other hand, is purposely built on the Unreal rendering engine to enable rendering of photorealistic camera streams from autonomous vehicles, and exempliﬁes the shift towards using video game rendering engines to improve realism in robotics simulation [10]. However, it is still limited by the ﬁdelity of the underlying physics engine when it comes to vehicle dynamics and inertial measurements.
Simulation offers an alternative to experimental data gathering in addressing the need for extensive labeled data sets, bolstered by the rise of data-driven algorithms for autonomous robotics. Clearly, there are many advantages to this approach, e.g., cost efﬁciency, safety, repeatability, and essentially unlimited quantity and diversity. In recent years, several synthetic, or virtual, datasets have appeared in literature. For example, Synthia [11] and Virtual KITTI [12] use Unity to generate photorealistic renders of an urban environment, and ICL-NUIM [13] provides synthetic renderings of an indoor environment based on pre-recorded handheld trajectories. The Blackbird Dataset [14] includes real-world ground truth and inertial measurements of a quadcopter in motion capture, and photorealistic camera imagery rendered in FlightGoggles. The open-source availability of FlightGoggles and its photorealistic assets enables users to straightforwardly generate additional data, including realtime photorealistic renders based on real-world vehicles and actors.
This paper is organized as follows. Section II provides an overview of the FlightGoggles system architecture, including interfacing with real-world vehicles and actors in motion capture facilities. Section III outlines the photogrammetry process and the resulting virtual environment. This section also details the rendering pipeline and the exteroceptive sensor models available. Section IV describes several applications of FlightGoggles, including results of the AlphaPilot qualiﬁcations. Finally, Section V concludes with remarks.
II. SYSTEM ARCHITECTURE
FlightGoggles is based on a modular architecture, as shown in Fig. 2. This architecture provides the ﬂexibility to tailor functionality for a speciﬁc simulation scenario involving real and/or simulated vehicles, and possibly human interactors. As shown in the ﬁgure, FlightGoggles’ central component is the Unity game engine. It utilizes position and orientation information to simulate camera imagery and exteroceptive sensors and to detect collisions. Collision checks are performed using mesh colliders, and results are output to be included in the dynamics of simulated vehicles.
FlightGoggles includes a multicopter physics engine for simulation of six degree-of-freedom ﬂight dynamics, inertial measurements, and ﬂight control. The physics model includes motor dynamics, basic vehicle aerodynamics, and

6942

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:44:52 UTC from IEEE Xplore. Restrictions apply.

Fig. 2: Overview of FlightGoggles system architecture. Pose data of real and simulated vehicles, and human interactors is used by the Unity rendering engine. All dynamics states, control inputs, and sensor outputs of real and simulated vehicles, and human interactors are available through the FlightGoggles API.

IMU bias dynamics and can be integrated using explicit Euler and 4th-order Runge-Kutta algorithms. A detailed description of the simulation is given in an elaborate version of this paper [15]. Additionally, the FlightGoggles API provides a simulation base class that can be used to simulate user-deﬁned vehicle equations of motion and measurement models. Simulation scenarios may also include real-world vehicles through the use of a motion capture system. In this case, Unity simulation of camera images and exteroceptive sensors, and collision detection are based on the real-world vehicle position and orientation. This type of vehicle-in-the-loop simulation can be seen as an extension of customary hardware-in-the-loop conﬁgurations. It not only includes the vehicle hardware, but also the actual physics of processes that are challenging to simulate accurately, such as aerodynamics (including effects of turbulent air ﬂows), and inertial measurements subject to vehicle vibrations. FlightGoggles provides the novel combination of realworld vehicle dynamics and proprioceptive measurements, and simulated photorealistic exteroceptive sensor simulation. It allows for real-world physics, ﬂexible exteroceptive sensor conﬁgurations, and obstacle-rich environments without the risk of actual collisions. FlightGoggles also allows scenarios

involving both humans and vehicles, co-located in simulation but placed in different motion capture rooms, e.g., for safety.
Dynamics states, control inputs, and sensor outputs of real and simulated vehicles, and human interactors are available to the user through the FlightGoggles API. In order to enable message passing between FlightGoggles nodes and the API, the framework can be used with either ROS [16] or LCM [17]. The FlightGoggles simulator can be run headlessly on an Amazon Web Services (AWS) cloud instance to enable real-time simulation on systems with limited hardware.
Dynamic elements, such as moving obstacles, lights, vehicles, and human actors, can be added and animated in the environment in real time. Using these added elements, users can change environment lighting or simulate complicated human-vehicle, vehicle-vehicle, and vehicle-object interactions in the virtual environment. In Section IV, we describe an use case involving a dynamic human actor. In this scenario, skeleton tracking motion capture data is used to render a 3D model of the human in the virtual FlightGoggles environment. The resulting render is observed in real time by a virtual camera attached to a quadcopter in real-world ﬂight in a different motion capture room, as shown in Fig. 6.

6943

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:44:52 UTC from IEEE Xplore. Restrictions apply.

III. EXTEROCEPTIVE SENSOR SIMULATION
This section describes the creation of the environment using photogrammetry, lists the features of the render pipeline, and describes each of the exteroceptive sensor models.
FlightGoggles provides a simulation environment with exceptional visual ﬁdelity. Its high level of photorealism is achieved using 84 unique 3D models captured from realworld objects using photogrammetry, as can be seen in Fig. 3. The resulting environment is comprised of over 40 million triangles and 1,050 object instances.
A. Photorealistic Sensor Simulation using Photogrammetry
Photogrammetry is the process in which multiple photographs of a real-world object from different viewpoints are used to efﬁciently construct a realistic high-resolution 3D model for use in virtual environments. This technique has two major advantages when compared to traditional 3D modeling techniques. Firstly, it requires virtually no manual modeling and texturing. The elimination of these timeconsuming and artistically demanding processes enables the creation of many high-resolution assets in a relatively short time and at a more moderate cost. Secondly, the resulting renderings are based directly on real-world data, i.e., photographs. Consequently, the simulation includes a photorealistic representation of the real-world object that is being modeled, which may be critical in robotics applications. Due to its advantages over traditional modeling methods, photogrammetry is already widely used in the video game industry; however, its application towards photorealistic robotics simulation, as introduced in FlightGoggles, is novel.
1) Photogrammetry asset capture pipeline: Photogrammetry was used to create 84 unique open-source 3D assets for the FlightGoggles environment. These assets are based on thousands of high-resolution digital photographs of realworld objects and environmental elements, such as walls and ﬂoors. The digital images were ﬁrst color-balanced, and then combined to reconstruct object meshes using the GPUbased reconstruction software Reality Capture [18]. After this step, the raw object meshes were manually cleaned to remove reconstruction artifacts. Mesh baking was performed to generate base color, normal, height and ambient occlusion maps for each object; which are then combined into one high-deﬁnition surface material in Unity3D. For a detailed overview of a typical photogrammetry capture workﬂow, we refer the reader to [19].
2) HD render pipeline: Fig. 3 shows several 3D assets that were generated using the process described above. The ﬁgure also shows examples of real-world reference imagery that was used in the photogrammetry process to construct these assets. To achieve photorealistic RGB camera rendering, FlightGoggles uses the Unity Game Engine High Deﬁnition Render Pipeline (HDRP) [20]. Using HDRP, cameras rendered in FlightGoggles have characteristics similar to those of real-world cameras including motion blur, lens dirt, bloom, real-time reﬂections, and precomputed ray-traced indirect lighting. Additional camera characteristics such as

chromatic aberration, vignetting, lens distortion, and depth of ﬁeld can be enabled in the simulation environment.
B. Performance Optimizations and System Requirements
Extensive performance and memory optimizations were performed to ensure that FlightGoggles is able to run on a wide spectrum of GPU rendering hardware with ≥ 2GB of video random access memory (VRAM). Additionally, FlightGoggles VRAM and GPU computation requirements can be reduced further by user-selectable quality proﬁles based on three major settings: real-time reﬂections, maximum object texture resolution, and maximum level of detail (i.e. polygon count).
1) Mesh level of detail: For each object mesh in the environment, three meshes with different levels of detail (LOD), i.e., polygon count and texture resolution, were generated: low, medium, and high. For meshes with lower levels of detail, textures were downsampled using subsampling and subsequent smoothing. During simulation, the real-time render pipeline improves render performance by selecting the appropriate level of detail object mesh and texture based on the size of the object mesh in camera image space. GPU VRAM usage can be decreased further by limiting the maximum level of detail across all meshes, through the userselectable quality proﬁles.
2) Pre-baked ray-traced lighting: In order to reduce runtime computation, all direct and indirect lighting, ambient occlusions, and shadow details from static light sources are pre-baked via NVIDIA RTX ray tracing. The resulting static lightmaps are layered onto object meshes in the environment. An NVIDIA Quadro RTX 8000 GPU was used to precompute the ray-traced lighting for each lighting condition in the Abandoned Warehouse environment, resulting in an average bake time of 45 minutes per lighting arrangement.
3) Render batching: Flightgoggles uses platform-speciﬁc render batching to increase rendering performance by reducing individual GPU draw calls. On Windows-based systems supporting DirectX11, FlightGoggles leverages the experimental Unity3D Scriptable Render Pipeline dynamic batcher, which drastically reduces GPU draw calls for all static and dynamic objects in the environment. On Linux and MacOS systems, FlightGoggles statically batches all static meshes in the environment. Static batching drastically increases rendering performance, but also increases VRAM usage as all meshes must be combined and pre-loaded onto the GPU memory at run time. If necessary, the latter increase can be addressed by reducing the VRAM usage through the userselectable quality proﬁles.
4) Dynamic clock scaling: FlightGoggles provides optional dynamic clock scaling to guarantee a nominal camera frame rate in simulation time, even on rendering hardware that is incapable of achieving reliable real-time frame rates. When automatic clock scaling is enabled, FlightGoggles monitors the frame rate of the renderer output and dynamically adjusts the ROS simulation time rate to achieve the desired nominal frame rate in simulation time. Since the built-in ROS time framework is used, changes in time rate do

6944

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:44:52 UTC from IEEE Xplore. Restrictions apply.

(a) Photo of barrel.

(b) Photo of rubble.

(c) Photo of corrugated metal.

(d) Photo of caged tank.

(e) Render of barrel.

(f) Render of rubble.

(g) Render of corrugated metal. (h) Render of caged tank.

Fig. 3: Object photographs that were used for photogrammetry and corresponding rendered assets in FlightGoggles.

not affect the relative timing of client nodes, which alleviates non-deterministic timing issues across simulation runs.

C. Exteroceptive Sensor Models
FlightGoggles is capable of high-ﬁdelity simulation of various types of exteroceptive sensors, such as RGB-D cameras, time-of-ﬂight distance sensors, and infrared radiation (IR) beacon sensors. Default noise characteristics, and intrinsic and extrinsic parameters are based on real sensor speciﬁcations, and can easily be adjusted. Moreover, users can instantiate multiple instances of each sensor type. This capability allows quick prototyping and evaluation of distinct exteroceptive sensor arrangements.
1) Camera: The default camera model provided by FlightGoggles is a perfect, i.e., distortion-free, projection model with optional motion blur, lens dirt, auto-exposure, and bloom. Major camera parameters, such as ﬁeld of view, image resolution, and stereo baseline, are exposed in the FlightGoggles API. The camera extrinsics Tcb where b is the vehicle ﬁxed body frame and c is the camera frame can also be changed in real time.
2) Infrared beacon sensor: An IR beacon sensor model is included to facilitate the quick development of guidance, navigation, and control algorithms. This sensor provides image-space u, v measurements of IR beacons in the camera ﬁeld of view. The beacons can be placed at static locations in the environment or on moving objects. Using real-time ray-casting from each RGB camera, simulated IR beacon measurements are tested for occlusion before being included in the IR sensor output. Fig. 4 shows a visual representation of the sensor output.
3) Time-of-ﬂight range sensor: FlightGoggles is able to simulate (multi-point) time-of-ﬂight range sensors using ray casts in any speciﬁed direction. In the default vehicle conﬁguration, a downward-facing single-point range ﬁnder for height estimation is provided. The noise characteristics

Fig. 4: Rendered camera view (faded) with IR marker locations overlayed. The unprocessed measurements and marker IDs from the simulated IR beacon sensor are indicated in red. The measurements are veriﬁed by comparison to imagespace reprojections of ground-truth IR marker locations, which are indicated in green. Note that IR markers can be arbitrarily placed by the user, including on dynamic objects.
of this sensor are similar to the commercially available LightWare SF11/B laser altimeter [21].
IV. APPLICATIONS
In this section, we discuss current and potential FlightGoggles applications, such as human-vehicle interaction, active sensor selection, multi-agent systems, and visual inertial navigation research for fast and agile vehicles [14], [22], [23]. We also describe the simulation stage of the AlphaPilot challenge, in which teams used FlightGoggles to compete in a simulated drone race [24].
A. Aircraft-in-the-Loop High-Speed Flight using Visual Inertial Odometry
Camera-IMU sensor packages are widely used in both commercial and research applications, because of their relatively low cost and low weight. Particularly in GPS-denied

6945

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:44:52 UTC from IEEE Xplore. Restrictions apply.

Fig. 6: A dynamic human actor in the FlightGoggles virtual environment is rendered in real time, based on skeleton tracking data of a human in a motion capture suit with markers.

Fig. 5: Visual features tracked using a typical visual inertial odometry pipeline on FlightGoggles simulated camera imagery.
environments, cameras may be essential for effective state estimation. Visual inertial odometry (VIO) algorithms combine camera images with pre-integrated IMU measurements to estimate the vehicle state. While these algorithms are often critical for safe navigation, it is challenging to verify their performance in varying conditions. Environment variables, e.g., lighting and object placement, and camera properties may signiﬁcantly affect performance, but generally cannot easily be varied in reality. Moreover, obstacle-rich environments may increase the risk of collisions, especially in highspeed ﬂight, increasing the cost of extensive experiments.
There are several examples of research in state estimation that relied on the FlightGoggles framework. In previous work, it was shown that the estimation error of a VIO algorithm remains similar when replacing data from a real on-board camera with simulated camera imagery from FlightGoggles [22]. Fig. 5 shows tracking of virtual visual features during a VIO ﬂight in FlightGoggles. In [23], the FlightGoggles simulation system enabled experimentation with various environment conﬁgurations to show the efﬁciency of a perception-aware planning algorithm. For these experiments, the quadcopter was tracking a perception-aware trajectory using a state-of-the-art controller [25], while relying on a pose estimate from VIO based on the virtual camera in FlightGoggles and real-world inertial measurements from the vehicle in ﬂight.
B. Interactions with Dynamic Actors
FlightGoggles is able to render dynamic actors, e.g., humans or vehicles, in real time from real-world models with ground-truth movement. Fig. 6 gives an overview of a simulation scenario involving a human actor. In this scenario, the human is rendered in real time based on skeleton tracking motion capture data, while a quadcopter is simultaneously ﬂying in a separate motion capture room. While both dynamic actors (i.e. human and quadcopter) are physically in separate spaces, they are both in the same virtual FlightGoggles environment. Consequently, both actors are visible

to each other and can interact through simulated camera imagery. This imagery can for example be displayed on virtual reality headsets, or used in vision-based autonomy algorithms. FlightGoggles provides the capability to simulate these realistic and versatile human-vehicle interactions in an inherently safe manner.
C. AlphaPilot Challenge
The AlphaPilot challenge [24] is an autonomous drone racing challenge organized by Lockheed Martin, NVIDIA, and the Drone Racing League (DRL). The challenge is split into two stages. A simulation phase open to the general public, and a real-world phase in which teams compete against each other by programming fully-autonomous racing drones built by the DRL. During the simulation phase, the FlightGoggles simulation framework was used as the main qualifying test for selecting nine teams that would progress to the next stage of the AlphaPilot challenge. To complete the test, contestants had to submit code to autonomously race a simulated quadcopter with simulated sensors through the 11gate race track shown in Fig. 7. Test details were revealed to all contestants on February 14th, 2019 and ﬁnal submissions were due on March 20th, 2019.
1) Challenge outline: The purpose of the AlphaPilot simulation challenge was for teams to demonstrate their autonomous guidance, navigation, and control capability in a realistic simulation environment. The participants’ aim was to complete the track as fast as possible using a simulated quadcopter based on the FlightGoggles multicopter dynamics model. To accomplish this, measurements from four simulated sensors were provided: (stereo) cameras, IMU, downward-facing time-of-ﬂight range sensor, and infrared gate beacons. Through the FlightGoggles ROS API, autonomous systems could obtain sensor measurements and provide collective thrust and attitude rate inputs to the quadcopter low-level acro/rate mode controller.
The race track was located in the FlightGoggles Abandoned Factory environment and consisted of 11 gates. To successfully complete the entire track, the quadcopter had to pass through all the gates in order. The ﬁnal score was calculated as score = 10 · gates − time where gates is the number of gates passed in order and time is the time taken

6946

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:44:52 UTC from IEEE Xplore. Restrictions apply.

2
3 FINISH

1 9
4
5 10

START 8 7 6

Fig. 7: Overhead visualization of speed proﬁles (in ms-1) and crash locations for top 20 AlphaPilot teams across all 25 runs. Nominal gate locations are numbered in track order and marked with boxes. Note that most crashes occur near gates, obstacles, or immediately after takeoff.

Camera IMU Ranger Infrared Learning VIO Filter Smoother Polynomial Visual Servo Other Linear MPC Other Final Score Score 1 Score 2 Score 3 Score 4 Score 5

in seconds to reach the ﬁnal gate. If the ﬁnal gate was not reached within the race time limit or the quadcopter collided with an environment object, a score of zero was recorded. To discourage memorization of the course, the exact gate locations were subject to random unknown perturbations. These perturbations were large enough to require adapting the vehicle trajectory, but did not change the track layout in a fundamental way. The ﬁnal score for each team was the average of their ﬁve highest scores over an evaluation set of 25 perturbed courses that was kept unknown to the teams. For development and veriﬁcation of their algorithms, participants were provided with the nominal gate locations, as well as another set of 25 perturbed courses with identically distributed gate locations.
2) FlightGoggles sensor usage: Table I shows the usage of provided sensors, the algorithm choices, and the ﬁnal and ﬁve highest scores for the 20 top teams (sorted by ﬁnal score). All of these 20 teams used both the simulated IMU sensor and the infrared beacon sensors. Several teams chose to also incorporate the camera and the time-of-ﬂight range sensor.
3) Algorithm choices: The contestants were tasked with developing guidance, navigation, and control algorithms. Table I tabulates the general estimation, planning, and control approaches used for each team alongside the sensor choices and their scores. Of the top 20 teams, only one used an endto-end learning-based method. The other 19 teams relied on more traditional pipelines (estimation, planning, and control) to complete the challenge. One of those teams used learning to determine the pose of the camera from the image. For state estimation, all but one team used a ﬁltering algorithm such as the extended Kalman ﬁlter, unscented Kalman ﬁlter, particle ﬁlter, or the Madgwick ﬁlter with the other team using a

91.4 91.5 91.5 91.4 91.3 91.2 84.5 85.4 84.6 84.3 84.2 84.1 81.0 81.5 81.0 80.9 80.9 80.8 80.6 81.0 80.9 80.4 80.3 80.3 78.6 78.8 78.6 78.6 78.5 78.5 78.6 78.7 78.6 78.5 78.5 78.5 76.1 76.6 76.2 75.9 75.9 75.8 74.2 74.5 74.2 74.2 74.1 74.1 71.4 71.5 71.5 71.4 71.4 71.4 71.1 71.3 71.1 71.1 71.0 71.0 70.9 73.6 72.8 72.8 72.7 62.5 70.5 71.0 70.8 70.2 70.2 70.0 69.9 71.4 70.7 69.3 69.2 69.0 57.3 77.0 76.3 66.5 66.5 0.0 56.3 56.5 56.4 56.2 56.2 56.2 55.9 57.5 56.2 55.7 55.3 54.7 29.8 74.8 74.5 0.0 0.0 0.0 13.0 25.2 19.9 19.9 0.0 0.0 12.6 41.0 21.8 0.0 0.0 0.0 11.8 59.1 0.0 0.0 0.0 0.0
TABLE I: Sensor usage, algorithm choices, and ﬁnal and ﬁve highest scores in AlphaPilot simulation challenge.
smoothing based technique. The teams that chose to use a visual inertial odometry algorithm opted to use off-the-shelf solutions for state estimation. The most common methods used for planning involved visual servo using infrared beacons or polynomial trajectory planning. Other methods used for planning either used manually-deﬁned waypoints or used sampling-based techniques for building trajectory libraries. Five of the 19 teams that used model-based techniques also incorporated some form of perception awareness in their

6947

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:44:52 UTC from IEEE Xplore. Restrictions apply.

planning algorithms. The predominant methods for control were linear control techniques and model predictive control. Additionally, geometric and backstepping control methods were used.
4) Analysis of trajectories: Fig. 7 shows the resulting trajectory speed proﬁles. To visualize the speed along the trajectories, we discretized the horizontal plane and colored each grid cell on a logarithmic scale according to the average of the local speeds. From the ﬁgure, we can observe that most teams chose to slow down for the sharp turns at gates 2 and 7. We can also observe that in general the average speed around gates is lower than at other portions of the environment, which can be attributed to the need to ‘search’ for the next gate. Fig. 7 also shows the crash locations of all the failed attempts. We observe that many of the crash locations are in the vicinity of the gates, which may be caused by widespread use of visual-servo-based techniques combined with the fact the infrared gate beacons are more likely to leave the camera ﬁeld of view at close range.
5) Individual performance of top teams: Given that the ﬁnal scoring function for the competition only included the ﬁve best scoring runs, teams were encouraged to take signiﬁcant risk to improve their top scores. Consequently, 75% of the contestants failed to complete the course in at least half of their 25 runs. Only one team completed the entire course in all of their 25 runs. Notably, this team also achieved very consistent scores across all runs. While their average score across all runs ranks among the highest of all teams; their ﬁnal score based on the ﬁve best runs is ranked signiﬁcantly lower, showing that risk-taking strategies are indeed rewarded.
V. CONCLUSIONS
This paper introduced FlightGoggles, a new modular framework for realistic simulation to aid robotics testing and development. FlightGoggles is enabled by photogrammetry and virtual reality technologies. Heavy utilization of photogrammetry helps provide realistic simulation of camera sensors. Virtual reality allows for integration of real vehicle motion and human behavior acquired in motion capture facilities directly into the simulation system. FlightGoggles is being actively utilized by a community of robotics researchers. In particular, FlightGoggles has served as the main test for selecting the contestants for the AlphaPilot autonomous drone racing challenge. This paper also presented a survey of approaches and results from the simulation challenge.
REFERENCES
[1] R. A. Brooks and M. J. Mataric, “Real robots, real learning problems,” in Robot learning. Springer, 1993, pp. 193–213.
[2] H. Chiu, V. Murali, R. Villamil, G. D. Kessler, S. Samarasekera, and R. Kumar, “Augmented reality driving using semantic georegistration,” in IEEE Conference on Virtual Reality and 3D User Interfaces (VR), 2018, pp. 423–430.
[3] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke, “Sim-to-real: Learning agile locomotion for quadruped robots,” arXiv preprint arXiv:1804.10332, 2018.

[4] “Unreal Engine,” https://www.unrealengine.com/, 2019, [Online; ac-

cessed 28-February-2019].

[5] “Unity3d Game Engine,” https://unity3d.com/, 2019, [Online; accessed

28-February-2019].

[6] T. Erez, Y. Tassa, and E. Todorov, “Simulation tools for model-based

robotics: Comparison of Bullet, Havok, MuJoCo, ODE and Physx,” in

IEEE International Conference on Robotics and Automation (ICRA),

2015, pp. 4397–4404.

[7] N. Koenig and A. Howard, “Design and use paradigms for Gazebo,

an open-source multi-robot simulator,” in IEEE/RSJ International

Conference on Intelligent Robots and Systems (IROS), 2004, pp. 2149–

2154.

[8] J. Meyer, A. Sendobry, S. Kohlbrecher, U. Klingauf, and O. Von Stryk,

“Comprehensive simulation of quadrotor UAVs using ROS and

Gazebo,” in International Conference on Simulation, Modeling, and

Programming for Autonomous Robots. Springer, 2012, pp. 400–411.

[9] F. Furrer, M. Burri, M. Achtelik, and R. Siegwart, “RotorS: A modular

Gazebo MAV simulator framework,” in Robot Operating System

(ROS). Springer, 2016, pp. 595–625.

[10] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-ﬁdelity

visual and physical simulation for autonomous vehicles,” in Field and

Service Robotics. Springer, 2018, pp. 621–635.

[11] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The

Synthia dataset: A large collection of synthetic images for semantic

segmentation of urban scenes,” in IEEE Conference on Computer

Vision and Pattern Recognition (CVPR), 2016, pp. 3234–3243.

[12] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, “Virtual worlds as proxy

for multi-object tracking analysis,” in CVPR, 2016.

[13] A. Handa, T. Whelan, J. McDonald, and A. Davison, “A benchmark

for RGB-D visual odometry, 3D reconstruction and SLAM,” in IEEE

Intl. Conf. on Robotics and Automation, ICRA, Hong Kong, China,

May 2014.

[14] A. Antonini, W. Guerra, V. Murali, T. Sayre-McCord, and S. Karaman,

“The Blackbird dataset: A large-scale dataset for UAV perception

in aggressive ﬂight,” in International Symposium on Experimental

Robotics (ISER), 2018.

[15] W. Guerra, E. Tal, V. Murali, G. Ryou, and S. Karaman, “Flightgog-

gles: Photorealistic sensor simulation for perception-driven robotics

using photogrammetry and virtual reality,” arXiv:1905.11377 [cs.RO],

2019.

[16] M. Quigley, K. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs,

R. Wheeler, and A. Y. Ng, “ROS: an open-source robot operating

system,” in ICRA Workshop on Open Source Software, 2009.

[17] A. S. Huang, E. Olson, and D. C. Moore, “LCM: Lightweight com-

munications and marshalling,” in IEEE/RSJ International Conference

on Intelligent Robots and Systems (IROS), 2010, pp. 4057–4062.

[18] “Reality Capture,” https://www.capturingreality.com/Product, 2019,

[Online; accessed 28-February-2019].

[19] S. Lachambre, S. Lagarde, and C. Jover, “Unity photogrammetry

workﬂow,” Unity Developer—Rendering Research. Retrieved

from

https:// unity3d.com/ ﬁles/ solutions/ photogrammetry/

Unity-Photogrammetry-Workﬂow 2017-07 v2.pdf , 2017.

[20] “High Deﬁnition Render Pipeline overview,” https://docs.unity3d.

com/Packages/com.unity.render-pipelines.high-deﬁnition@6.9/

manual/index.html, 2019, [Online; accessed 30-July-2019].

[21] “LightWare SF11/B Laser Range Finder,” http://documents.lightware.

co.za/SF11%20-%20Laser%20Altimeter%20Manual%20-%20Rev%

208.pdf, 2019, [Online; accessed 28-February-2019].

[22] T. Sayre-McCord, W. Guerra, A. Antonini, J. Arneberg, A. Brown,

G. Cavalheiro, Y. Fang, A. Gorodetsky, D. McCoy, S. Quilter, F. Ri-

ether, E. Tal, Y. Terzioglu, L. Carlone, and S. Karaman, “Visual-

inertial navigation algorithm development using photorealistic camera

simulation in the loop,” in IEEE International Conference on Robotics

and Automation (ICRA), 2018, pp. 2566–2573.

[23] V. Murali, I. Spasojevic, W. Guerra, and S. Karaman, “Perception-

aware trajectory generation for aggressive quadrotor ﬂight using

differential ﬂatness,” in American Control Conference (ACC), 2019.

[24] “AlphaPilot – Lockheed Martin AI Drone Racing Innovation Chal-

lenge,” https://www.herox.com/alphapilot, 2019, [Online; accessed 28-

February-2019].

[25] E. Tal and S. Karaman, “Accurate tracking of aggressive quadrotor

trajectories using incremental nonlinear dynamic inversion and differ-

ential ﬂatness,” in IEEE Conference on Decision and Control (CDC),

2018, pp. 4282–4288.

6948 Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:44:52 UTC from IEEE Xplore. Restrictions apply.

