Reinforcement Learning: Theory and Algorithms
Alekh Agarwal Nan Jiang Sham M. Kakade Wen Sun January 31, 2022
WORKING DRAFT: Please email bookrltheory@gmail.com with any typos or errors you ﬁnd.
We appreciate it!

ii

Contents

1 Fundamentals

3

1 Markov Decision Processes

5

1.1 Discounted (Inﬁnite-Horizon) Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . 5

1.1.1 The objective, policies, and values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.1.2 Bellman Consistency Equations for Stationary Policies . . . . . . . . . . . . . . . . . . . . . 7

1.1.3 Bellman Optimality Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

1.2 Finite-Horizon Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

1.3 Computational Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

1.3.1 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

1.3.2 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

1.3.3 Value Iteration for Finite Horizon MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

1.3.4 The Linear Programming Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

1.4 Sample Complexity and Sampling Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

1.5 Bonus: Advantages and The Performance Difference Lemma . . . . . . . . . . . . . . . . . . . . . . 18

1.6 Bibliographic Remarks and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

2 Sample Complexity with a Generative Model

21

2.1 Warmup: a naive model-based approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2.2 Sublinear Sample Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

2.3 Minmax Optimal Sample Complexity (and the Model Based Approach) . . . . . . . . . . . . . . . . 24

2.3.1 The Discounted Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

2.3.2 Finite Horizon Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

2.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

2.4.1 Variance Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

iii

2.4.2 Completing the proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 2.5 Scalings and Effective Horizon Dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.6 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

3 Linear Bellman Completeness

31

3.1 The Linear Bellman Completeness Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

3.2 The LSVI Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

3.3 LSVI with D-Optimal Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

3.3.1 D-Optimal Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

3.3.2 Performance Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.3.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

3.4 How Strong is Bellman Completion as a Modeling? . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

3.5 Ofﬂine Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

3.5.1 Ofﬂine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

3.5.2 Ofﬂine Policy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

3.6 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

4 Fitted Dynamic Programming Methods

39

4.1 Fitted Q-Iteration (FQI) and Ofﬂine RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

4.1.1 The FQI Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

4.1.2 Performance Guarantees of FQI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

4.2 Fitted Policy-Iteration (FPI) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

4.3 Failure Cases Without Assumption 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

4.4 FQI for Policy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

4.5 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

5 Statistical Limits of Generalization

45

5.1 Agnostic Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

5.1.1 Review: Binary Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

5.1.2 Importance Sampling and a Reduction to Supervised Learning . . . . . . . . . . . . . . . . . 47

5.2 Linear Realizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

5.2.1 Ofﬂine Policy Evaluation with Linearly Realizable Values . . . . . . . . . . . . . . . . . . . 49

5.2.2 Linearly Realizable Q . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

iv

5.2.3 Linearly Realizable π . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 5.3 Discussion: Studying Generalization in RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 5.4 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

2 Strategic Exploration

61

6 Multi-Armed & Linear Bandits

63

6.1 The K-Armed Bandit Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

6.1.1 The Upper Conﬁdence Bound (UCB) Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 63

6.2 Linear Bandits: Handling Large Action Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

6.2.1 The LinUCB algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

6.2.2 Upper and Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

6.3 LinUCB Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68

6.3.1 Regret Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

6.3.2 Conﬁdence Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

6.4 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

7 Strategic Exploration in Tabular MDPs

73

7.1 On The Need for Strategic Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73

7.2 The UCB-VI algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

7.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75

7.3.1 Proof of Lemma 7.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

7.4 An Improved Regret Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

7.5 Phased Q-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

7.6 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

8 Linearly Parameterized MDPs

85

8.1 Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85

8.1.1 Low-Rank MDPs and Linear MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85

8.2 Planning in Linear MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86

8.3 Learning Transition using Ridge Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

8.4 Uniform Convergence via Covering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89

8.5 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92

v

8.6 Analysis of UCBVI for Linear MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 8.6.1 Proving Optimism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 8.6.2 Regret Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 8.6.3 Concluding the Final Regret Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
8.7 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

9 Generalization with

Bounded Bilinear Rank

97

9.1 Hypothesis Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97

9.2 The Bellman Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98

9.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

9.3.1 Examples that have small Q-Bellman rank . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

9.3.2 Examples that have small V -Bellman rank . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

9.4 Bilinear Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

9.4.1 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

9.5 PAC-RL with Bounded Bilinear Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

9.5.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

9.5.2 Sample Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

9.5.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

9.6 The Eluder Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

9.7 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111

10 Deterministic MDPs with Linearly Parameterized Q

113

3 Policy Optimization

115

11 Policy Gradient Methods and Non-Convex Optimization

117

11.1 Policy Gradient Expressions and the Likelihood Ratio Method . . . . . . . . . . . . . . . . . . . . . 118

11.2 (Non-convex) Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

11.2.1 Gradient ascent and convergence to stationary points . . . . . . . . . . . . . . . . . . . . . . 120

11.2.2 Monte Carlo estimation and stochastic gradient ascent . . . . . . . . . . . . . . . . . . . . . 120

11.3 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

12 Optimality

123

vi

12.1 Vanishing Gradients and Saddle Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 12.2 Policy Gradient Ascent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 12.3 Log Barrier Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 12.4 The Natural Policy Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 12.5 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

13 Function Approximation and the NPG

133

13.1 Compatible function approximation and the NPG . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133

13.2 Examples: NPG and Q-NPG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

13.2.1 Log-linear Policy Classes and Soft Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . 135

13.2.2 Neural Policy Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136

13.3 The NPG “Regret Lemma” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136

13.4 Q-NPG: Performance Bounds for Log-Linear Policies . . . . . . . . . . . . . . . . . . . . . . . . . 138

13.4.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140

13.5 Q-NPG Sample Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142

13.6 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142

14 CPI, TRPO, and More

143

14.1 Conservative Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143

14.1.1 The CPI Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144

14.2 Trust Region Methods and Covariant Policy Search . . . . . . . . . . . . . . . . . . . . . . . . . . . 148

14.2.1 Proximal Policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

14.3 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151

4 Further Topics

153

15 Imitation Learning

155

15.1 Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155

15.2 Ofﬂine IL: Behavior Cloning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155

15.3 The Hybrid Setting: Statistical Beneﬁt and Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 156

15.3.1 Extension to Agnostic Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158

15.4 Maximum Entropy Inverse Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

15.4.1 MaxEnt IRL: Formulation and The Principle of Maximum Entropy . . . . . . . . . . . . . . 160

vii

15.4.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 15.4.3 Maximum Entropy RL: Implementing the Planning Oracle in Eq. 0.4 . . . . . . . . . . . . . 161 15.5 Interactive Imitation Learning: AggreVaTe and Its Statistical Beneﬁt over Ofﬂine IL Setting . . . . . . . . . . . . . . . . . . . . . . 162 15.6 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165

16 Linear Quadratic Regulators

167

16.1 The LQR Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167

16.2 Bellman Optimality: Value Iteration & The Algebraic Riccati Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 168

16.2.1 Planning and Finite Horizon LQRs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168

16.2.2 Planning and Inﬁnite Horizon LQRs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170

16.3 Convex Programs to ﬁnd P and K . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170

16.3.1 The Primal for Inﬁnite Horizon LQR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171

16.3.2 The Dual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171

16.4 Policy Iteration, Gauss Newton, and NPG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172

16.4.1 Gradient Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172

16.4.2 Convergence Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

16.4.3 Gauss-Newton Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

16.5 System Level Synthesis for Linear Dynamical Systems . . . . . . . . . . . . . . . . . . . . . . . . . 176

16.6 Bibliographic Remarks and Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179

17 Partially Observable Markov Decision Processes

181

Bibliography

183

A Concentration

193

viii

Notation

The reader might ﬁnd it helpful to refer back to this notation section.

• We slightly abuse notation and let [K] denote the set {0, 1, 2, . . . K − 1} for an integer K.

• We let ∆(X ) denote the set of probability distribution over the set X .

•

For

a

vector

v,

we

let

(v)2

,

√ v,

and

|v|

be

the

component-wise

square,

square

root,

and

absolute

value

operations.

• Inequalities between vectors are elementwise, e.g. for vectors v, v , we say v ≤ v , if the inequality holds elementwise.

• For a vector v, we refer to the j-th component of this vector by either v(j) or [v]j • Denote the variance of any real valued f under a distribution D as:
VarD(f ) := Ex∼D[f (x)2] − (Ex∼D[f (x)])2

• We overload notation where, for a distribution µ over S, we write: V π(µ) = Es∼µ [V π(s)] .

• It is helpful to overload notation and let P also refer to a matrix of size (S · A) × S where the entry P(s,a),s is equal to P (s |s, a). We also will deﬁne P π to be the transition matrix on state-action pairs induced by a deterministic policy π. In particular, P(πs,a),(s ,a ) = P (s |s, a) if a = π(s ) and P(πs,a),(s ,a ) = 0 if a = π(s ). With this notation,
Qπ = r + γP V π
Qπ = r + γP πQπ
Qπ = (I − γP π)−1r

• For a vector Q ∈ R|S×A|, denote the greedy policy and value as: πQ(s) := argmaxa∈A Q(s, a) VQ(s) := max Q(s, a). .
a∈A
• For a vector Q ∈ R|S×A|, the Bellman optimality operator T : R|S×A| → R|S×A| is deﬁned as: T Q := r + γP VQ .

(0.1)

• For a vector x and a matrix M , unless explicitly stated otherw√ise, we let x and M denote the Euclidean and spectral norms, respectively. We use the notation x M = x M x, where x and M are assumed to be of appropriate size.

1

2

Part 1
Fundamentals
3

Chapter 1
Markov Decision Processes
1.1 Discounted (Inﬁnite-Horizon) Markov Decision Processes
In reinforcement learning, the interactions between the agent and the environment are often described by an inﬁnitehorizon, discounted Markov Decision Process (MDP) M = (S, A, P, r, γ, µ), speciﬁed by:
• A state space S, which may be ﬁnite or inﬁnite. For mathematical convenience, we will assume that S is ﬁnite or countably inﬁnite.
• An action space A, which also may be discrete or inﬁnite. For mathematical convenience, we will assume that A is ﬁnite.
• A transition function P : S × A → ∆(S), where ∆(S) is the space of probability distributions over S (i.e., the probability simplex). P (s |s, a) is the probability of transitioning into state s upon taking action a in state s. We use Ps,a to denote the vector P (· s, a).
• A reward function r : S × A → [0, 1]. r(s, a) is the immediate reward associated with taking action a in state s. More generally, the r(s, a) could be a random variable (where the distribution depends on s, a). While we largely focus on the case where r(s, a) is deterministic, the extension to methods with stochastic rewards are often straightforward.
• A discount factor γ ∈ [0, 1), which deﬁnes a horizon for the problem. • An initial state distribution µ ∈ ∆(S), which speciﬁes how the initial state s0 is generated.
In many cases, we will assume that the initial state is ﬁxed at s0, i.e. µ is a distribution supported only on s0.
1.1.1 The objective, policies, and values
Policies. In a given MDP M = (S, A, P, r, γ, µ), the agent interacts with the environment according to the following protocol: the agent starts at some state s0 ∼ µ; at each time step t = 0, 1, 2, . . ., the agent takes an action at ∈ A, obtains the immediate reward rt = r(st, at), and observes the next state st+1 sampled according to st+1 ∼ P (·|st, at). The interaction record at time t,
τt = (s0, a0, r0, s1, . . . , st, at, rt),
5

is called a trajectory, which includes the observed state at time t.
In the most general setting, a policy speciﬁes a decision-making strategy in which the agent chooses actions adaptively based on the history of observations; precisely, a policy is a (possibly randomized) mapping from a trajectory to an action, i.e. π : H → ∆(A) where H is the set of all possible trajectories (of all lengths) and ∆(A) is the space of probability distributions over A. A stationary policy π : S → ∆(A) speciﬁes a decision-making strategy in which the agent chooses actions based only on the current state, i.e. at ∼ π(·|st). A deterministic, stationary policy is of the form π : S → A.

Values. We now deﬁne values for (general) policies. For a ﬁxed policy and a starting state s0 = s, we deﬁne the value function VMπ : S → R as the discounted sum of future rewards

∞

VMπ (s) = E

γtr(st, at) π, s0 = s .

t=0

where expectation is with respect to the randomness of the trajectory, that is, the randomness in state transitions and the stochasticity of π. Here, since r(s, a) is bounded between 0 and 1, we have 0 ≤ VMπ (s) ≤ 1/(1 − γ).

Similarly, the action-value (or Q-value) function QπM : S × A → R is deﬁned as

∞

QπM (s, a) = E

γtr(st, at) π, s0 = s, a0 = a .

t=0

and QπM (s, a) is also bounded by 1/(1 − γ).

Goal. Given a state s, the goal of the agent is to ﬁnd a policy π that maximizes the value, i.e. the optimization problem the agent seeks to solve is:

max
π

VMπ

(s)

(0.1)

where the max is over all (possibly non-stationary and randomized) policies. As we shall see, there exists a deterministic and stationary policy which is simultaneously optimal for all starting states s.

We drop the dependence on M and write V π when it is clear from context.

Example 1.1 (Navigation). Navigation is perhaps the simplest to see example of RL. The state of the agent is their current location. The four actions might be moving 1 step along each of east, west, north or south. The transitions in the simplest setting are deterministic. Taking the north action moves the agent one step north of their location, assuming that the size of a step is standardized. The agent might have a goal state g they are trying to reach, and the reward is 0 until the agent reaches the goal, and 1 upon reaching the goal state. Since the discount factor γ < 1, there is incentive to reach the goal state earlier in the trajectory. As a result, the optimal behavior in this setting corresponds to ﬁnding the shortest path from the initial to the goal state, and the value function of a state, given a policy is γd, where d is the number of steps required by the policy to reach the goal state.

Example 1.2 (Conversational agent). This is another fairly natural RL problem. The state of an agent can be the current transcript of the conversation so far, along with any additional information about the world, such as the context for the conversation, characteristics of the other agents or humans in the conversation etc. Actions depend on the domain. In the most basic form, we can think of it as the next statement to make in the conversation. Sometimes, conversational agents are designed for task completion, such as travel assistant or tech support or a virtual ofﬁce receptionist. In these cases, there might be a predeﬁned set of slots which the agent needs to ﬁll before they can ﬁnd a good solution. For instance, in the travel agent case, these might correspond to the dates, source, destination and mode of travel. The actions might correspond to natural language queries to ﬁll these slots.

6

In task completion settings, reward is naturally deﬁned as a binary outcome on whether the task was completed or not, such as whether the travel was successfully booked or not. Depending on the domain, we could further reﬁne it based on the quality or the price of the travel package found. In more generic conversational settings, the ultimate reward is whether the conversation was satisfactory to the other agents or humans, or not.
Example 1.3 (Strategic games). This is a popular category of RL applications, where RL has been successful in achieving human level performance in Backgammon, Go, Chess, and various forms of Poker. The usual setting consists of the state being the current game board, actions being the potential next moves and reward being the eventual win/loss outcome or a more detailed score when it is deﬁned in the game. Technically, these are multi-agent RL settings, and, yet, the algorithms used are often non-multi-agent RL algorithms.

1.1.2 Bellman Consistency Equations for Stationary Policies

Stationary policies satisfy the following consistency conditions:
Lemma 1.4. Suppose that π is a stationary policy. Then V π and Qπ satisfy the following Bellman consistency equations: for all s ∈ S, a ∈ A,
V π(s) = Qπ(s, π(s)). Qπ(s, a) = r(s, a) + γEa∼π(·|s),s ∼P (·|s,a) V π(s ) .

We leave the proof as an exercise to the reader.

It is helpful to view V π as vector of length |S| and Qπ and r as vectors of length |S| · |A|. We overload notation and let P also refer to a matrix of size (|S| · |A|) × |S| where the entry P(s,a),s is equal to P (s |s, a).

We also will deﬁne P π to be the transition matrix on state-action pairs induced by a stationary policy π, speciﬁcally:

P(πs,a),(s ,a ) := P (s |s, a)π(a |s ).

In particular, for deterministic policies we have:

P(πs,a),(s ,a ) :=

P (s |s, a) if a = π(s ) 0 if a = π(s )

With this notation, it is straightforward to verify:
Qπ = r + γP V π Qπ = r + γP πQπ .

Corollary 1.5. Suppose that π is a stationary policy. We have that:

Qπ = (I − γP π)−1r

(0.2)

where I is the identity matrix.

Proof: To see that the I − γP π is invertible, observe that for any non-zero vector x ∈ R|S||A|,

(I − γP π)x ∞ = x − γP πx ∞ ≥ x ∞ − γ Pπx ∞
≥ x ∞−γ x ∞
= (1 − γ) x ∞ > 0

(triangule inequality for norms) (each element of P πx is an average of x)
(γ < 1, x = 0)

which implies I − γP π is full rank.

The following is also a helpful lemma:

7

Lemma 1.6. We have that:
∞
[(1 − γ)(I − γP π)−1](s,a),(s ,a ) = (1 − γ) γtPπ(st = s , at = a |s0 = s, a0 = a)
t=0
so we can view the (s, a)-th row of this matrix as an induced distribution over states and actions when following π after starting with s0 = s and a0 = a.
We leave the proof as an exercise to the reader.

1.1.3 Bellman Optimality Equations

A remarkable and convenient property of MDPs is that there exists a stationary and deterministic policy that simultaneously maximizes V π(s) for all s ∈ S. This is formalized in the following theorem:
Theorem 1.7. Let Π be the set of all non-stationary and randomized policies. Deﬁne:
V (s) := sup V π(s)
π∈Π
Q (s, a) := sup Qπ(s, a).
π∈Π
which is ﬁnite since V π(s) and Qπ(s, a) are bounded between 0 and 1/(1 − γ).
There exists a stationary and deterministic policy π such that for all s ∈ S and a ∈ A,
V π(s) = V (s) Qπ(s, a) = Q (s, a).
We refer to such a π as an optimal policy.

Proof: For any π ∈ Π and for any time t, π speciﬁes a distribution over actions conditioned on the history of observations; here, we write π(At = a|S0 = s0, A0 = a0, R0 = r0, . . . St−1 = st−1, At−1 = at−1, Rt−1 = rt−1, St = st) as the probability that π selects action a at time t given an observed history s0, a0, r0, . . . st−1, at−1, rt−1, st. For the purposes of this proof, it is helpful to formally let St, At and Rt denote random variables, which will distinguish them from outcomes, which is denoted by lower case variables. First, let us show that conditioned on
(S0, A0, R0, S1) = (s, a, r, s ), the maximum future discounted value, from time 1 onwards, is not a function of s, a, r. More precisely, we seek to show that:

∞
sup E γtr(st, at) π, (S0, A0, R0, S1) = (s, a, r, s ) = γV (s )
π∈Π t=1

(0.3)

For any policy π, deﬁne an “offset” policy π(s,a,r), which is the policy that chooses actions on a trajectory τ according to the same distribution that π chooses actions on the trajectory (s, a, r, τ ). Precisely, for all t, deﬁne

π(s,a,r)(At = a|S0 = s0, A0 = a0, R0 = r0, . . . St = st) := π(At+1 = a|S0 = s, A0 = a, R0 = r, S1 = s0, A1 = a0, R1 = r0, . . . St+1 = st).

By the Markov property, we have that:

∞

∞

E

γtr(st, at) π, (S0, A0, R0, S1) = (s, a, r, s ) = γE

γtr(st, at) π(s,a,r), S0 = s = γV (s π(s,a,r) ),

t=1

t=0

8

where the ﬁrst equality follows from a change of variables on the time index, along with the deﬁnition of the policy π(s,a,r). Also, we have that, for all (s, a, r), that the set {π(s,a,r)|π ∈ Π} is equal to Π itself, by the deﬁnition of Π and π(s,a,r). This implies:

∞

sup E

γtr(st, at) π, (S0, A0, R0, S1) = (s, a, r, s ) = γ · sup V (s π(s,a,r) ) = γ · sup V π(s ) = γV (s ),

π∈Π t=1

π∈Π

π∈Π

thus proving Equation 0.3.

We now show the deterministic and stationary policy

π(s) = sup E r(s, a) + γV (s1) (S0, A0) = (s, a)
a∈A

is optimal, i.e. that V π(s) = V (s). For this, we have that:

∞

V (s0) = sup E r(s0, a0) + γtr(st, at)

π∈Π

t=1

∞

(a)
= sup E r(s0, a0) + E

γtr(st, at) π, (S0, A0, R0, S1) = (s0, a0, r0, s1)

π∈Π

t=1

∞

≤ sup E r(s0, a0) + sup E γtr(st, at) π , (S0, A0, R0, S1) = (s0, a0, r0, s1)

π∈Π

π ∈Π t=1

(b)
= sup E r(s0, a0) + γV (s1)
π∈Π

= sup E r(s0, a0) + γV (s1)
a0 ∈A
(=c) E r(s0, a0) + γV (s1) π .

where step (a) uses the law of iterated expectations; step (b) uses Equation 0.3; and step (c) follows from the deﬁnition of π. Applying the same argument recursively leads to:
V (s0) ≤ E r(s0, a0) + γV (s1) π ≤ E r(s0, a0) + γr(s1, a1) + γ2V (s2) π ≤ . . . ≤ V π(s0).

Since V π(s) ≤ supπ∈Π V π(s) = V (s) for all s, we have that V π = V , which completes the proof of the ﬁrst claim.
For the same policy π, an analogous argument can be used prove the second claim.
This shows that we may restrict ourselves to using stationary and deterministic policies without any loss in performance. The following theorem, also due to [Bellman, 1956], gives a precise characterization of the optimal value function.
Theorem 1.8 (Bellman optimality equations). We say that a vector Q ∈ R|S||A| satisﬁes the Bellman optimality equations if:
Q(s, a) = r(s, a) + γEs ∼P (·|s,a) max Q(s , a ) .
a ∈A
For any Q ∈ R|S||A|, we have that Q = Q if and only if Q satisﬁes the Bellman optimality equations. Furthermore, the deterministic policy deﬁned by π(s) ∈ argmaxa∈A Q (s, a) is an optimal policy (where ties are broken in some arbitrary manner).

9

Before we prove this claim, we will provide a few deﬁnitions. Let πQ denote the greedy policy with respect to a vector Q ∈ R|S||A|, i.e
πQ(s) := argmaxa∈A Q(s, a) .
where ties are broken in some arbitrary manner. With this notation, by the above theorem, the optimal policy π is given by:
π = πQ .

Let us also use the following notation to turn a vector Q ∈ R|S||A| into a vector of length |S|.

VQ(s) := max Q(s, a).
a∈A
The Bellman optimality operator TM : R|S||A| → R|S||A| is deﬁned as: T Q := r + γP VQ .

(0.4)

This allows us to rewrite the Bellman optimality equation in the concise form:

Q = T Q,

and, so, the previous theorem states that Q = Q if and only if Q is a ﬁxed point of the operator T .

Proof: Let us begin by showing that:

V (s) = max Q (s, a).

(0.5)

a

Let π be an optimal stationary and deterministic policy, which exists by Theorem 1.7. Consider the policy which

takes action a and then follows π . Due to that V (s) is the maximum value over all non-stationary policies (as shown

in Theorem 1.7),

V (s) ≥ Qπ (s, a) = Q (s, a),

which shows that V (s) ≥ maxa Q (s, a) since a is arbitrary in the above. Also, by Lemma 1.4 and Theorem 1.7,

V (s) = V π (s) = Qπ (s, π (s)) ≤ max Qπ (s, a) = max Q (s, a),

a

a

which proves our claim since we have upper and lower bounded V (s) by maxa Q (s, a).
We ﬁrst show sufﬁciency, i.e. that Q (the state-action value of an optimal policy) satisﬁes Q = T Q . Now for all actions a ∈ A, we have:

Q

(s, a)

=

max Qπ(s, a)
π

=

r(s,

a)

+

γ

max
π

Es

∼P (·|s,a)[V π(s

)]

= r(s, a) + γEs ∼P (·|s,a)[V (s )]

= r(s, a) + γEs ∼P (·|s,a)[max Q (s , a )].
a

Here, the second equality follows from Theorem 1.7 and the ﬁnal equality follows from Equation 0.5. This proves sufﬁciency.

For the converse, suppose Q = T Q for some Q. We now show that Q = Q . Let π = πQ. That Q = T Q implies that Q = r + γP πQ, and so:
Q = (I − γP π)−1r = Qπ

using Corollary 1.5 in the last step. In other words, Q is the action value of the policy πQ. Also, let us show that (P π − P π )Qπ ≥ 0. To see this, observe that:

[(P π − P π )Qπ]s,a = Es ∼P (·|s,a)[Qπ(s , π(s )) − Qπ(s , π (s ))] ≥ 0

10

where the last step uses that π = πQ. Now observe for any other deterministic and stationary policy π :

Q − Qπ

= Qπ − Qπ = Qπ − (I − γP π )−1r = (I − γP π )−1((I − γP π ) − (I − γP π))Qπ = γ(I − γP π )−1(P π − P π )Qπ ≥ 0,

where the last step follows since we have shown (P π − P π )Qπ ≥ 0 and since (1 − γ)(I − γP π )−1 is a matrix with all positive entries (see Lemma 1.6). Thus, Qπ = Q ≥ Qπ for all deterministic and stationary π , which shows that π is an optimal policy. Thus, Q = Qπ = Q , using Theorem 1.7. This completes the proof.

1.2 Finite-Horizon Markov Decision Processes

In some cases, it is natural to work with ﬁnite-horizon (and time-dependent) Markov Decision Processes (see the discussion below). Here, a ﬁnite horizon, time-dependent Markov Decision Process (MDP) M = (S, A, {P }h, {r}h, H, µ) is speciﬁed as follows:

• A state space S, which may be ﬁnite or inﬁnite.
• An action space A, which also may be discrete or inﬁnite.
• A time-dependent transition function Ph : S × A → ∆(S), where ∆(S) is the space of probability distributions over S (i.e., the probability simplex). Ph(s |s, a) is the probability of transitioning into state s upon taking action a in state s at time step h. Note that the time-dependent setting generalizes the stationary setting where all steps share the same transition.
• A time-dependent reward function rh : S × A → [0, 1]. rh(s, a) is the immediate reward associated with taking action a in state s at time step h.
• A integer H which deﬁnes the horizon of the problem.
• An initial state distribution µ ∈ ∆(S), which species how the initial state s0 is generated.

Here, for a policy π, a state s, and h ∈ {0, . . . H − 1}, we deﬁne the value function Vhπ : S → R as

H −1

Vhπ(s) = E

rh(st, at) π, sh = s ,

t=h

where again the expectation is with respect to the randomness of the trajectory, that is, the randomness in state tran-

sitions and the stochasticity of π. Similarly, the state-action value (or Q-value) function Qπh : S × A → R is deﬁned as
H −1

Qπh(s, a) = E

rh(st, at) π, sh = s, ah = a .

t=h

We also use the notation V π(s) = V0π(s).

Again, given a state s, the goal of the agent is to ﬁnd a policy π that maximizes the value, i.e. the optimization problem the agent seeks to solve is:

where recall that V π(s) = V0π(s).

max V π(s)
π

(0.6)

11

Theorem 1.9. (Bellman optimality equations) Deﬁne
Qh(s, a) = sup Qπh(s, a)
π∈Π
where the sup is over all non-stationary and randomized policies. Suppose that QH = 0. We have that Qh = Qh for all h ∈ [H] if and only if for all h ∈ [H],

Qh(s, a) = rh(s, a) + Es ∼Ph(·|s,a)

max Qh+1(s , a )
a ∈A

.

Furthermore, π(s, h) = argmaxa∈A Qh(s, a) is an optimal policy.

We leave the proof as an exercise to the reader.

(0.7)

Discussion: Stationary MDPs vs Time-Dependent MDPs For the purposes of this book, it is natural for us to study both of these models, where we typically assume stationary dynamics in the inﬁnite horizon setting and timedependent dynamics in the ﬁnite-horizon setting. From a theoretical perspective, the ﬁnite horizon, time-dependent setting is often more amenable to analysis, where optimal statistical rates often require simpler arguments. However, we should note that from a practical perspective, time-dependent MDPs are rarely utilized because they lead to policies and value functions that are O(H) larger (to store in memory) than those in the stationary setting. In practice, we often incorporate temporal information directly into the deﬁnition of the state, which leads to more compact value functions and policies (when coupled with function approximation methods, which attempt to represent both the values and policies in a more compact form).

1.3 Computational Complexity
This section will be concerned with computing an optimal policy, when the MDP M = (S, A, P, r, γ) is known; this can be thought of as the solving the planning problem. While much of this book is concerned with statistical limits, understanding the computational limits can be informative. We will consider algorithms which give both exact and approximately optimal policies. In particular, we will be interested in polynomial time (and strongly polynomial time) algorithms.
Suppose that (P, r, γ) in our MDP M is speciﬁed with rational entries. Let L(P, r, γ) denote the total bit-size required to specify M , and assume that basic arithmetic operations +, −, ×, ÷ take unit time. Here, we may hope for an algorithm which (exactly) returns an optimal policy whose runtime is polynomial in L(P, r, γ) and the number of states and actions.
More generally, it may also be helpful to understand which algorithms are strongly polynomial. Here, we do not want to explicitly restrict (P, r, γ) to be speciﬁed by rationals. An algorithm is said to be strongly polynomial if it returns an optimal policy with runtime that is polynomial in only the number of states and actions (with no dependence on L(P, r, γ)).
The ﬁrst two subsections will cover classical iterative algorithms that compute Q , and then we cover the linear programming approach.
1.3.1 Value Iteration
Perhaps the simplest algorithm for discounted MDPs is to iteratively apply the ﬁxed point mapping: starting at some Q, we iteratively apply T :
Q ← TQ,
12

Poly? Strongly Poly?

Value Iteration

|S

|2|A|

L(P,r,γ) log 1−γ

1 1−γ



Policy Iteration

(|S |3

+

|S |2 |A|)

L(P,r,γ) log 1−γ

1 1−γ

(|S|3 + |S|2|A|) · min

, |A||S|

|S |2 |A|

log

|S|2 1−γ

|S |

1−γ

LP-Algorithms |S|3|A|L(P, r, γ)

|S |4 |A|4

log

|S | 1−γ

Table 0.1: Computational complexities of various approaches (we drop universal constants). Polynomial time algorithms depend on the bit complexity, L(P, r, γ), while strongly polynomial algorithms do not. Note that only for a ﬁxed value of γ are value and policy iteration polynomial time algorithms; otherwise, they are not polynomial time algorithms. Similarly, only for a ﬁxed value of γ is policy iteration a strongly polynomial time algorithm. In contrast,
the LP-approach leads to both polynomial time and strongly polynomial time algorithms; for the latter, the approach is an interior point algorithm. See text for further discussion, and Section 1.6 for references. Here, |S|2|A| is the assumed runtime per iteration of value iteration, and |S|3 + |S|2|A| is the assumed runtime per iteration of policy iteration (note that for this complexity we would directly update the values V rather than Q values, as described in the
text); these runtimes are consistent with assuming cubic complexity for linear system solving.

This is algorithm is referred to as Q-value iteration. Lemma 1.10. (contraction) For any two vectors Q, Q ∈ R|S||A|,
TQ−TQ ∞ ≤γ Q−Q ∞

Proof: First, let us show that for all s ∈ S, |VQ(s)−VQ (s)| ≤ maxa∈A |Q(s, a)−Q (s, a)|. Assume VQ(s) > VQ (s) (the other direction is symmetric), and let a be the greedy action for Q at s. Then

|VQ(s) − VQ (s)| = Q(s, a) − max Q (s, a ) ≤ Q(s, a) − Q (s, a) ≤ max |Q(s, a) − Q (s, a)|.

a ∈A

a∈A

Using this,

T Q − T Q ∞ = γ P VQ − P VQ ∞

= γ P (VQ − VQ ) ∞

≤ γ VQ − VQ ∞

=

γ

max
s

|VQ(s)

−

VQ

(s)|

≤ γ max max |Q(s, a) − Q (s, a)|
sa

= γ Q−Q ∞

where the ﬁrst inequality uses that each element of P (VQ − VQ ) is a convex average of VQ − VQ and the second inequality uses our claim above.
The following result bounds the sub-optimality of the greedy policy itself, based on the error in Q-value function.

Lemma 1.11. (Q-Error Ampliﬁcation) For any vector Q ∈ R|S||A|,

V πQ ≥ V

−

2

Q−Q 1−γ

∞ 1,

where 1 denotes the vector of all ones.

13

Proof: Fix state s and let a = πQ(s). We have:

V (s) − V πQ (s) = Q (s, π (s)) − QπQ (s, a) = Q (s, π (s)) − Q (s, a) + Q (s, a) − QπQ (s, a) = Q (s, π (s)) − Q (s, a) + γEs ∼P (·|s,a)[V (s ) − V πQ (s )] ≤ Q (s, π (s)) − Q(s, π (s)) + Q(s, a) − Q (s, a) +γEs ∼P (s,a)[V (s ) − V πQ (s )] ≤ 2 Q − Q ∞ + γ V − V πQ ∞.

where the ﬁrst inequality uses Q(s, π (s)) ≤ Q(s, πQ(s)) = Q(s, a) due to the deﬁnition of πQ. Theorem 1.12. (Q-value iteration convergence). Set Q(0) = 0. For k = 0, 1, . . ., suppose:

Q(k+1) = T Q(k)

Let π(k)

=

πQ(k) .

For k

≥

log

2 (1−γ)2

1−γ

,

V π(k) ≥ V − 1 .

Proof: Since Q ∞ ≤ 1/(1 − γ), Q(k) = T kQ(0) and Q = T Q , Lemma 1.10 gives

Q(k) − Q

∞ = T kQ(0) − T kQ

∞ ≤ γk Q(0) − Q

∞ = (1 − (1 − γ))k Q

∞

≤

exp(−(1 − 1−γ

γ)k)

.

The proof is completed with our choice of k and using Lemma 1.11.

Iteration complexity for an exact solution. With regards to computing an exact optimal policy, when the gap between the current objective value and the optimal objective value is smaller than 2−L(P,r,γ), then the greedy policy will be optimal. This leads to claimed complexity in Table 0.1. Value iteration is not strongly polynomial algorithm due to that, in ﬁnite time, it may never return the optimal policy.

1.3.2 Policy Iteration

The policy iteration algorithm, for discounted MDPs, starts from an arbitrary policy π0, and repeats the following iterative procedure: for k = 0, 1, 2, . . .

1. Policy evaluation. Compute Qπk 2. Policy improvement. Update the policy:

πk+1 = πQπk

In each iteration, we compute the Q-value function of πk, using the analytical form given in Equation 0.2, and update the policy to be greedy with respect to this new Q-value. The ﬁrst step is often called policy evaluation, and the second step is often called policy improvement.
Lemma 1.13. We have that:

1. Qπk+1 ≥ T Qπk ≥ Qπk

14

2. Qπk+1 − Q ∞ ≤ γ Qπk − Q ∞
Proof: First let us show that T Qπk ≥ Qπk . Note that the policies produced in policy iteration are always deterministic, so V πk (s) = Qπk (s, πk(s)) for all iterations k and states s. Hence,
T Qπk (s, a) = r(s, a) + γEs ∼P (·|s,a)[max Qπk (s , a )]
a
≥ r(s, a) + γEs ∼P (·|s,a)[Qπk (s , πk(s ))] = Qπk (s, a).

Now let us prove that Qπk+1 ≥ T Qπk . First, let us see that Qπk+1 ≥ Qπk :

∞

Qπk = r + γP πk Qπk ≤ r + γP πk+1 Qπk ≤

γt(P πk+1 )tr = Qπk+1 .

t=0

where we have used that πk+1 is the greedy policy in the ﬁrst inequality and recursion in the second inequality. Using this,

Qπk+1 (s, a) = r(s, a) + γEs ∼P (·|s,a)[Qπk+1 (s , πk+1(s ))] ≥ r(s, a) + γEs ∼P (·|s,a)[Qπk (s , πk+1(s ))] = r(s, a) + γEs ∼P (·|s,a)[max Qπk (s , a )] = T Qπk (s, a)
a

which completes the proof of the ﬁrst claim.

For the second claim,

Q − Qπk+1 ∞ ≤ Q − T Qπk ∞ = T Q − T Qπk ∞ ≤ γ Q − Qπk ∞

where we have used that Q ≥ Qπk+1 ≥ T Qπk in second step and the contraction property of T (see Lemma 1.10) in the last step.

With this lemma, a convergence rate for the policy iteration algorithm immediately follows.

Theorem 1.14.

(Policy iteration convergence).

Let π0

be any initial policy.

For k

≥

log

1 (1−γ)

1−γ

, the k-th policy in

policy iteration has the following performance bound:

Qπk ≥ Q − 1 .

Iteration complexity for an exact solution. With regards to computing an exact optimal policy, it is clear from the

previous results that policy iteration is no worse than value iteration. However, with regards to obtaining an exact

solution MDP that is independent of the bit complexity, L(P, r, γ), improvements are possible (and where we assume

basic arithmetic operations on real numbers are order one cost). Naively, the number of iterations of policy iterations is bounded by the number of policies, namely |A||S|; here, a small improvement is possible, where the number of

iterations of policy iteration can be bounded by

|A||S| |S |

.

Remarkably, for a ﬁxed value of γ, policy iteration can be

show

to

be

a

strongly

polynomial

time

algorithm,

where

policy

iteration

ﬁnds

an

exact

policy

in

at

most

|S|2|A| log 1−γ

|S|2 1−γ

iterations. See Table 0.1 for a summary, and Section 1.6 for references.

15

1.3.3 Value Iteration for Finite Horizon MDPs

Let us now specify the value iteration algorithm for ﬁnite-horizon MDPs. For the ﬁnize-horizon setting, it turns out that the analogues of value iteration and policy iteration lead to identical algorithms. The value iteration algorithm is speciﬁed as follows:

1. Set QH−1(s, a) = rH−1(s, a). 2. For h = H − 2, . . . 0, set:

Qh(s, a) = rh(s, a) + γEs ∼Ph(·|s,a)

max Qh+1(s , a )
a ∈A

.

By Theorem 1.9, it follows that Qh(s, a) = Qh(s, a) and that π(s, h) = argmaxa∈A Qh(s, a) is an optimal policy.

1.3.4 The Linear Programming Approach

It is helpful to understand an alternative approach to ﬁnding an optimal policy for a known MDP. With regards to

computation, consider the setting where our MDP M = (S, A, P, r, γ, µ) is known and P , r, and γ are all speciﬁed by

rational numbers. Here, from a computational perspective, the previous iterative algorithms are, strictly speaking, not

polynomial time algorithms, due to that they depend polynomially on 1/(1 − γ), which is not polynomial in the de-

scription

length

of

the

MDP

.

In

particular,

note

that

any

rational

value

of

1−γ

may

be

speciﬁed

with

only

O(log

1 1−γ

)

bits of precision. In this context, we may hope for a fully polynomial time algorithm, when given knowledge of the

MDP, which would have a computation time which would depend polynomially on the description length of the MDP

M , when the parameters are speciﬁed as rational numbers. We now see that the LP approach provides a polynomial

time algorithm.

The Primal LP and A Polynomial Time Algorithm

Consider the following optimization problem with variables V ∈ R|S|:

min subject to

µ(s)V (s)
s
V (s) ≥ r(s, a) + γ P (s |s, a)V (s )
s

∀a ∈ A, s ∈ S

Provided that µ has full support, then the optimal value function V (s) is the unique solution to this linear program. With regards to computation time, linear programming approaches only depend on the description length of the coefﬁcients in the program, due to that this determines the computational complexity of basic additions and multiplications. Thus, this approach will only depend on the bit length description of the MDP, when the MDP is speciﬁed by rational numbers.

Computational complexity for an exact solution. Table 0.1 shows the runtime complexity for the LP approach, where we assume a standard runtime for solving a linear program. The strongly polynomial algorithm is an interior point algorithm. See Section 1.6 for references.
16

Policy iteration and the simplex algorithm. It turns out that the policy iteration algorithm is actually the simplex method with block pivot. While the simplex method, in general, is not a strongly polynomial time algorithm, the policy iteration algorithm is a strongly polynomial time algorithm, provided we keep the discount factor ﬁxed. See [Ye, 2011].

The Dual LP and the State-Action Polytope

For a ﬁxed (possibly stochastic) policy π, let us deﬁne a visitation measure over states and actions induced by following π after starting at s0. Precisely, deﬁne this distribution, dπs0 , as follows:

∞
dπs0 (s, a) := (1 − γ) γtPrπ(st = s, at = a|s0)
t=0

(0.8)

where Prπ(st = s, at = a|s0) is the probability that st = s and at = a, after starting at state s0 and following π thereafter. It is straightforward to verify that dπs0 is a distribution over S × A. We also overload notation and write:
dπµ(s, a) = Es0∼µ dπs0 (s, a) .
for a distribution µ over S. Recall Lemma 1.6 provides a way to easily compute dπµ(s, a) through an appropriate vector-matrix multiplication.

It is straightforward to verify that dπµ satisﬁes, for all states s ∈ S:

dπµ(s, a) = (1 − γ)µ(s) + γ P (s|s , a )dπµ(s , a ).

a

s ,a

Let us deﬁne the state-action polytope as follows:

Kµ := {d| d ≥ 0 and d(s, a) = (1 − γ)µ(s) + γ P (s|s , a )d(s , a )}

a

s ,a

We now see that this set precisely characterizes all state-action visitation distributions.

Proposition 1.15. We have that Kµ is equal to the set of all feasible state-action distributions, i.e. d ∈ Kµ if and only if there exists a stationary (and possibly randomized) policy π such that dπµ = d.

With respect the variables d ∈ R|S|·|A|, the dual LP formulation is as follows:

max subject to

1 1 − γ dµ(s, a)r(s, a)
s,a
d ∈ Kµ

Note that Kµ is itself a polytope, and one can verify that this is indeed the dual of the aforementioned LP. This approach provides an alternative approach to ﬁnding an optimal solution.

If d is the solution to this LP, and provided that µ has full support, then we have that:

π (a|s) =

d (s, a) ,

a d (s, a )

is an optimal policy. An alternative optimal policy is argmaxa d (s, a) (and these policies are identical if the optimal policy is unique).

17

1.4 Sample Complexity and Sampling Models
Much of reinforcement learning is concerned with ﬁnding a near optimal policy (or obtaining near optimal reward) in settings where the MDPs is not known to the learner. We will study these questions in a few different models of how the agent obtains information about the unknown underlying MDP. In each of these settings, we are interested understanding the number of samples required to ﬁnd a near optimal policy, i.e. the sample complexity. Ultimately, we interested in obtaining results which are applicable to cases where number of states and actions is large (or, possibly, countably or uncountably inﬁnite). This is many ways analogous to the supervised learning question of generalization, though, as we shall see, this question is fundamentally more challenging in the reinforcement learning setting.
The Episodic Setting. In the episodic setting, in every episode, the learner acts for some ﬁnite number of steps, starting from a ﬁxed starting state s0 ∼ µ, the learner observes the trajectory, and the state resets to s0 ∼ µ. This episodic model of feedback is applicable to both the ﬁnte-horizon and inﬁnite horizon settings.
• (Finite Horizon MDPs) Here, each episode lasts for H-steps, and then the state is reset to s0 ∼ µ.
• (Inﬁnite Horizon MDPs) Even for inﬁnite horizon MDPs it is natural to work in an episodic model for learning, where each episode terminates after a ﬁnite number of steps. Here, it is often natural to assume either the agent can terminate the episode at will or that the episode will terminate at each step with probability 1 − γ. After termination, we again assume that the state is reset to s0 ∼ µ. Note that, if each step in an episode is terminated with probability 1 − γ, then the observed cumulative reward in an episode of a policy provides an unbiased estimate of the inﬁnite-horizon, discounted value of that policy.
In this setting, we are often interested in either the number of episodes it takes to ﬁnd a near optimal policy, which is a PAC (probably, approximately correct) guarantee, or we are interested in a regret guarantee (which we will study in Chapter 7). Both of these questions are with regards to statistical complexity (i.e. the sample complexity) of learning.
The episodic setting is challenging in that the agent has to engage in some exploration in order to gain information at the relevant state. As we shall see in Chapter 7, this exploration must be strategic, in the sense that simply behaving randomly will not lead to information being gathered quickly enough. It is often helpful to study the statistical complexity of learning in a more abstract sampling model, a generative model, which allows to avoid having to directly address this exploration issue. Furthermore, this sampling model is natural in its own right.
The generative model setting. A generative model takes as input a state action pair (s, a) and returns a sample s ∼ P (·|s, a) and the reward r(s, a) (or a sample of the reward if the rewards are stochastic).
The ofﬂine RL setting. The ofﬂine RL setting is where the agent has access to an ofﬂine dataset, say generated under some policy (or a collection of policies). In the simplest of these settings, we may assume our dataset is of the form {(s, a, s , r)} where r is the reward (corresponding to r(s, a) if the reward is deterministic) and s ∼ P (·|s, a). Furthermore, for simplicity, it can be helpful to assume that the s, a pairs in this dataset were sampled i.i.d. from some ﬁxed distribution ν over S × A.
1.5 Bonus: Advantages and The Performance Difference Lemma
Throughout, we will overload notation where, for a distribution µ over S, we write:
V π(µ) = Es∼µ [V π(s)] .
18

The advantage Aπ(s, a) of a policy π is deﬁned as Aπ(s, a) := Qπ(s, a) − V π(s) .

Note that: for all state-action pairs.

A∗(s, a) := Aπ∗ (s, a) ≤ 0

Analogous to the state-action visitation distribution (see Equation 0.8), we can deﬁne a visitation measure over just the states. When clear from context, we will overload notation and also denote this distribution by dπs0 , where:

∞
dπs0 (s) = (1 − γ) γtPrπ(st = s|s0).
t=0

(0.9)

Here, Prπ(st = s|s0) is the state visitation probability, under π starting at state s0. Again, we write:

dπµ(s) = Es0∼µ dπs0 (s) .

for a distribution µ over S.

The following lemma is helpful in the analysis of RL algorithms.

Lemma 1.16. (The performance difference lemma) For all policies π, π and distributions µ over S,

V π(µ) − V π

(µ)

=

1 1 − γ Es

∼dπµ Ea

∼π(·|s

)

Aπ (s , a )

.

Proof: Let Prπ(τ |s0 = s) denote the probability of observing a trajectory τ when starting in state s and following the policy π. By deﬁnition of dπs0θ , observe that for any function f : S × A → R,

Eτ ∼Prπ

∞
γtf (st, at)

1 = 1 − γ Es∼dπs0θ Ea∼πθ(·|s) f (s, a) .

t=0

(0.10)

Using a telescoping argument, we have:

∞

V π(s) − V π (s) = Eτ∼Prπ(τ|s0=s)

γtr(st, at) − V π (s)

t=0

∞

= Eτ ∼Prπ(τ |s0=s)

γt r(st, at) + V π (st) − V π (st) − V π (s)

t=0

∞

(a)
= Eτ ∼Prπ(τ |s0=s)

γt r(st, at) + γV π (st+1) − V π (st)

t=0

∞

(b)
= Eτ ∼Prπ(τ |s0=s)

γt r(st, at) + γE[V π (st+1)|st, at] − V π (st)

t=0

∞

(c)
= Eτ ∼Prπ(τ |s0=s)

γt Qπ (st, at) − V π (st)

t=0

∞

= Eτ ∼Prπ(τ |s0=s)

γtAπ (st, at)

t=0

=

1

1 −

γ Es

∼dπs

Ea∼π(·|s)Aπ

(s

, a),

where step (a) rearranges terms in the summation via telescoping; step (b) uses the law of iterated expectations; step (c) follows by deﬁnition; and the ﬁnal equality follows from Equation 0.10.

19

1.6 Bibliographic Remarks and Further Reading

We refer the reader to [Puterman, 1994] for a more detailed treatment of dynamic programming and MDPs. [Puterman, 1994] also contains a thorough treatment of the dual LP, along with a proof of Lemma 1.15

With regards to the computational complexity of policy iteration, [Ye, 2011] showed that policy iteration is a strongly

polynomial time algorithm for a ﬁxed discount rate 1. Also, see [Ye, 2011] for a good summary of the computa-

tional complexities of various approaches. [Mansour and Singh, 1999] showed that the number of iterations of policy

iteration

can

be

bounded

by

|A||S| |S |

.

With regards to a strongly polynomial algorithm, the CIPA algorithm [Ye, 2005] is an interior point algorithm with the claimed runtime in Table 0.1.

Lemma 1.11 is due to Singh and Yee [1994].

The performance difference lemma is due to [Kakade and Langford, 2002, Kakade, 2003], though the lemma was implicit in the analysis of a number of prior works.

1The stated strongly polynomial runtime in Table 0.1 for policy iteration differs from that in [Ye, 2011] due to we assume that the runtime per iteration of policy iteration is |S|3 + |S|2|A|.
20

Chapter 2
Sample Complexity with a Generative Model
This chapter begins our study of the sample complexity, where we focus on the (minmax) number of transitions we need to observe in order to accurately estimate Q or in order to ﬁnd a near optimal policy. We assume that we have access to a generative model (as deﬁned in Section 1.4) and that the reward function is deterministic (the latter is often a mild assumption, due to that much of the difﬁculty in RL is due to the uncertainty in the transition model P ). This chapter follows the results due to [Azar et al., 2013], along with some improved rates due to [Agarwal et al., 2020c]. One of the key observations in this chapter is that we can ﬁnd a near optimal policy using a number of observed transitions that is sublinear in the model size, i.e. use a number of samples that is smaller than O(|S|2|A|). In other words, we do not need to learn an accurate model of the world in order to learn to act near optimally.
Notation. We deﬁne M to be the empirical MDP that is identical to the original M , except that it uses P instead of P for the transition model. When clear from context, we drop the subscript on M on the values, action values (and one-step variances and variances which we deﬁne later). We let V π, Qπ, Q , and π denote the value function, state-action value function, optimal state-action value, and optimal policy in M , respectively.
2.1 Warmup: a naive model-based approach
A central question in this chapter is: Do we require an accurate model of the world in order to ﬁnd a near optimal policy? Recall that a generative model takes as input a state action pair (s, a) and returns a sample s ∼ P (·|s, a) and the reward r(s, a) (or a sample of the reward if the rewards are stochastic). Let us consider the most naive approach to learning (when we have access to a generative model): suppose we call our simulator N times at each state action pair. Let P be our empirical model, deﬁned as follows:
P (s |s, a) = count(s , s, a) N
where count(s , s, a) is the number of times the state-action pair (s, a) transitions to state s . As the N is the number of calls for each state action pair, the total number of calls to our generative model is |S||A|N . As before, we can view P as a matrix of size |S||A| × |S|.
21

Note that since P has a |S|2|A| parameters, we would expect that observing O(|S|2|A|) transitions is sufﬁcient to provide us with an accurate model. The following proposition shows that this is the case.

Proposition 2.1. There exists an absolute constant c such that the following holds. Suppose

∈

0,

1 1−γ

obtain

# samples from generative model

= |S||A|N

≥

γ |S|2|A| log(c|S||A|/δ)

(1 − γ)4

2

and that we

where we uniformly sample every state action pair. Then, with probability greater than 1 − δ, we have:

• (Model accuracy) The transition model has error bounded as:

max
s,a

P (·|s, a) − P (·|s, a)

1 ≤ (1 − γ)2

.

• (Uniform value accuracy) For all policies π, Qπ − Qπ ∞ ≤

• (Near optimal planning) Suppose that π is the optimal policy in M . We have that: Q − Q ∞ ≤ , and Qπ − Q ∞ ≤ 2 .

Before we provide the proof, the following lemmas will be helpful throughout: Lemma 2.2. (Simulation Lemma) For all π we have that:
Qπ − Qπ = γ(I − γP π)−1(P − P )V π

Proof: Using our matrix equality for Qπ (see Equation 0.2), we have:
Qπ − Qπ = (I − γP π)−1r − (I − γP π)−1r = (I − γP π)−1((I − γP π) − (I − γP π))Qπ = γ(I − γP π)−1(P π − P π)Qπ = γ(I − γP π)−1(P − P )V π

which proves the claim. Lemma 2.3. For any policy π, MDP M and vector v ∈ R|S|×|A|, we have (I − γP π)−1v ∞ ≤ v ∞ /(1 − γ).

Proof: Note that v = (I − γP π)(I − γP π)−1v = (I − γP π)w, where w = (I − γP π)−1v. By triangle inequality, we have
v ∞ = (I − γP π)w ∞ ≥ w ∞ − γ P πw ∞ ≥ w ∞ − γ w ∞ ,
where the ﬁnal inequality follows since P πw is an average of the elements of w by the deﬁnition of P π so that P πw ∞ ≤ w ∞. Rearranging terms completes the proof.
Now we are ready to complete the proof of our proposition.
Proof: Using the concentration of a distribution in the 1 norm (Lemma A.8), we have that for a ﬁxed s, a that, with probability greater than 1 − δ, we have:

P (·|s, a) − P (·|s, a) 1 ≤ c

|S| log(1/δ) m

22

where m is the number of samples used to estimate P (·|s, a). The ﬁrst claim now follows by the union bound (and redeﬁning δ and c appropriately).

For the second claim, we have that:

Qπ − Qπ ∞ =

γ(I − γP π)−1(P

− P )V π

∞≤

γ 1−γ

(P − P )V π ∞

≤

γ 1−γ

max P (·|s, a) − P (·|s, a) 1
s,a

Vπ

∞

≤

γ (1 − γ)2

max
s,a

P (·|s, a) − P (·|s, a)

1

where the penultimate step uses Holder’s inequality. The second claim now follows.
For the ﬁnal claim, ﬁrst observe that | supx f (x) − supx g(x)| ≤ supx |f (x) − g(x)|, where f and g are real valued functions. This implies:

|Q (s, a) − Q (s, a)| = | sup Qπ(s, a) − sup Qπ(s, a)| ≤ sup |Qπ(s, a) − Qπ(s, a)| ≤

π

π

π

which proves the ﬁrst inequality. The second inequality is left as an exercise to the reader.

2.2 Sublinear Sample Complexity

In the previous approach, we are able to accurately estimate the value of every policy in the unknown MDP M . However, with regards to planning, we only need an accurate estimate Q of Q , which we may hope would require less samples. Let us now see that the model based approach can be reﬁned to obtain minmax optimal sample complexity, which we will see is sublinear in the model size.
We will state our results in terms of N , and recall that N is the # of calls to the generative models per state-action pair, so that:
# samples from generative model = |S||A|N.

Let us start with a crude bound on the optimal action-values, which provides a sublinear rate. In the next section, we will improve upon this to obtain the minmax optimal rate.

Proposition 2.4. (Crude Value Bounds) Let δ ≥ 0. With probability greater than 1 − δ,

Q − Q ∞ ≤ ∆δ,N Q − Qπ ∞ ≤ ∆δ,N ,

where:

γ

2 log(2|S||A|/δ)

∆δ,N := (1 − γ)2

N

Note that the ﬁrst inequality above shows a sublinear rate on estimating the value function. Ultimately, we are interested in the value V π when we execute π , not just an estimate Q of Q . Here, by Lemma 1.11, we lose an

additional horizon factor and have:

Q − Qπ

∞

≤

1 1−

γ ∆δ,N .

As we see in Theorem 2.6, this is improvable.

Before we provide the proof, the following lemma will be helpful throughout.

23

Lemma 2.5. (Component-wise Bounds) We have that:
Q − Q ≤ γ(I − γP π )−1(P − P )V Q − Q ≥ γ(I − γP π )−1(P − P )V

Proof: For the ﬁrst claim, the optimality of π in M implies: Q − Q = Qπ − Qπ ≤ Qπ − Qπ = γ(I − γP π )−1(P − P )V ,

where we have used Lemma 2.2 in the ﬁnal step. This proves the ﬁrst claim.

For the second claim,

Q −Q

= Qπ − Qπ = (1 − γ) (I − γP π )−1r − (I − γP π )−1r

= (I − γP π )−1((I − γP π ) − (I − γP π ))Q = γ(I − γP π )−1(P π − P π )Q ≥ γ(I − γP π )−1(P π − P π )Q = γ(I − γP π )−1(P − P )V ,

where the inequality follows from P π Q ≤ P π Q , due to the optimality of π . This proves the second claim.

Proof: Following from the simulation lemma (Lemma 2.2) and Lemma 2.3, we have:

Q − Qπ

∞

≤

1

γ −

γ

(P − P )V

∞.

Also, the previous lemma, implies that:

Q −Q

∞

≤

1

γ −

γ

(P − P )V

∞

By applying Hoeffding’s inequality and the union bound,

(P − P )V

∞

=

max
s,a

|Es

∼P (·|s,a)[V

(s )] − Es ∼P (·|s,a)[V

(s

)]|

≤

1

1 −

γ

2 log(2|S||A|/δ) N

which holds with probability greater than 1 − δ. This completes the proof.

2.3 Minmax Optimal Sample Complexity (and the Model Based Approach)
We now see that the model based approach is minmax optimal, for both the discounted case and the ﬁnite horizon setting.
2.3.1 The Discounted Case
Upper bounds. The following theorem reﬁnes our crude bound on Q . Theorem 2.6. For δ ≥ 0 and for an appropriately chosen absolute constant c, we have that:
24

• (Value estimation) With probability greater than 1 − δ,

Q −Q

∞≤γ

c log(c|S||A|/δ)

cγ log(c|S||A|/δ)

(1 − γ)3

N

+ (1 − γ)3

N

.

•

(Sub-optimality) If N

≥

1 (1−γ)2

,

then

with

probability

greater

than

1

−

δ,

Q − Qπ

∞≤γ

c log(c|S||A|/δ)

(1 − γ)3

N

.

This immediately provides the following corollary.

Corollary 2.7. Provided that ≤ 1 and that

# samples from generative model

= |S||A|N

≥

c|S||A| log(c|S||A|/δ)

(1 − γ)3

2

,

then with probability greater than 1 − δ,

Q −Q ∞ ≤ .

Furthermore, provided that ≤

1 1−γ

and

that

# samples from generative model

= |S||A|N

≥

c|S||A| log(c|S||A|/δ)

(1 − γ)3

2

,

then with probability greater than 1 − δ,

Q − Qπ ∞ ≤ .

We only prove the ﬁrst claim in Theorem 2.6 on the estimation accuracy. With regards to the sub-optimality, note that Theorem 1.11 already implies a sub-optimality gap, though with an ampliﬁcation of the estimation error by 2/(1 − γ). The argument for the improvement provided in the second claim is more involved (See Section 2.6 for further discussion).

Lower Bounds. Let us say that an estimation algorithm A, which is a map from samples to an estimate Q , is ( , δ)-good on MDP M if Q − Q ∞ ≤ holds with probability greater than 1 − δ.
Theorem 2.8. There exists 0, δ0, c and a set of MDPs M such that for ∈ (0, 0) and δ ∈ (0, δ0) if algorithm A is ( , δ)-good on all M ∈ M, then A must use a number of samples that is lower bounded as follows

# samples from generative model

≥

c |S||A| log(c|S||A|/δ)

(1 − γ)3

2

.

In other words, this theorem shows that the model based approach minmax optimal.

2.3.2 Finite Horizon Setting
Recall the setting of ﬁnite horizon MDPs deﬁned in Section 1.2. Again, we can consider the most naive approach to learning (when we have access to a generative model): suppose we call our simulator N times for every (s, a, h) ∈ S × A × [H], i.e. we obtain N i.i.d. samples where s ∼ Ph(·|s, a), for every (s, a, h) ∈ S × A × [H]. Note that the total number of observed transitions is H|S||A|N .
25

Upper bounds. The following theorem provides an upper bound on the model based approach. Theorem 2.9. For δ ≥ 0 and with probability greater than 1 − δ, we have that:

• (Value estimation) • (Sub-optimality)

Q0 − Q0 ∞ ≤ cH

log(c|S ||A|/δ )

log(c|S ||A|/δ )

+ cH

,

N

N

Q0 − Qπ0

∞ ≤ cH

log(c|S ||A|/δ )

log(c|S ||A|/δ )

+ cH

,

N

N

where c is an absolute constant.

Note that the above bound requires N to be O(H2) in order to achieve an -optimal policy, while in the discounted case, we require N to be O(1/(1 − γ)3) for the same guarantee. While this may seem like an improvement by a horizon factor, recall that for the ﬁnite horizon case, N corresponds to observing O(H) more transitions than in the
discounted case.

Lower Bounds. In the minmax sense of Theorem 2.8, the previous upper bound provided by the model based approach for the ﬁnite horizon setting achieves the minmax optimal sample complexity.

2.4 Analysis
We now prove (the ﬁrst claim in) Theorem 2.6.

2.4.1 Variance Lemmas

The key to the sharper analysis is to more sharply characterize the variance in our estimates. Denote the variance of any real valued f under a distribution D as:

VarD(f ) := Ex∼D[f (x)2] − (Ex∼D[f (x)])2

Slightly abusing the notation, for V ∈ R|S|, we deﬁne the vector VarP (V ) ∈ R|S||A| as:

VarP (V )(s, a) := VarP (·|s,a)(V )

Equivalently,

VarP (V ) = P (V )2 − (P V )2 .

Now we characterize a relevant deviation in terms of the its variance. Lemma 2.10. Let δ > 0. With probability greater than 1 − δ,

|(P − P )V | ≤

2 log(2|S||A|/δ)

1 2 log(2|S||A|/δ)

N

VarP (V ) + 1 − γ

3N

1.

26

Proof: The claims follows from Bernstein’s inequality along with a union bound over all state-action pairs.

The key ideas in the proof are in how we bound (I − γP π )−1 VarP (V ) ∞ and (I − γP π )−1 VarP (V ) ∞.

It is helpful to deﬁne ΣπM as the variance of the discounted reward, i.e.


∞

ΣπM (s, a) := E 

γtr(st, at) − QπM (s, a)

t=0

2



s0 = s, a0 = a

where the expectation is induced under the trajectories induced by π in M . It is straightforward to verify that ΣπM ∞ ≤ γ2/(1 − γ)2.
The following lemma shows that ΣπM satisﬁes a Bellman consistency condition. Lemma 2.11. (Bellman consistency of Σ) For any MDP M ,

ΣπM = γ2VarP (VMπ ) + γ2P πΣπM where P is the transition model in MDP M .

(0.1)

The proof is left as an exercise to the reader. Lemma 2.12. (Weighted Sum of Deviations) For any policy π and MDP M ,

(I − γP π)−1

VarP (VMπ )

≤
∞

2 (1 − γ)3 ,

where P is the transition model of M .

Proof:Note that (1 − γ)(I a distribution ν (where ν is

− γP π)−1 is vector of the

matrix whose rows are a probability distribution. same dimension of v), Jensen’s inequality implies

Fthoartaνp·o√sivtiv≤e

√veνct·ovr .vTahnids

implies:

(I

−

γP

π

)−1

√ v

∞

=

1 1−γ

(1

−

γ)(I

−

γP

π

)−1

√ v

∞

≤

1

1 −

γ

(I

−

γP

π )−1 v

∞

≤

1

2 −

γ

(I

−

γ2P

π )−1 v

.
∞

where we have used that (I − γP π)−1v ∞ ≤ 2 (I − γ2P π)−1v ∞ (which we will prove shortly). The proof is completed as follows: by Equation 0.1, ΣπM = γ2(I − γ2P π)−1VarP (VMπ ), so taking v = VarP (VMπ ) and using that ΣπM ∞ ≤ γ2/(1 − γ)2 completes the proof.

Finally, to see that (I − γP π)−1v ∞ ≤ 2 (I − γ2P π)−1v ∞, observe:

(I − γP π)−1v ∞ = (I − γP π)−1(I − γ2P π)(I − γ2P π)−1v ∞

= (I − γP π)−1 (1 − γ)I + γ(I − γP π) (I − γ2P π)−1v ∞

= (1 − γ)(I − γP π)−1 + γI (I − γ2P π)−1v ∞

≤ (1 − γ) (I − γP π)−1(I − γ2P π)−1v ∞ + γ (I − γ2P π)−1v ∞

≤

1−γ 1−γ

(I − γ2P π)−1v

∞+γ

(I − γ2P π)−1v

∞

≤ 2 (I − γ2P π)−1v ∞

which proves the claim.

27

2.4.2 Completing the proof

Lemma 2.13. Let δ ≥ 0. With probability greater than 1 − δ, we have:

VarP (V ) ≤ 2VarP (V π ) + ∆δ,N 1 VarP (V ) ≤ 2VarP (V ) + ∆δ,N 1

where

1

18 log(6|S||A|/δ)

1 4 log(6|S||A|/δ)

∆δ,N := (1 − γ)2

N

+ (1 − γ)4

N

.

Proof: By deﬁnition,

VarP (V ) = VarP (V ) − VarP (V ) + VarP (V ) = P (V )2 − (P V )2 − P (V )2 + (P V )2 + VarP (V ) = (P − P )(V )2 − (P V )2 − (P V )2 + VarP (V )

Now we bound each of these terms with Hoeffding’s inequality and the union bound. For the ﬁrst term, with probability greater than 1 − δ,

(P − P )(V

)2

∞

≤

1 (1 − γ)2

2 log(2|S||A|/δ) .
N

For the second term, again with probability greater than 1 − δ,

(P V )2 − (P V )2 ∞ ≤ P V + P V ∞ P V − P V ∞

≤

2 1−γ

(P − P )V

∞

≤

(1

2 − γ)2

2 log(2|S||A|/δ) .
N

where we have used that (·)2 is a component-wise operation in the second step. For the last term:

VarP (V ) = VarP (V − V π + V π )

≤ 2VarP (V − V π ) + 2VarP (V π )

≤

2V

−Vπ

2 ∞

+

2VarP

(V

π

)

= 2∆2δ,N + 2VarP (V π ) .

where ∆δ,N is deﬁned in Proposition 2.4. To obtain a cumulative probability of error less than δ, we replace δ in the above claims with δ/3. Combining these bounds completes the proof of the ﬁrst claim. The argument in the above display also implies that VarP (V ) ≤ 2∆2δ,N + 2VarP (V ) which proves the second claim.

Using Lemma 2.10 and 2.13, we have the following corollary.

Corollary 2.14. Let δ ≥ 0. With probability greater than 1 − δ, we have:

|(P − P )V | ≤ c

VarP (V π

) log(c|S||A|/δ) N

+ ∆δ,N 1

|(P − P )V | ≤ c

VarP (V

) log(c|S||A|/δ)

N

+ ∆δ,N 1 ,

where

1 ∆δ,N := c 1 − γ

and where c is an absolute constant.

log(c|S||A|/δ) 3/4

c log(c|S||A|/δ)

N

+ (1 − γ)2

N

,

28

Proof:(of Theorem 2.6) The proof consists of bounding the terms in Lemma 2.5. We have:

Q −Q

≤ γ (I − γP π )−1(P − P )V ∞

≤

cγ

log(c|S||A|/δ) (I − γP π )−1 N

VarP (V π

)

∞+

cγ (1 − γ)2

log(c|S||A|/δ) 3/4 N

cγ log(c|S||A|/δ)

+ (1 − γ)3

N

≤γ

2 (1 − γ)3

log(c|S ||A|/δ )

cγ

N

+ (1 − γ)2

log(c|S||A|/δ) 3/4

cγ log(c|S||A|/δ)

N

+ (1 − γ)3

N

≤

3γ

1 (1 − γ)3 c

log(c|S ||A|/δ )

cγ log(c|S||A|/δ)

N

+ 2 (1 − γ)3

N

,

where the ﬁrst step uses Corollary 2.14; the second uses Lemma 2.12; and the last step uses that 2ab ≤ a2 + b2 (and choosing a, b appropriately). The proof of the lower bound is analogous. Taking a different absolute constant completes the proof.

2.5 Scalings and Effective Horizon Dependencies

It will be helpful to more intuitively understand why 1/(1 − γ)3 is the effective horizon dependency one might hope to expect, from a dimensional analysis viewpoint. Due to that Q is a quantity that is as large as 1/(1 − γ), to account
for this scaling, it is natural to look at obtaining relative accuracy.

In particular, if

N

≥

1

c −

γ

|S||A| log(c|S||A|/δ)

2

,

then with probability greater than 1 − δ, then

Q − Qπ ∞ ≤ 1 − γ , and Q − Q ∞ ≤ 1 − γ .

(provided that

≤

√ 1

−

γ

using

Theorem

2.6).

In

other

words,

if

we

had

normalized

the

value

functions

1,

then

for

additive accuracy (on our normalized value functions) our sample size would scale linearly with the effective horizon.

2.6 Bibliographic Remarks and Further Readings
The notion of a generative model was ﬁrst introduced in [Kearns and Singh, 1999], which made the argument that, up to horizon factors and logarithmic factors, both model based methods and model free methods are comparable. [Kakade, 2003] gave an improved version of this rate (analogous to the crude bounds seen here).
The ﬁrst claim in Theorem 2.6 is due to [Azar et al., 2013], and the proof in this section largely follows this work. Improvements are possible with regards to bounding the quality of π ; here, Theorem 2.6 shows that the model based approach is near optimal even for policy itself; showing that the quality of π does suffer any ampliﬁcation factor of 1/(1 − γ). [Sidford et al., 2018] provides the ﬁrst proof of this improvement using a variance reduction algorithm with value iteration. The second claim in Theorem 2.6 is due to [Agarwal et al., 2020c], which shows that the naive model based approach is sufﬁcient. The lower bound in Theorem 2.8 is due to [Azar et al., 2013].
1Rescaling the value functions by multiplying by (1 − γ), i.e. Qπ ← (1 − γ)Qπ, would keep the values bounded between 0 and 1. Throughout,
29

We also remark that we may hope for the sub-optimality bounds (on the value of the argmax policy) to hold up to for “large” , i.e. up to ≤ 1/(1 − γ) (see the second claim in Theorem 2.6). Here, the work in [Li et al., 2020] shows this limit is achievable, albeit with a slightly different algorithm where they introduce perturbations. It is currently an open question if the naive model based approach also achieves the non-asymptotic statistical limit. This chapter also provided results, without proof, on the optimal sample complexity in the ﬁnite horizon setting (see Section 2.3.2). The proof of this claim would also follow from the line of reasoning in [Azar et al., 2013], with the added simpliﬁcation that the sub-optimality analysis is simpler in the ﬁnite-horizon setting with time-dependent transition matrices (e.g. see [Yin et al., 2021]).
this book it is helpful to understand sample size with regards to normalized quantities.
30

Chapter 3
Linear Bellman Completeness

Up to now we have focussed on “tabular” MDPs, where the sample complexity (and the computational complexity) scaled polynomially in the size of the state and action spaces. Ultimately, we seek to obtain methods which are applicable to cases where number of states and actions is large (or, possibly, countably or uncountably inﬁnite).
In this chapter, we will consider one of the simplest conditions, where such a result is possible. In particular, we will consider a setting where we have a set of features which satisfy the linear Bellman completion condition, and we show that, with access to a generative model, there is a simple algorithm that learns a near optimal policy with polynomial sample complexity in the dimension of these features.
The linear Bellman completeness concept and ﬁrst analysis was due to [Munos, 2005]. This chapter studies this concept in the setting of ﬁnite horizon MDPs, where we have access to a generative model.

3.1 The Linear Bellman Completeness Condition

We will work with given a feature mapping φ : S × A → Rd, where we assume the following property is true: given any linear function f (s, a) := θ φ(s, a), we have that the Bellman operator applied to f (s, a) also returns a linear function with respect to φ. Precisely, we have:

Deﬁnition 3.1 (Linear Bellman Completeness). We say the features φ satisfy the linear Bellman completeness property if for all θ ∈ Rd and (s, a, h) ∈ S × A × [H], there exists w ∈ Rd such that:

w

φ(s,

a)

=

r(s,

a)

+

Es

∼Ph (s,a)

max
a

θ

φ(s , a ).

As w depends on θ, we use the notation Th : Rd → Rd to represent such a w, i.e., w := Th(θ) in the above equation.

Note that the above implies that r(s, a) is in the span of φ (to see this, take θ = 0). Furthermore, it also implies that Qh(s, a) is linear in φ, i.e., there exists θh such that Qh(s, a) = (θh) φ(s, a). We can easily see that tabular MDP is captured by the linear Bellman completeness with φ(s, a) ∈ R|S||A| being a one-hot encoding vector with zeros everywhere except one at the entry corresponding to state-action (s, a). In Chapters 8 and 9, we will provide precise examples of models which satisfy the linear Bellman completeness condition.
We focus on the generative model setting, i.e., where we can input any (s, a) pair to obtain a next state sample s ∼ P (s, a) and reward r(s, a). Our goal here is to design an algorithm that ﬁnds a policy π such that V π ≥ V − , with number of samples scaling polynomially with respect to all relevant parameters d, H, 1/ .

31

Algorithm 1 Least Squares Value Iteration
1: Input: D0, . . . , DH−1 2: Set VH (s) = 0 for all s ∈ S 3: for h = H − 1 → 0 do
4: Solve least squares

θh = arg min

θ φ(s, a) − r − Vh+1(s ) 2

θ

s,a,r,s ∈Dh

5: Set Vh(s) = maxa∈A θh φ(s, a), ∀s 6: end for 7: Return: {πh(s)}Hh=−01 where πh(s) := arg maxa θh φ(s, a), ∀s, h.

(0.1)

3.2 The LSVI Algorithm

We ﬁrst present the least square value iteration (LSVI) algorithm (Alg. 1) here. The algorithm takes H many datasets D0, . . . , DH−1 as inputs, where each Dh = {s, a, r, s }.
For each time step, LSVI estimate Qh(s, a) via θh φ(s, a) with θh computed from a least square problem.
To make sure the algorithm succeeds in terms of ﬁnding a near optimal policy, we need to design the datasets D0, . . . , DH−1. Intuitively, we may want to make sure that in each Dh, we have s,a∈Dh φ(s, a)φ(s, a) being full rank, so that least square has closed-form solution, and θh has a good generalization bound. In the next section, we will leverage the generative model setting and the D-optimal design to construct such datasets.

3.3 LSVI with D-Optimal Design

In this section, we start by introducing the D-optimal design and its properties. We then use the D-optimal design to construct a dataset and give a generalization bound for the solution to the least squares problem on the constructed dataset. Finally, we present the sample complexity bound for LSVI and its analysis.

We will use the notation

x

2 M

=x

M x for a matrix M and vector x of appropriate dimensions.

3.3.1 D-Optimal Design

We now specify a sampling distribution which, roughly speaking, ensures good coverage (in a spectral sense) over our feature set.
Theorem 3.2. Suppose X ⊂ Rd is a compact set (and full dimensional). There exists a distribution ρ on X such that:

• ρ is supported on at most d(d + 1)/2 points (all of which are in X ).

• Deﬁne We have that for all x ∈ X ,

Σ = Ex∼ρ[xx .]

x

2 Σ−1

≤ d.

The distribution ρ is referred to as the D-optimal design.

32

With these two properties, one can show that for any x ∈ X , it can be written as a linear combination of the points on

the support of ρ, i.e., x =

xi∈support(ρ) αiρ(xi)xi, where

α

2 2

≤ d.

The following speciﬁes an explicit construction: ρ is the following maximizer:

ρ ∈ argmaxν∈∆(X ) ln det Ex∼ν xx .

(0.2)

(and there exists an optimizer which is supported on at most d(d + 1)/2 points). The design ρ has a geometric

interpretation: the centered ellipsoid E = {v :

v

2 Σ−1

≤

d} is the unique minimum volume centered ellipsoid

containing X We do not provide the proof here (See Section 3.6).

In our setting, we will utilize the D-optimal design on the set Φ := {φ(s, a) : s, a ∈ S × A}.

Change of basis interpretation with D-optimal design. Consider the coordinate transformation: x := Σ−1/2x.

Here we have that:

Σ := Ex∼ρ[xx ] = I.

Furthermore, we still have that

x

2 Σ−1

≤

d.

In this sense, we can interpret the D-optimal design as providing a

well-conditioned sampling distribution for the set X .

3.3.2 Performance Guarantees

Now we explain how we can construct a dataset Dh for h = 0, . . . , H − 1 using the D-optimal design ρ and the generative model:

• At h, for each s, a ∈ support(ρ), we sample ns,a := ρ(s, a)N many next states independently from Ph(·|s, a); combine all (s, a, r(s, a), s ) and denote the whole dataset as Dh
• Repeat for all h ∈ [0, . . . , H − 1] to construct D0, . . . , DH−1.

The key property of the dataset Dh is that the empirical covariance matrix s,a∈Dh φ(s, a)φ(s, a) is full rank. Denote Λh = s,a∈Dh φ(s, a)φ(s, a) , we can verify that

Λh =

φ(s, a)φ(s, a) N Σ

s,a∈D

(0.3)

where the inequality comes from the fact that in Dh, for each s, a ∈ support(ρ), we have ρ(s, a)N many copies of it. The D-optimal design ensures that Λ covers all φ(s, a) in the sense that maxs,a φ(s, a) Λ−h 1φ(s, a) ≤ d due to the property of the D-optimal design. This is the key property that ensures the solution of the least squares on Dh is
accurate globally.

Now we state the main sample complexity bound theorem for LSVI.

Theorem 3.3 (Sample Complexity of LSVI). Suppose our features satisfy the linear Bellman completion property.

Fix δ ∈ (0, 1) and

∈ (0, 1). Set parameter N :=

64H 6 d2 ln(1/δ)
2

.

With probability at least 1 − δ, Algorithm 1

outputs π such that:

Es∼µV0 (s) − Es∼µV0π(s) ≤ ,

with total number of samples H

d2

+

64H 6 d2 ln(1/δ)
2

.

33

3.3.3 Analysis

To prove the theorem, we ﬁrst show that if we had {Qh}h such that the Bellman residual Qh − ThQh+1 ∞ is small, then the performance of the greedy policies with respect to Qh is already near optimal. Note that this result is general and has nothing to do with the LSVI algorithm.
Lemma 3.4. Assume that for all h, we have Qh − Th+1Qh+1 ≤ . Then we have:
∞

1. Accuracy of Qh: for all h:

Qh − Qh

≤ (H − h) ,
∞

2. Policy performance: for πh(s) := argmaxa Qh(s, a), we have V π − V

≤ 2H2 .

Proof: The proof of this lemma is elementary, which is analog to what we have proved in Value Iteration in the discounted setting. For completeness, we provide a proof here.
Starting from QH (s, a) = 0, ∀s, a, we have TH−1QH = r by deﬁnition. The condition implies that QH−1 − r ∞ ≤ , which means that QH−1 − QH−1 ∞ ≤ . This proves the base case.
Our inductive hypothesis is that Qh+1 − Qh+1 ∞ ≤ (H − h − 1) . Note that:
|Qh(s, a) − Qh(s, a)| ≤ |Qh(s, a) − ThQh+1(s, a)| + |ThQh+1(s, a) − Qh(s, a)| ≤ + |ThQh+1(s, a) − ThQh+1(s, a)| ≤ + (H − h − 1) = (H − h) ,
which concludes the proof for the ﬁrst claim.
For the second claim, Starting from time step H − 1, for any s, we have: VHπ−1(s) − VH−1(s) = QπH−1(s, πH−1(s)) − QH−1(s, π (s)) = QπH−1(s, πH−1(s)) − QH−1(s, πH−1(s)) + QH−1(s, πH−1(s)) − QH−1(s, π (s)) = QH−1(s, πH−1(s)) − QH−1(s, π (s)) ≥ QH−1(s, πH−1(s)) − QH−1(s, πH−1(s)) + QH−1(s, π (s)) − QH−1(s, π (s)) ≥2 ,
where the third equality uses the fact that QπH−1(s, a) = QH−1(s, a) = r(s, a), the ﬁrst inequality uses the deﬁnition of πH−1 and QH−1(s, πH−1(s)) ≥ QH−1(s, a), ∀a. This concludes the base case. For time step h + 1, our inductive hypothesis is deﬁned as Vhπ+1(s) − Vh+1(s) ≥ −2(H − h − 1)H . For time step h, we have:
Vhπ(s) − Vh (s) = Qπh(s, πh(s)) − Qh(s, π (s)) = Qπh(s, πh(s)) − Qh(s, πh(s)) + Qh(s, πh(s)) − Qh(s, π (s)) = Es ∼Ph(s,πh(s)) Vhπ+1(s ) − Vh+1(s ) + Qh(s, πh(s)) − Qh(s, π (s))
≥ −2(H − h − 1) + Qh(s, πh(s)) − Qh(s, πh(s)) + Qh(s, πh(s)) − Qh(s, π (s)) ≥ −2(H − h − 1)H − 2(H − h) ≥ −2(H − h)H .
This concludes that for any s, at time step h = 0, we have V0π(s) − V0 (s) ≥ −2H2 .
To conclude the proof, what we left to show is that the Qh that is returned by LSVI satisﬁes the condition in the above lemma. Note that Qh(s, a) = θˆh φ(s, a), and ThQh+1 is indeed the Bayes optimal solution of the least squares in Eq. 0.1, and via the Bellman completion assumption, we have ThQh+1(s, a) = Th(θˆh+1) φ(s, a), ∀s, a. With the constructed Dh based on the D-optimal design, Linear regression immediately gives us a generalization bound on Qh − ThQh+1 ∞. The following theorem formalize the above statement.

34

Lemma 3.5. Fix δ ∈ (0, 1) and

∈ (0, 1), set N =

16H 2 d2 ln(H/δ)
2

. With probability at least 1 − δ, for all h, we

have:

∀h : Qh − ThQh+1 ≤ .
∞

Proof: The proof uses a standard result for ordinary least squares (see Theorem A.10 in the Appendix). Consider a time step h. In time step h, we perform linear regression from φ(s, a) to r(s, a) + maxa θˆh+1φ(s , a ). Note that the
Bayes optimal here is

Es ∼Ph(s,a)

r(s,

a)

+

max
a

θˆh+1φ(s

,

a

)

= Th(θˆh+1)

φ(s, a).

Also we can show that the noise s,a := r(s, a) + maxa Qh+1(s , a ) − ThQh(s, a) is bounded by 2H, and also
note that the noises are independent, and has mean zero: Es ∼Ph(s,a) r(s, a) + maxa Qh+1(s , a ) − ThQh(s, a) = 0. Using Theorem A.10, we have that with probability at least 1 − δ,

θˆh − Th(θˆh+1) 2 ≤ 4H2 d +
Λh
Now for any s, a, we must have:

d ln(1/δ) + 2 ln(1/δ) .

(θˆh − Th(θˆh+1))

2
φ(s, a) ≤

2
θˆh − Th(θˆh+1)
Λh

φ(s, a)

2 Λ− h 1

≤

θˆh − Th(θˆh+1)

2d Λh N

≤ 4H2 d2 + d1.5

ln(1/δ) + 2d ln(1/δ)

≤

16H 2 d2

ln(1/δ) ,

N

N

which implies that:

Qh − ThQh+1 = max (θˆh − Th(θˆh+1)) φ(s, a) ≤ 4Hd √ln(1/δ) .

∞

s,a

N

Set the right hand side to be

,

we

have

N

=

16H

2

d2 ln(1/δ )
2

.

Finally, apply union bound over all h = 0, . . . , H − 1, we conclude the proof.

Now we are ready to conclude the proof for the main theorem.

Proof:[Proof of Theorem 3.3] Using Lemma 3.5, with N =

64H6d2 ln(H/δ)
2

, we have that

Qh − ThQh+1 ∞ ≤

/(2H2) for all h, with probability at least 1 − δ. Now apply Lemma 3.4, we immediately have that |V πˆ − V | ≤ .

Regarding sample complexity, we have:

H ρ(s, a)N ≤ H

(1 + ρ(s, a)N ) ≤ H

d2

+

64H 6 d2 ln(H/δ)
2

,

s,a

s,a∈support(ρ)

where the second inequality use the support size upper bound of the D-optimal design ρ (see Lemma 3.2). This completes the proof.

3.4 How Strong is Bellman Completion as a Modeling?
To be added. 35

3.5 Ofﬂine Reinforcement Learning

One notable observation about LSVI is that the algorithm is non-adaptive, in the sense that it works using a ﬁxed set of observed transitions and rewards. Hence, it is applicable to the ofﬂine setting discussed in Chapter 1.

We now discuss two ofﬂine objectives: that of learning a near optimal policy and that of policy evaluation (i.e. evaluating the quality of a given policy).
We consider the setting where have a H datasets of the form Dh = {(si, ai, si, r(si, ai))}Ni=1, where we assume for each i and h, that we have independent samples si ∼ Ph(·|si, ai). In other words, Dh is a dataset of observed transitions corresponding to stage h, where each next state has been sampled independently. Note that here we have not made explicit distributional assumption about how the (si, ai)’s have been generated. The results we present can also be extended to where we have a ﬁxed data generation distribution over these quadruples.

It is straight forward to extend the LSVI guarantees to these setting provided our dataset has coverage in the following sense:

Assumption 3.6. (Coverage) Suppose that for each h ∈ [H], we have that:

1

1

N

φ(si, ai)φ(si, ai)

Σ κ

(si ,ai )∈Dh

where Σ is the D-optimal design covariance (see Equation 0.2).

3.5.1 Ofﬂine Learning

A minor modiﬁcation of the proof in Theorem 3.3 leads to the following guarantee:

Theorem 3.7 (Sample Complexity of LSVI). Suppose that Assumption 3.6 holds and that our features satisfy the

linear Bellman completion property. Fix δ ∈ (0, 1) and

∈ (0, 1). Set parameter N :=

cκH 6 d2 ln(1/δ)
2

, where c is

an appropriately chosen absolute constant. With probability at least 1 − δ, Algorithm 1 outputs π such that:

Es∼µ[V0 (s)] − Es∼µ[V0π(s)] ≤ .

3.5.2 Ofﬂine Policy Evaluation

Here we are interested in question of evaluating some given policy π using the ofﬂine data. For this, we will make a completeness assumption with respect to π as follows:
Deﬁnition 3.8 (Linear Completeness for π). We say the features φ satisfy the policy completeness property for π if for all θ ∈ Rd and (s, a, h) ∈ S × A × [H], there exists w ∈ Rd such that:
w φ(s, a) = r(s, a) + Es ∼Ph(s,a)θ φ(s , π(s )).

For this case, Algorithm 2, Least Squares Policy Evaluation (LSPE), is a modiﬁed version of LSVI for the purposes of estimation; note the algorithm no longer estimates Vh(s) using the greedy policy. Again, a nearly identical argument to the proof in Theorem 3.3 leads to the following guarantee. Here, we set:

V (µ) =

θ0 · φ(si, π(si))

si ∈D0

where θ0 is the parameter returned by LSVI.

36

Algorithm 2 Least Squares Policy Evaluation
1: Input: π, D0, . . . , DH−1 2: Set VH (s) = 0 for all s ∈ S 3: for h = H − 1 → 0 do
4: Solve least squares

θh = arg min

θ φ(s, a) − r − Vh+1(s ) 2

θ

s,a,r,s ∈Dh

5: Set Vh(s) = θh φ(s, π(s)), ∀s 6: end for 7: Return: {θh}Hh=−01.

(0.4)

Theorem 3.9 (Sample Complexity of LSPE). Suppose that Assumption 3.6 holds and that our features satisfy the lin-

ear policy completion property (with respect to π). Fix δ ∈ (0, 1) and

∈ (0, 1). Set parameter N :=

cκH 4 d2 ln(1/δ)
2

,

where c is an appropriately chosen absolute constant. With probability at least 1 − δ, Algorithm 2 outputs θ0 such that
for all s, |V0π(s) − θ0 φ(s, π(s))| ≤ .

Note that the above theorem has a sample complexity improvement by a factor of H. This is due to that the analysis is improvable, as we only care about value accuracy (the ﬁrst claim in Lemma 3.4, rather than the second, is what is relevant here). We should note that the sample size bounds presented in this chapter have not been optimized with regards to their H dependencies.

3.6 Bibliographic Remarks and Further Readings
The idea of Bellman completion under general function class was introduced in [Munos, 2005] under the setting of batch RL. For the episodic online learning setting, Zanette et al. [2020] provided a statistically efﬁcient algorithm under the linear Bellman completion condition, and [Jin et al., 2021] proposes a statistically efﬁcient algorithms under the Bellman completion condition with general function approximation.
We refer readers to to [Lattimore and Szepesva´ri, 2020] for a proof of the D-optimal design; the idea directly follows from John’s theorem (e.g. see [Ball, 1997, Todd, 2016]).

37

38

Chapter 4
Fitted Dynamic Programming Methods

Let us again consider the question of learning in large MDPs, when the underlying MDPs is unknown. In the previous chapter, we relied on the linear Bellman completeness assumption to provide strong guarantees with only polynomial dependence on the dimension of the feature space and the horizon H. This chapter considers the approach of using function approximation methods with iterative dynamic programming approaches.
In particular, we now consider approaches which rely on (average-case) supervised learning methods (namely regression), where we use regression to approximate the target functions in both the value iteration and policy iteration algorithms. We refer to these algorithms as ﬁtted Q-iteration (FQI) and ﬁtted policy iteration (FPI). The FQI algorithm can be implemented in an ofﬂine RL sampling model while the FPI algorithm requires the sampling access to an episodic model (see Chapter 1 for review of these sampling models).
This chapter focuses on obtaining of average case function approximation error bounds, provided we have a somewhat stringent condition on how the underlying MDP behaves, quantiﬁed by the concentrability coefﬁcient. This notion was introduced in [Munos, 2003, 2005]. While the notion is somewhat stringent, we will see that it is not avoidable without further assumptions. The next chapter more explicitly considers lower bounds, while Chapters 13 and 14 seek to relax the concentrability notion.

4.1 Fitted Q-Iteration (FQI) and Ofﬂine RL

We consider inﬁnite horizon discounted MDP M = {S, A, γ, P, r, µ} where µ is the initial state distribution. We assume reward is bounded, i.e., sups,a r(s, a) ∈ [0, 1]. For notation simplicity, we denote Vmax := 1/(1 − γ). Given any f : S × A → R, we denote the Bellman operator T f : S × A → R as follows. For all s, a ∈ S × A,
T f (s, a) := r(s, a) + γEs ∼P (·|s,a) max f (s , a ).
a ∈A

We assume that we have a distribution ν ∈ ∆(S × A). We collect a dataset D := {(si, ai, ri, si)}ni=1 where si, ai ∼ ν, ri = r(si, ai), si ∼ P (·|si, ai). Given D, our goal is to output a near optimal policy for the MDP, that is we would like our algorithm to produce a policy πˆ such that, with probability at least 1 − δ, V (πˆ) ≥ V − , for some

( , δ) pair. As usual, the number of samples n will depend on the accuracy parameters ( , δ) and we would like n

to scale favorably with these. Given any distribution ν ∈ S × A, and any function f : S × A → R, we write

f

2 2,ν

:=

Es,a∼ν f 2(s, a)

Denote a function class F = {f : S × A → [0, Vmax]}.

39

We require the data distribution ν is exploratory enough.
Assumption 4.1 (Concentrability). There exists a constant C such that for any policy π (including non-stationary policies), we have:
∀π, s, a : dπ(s, a) ≤ C. ν(s, a)

Note that concentrability does not require that the state space is ﬁnite, but it does place some constraints on the system dynamics. Note that the above assumption requires that ν to cover all possible policies’s state-action distribution, even including non-stationary policies.

In additional to the above two assumptions, we need an assumption on the representational condition of class F.

Assumption 4.2 (Inherent Bellman Error). We assume the following error bound holds:

approx,ν := max min
f ∈F f ∈F

f

−Tf

2 2,ν

.

We refer to approx,ν as the inherent Bellman error with respect to the distribution ν.

4.1.1 The FQI Algorithm

Fitted Q Iteration (FQI) simply performs the following iteration. Start with some f0 ∈ F, FQI iterates:

FQI:

n

2

ft ∈ argminf∈F

f

(si,

ai)

−

ri

−

γ

max
a ∈A

ft−1(si,

ai)

.

i=1

(0.1)

After k many iterations, we output a policy πk(s) := argmaxa fk(s, a), ∀s.
To get an intuition why this approach can work, let us assume the inherent Bellman error approx = 0 which is saying that for any f ∈ F , we have T f ∈ F (i.e., Bellman completion). Note that the Bayes optimal solution is T ft−1. Due to the Bellman completion assumption, the Bayes optimal solution T ft−1 ∈ F . Thus, we should expect that ft is close to the Bayes optimal T ft−1 under the distribution ν, i.e., with a uniform convergence argument, for the generalization bound, we should expect that:

Es,a∼ν (ft(s, a) − T ft−1(s, a))2 ≈ 1/n.

Thus in high level, ft ≈ T ft−1 as our data distribution ν is exploratory, and we know that based on value iteration, T fk−1 is a better approximation of Q than fk, i.e., T ft−1 − Q ∞ ≤ γ ft−1 − Q ∞, we can expect FQI to converge to the optimal solution when n → ∞, t → ∞. We formalize the above intuition below.

4.1.2 Performance Guarantees of FQI

We ﬁrst state the performance guarantee of FQI. Theorem 4.3 (FQI guarantee). Fix K ∈ N+. Fitted Q Iteration guarantees that with probability 1 − δ,

V

− V πK

≤

1 (1 − γ)2

22CVm2ax ln(|F |2K/δ) + n

20C approx,ν

+

γK Vmax (1 − γ)

40

We start from the following lemma which shows that if for all t,

ft+1 − T ft

2 2,ν

≤ ε, then the greedy policy with

respect to ft is approaching to π .

Lemma 4.4. Assume that for all t, we have ft+1 − T ft 2,ν ≤ ε, then for any k, we have:

√

V πk − V

≤

Cε (1 − γ)2

+

γk 1

Vmax −γ

.

Proof: We start from the Performance Difference Lemma:

(1 − γ) (V

− V πk ) = Es∼dπk [−A (s, πk(s))] = Es∼dπk [Q (s, π (s)) − Q (s, πk(s))] ≤ Es∼dπk [Q (s, π (s)) − fk(s, π (s)) + fk(s, πk(s)) − Q (s, πk(s))] ≤ Q − fk 1,dπk ◦π + Q − fk 1,dπk ◦πk ≤ Q − fk 2,dπk ◦π + Q − fk , 2,dπk ◦πk

where the ﬁrst inequality comes from the fact that πk is a greedy policy of fk, i.e., fk(s, πk(s)) ≥ fk(s, a) for any other a including π (s). Now we bound each term on the RHS of the above inequality. We do this by consider a state-action distribution β that is induced by some policy. We have:

Q − fk 2,β ≤ Q − T fk−1 2,β + fk − T fk−1 2,β

2

≤ γ Es,a∼β

Es

∼P (·|s,a)

max
a

Q

(s

,

a)

−

max
a

fk−1(s

,

a)

+ fk − T fk−1 2,β

≤γ

Es,a∼β,s

∼P (·|s,a)

max
a

(Q

(s

,

a)

−

fk−1(s

,

a))2

+

√ C

fk − T fk−1

2,ν ,

where in the last inequality, we use the fact that (E[x])2 ≤ E[x2], (maxx f (x) − maxx g(x))2 ≤ maxx(f (x) − g(x))2 for any two functions f and g, and assumption 4.1.

Denote β (s , a ) = comes:

s,a β(s, a)P (s |s, a)1{a = argmaxa (Q (s , a) − fk−1(s , a))2}, the above inequality be√
Q − fk 2,β ≤ γ Q − fk−1 2,β + C fk − T fk−1 2,ν .

We can recursively repeat the same process for Q − fk−1 2,ν till step k = 0:

√ k−1 Q − fk 2,β ≤ C γt fk−t − T fk−t−1 2,ν + γk Q − f0 2,ν ,
t=0
where ν is some valid state-action distribution.

Note that for the ﬁrst term on the RHS of the above inequality, we can bound it as follow:

√ k−1 C γt

fk−t − T fk−t−1

2,µ

≤

√ C

k−1

γkε

≤

√ Cε

1

1 −

γ

.

t=0

t=0

For the second term, we have:

γ Q − f0 2,ν ≤ γkVmax.

Thus, we have:

√

Q

− fk

2,β

=

Cε 1−γ

+

γ k Vmax ,

41

for all β, including β = dπk ◦ π , and β = dπk ◦ πk. This concludes the proof.
What left is to show that least squares control the error ε. We will use the generalization bound for least squares shown in Lemma A.11.

Lemma 4.5 (Bellman error). With probability at least 1 − δ, for all t = 0, . . . , K, we have:

ft+1 − T ft

2 2,ν

≤

22Vm2ax ln(|F |2K/δ) + 20 n

approx,ν .

Proof:

Let us ﬁx a function g ∈ F, and consider the regression problem on dataset {φ(s, a), r(s, a) + γ maxa g(s , a )}.

Denote fˆg = argminf∈F

n i=1

(f

(si,

ai)

−

ri

−

γ

maxa

g(s , a )), as the least square solution.

Note that in this regression problem, we have |ri + γ maxa g(s , a )| ≤ 1 + γVmax ≤ 2Vmax, and for our Bayes
optimal solution, we have |T g(s, a)| = |r(s, a) + γEs ∼P (·|s,a) maxa g(s , a )| ≤ 1 + γVmax ≤ 2Vmax. Also note that our inherent Bellman error condition implies that minf∈F Es,a∼ν (f (s, a) − T g(s, a))2 ≤ approx,ν . Thus, we
can apply Lemma A.11 directly here. With probability at least 1 − δ, we get:

fˆg − T g

2 2,ν

≤

22Vm2ax ln(|F |/δ) + 20 n

approx,ν .

Note that the above inequality holds for the ﬁxed g. We apply a union bound over all possible g ∈ F, we get that with probability at least 1 − δ:

∀g ∈ F :

fˆg − T g

2 2,ν

≤

22Vm2ax ln(|F |2/δ) + 20 n

approx,ν .

(0.2)

Consider iteration t, set g = ft+1 in Eq. 0.2, and by the deﬁnition of ft, we have fˆg = ft in this case, which means that above inequality in Eq. 0.2 holds for the pair (fˆg, g) = (ft, ft+1). Now apply a union bound over all t = 0, . . . , K, we conclude the proof.
Now we are ready to prove the main theorem.
Proof:[Proof of Theorem 4.3]
From Lemma 4.5, we see that with probability at least 1 − δ, we have for all t ≤ K:

ft+1 − T ft 2,ν ≤

22Vm2ax ln(|F |2K/δ) + n

20 approx,ν .

Set ε in Lemma 4.4 to be

+ 22Vm2ax ln(|F |2K/δ)
n

20 approx,ν , we conclude the proof.

4.2 Fitted Policy-Iteration (FPI)
to be added..

42

4.3 Failure Cases Without Assumption 4.1 4.4 FQI for Policy Evaluation 4.5 Bibliographic Remarks and Further Readings
The notion of concentrability was developed in [Munos, 2003, 2005] in order to permitting sharper bounds in terms of average case function approximation error, provided that the concentrability coefﬁcient is bounded. These methods also permit sample based ﬁtting methods, with sample size and error bounds, provided there is a data collection policy that induces a bounded concentrability coefﬁcient [Munos, 2005, Szepesva´ri and Munos, 2005, Antos et al., 2008, Lazaric et al., 2016]. Chen and Jiang [2019] provide a more detailed discussion on this quantity.
43

44

Chapter 5
Statistical Limits of Generalization
In reinforcement learning, we seek to have learnability results which are applicable to cases where number of states is large (or, possibly, countably or uncountably inﬁnite). This is a question of generalization, which, more broadly, is one of the central challenges in machine learning.
The previous two chapters largely focussed on sufﬁcient conditions under which we can obtain sample complexity results which do not explicitly depend on the size of the state (or action) space. This chapter focusses on what are necessary conditions for generalization. Here, we can frame our questions by examining the extent to which generalization in RL is similar to (or different from) that in supervised learning. Two most basic settings in supervised learning are: (i) agnostic learning (i.e. ﬁnding the best classiﬁer or hypothesis in some class) and (ii) learning with linear models (i.e. learning the best linear regressor or the best linear classiﬁer). This chapter will focus on lower bounds with regards to the analogues of these two questions for reinforcement learning:
• (Agnostic learning) Given some hypothesis class (of policies, value functions, or models), what is the sample complexity of ﬁnding (nearly) the best hypothesis in this class?
• (Linearly realizable values or policies) Suppose we are given some d-dimensional feature mapping where we are guaranteed that either the optimal value function is linear in these given features or that the optimal policy has a linear parameterization. Are we able to obtain sample complexity guarantees that are polynomial in d, with little to no explicit dependence on the size of the state or action spaces? We will consider this question in both the ofﬂine setting (for the purposes of policy evaluation, as in Chapter 3) and for in online setting where our goal is to learn a near optimal optimal policy.
Observe that supervised learning can be viewed as horizon one, H = 1, RL problem (where the learner only receives feedback for the “label”, i.e. the action, chosen). We can view the second question above as the analogue of linear regression or classiﬁcation with halfspaces. In supervised learning, both of these settings have postive answers, and they are fundamental in our understanding of generalization. Perhaps surprisingly, we will see negative answers to these questions in the RL setting. The signiﬁcance of this provides insights as to why our study of generalization in reinforcement learning is substantially more subtle than in supervised learning. Importantly, the insights we develop here will also help us to motivate the more reﬁned assumptions and settings that we consider in subsequent chapters (see Section 5.3 for discussion).
This chapter will work with ﬁnite horizon MDPs, where we consider both the episodic setting and the generative model setting. With regards to the ﬁrst question on agnostic learning, this chapter follows the ideas ﬁrst introduced in [Kearns et al., 2000]. With regards to the second question on linear realizability, this chapter follows the results in [Du et al., 2019, Wang et al., 2020a, Weisz et al., 2021a, Wang et al., 2021].
45

5.1 Agnostic Learning

Suppose we have a hypothesis class H (either ﬁnite or inﬁnite), where for each f ∈ H we have an associated policy πf : S → A, which, for simplicity, we assume is deterministic. Here, we could have that:

• H itself is a class of policies.
• H is a set of state-action values, where for f ∈ H, we associate it with the greedy policy πf (s, h) = argmaxa fh(s, a), where s ∈ S and h ∈ [H].
• H could be a class of models (i.e. each f ∈ H is an MDP itself). Here, for each f ∈ H, we can let πf be the optimal policy in the MDP f .

We let Π denote the induced set of policies from our hypothesis class H, i.e. Π = {πf |f ∈ H}.

The goal of agnostic learning can be formulated by the following optimization problem:

max
π∈Π

Es0

∼µ

V

π

(s0

),

(0.1)

where we are interested in the number of samples required to approximately solve this optimization problem. We will work both in the episodic setting and the generative model setting, and, for simplicity, we will restrict ourselves to ﬁnite hypothesis classes.

Binary classiﬁcation as an H = 1 RL problem: Observe that the problem of binary classiﬁcation can be viewed as learning in an MDP with a horizon of one. In particular, take H = 1; take |A| = 2; let the distribution over starting states s0 ∼ µ correspond to the input distribution; and take the reward function as r(s, a) = 1(label(s) = a). In other words, we equate our action with the prediction of the binary class membership, and the reward function is determined
by if our prediction is correct or not.

5.1.1 Review: Binary Classiﬁcation

One of the most important concepts for learning binary classiﬁers is that it is possible to generalize even when the state space is inﬁnite. Here note that the domain of our classiﬁers, often denoted by X , is analogous to the state space S. We now brieﬂy review some basics of supervised learning before we turn to the question of generalization in reinforcement learning.
Consider the problem of binary classiﬁcation with N labeled examples of the form (xi, yi)Ni=1, with xi ∈ X and yi ∈ {0, 1}. Suppose we have a (ﬁnite or inﬁnte) set H of binary classiﬁers where each h ∈ H is a mapping of the form h : X → {0, 1}. Let 1(h(x) = y) be an indicator which takes the value 0 if h(x) = y and 1 otherwise. We assume that our samples are drawn i.i.d. according to a ﬁxed joint distribution D over (x, y).

Deﬁne the empirical error and the true error as:

1N

err(h) = N

1(h(xi) = yi), err(h) = E(X,Y )∼D1(h(X) = Y ).

i=1

For a given h ∈ H, Hoeffding’s inequality implies that with probability at least 1 − δ:

|err(h) − err(h)| ≤

12 log .

2N δ

This and the union bound give rise to what is often referred to as the “Occam’s razor” bound:

46

Proposition 5.1. (The “Occam’s razor” bound) Suppose H is ﬁnite. Let h = arg minh∈H err(h). With probability at

least 1 − δ:

err(h) ≤ min err(h) +

2 2|H| log .

h∈H

N

δ

Hence, provided that

N

≥

2

log

2|H| δ

2

,

then with probability at least 1 − δ, we have that:

err(h) ≤ min err(h) + .
h∈H
A key observation here is that the our regret — the regret is the left hand side of the above inequality — has no dependence on the size of X (i.e. S) which may be inﬁnite and is only logarithmic in the number of hypothesis in our class.

5.1.2 Importance Sampling and a Reduction to Supervised Learning
Now let us return to the agnostic learning question in Equation 0.1. We will see that, provided we are willing to be exponential in the horizon H, then agnostic learning is possible. Furthermore, it is a straightforward argument to see that we are not able to do better.

An Occam’s Razor Bound for RL

We now provide a reduction of RL to the supervised learning problem, given only sampling access in the episodic setting. The key issue is how to efﬁciently reuse data. The idea is that we will simply collect N trajectories by executing a policy which chooses actions uniformly at random; let UnifA denote this policy.

The following shows how we can obtain an unbiased estimate of the value of any policy π using this uniform policy UnifA:
Lemma 5.2. (Unbiased estimation of V0π(µ)) Let π be any deterministic policy. We have that:

H −1

V0π(µ) = |A|H · Eτ∼PrUnifA 1 π(s0) = a0, . . . , π(sH−1) = aH−1

r(sh, ah)

h=0

where PrUnifA speciﬁes the distribution over trajectories τ = (s0, a0, r0, . . . sH−1, aH−1, rH−1) under the policy UnifA.

The proof follows from a standard importance sampling argument (applied to the distribution over the trajectories). Proof: We have that:

H −1

V0π(µ) = Eτ∼Prπ

rh

h=0

= Eτ ∼PrUnifA

Prπ(τ ) PrUnifA (τ )

H −1 h=0

rh

= |A|H · Eτ∼PrUnifA 1 π(s0) = a0, . . . , π(sH−1) = aH−1

H −1
rh
h=0

47

where the last step follows due to that the probability ratio is only nonzero when UnifA choose actions identical to that of π.

Crudely, the factor of |A|H is due to that the estimated reward of π on a trajectory is nonzero only when π takes exactly identical actions to those taken by UnifA on the trajectory, which occurs with probability 1/|A|H .

Now, given sampling access in the episodic model, we can use UnifA to get any estimate of any other policy π ∈ Π. Note that the factor of |A|H in the previous lemma will lead to this approach being a high variance estimator. Suppose we draw N trajectories under UnifA. Denote the n-th sampled trajectory by (sn0 , an0 , r1n, sn1 , . . . , snH−1, anH−1, rHn −1). We can then use following to estimate the ﬁnite horizon reward of any given policy π:

V0π (µ)

=

|A|H N

N

1 π(sn0 ) = an0 , . . . π(snH−1) = anH−1

H −1
r(snt , ant ).

n=1

t=0

Proposition 5.3. (An “Occam’s razor bound” for RL) Let δ ≥ 0. Suppose Π is a ﬁnite and suppose we use the
aforementioned estimator, V0π(µ), to estimate the value of every π ∈ Π. Let π = arg maxπ∈Π V0π(µ). We have that with probability at least 1 − δ,

V0π (µ)

≥

max
π∈Π

V0π (µ)

−

H

|A|H

2 2|Π| log .
Nδ

Proof: Note that, ﬁxing π, our estimator is sum of i.i.d. random variables, where each independent estimator,

|A|H 1 π(sn0 ) = an0 , . . . π(snH−1) = anH−1

H −1 t=0

r(snt ,

ant ),

is

bounded

by

H|A|H .

The remainder of the argu-

ment is identical to that used in Proposition 5.1.

Hence, provided that

N

≥

H |A|H

2 log(2|Π|/δ)

2

,

then with probability at least 1 − δ, we have that:

V0π (s0 )

≥

max
π∈Π

V0π (s0 )

−

.

This is the analogue of the Occam’s razor bound for RL.

Importantly, the above shows that we can avoid dependence on the size of the state space, though this comes at the price of an exponential dependence on the horizon. As we see in the next section, this dependence is unavoidable (without making further assumptions).

Inﬁnite Policy Classes. In the supervised learning setting, a crucial observation is that even though a hypothesis set H of binary classiﬁers may be inﬁnite, we may still be able to obtain non-trivial generalization bounds. A crucial observation here is that even though the set H may be inﬁnite, the number of possible behaviors of on a ﬁnite set of states is not necessarily exhaustive. The Vapnik–Chervonenkis (VC) dimension of H, V C(H), is formal way to characterize this intuition, and, using this concept, we are able to obtain generalization bounds in terms of the V C(H).
With regards to inﬁnite hypothesis classes of policies (say for the case where |A| = 2), extending our Occam’s razor bound can be done with precisely the same approach. In particular, when |A| = 2, each π ∈ Π can be viewed as Boolean function, and this gives rise to the VC dimension VC(Π) of our policy class. The bound in Proposition 5.3 can be replaced with a VC-dimension bound that is analogous to that of binary classiﬁcation (see Section 5.4).

Lower Bounds
Clearly, the drawback of this agnostic learning approach is that we would require a number of samples that is exponential in the problem horizon. We now see that if we desire a sample complexity that scales with O(log |Π|), then an
48

exponential dependence on the effective horizon is unavoidable, without making further assumptions.

Here, our lower bound permits a (possibly randomized) algorithm to utilize a generative model (which is a more powerful sampling model than the episodic one).

Proposition 5.4. (Lower Bound with a Generative Model) Suppose algorithm A has access to a generative model.

There exists a policy class Π, where |Π| = |A|H such that if algorithm A returns any policy π (not necessarily in Π)

such that

V0π

(µ)

≥

max
π∈Π

V0π

(µ)

−

0.5.

with probability greater than 1/2, then A must make a number of number calls N to the generative model where:

N ≥ c|A|H

(where c is an absolute constant).

Proof: The proof is simple. Consider a |A|-ary balanced tree, with |A|H states and |A| actions, where states correspond nodes and actions correspond to edges; actions always move the agent from the root towards a leaf node. We make only one leaf node rewarding, which is unknown to the algorithm. We consider the policy class to be all |A|H policies. The theorem now immediately follows since the algorithm gains no knowledge of the rewarding leaf node unless it queries that node.
Note this immediately rules out the possibility that any algorithm which can obtain a log |Π| dependence without paying a factor of |A|H in the sample size due to that log |Π| = H log |A| in the example above.

5.2 Linear Realizability
In supervised learning, two of the most widely studied settings are those of linear regression and binary classiﬁcation with halfspaces. In both settings, we are able to obtain sample complexity results that are polynomial in the feature dimension. We now consider the analogue of these assumptions for RL, starting with the analogue of linear regression.
When the state space is large or inﬁnite, we may hope that linearly realizability assumptions may permit a more sample efﬁcient approach. We will start with linear realizability on Qπ and consider the ofﬂine policy evaluation problem. Then we will consider the problem of learning with only a linearly realizability assumption on Q (along with access to either a generative model or sampling access in the episodic setting).
5.2.1 Ofﬂine Policy Evaluation with Linearly Realizable Values
In Chapter 3, we observed that the LSVI and LSPE algorithm could be used with an ofﬂine dataset for the purposes of policy evaluation. Here, we made the linear Bellman completeness assumption on our features. Let us now show that with only a linear realizability assumption, then not only is LSPE sample inefﬁcient but we will also see that, information theoretically, every algorithm is sample inefﬁcient, in a minmax sense.
This section is concerned with the ofﬂine RL setting. In this setting, the agent does not have direct access to the MDP and instead is given access to data distributions {µh}Hh=−01 where for each h ∈ [H], µh ∈ ∆(Sh × A). The inputs of the agent are H datasets {Dh}Hh=−01, and for each h ∈ [H], Dh consists i.i.d. samples of the form (s, a, r, s ) ∈ Sh × A × R × Sh+1 tuples, where (s, a) ∼ µh, r ∼ r(s, a), s ∼ P (s, a). We now focus on the ofﬂine policy evaluation problem: given a policy π : S → ∆ (A) and a feature mapping φ : S × A → Rd, the goal is to output an accurate estimate of the value of π (i.e., V π) approximately, using the collected datasets {Dh}Hh=−01, with as few samples as possible.
49

We will make the following linear realizability assumption with regards to a feature mapping φ : S × A → Rd, which we can think of as either being hand-crafted or coming from a pre-trained neural network that transforms a stateaction pair to a d-dimensional embedding, and the Q-functions can be predicted by linear functions of the features. In particular, this section will assume the following linear realizability assumption with regards to every policy π.
Assumption 5.5 (Realizable Linear Function Approximation). For every policy π : S → ∆(A), there exists θ0π, . . . θHπ −1 ∈ Rd such that for all (s, a) ∈ S × A and h ∈ [H],
Qπh(s, a) = (θhπ) φ(s, a).

Note that this assumption is substantially stronger than assuming realizability with regards to a single target policy π (say the policy that we wish to evaluate); this assumption imposes realizability for all policies.
We will also assume a coverage assumption, analogous to Assumption 3.6. It should be evident that without feature coverage in our dataset, realizability alone is clearly not sufﬁcient for sample-efﬁcient estimation. Note that , we will make the strongest possible assumption, with regards to the conditioning of the feature covariance matrix; in particular, this assumption is equivalent to µ being a D-optimal design.

Assumption 5.6 (Coverage). For all (s, a) ∈ S × A, assume our feature map is bounded such that φ(s, a) 2 ≤ 1. Furthermore, suppose for each h ∈ [H], the data distributions µh satisﬁes the following:

1

E(s,a)∼µh [φ(s, a)φ(s, a)

] = I. d

Note that the minimum eigenvalue of the above matrix is 1/d, which is the largest possible minimum eigenvalue over
all data distributions µh, since σmin(E(s,a)∼µh [φ(s, a)φ(s, a) ]) is less than or equal to 1/d (due to that φ(s, a) 2 ≤ 1 for all (s, a) ∈ S × A). Also, it is not difﬁcult to see that this distribution satisﬁes the D-optimal design property.

Clearly, for the case where H = 1, the realizability assumption (Assumption 5.5), and coverage assumption (Assumption 5.6) imply that the ordinary least squares estimator will accurately estimate θ0π. The following shows these assumptions are not sufﬁcient for ofﬂine policy evaluation for long horizon problems.
Theorem 5.7. Suppose Assumption 5.6 holds. Fix an algorithm that takes as input both a policy and a feature mapping. There exists a (deterministic) MDP satisfying Assumption 5.5, such that for any policy π : S → ∆(A), the algorithm requires Ω((d/2)H ) samples to output the value of π up to constant additive approximation error with probability at least 0.9.

Although we focus on ofﬂine policy evaluation, this hardness result also holds for ﬁnding near-optimal policies under Assumption 5.5 in the ofﬂine RL setting with linear function approximation. Below we give a simple reduction. At the initial state, if the agent chooses action a1, then the agent receives a ﬁxed reward value (say 0.5) and terminates. If the agent chooses action a2, then the agent transits to the hard instance. Therefore, in order to ﬁnd a policy with suboptimality at most 0.5, the agent must evaluate the value of the optimal policy in the hard instance up to an error of 0.5, and hence the hardness result holds.

Least-Squares Policy Evaluation (LSPE) has exponential variance. For ofﬂine policy evaluation with linear function approximation, it is not difﬁcult to see that LSPE, Algorithm 2, will provide an unbiased estimate (provided the feature covariance matrices are full rank, which will occur with high probability). Interestingly, as a direct corollary, the above theorem implies that LSPE has exponential variance in H. More generally, this theorem implies that there is no estimator that can avoid such exponential dependence in the ofﬂine setting.
50

𝜙 𝑠-., 𝑎! = 𝑒. 𝜙 𝑠-., 𝑎( = 𝑒.,+* 𝜙 𝑠-+*,!, 𝑎 = (𝑒! + 𝑒( + ⋯ + 𝑒+*)/𝑑'!/(

𝑄 𝑠, 𝑎! = 𝑟"𝑑'($%!)/( 𝑟 𝑠, 𝑎 = 0

𝑠)!

𝑠)(

𝑠)/

…

𝑠)+*

𝑎! 𝑎(

𝑠)+*,!

𝑄 𝑠)+*,!, 𝑎 = 𝑟"𝑑'$/( 𝑟 𝑠)+*,!, 𝑎 = 𝑟"(𝑑'$/( − 𝑑'($%!)/()

𝑄 𝑠, 𝑎! = 𝑟"𝑑'($%()/( 𝑟 𝑠, 𝑎 = 0

𝑠!!

𝑠!(

𝑠!/

…

𝑠!+*

𝑠!+*,!

𝑄 𝑠!+*,!, 𝑎 = 𝑟"𝑑'($%!)/( 𝑟 𝑠!+*,!, 𝑎 = 𝑟"(𝑑'($%!)/( − 𝑑'($%()/()

𝑄 𝑠, 𝑎! = 𝑟"𝑑'($%-)/(

⋯

⋯

⋯

…

⋯

𝑟 𝑠, 𝑎 = 0

⋯

𝑄 𝑠-+*,!, 𝑎 = 𝑟"𝑑'($%-,!)/(

𝑟 𝑠-+*,!, 𝑎 = 𝑟"(𝑑'($%-,!)/( − 𝑑'($%-)/()

𝑄 𝑠, 𝑎! = 𝑟"𝑑'!/( 𝑟 𝑠, 𝑎 = 0

𝑠$! %(

𝑠$( %(

𝑠$/ %(

…

𝑠$+*%(

𝑠$+*,%!(

𝑄 𝑠$+*,%!(, 𝑎 = 𝑟"𝑑' 𝑟 𝑠$+*,%!(, 𝑎 = 𝑟"(𝑑' − 𝑑'!/()

𝑄 𝑠, 𝑎 = 𝑟" 𝔼[𝑟 𝑠, 𝑎 ] = 𝑟"

𝑠$! %!

𝑠$( %!

𝑠$/ %!

…

𝑠$+*%!

𝑠$+*,%!!

𝑄 𝑠$+*,%!!, 𝑎 = 𝑟"𝑑'!/( 𝑟 𝑠$+*,%!!, 𝑎 = 𝑟"𝑑'!/(

Figure 0.1: An illustration of the hard instance. Recall that dˆ = d/2. States on the top are those in the ﬁrst level
(h = 0), while states at the bottom are those in the last level (h = H − 1). Solid line (with arrow) corresponds to
transitions associated with action a1, while dotted line (with arrow) corresponds to transitions associated with action a2. For each level h ∈ [H], reward values and Q-values associated with s1h, s2h, . . . , sdhˆ are marked on the left, while reward values and Q-values associated with sdhˆ+1 are mark on the right. Rewards and transitions are all deterministic, except for the reward distributions associated with s1H−1, s2H−1, . . . , sdHˆ −1. We mark the expectation of the reward value when it is stochastic. For each level h ∈ [H], for the data distribution µh, the state is chosen uniformly at random from those states in the dashed rectangle, i.e., {s1h, s2h, . . . , sdhˆ}, while the action is chosen uniformly at random from {a1, a2}. Suppose the initial state is sd1ˆ+1. When r∞ = 0, the value of the policy is 0. When r∞ = dˆ−H/2, the value of the policy is r∞ · dˆH/2 = 1.

A Hard Instance for Ofﬂine Policy Evaluation
We now provide the hard instance construction and the proof of Theorem 5.7. We use d to denote the feature dimension, and we assume d is even for simplicity. We use dˆto denote d/2 for convenience. We also provide an illustration of the construction in Figure 0.1.

State Space, Action Space and Transition Operator. The action space A = {a1, a2}. For each h ∈ [H], Sh contains dˆ+ 1 states s1h, s2h, . . . , sdhˆ and sdhˆ+1. For each h ∈ {0, 1, . . . , H − 2}, for each c ∈ {1, 2, . . . , dˆ+ 1}, we have

 1  P (s|sch, a) = 1
0

s = sdhˆ++11, a = a1 s = sch+1, a = a2 .
else

51

Reward Distributions. Let 0 ≤ r∞ ≤ dˆ−H/2 be a parameter to be determined. For each (h, c) ∈ {0, 1, . . . , H − 2} × [dˆ] and a ∈ A, we set r(sch, a) = 0 and r(sdhˆ+1, a) = r∞ · (dˆ1/2 − 1) · dˆ(H−h−1)/2. For the last level, for each c ∈ [dˆ] and a ∈ A, we set

r(scH−1, a) =

1 −1

with probability (1 + r∞)/2 with probability (1 − r∞)/2

so that E[r(scH−1, a)] = r∞. Moreover, for all actions a ∈ A, r(sdHˆ+−11, a) = r∞ · dˆ1/2.

Feature Mapping. Let e1, e2, . . . , ed be a set of orthonormal vectors in Rd. Here, one possible choice is to set

e1, e2, . . . , ed to be the standard basis vectors. For each (h, c) ∈ [H] × [dˆ], we set φ(sch, a1) = ec, φ(sch, a2) = ec+dˆ,

and

φ(sdhˆ+1, a)

=

1 dˆ1/2

ec
c∈dˆ

for all a ∈ A.

Verifying Assumption 5.5. Now we verify that Assumption 5.5 holds for this construction. Lemma 5.8. For every policy π : S → ∆(A), for each h ∈ [H], for all (s, a) ∈ Sh × A, we have Qπh(s, a) = (θhπ) φ(s, a) for some θhπ ∈ Rd.
Proof: We ﬁrst verify Qπ is linear for the ﬁrst H − 1 levels. For each (h, c) ∈ {0, 1, . . . , H − 2} × [dˆ], we have Qπh(sch, a1) =r(sch, a1) + r(sdhˆ++11, a1) + r(sdhˆ++12, a1) + . . . + r(sdHˆ+−11, a1) = r∞ · dˆ(H−h−1)/2.

Moreover, for all a ∈ A, Qπh(sdhˆ+1, a) =r(sdhˆ+1, a) + r(sdhˆ++11, a1) + r(sdhˆ++12, a1) + . . . + r(sdHˆ+−11, a1) = r∞ · dˆ(H−h)/2.

Therefore, if we deﬁne

dˆ

dˆ

θhπ =

r∞ · dˆ(H−h−1)/2 · ec +

Qπh(sch, a2) · ec+dˆ,

c=1

c=1

then Qπh(s, a) = (θhπ) φ(s, a) for all (s, a) ∈ Sh × A.

Now we verify that the Q-function is linear for the last level. Clearly, for all c ∈ [dˆ] and a ∈ A, QπH−1(scH−1, a) = r∞

and QπH−1(sdHˆ+−11, a) = r∞ ·

dˆ. Thus by deﬁning θHπ −1 =

d c=1

r∞

·

ec,

we

have

QπH−1(s,

a)

=

θHπ −1

φ(s, a)

for all (s, a) ∈ SH−1 × A.

The Data Distributions. For each level h ∈ [H], the data distribution µh is a uniform distribution over the set {(s1h, a1), (s1h, a2), (s2h, a1), (s2h, a2), . . . , (sdhˆ, a1), (sdhˆ, a2)}. Notice that (sdhˆ+1, a) is not in the support of µh for all a ∈ A. It can be seen that,

E(s,a)∼µh φ(s, a)φ(s, a)

1d

1

= d

ecec

=

I. d

c=1

52

The Information-Theoretic Argument

We show that it is information-theoretically hard for any algorithm to distinguish the case r∞ = 0 and r∞ = dˆ−H/2. We ﬁx the initial state to be sd0ˆ+1, and consider any policy π : S → ∆(A). When r∞ = 0, all reward values will be zero, and thus the value of π would be zero. On the other hand, when r∞ = dˆ−H/2, the value of π would be r∞ · dˆH/2 = 1. Thus, if the algorithm approximates the value of the policy up to an error of 1/2, then it must distinguish the case that r∞ = 0 and r∞ = dˆ−H/2.
We ﬁrst notice that for the case r∞ = 0 and r∞ = dˆ−H/2, the data distributions {µh}hH=−01, the feature mapping φ : S × A → Rd, the policy π to be evaluated and the transition operator P are the same. Thus, in order to distinguish the case r∞ = 0 and r∞ = dˆ−H/2, the only way is to query the reward distribution by using sampling taken from the data distributions.
For all state-action pairs (s, a) in the support of the data distributions of the ﬁrst H − 1 levels, the reward distributions will be identical. This is because for all s ∈ Sh \ {sdhˆ+1} and a ∈ A, we have r(s, a) = 0. For the case r∞ = 0 and r∞ = dˆ−H/2, for all state-action pairs (s, a) in the support of the data distribution of the last level,

1 r(s, a) =

with probability (1 + r∞)/2 .

−1 with probability (1 − r∞)/2

Therefore, to distinguish the case that r∞ = 0 and r∞ = dˆ−H/2, the agent needs to distinguish two reward distributions
r(1) = 1 with probability 1/2 −1 with probability 1/2

and

r(2) =

1 −1

with probability (1 + dˆ−H/2)/2 with probability (1 − dˆ−H/2)/2 .

It is standard argument that in order to distinguish r(1) and r(2) with probability at least 0.9, any algorithm requires Ω(dˆH ) samples.

The key in this construction is the state sdhˆ+1 in each level, whose feature vector is deﬁned to be c∈dˆ ec/dˆ1/2. In each level, sdhˆ+1 ampliﬁes the Q-values by a dˆ1/2 factor, due to the linearity of the Q-function. After all the H levels, the value will be ampliﬁed by a dˆH/2 factor. Since sdhˆ+1 is not in the support of the data distribution, the only way for the agent to estimate the value of the policy is to estimate the expected reward value in the last level. This construction
forces the estimation error of the last level to be ampliﬁed exponentially and thus implies an exponential lower bound.

5.2.2 Linearly Realizable Q
Speciﬁcally, we will assume access to a feature map φ : S × A → Rd, and we will assume that a linear function of φ can accurately represent the Q -function. Speciﬁcally, Assumption 5.9 (Linear Q Realizability). For all h ∈ [H], assume there exists θh∗ ∈ Rd such that for all (s, a) ∈ S × A,
Q∗h(s, a) = θh∗ · φ(s, a).
The hope is that this assumption may permit a sample complexity that is polynomial in d and H, with no explicit |S| or |A| dependence.
Another assumption that we will use is that the minimum suboptimality gap is lower bounded.
53

...

... ...

h=0 1 2

H1

Figure 0.2: The Leaking Complete Graph Construction. Illustration of a hard MDP. There are m + 1 states in the MDP, where f is an absorbing terminal state. Starting from any non-terminal state a, regardless of the action, there is at least α = 1/6 probability that the next state will be f .

Assumption 5.10 (Constant Sub-optimality Gap). For any state s ∈ S, a ∈ A, the suboptimality gap is deﬁned as ∆h(s, a) := Vh∗(s) − Q∗h(s, a). We assume that
min {∆h(s, a) : ∆h(s, a) > 0} ≥ ∆min.
h∈[H ],s∈S ,a∈A
The hope is that with a “large gap”, the identiﬁcation of the optima1l policy itself (as opposed to just estimating its value accurately) may be statistically easier, thus making the problem easier.
We now present two hardness results. We start with the case where we have access to a generative model.
Theorem 5.11. (Linear Q ; Generative Model Case) Consider any algorithm A which has access to a generative model and which takes as input the feature mapping φ : S × A → Rd. There exists an MDP with a feature mapping φ satisfying Assumption 5.9 and where the size of the action space is |A| = c1 min{d1/4, H1/2} such that if A (when given φ as input) ﬁnds a policy π such that
Es1∼µV π(s1) ≥ Es1∼µV ∗(s1) − 0.05 with probability 0.1, then A requires min{2c2d, 2c2H } samples (c1 and c2 are absolute constants).
Note that theorem above uses an MDP whose size of the action space is only of moderate size (actually sublinear in both d and H). Of course, in order to prove such a result, we must rely on a state space which is exponential in d or H (else, we could apply a tabular algorithm to obtain a polynomial result). The implications of the above show that the linear Q assumption, alone, is not sufﬁcient for sample efﬁcient RL, even with access to a generative model.
We now see that in the (online) episodic setting, even if we have a constant gap in the MDP, the hardness of learning is still unavoidable.
Theorem 5.12. (Linear Q Realizability + Gap; Episodic Setting) Consider any algorithm A which has access to the episodic sampling model and which takes as input the feature mapping φ : S × A → Rd. There exists an MDP with a feature mapping φ satisfying Assumption 5.9 and Assumption 5.10 (where ∆min is an absolute constant) such that if A (using φ as input) ﬁnds a policy π such that
Es1∼µV π(s1) ≥ Es1∼µV ∗(s1) − 0.05 with probability 0.1, then A requires min{2cd, 2cH } samples (where c is an absolute constant).
54

The theorem above also holds with an MDP whose action space is of size |A| = Ω( min{d1/4, H1/2} ). However, for ease of exposition, we will present a proof of this theorem that uses a number of actions that is exponential in d (see Section 5.4).

Linear Q Realizability + Gap; Generative Model Case. For this case, it is not difﬁcult to see that, with a generative model, it is possible to exactly learn an optimal policy with a number of samples that is poly(d, H, 1/∆min) (see Section 5.4). The key idea is that, at the last stage, the large gap permits us to learn the optimal policy itself (using linear regression). Furthermore, if we have an optimal policy from timestep h onwards, then we are able to obtain unbiased estimates of Qh−1(s, a), at any state action pair, using the generative model. We leave the proof as an exercise to the interested reader.

A Hard Instance with Constant Suboptimality Gap

We now prove Theorem 5.12, and we do not prove Theorem 5.11 (see Section 5.4). The remainder of this section provides the construction of a hard family of MDPs where Q∗ is linearly realizable and has constant suboptimality gap and where it takes exponential samples to learn a near-optimal policy. Each of these hard MDPs can roughly be seen as a “leaking complete graph” (see Fig. 0.2). Information about the optimal policy can only be gained by: (1) taking the optimal action; (2) reaching a non-terminal state at level H. We will show that when there are exponentially many actions, either events happen with negligible probability unless exponentially many trajectories are played.
Let m be an integer to be determined. The state space is {¯1, · · · , m¯ , f }. The special state f is called the terminal state. The action space is simply A = {1, 2, . . . , m}. Each MDP in this family is speciﬁed by an index a∗ ∈ {1, 2, . . . , m} and denoted by Ma∗ . In other words, there are m MDPs in this family. We will make use of a (large) set of approximately orthogonal vectors, which exists by the Johnson-Lindenstrauss lemma. We state the following lemma without proof:

Lemma 5.13 (Johnson-Lindenstrauss).

For

any

α

>

0,

if

m

≤

exp(

1 8

α2d

),

there

exists

m

unit

vectors

{v1,

·

·

·

, vm}

in Rd such that ∀i, j ∈ {1, 2, . . . , m} such that i = j, | vi, vj | ≤ α.

We will set α

=

1 6

and m

=

exp(

1 8

α2d)

.

By Lemma 5.13,

we can ﬁnd such a set of d-dimensional unit vectors

{v1, · · · , vm}. For the clarity of presentation, we will use vi and v(i) interchangeably. The construction of Ma∗ is

speciﬁed below. Note that in this construction, the features, the rewards and the transitions are deﬁned for all a1, a2

with a1, a2 ∈ {1, 2, . . . , m} and a1 = a2. In particular, this construction is properly deﬁned even when a1 = a∗.

Features. The feature map, which maps state-action pairs to d + 1 dimensional vectors, is deﬁned as follows.

φ(a1, a2) := 0, v(a1), v(a2) + 2α · v(a2) ,

3

φ(a1, a1) :=

α, 0 , 4

φ(f, 1) = (0, 0) , φ(f, a) := (−1, 0) .

(∀a1, a2 ∈ {1, 2, . . . , m}, a1 = a2) (∀a1 ∈ {1, 2, . . . , m})
(∀a = 1)

Here 0 is the zero vector in Rd. Note that the feature map is independent of a∗ and is shared across the MDP family.

55

Rewards. For h ∈ [H], the rewards are deﬁned as

rh(a1, a∗) := v(a1), v(a∗) + 2α,

rh(a1, a2) := −2α

3

rh(a1, a1)

:=

α, 4

rh(f, 1) := 0,

rh(f, a) := −1.

v(a1), v(a2) + 2α ,

For h = H − 1, rH−1(s, a) := φ(s, a), (1, v(a∗)) for every state-action pair.

(a1 = a∗) (a2 = a∗, a2 = a1)
(∀a1)
(a = 1)

Transitions. The initial state distribution µ is set as a uniform distribution over {¯1, · · · , m¯ }. The transition probabilities are set as follows.

Pr[f |a1, a∗] = 1,

Pr[f |a1, a1] = 1,



a2 : v(a1), v(a2) + 2α

Pr[·|a1, a2] =

,

f : 1 − v(a1), v(a2) − 2α

Pr[f |f, ·] = 1.

(a2 = a∗, a2 = a1)

After taking action a2, the next state is either a2 or f . Thus this MDP looks roughly like a “leaking complete graph” (see Fig. 0.2): starting from state a, it is possible to visit any other state (except for a∗); however, there is always at least 1 − 3α probability of going to the terminal state f . The transition probabilities are indeed valid, because
0 < α ≤ v(a1), v(a2) + 2α ≤ 3α < 1.

We now verify that realizability, i.e. Assumption 5.9, is satisﬁed. Lemma 5.14. (Linear realizability) In the MDP Ma∗ , ∀h ∈ [H], for any state-action pair (s, a), Q∗h(s, a) = φ(s, a), θ∗ with θ∗ = (1, v(a∗)).

Proof: We ﬁrst verify the statement for the terminal state f . Observe that at the terminal state f , the next state is always f and the reward is either 0 (if action 1 is chosen) or −1 (if an action other than 1 is chosen). Hence, we have

Q∗h(f, a) =

0 −1

a=1 a=1

and Vh∗(f ) = 0.
This implies Q∗h(f, ·) = φ(f, ·), (1, v(a∗)) .
We now verify realizability for other states via induction on h = H − 1, · · · , 0. The induction hypothesis is that for all a1, a2 ∈ {1, 2, . . . , m}, we have

Q∗h(a1, a2) =

( v(a1), v(a2) + 2α) · v(a2), v(a∗)

3 4

α

a1 = a2 a1 = a2

(0.2)

56

and

Vh∗(a1) =

v(a1), v(a∗) + 2α

3 4

α

a1 a1

= =

a∗ a∗

.

(0.3)

Note that (0.2) implies that realizability is satisﬁed. In the remaining part of the proof we verify Eq. (0.2) and (0.3).

When h = H − 1, (0.2) holds by the deﬁnition of rewards. Next, note that for all h ∈ [H], (0.3) follows from (0.2). This is because for all a1 = a∗, for all a2 ∈/ {a1, a∗}.

Q∗h(a1, a2) = ( v(a1), v(a2) + 2α) · v(a2), v(a∗) ≤ 3α2,

Moreover, for all a1 = a∗,

Furthermore, for all a1 = a∗,

Q∗h(a1, a1)

=

3 α
4

<

α.

Q∗h(a1, a∗) = v(a1), v(a∗) + 2α ≥ α > 3α2.

In other words, (0.2) implies that a∗ is always the optimal action for all state a1 with a1 = a∗. Now, for state a∗, for all a = a∗, we have

Q∗h(a∗, a) = ( v(a∗), v(a)

+ 2α) ·

v(a∗), v(a)

≤

3α2

<

3 α
4

=

Q∗h(a∗, a∗).

Hence, (0.2) implies that a∗ is always the optimal action for all states a with a ∈ {1, 2, . . . , m}.

Thus, it remains to show that (0.2) holds for h assuming (0.3) holds for h + 1. Here we only consider the case that a2 = a1 and a2 = a∗, since otherwise Pr[f |a1, a2] = 1 and thus (0.2) holds by the deﬁnition of the rewards and the fact that Vh∗(f ) = 0. When a2 ∈/ {a1, a∗}, we have
Q∗h(a1, a2) = rh(a1, a2) + Esh+1 Vh∗+1(sh+1) a1, a2 = −2α v(a1), v(a2) + 2α + Pr[sh+1 = a2] · Vh∗+1(a2) + Pr[sh+1 = f ] · Vh∗+1(f )
= −2α v(a1), v(a2) + 2α + v(a1), v(a2) + 2α · v(a2), v(a∗) + 2α
= v(a1), v(a2) + 2α · v(a2), v(a∗) .

This is exactly (0.2) for h. Hence both (0.2) and (0.3) hold for all h ∈ [H].

We now verify the constant sub-optimality gap, i.e. Assumption 5.10, is satisﬁed. Lemma 5.15. (Constant Gap) Assumption 5.10 is satisﬁed with ∆min = 1/24.

Proof: From Eq. (0.2) and (0.3), it is easy to see that at state a1 = a∗, for a2 = a∗, the suboptimality gap is

∆h(a1, a2) := Vh∗(a1) − Q∗h(a1, a2) ≥ α − max

3α2, 3 α 4

1 =.
24

Moreover, at state a∗, for a = a∗, the suboptimality gap is

∆h(a∗, a)

:=

Vh∗(a∗)

− Q∗h(a∗, a)

≥

3α − 4

3α2

=

1 .
24

Finally, for the terminal state f , the suboptimality gap is obviously 1.

Therefore ∆min

≥

1 24

for all MDPs under

consideration.

57

The Information-Theoretic Argument We provide a proof sketch for the lower bound below in Theorem 5.12.

Proof sketch. We will show that for any algorithm, there exists a∗ ∈ {1, 2, . . . , m} such that in order to output π

with

Es1∼µV π(s1) ≥ Es1∼µV ∗(s1) − 0.05

with probability at least 0.1 for Ma∗ , the number of samples required is 2Ω(min{d,H}).

Observe that the feature map of Ma∗ does not depend on a∗, and that for h < H − 1 and a2 = a∗, the reward Rh(a1, a2) also contains no information about a∗. The transition probabilities are also independent of a∗, unless the action a∗ is taken. Moreover, the reward at state f is always 0. Thus, to receive information about a∗, the agent either
needs to take the action a∗, or be at a non-game-over state at the ﬁnal time step (h = H − 1).

However, note that the probability of remaining at a non-terminal state at the next layer is at most

sup v(a1), v(a2)
a1 =a2

+

2α

≤

3α

≤

3 .

4

Thus for any algorithm, Pr[sH−1 = f ] ≤

3 4

H , which is exponentially small.

In other words, any algorithm that does not know a∗ either needs to “be lucky” so that sH−1 = f , or needs to take a∗ “by accident”. Since the number of actions is m = 2Θ(d), either event cannot happen with constant probability unless
the number of episodes is exponential in min{d, H}.

In order to make this claim rigorous, we can construct a reference MDP M0 as follows. The state space, action space, and features of M0 are the same as those of Ma. The transitions are deﬁned as follows:



Pr[·|a1, a2] = a2 : v(a1), v(a2) + 2α

,

f : 1 − v(a1), v(a2) − 2α

(∀a1, a2 s.t. a1 = a2)

Pr[f |f, ·] = 1.

The rewards are deﬁned as follows:

rh(a1, a2) := −2α v(a1), v(a2) + 2α , rh(f, ·) := 0.

( ∀a1, a2 s.t. a1 = a2)

Note that M0 is identical to Ma∗ , except when a∗ is taken, or when an trajectory ends at a non-terminal state. Since
the latter event happens with an exponentially small probability, we can show that for any algorithm the probability of taking a∗ in Ma∗ is close to the probability of taking a∗ in M0. Since M0 is independent of a∗, unless an exponential number of samples are used, for any algorithm there exists a∗ ∈ {1, 2, . . . , m} such that the probability of taking a∗ in M0 is o(1). It then follows that the probability of taking a∗ in Ma∗ is o(1). Since a∗ is the optimal action for every state, such an algorithm cannot output a near-optimal policy for Ma∗ .

5.2.3 Linearly Realizable π
Similar to value-based learning, a natural assumption for policy-based learning is that the optimal policy is realizable by a halfspace.

58

Assumption 5.16 (Linear π∗ Realizability). For any h ∈ [H], there exists θh ∈ Rd that satisﬁes for any s ∈ S, we have
π∗ (s) ∈ argmaxa θh, φ (s, a) .
As before (and as is natural in supervised learning), we will also consider the case where there is a large margin (which is analogous to previous large gap assumption). Assumption 5.17 (Constant Margin). Without loss of generality, assume the scalings are such that for all (s, a, h) ∈ S × A × [H], φ(s, a) 2 ≤ 1 and θh ≤ 1 Now suppose that for all s ∈ S and any a ∈/ π∗(s),
θh, φ (s, π∗(s)) − θh, φ (s, a) ≥ ∆min.
Here we restrict the linear coefﬁcients and features to have unit norm for normalization.
We state the following without proof (see Section 5.4): Theorem 5.18. (Linear π Realizability + Margin) Consider any algorithm A which has access to a generative model and which takes as input the feature mapping φ : S × A → Rd. There exists an MDP with a feature mapping φ satisfying Assumption 5.16 and Assumption 5.17 (where ∆min is an absolute constant) such that if A (with φ as input) ﬁnds a policy π such that
Es1∼µV π(s1) ≥ Es1∼µV ∗(s1) − 0.05 with probability 0.1, then A requires min{2cd, 2cH } samples, where c is an absolute constant.
5.3 Discussion: Studying Generalization in RL
The previous lower bounds shows that: (i) without further assumptions, agnostic learning (in the standard supervised learning sense) is not possible in RL, unless we can tolerate an exponential dependence on the horizon, and (ii) simple realizability assumptions are also not sufﬁcient for sample efﬁcient RL.
This motivates the study of RL to settings where we either make stronger assumptions or have a means in which the agent can obtain side information. Three examples of these approaches that are considered in this book are:
• Structural (and Modelling) Assumptions: By making stronger assumptions about the how the hypothesis class relates to the underlying MDP, we can move away from agnostic learning lower bound. We have seen one example of this with the stronger Bellman completeness assumption that we considered in Chapter 3. We will see more examples of this in Part 2.
• Distribution Dependent Results (and Distribution Shift): We have seen one example of this approach when we consider the approximate dynamic programming approach in Chapter 4. When we move to policy gradient methods (in Part 3), we will also consider results which depend on the given distribution (i.e. where we suppose our starting state distribution µ has some reasonable notion of coverage).
• Imitation Learning and Behavior Cloning: here we will consider settings where the agent has input from, effectively, a teacher, and we see how this can circumvent statistical hardness results.
5.4 Bibliographic Remarks and Further Readings
The reduction from reinforcement learning to supervised learning was ﬁrst introduced in [Kearns et al., 2000], which used a different algorithm (the “trajectory tree” algorithm), as opposed to the importance sampling approach presented
59

here. [Kearns et al., 2000] made the connection to the VC dimension of the policy class. The fundamental sample complexity tradeoff — between polynomial dependence on the size of the state space and exponential dependence on the horizon — was discussed in depth in [Kakade, 2003]. Utilizing linear methods for dynamic programming goes back to, at least, [Shannon, 1959, Bellman and Dreyfus, 1959]. Formally considering the linear Q assumption goes back to at least [Wen and Van Roy, 2017]. Resolving the learnability under this assumption was an important open question discussed in [Du et al., 2019], which is now resolved. In the ofﬂine (policy evaluation) setting, the lower bound in Theorem 5.7 is due to [Wang et al., 2020a, Zanette, 2021]. With a generative model, the breakthrough result of [Weisz et al., 2021a] established the impossibility result with the linear Q assumption. Furthermore, Theorem 5.12, due to [Wang et al., 2021], resolves this question in the online setting with the additional assumption of having a constant sub-optimality gap. Also, [Weisz et al., 2021b] extends the ideas from [Weisz et al., 2021a], so that the lower bound is applicable with action spaces of polynomial size (in d and H); this is the result we use for Theorem 5.11. Theorem 5.18, which assumed a linearly realizable optimal policy, is due to [Du et al., 2019].
60

Part 2
Strategic Exploration
61

Chapter 6
Multi-Armed & Linear Bandits

For the case, where γ = 0 (or H = 1 in the undiscounted case), the problem of learning in an unknown MDP reduce to the multi-armed bandit problem. The basic algorithms and proof methodologies here are important to understand in their own right, due to that we will have to extend these with more sophisticated variants to handle the explorationexploitation tradeoff in the more challenging reinforcement learning problem.
This chapter follows analysis of the LinUCB algorithm from the original proof in [Dani et al., 2008], with a simpliﬁed concentration analysis due to [Abbasi-Yadkori et al., 2011].
Throughout this chapter, we assume reward is stochastic.

6.1 The K-Armed Bandit Problem

The setting is where we have K decisions (the “arms”), where when we play arm a ∈ {1, 2, . . . K} we obtain a random reward ra ∈ [−1, 1] from R(a) ∈ ∆([−1, 1]) which has mean reward:

where it is easy to see that µa ∈ [−1, 1].

Era∼R(a)[ra] = µa

Every iteration t, the learner will pick an arm It ∈ [1, 2, . . . K]. Our cumulative regret is deﬁned as:

T −1

RT = T · max µi −
i

µIt

t=0

We denote a = argmaxi µi as the optimal arm. We deﬁne gap ∆a = µa − µa for any arm a.

Theorem 6.1. There exists an algorithm such that with probability at least 1 − δ, we have:





 RT = O min


KT · ln(T K/δ),

ln(T K/δ)  + K.

a=a

∆a 

6.1.1 The Upper Conﬁdence Bound (UCB) Algorithm
We summarize the upper conﬁdence bound (UCB) algorithm in Alg. 3. For simplicity, we allocate the ﬁrst K rounds to pull each arm once.
63

Algorithm 3 UCB

1: Play each arm once and denote received reward as ra for all a ∈ {1, 2, . . . K}

2: for t = 1 → T − K do

3:

Execute arm It = arg maxi∈[K] µˆti +

log(T K/δ) Nit

4: Observe rt := rIt

5: end for

where every iteration t, we maintain counts of each arm:

t−1
Nat = 1 + 1{Ii = a},
i=1

where It is the index of the arm that is picked by the algorithm at iteration t. We main the empirical mean for each arm as follows:

µta

=

1 Nat

t−1
ra + 1{Ii = a}ri .
i=1

Recall that ra is the reward of arm a we got during the ﬁrst K rounds.

We also main the upper conﬁdence bound for each arm as follows:

µta + 2

ln(T K/δ) Nat .

The following lemma shows that this is a valid upper conﬁdence bound with high probability.

Lemma 6.2 (Upper Conﬁdence Bound). For all t ∈ [1, . . . , T ] and a ∈ [1, 2, . . . K], we have that with probability at least 1 − δ,

µta − µa ≤ 2

ln(T K/δ) Nat .

(0.1)

The proof of the above lemma uses Azuma-Hoeffding’s inequality (Theorem A.5) for each arm a and iteration t and then apply a union bound over all T iterations and K arms. The reason one needs to use Azuma-Hoeffding’s inequality is that here the number of trials of arm a, i.e. Nat, itself is a random variable while the classic Hoeffding’s inequality only applies to the setting where the number of trials is a ﬁxed number.
Proof: We ﬁrst consider a ﬁxed arm a. Let us deﬁne the following random variables, X0 = ra − µa, X1 = 1{I1 = a}(r1 − µa), X2 = 1{I2 = a}(r2 − µa), . . . , Xi = 1{Ii = a}(ri − µa), . . . .
Regarding the boundedness on these random variables, we have that for i where 1{Ii = a} = 1, we have |Xi| ≤ 1, and for i where 1{Ii = a} = 0, we have |Xi| = 0. Now, consider E [Xi|H<i], where H<i is all history up to but not including iteration i. Note that we have E [Xi|H<i] = 0 since condition on history H<i, 1{Ii = a} is a deterministic quantity (i.e., UCB algorithm determines whether or not to pull arm a at iteration i only based on the information from H<i). Thus, we can conclude that {Xi}i=0 is a Martingale difference sequence. Via Azuma-Hoeffding’s inequality, we have that with probability at least 1 − δ, for any ﬁxed t, we have:
t−1
Xi = Natµta − Natµa ≤ 2 ln(1/δ)Nat,
i=0

64

where we use the fact that

t−1 i=0

|Xi|2

≤

Nat .

Divide

Nat

on

both

side,

we

have:

µta − µa ≤ 2 ln(1/δ)/Nat

Apply union bound over all 0 ≤ t ≤ T and a ∈ {1, . . . , K}, we have that with probability 1 − δ, for all t, a:

This concludes the proof.

µta − µa ≤ 2 ln(1/δ)/Nat ≤ 2 ln(T K/δ)/Nat

Now we can conclude the proof of the main theorem.

Proof: Below we conditioned on the above inequality 0.1 holds. This gives us the following optimism:

µa ≤ µta + 2

ln(T K/δ) Nat

,

∀a,

t.

Thus, we can upper bound the regret as follows:

µa − µIt ≤ µtIt + 2 Sum over all iterations, we get:

ln(T K/δ) NItt

−

µIt

≤

4

ln(T K/δ)

NItt

.

T −1

T −1

µa − µIt ≤ 4 ln(T K/δ)

t=0

t=0

1 NItt

= 4 ln(T K/δ)

NaT √1 ≤ 8 ln(T K/δ)

a i=1 i

a

√

≤ 8 ln(T K/δ) KT .

NaT ≤ 8 ln(T K/δ) K NaT
a

Note that our algorithm has regret K at the ﬁrst K rounds.

On the other hand, if for each arm a, the gap ∆a > 0, then, we must have:

NaT

≤

4 ln(T K/δ)

∆2a

.

which is because after the UCB of an arm a is below µa , UCB algorithm will never pull this arm a again (the UCB of a is no smaller than µa ).

Thus for the regret calculation, we get:

T −1
µa
t=0

− µIt ≤
a=a

NaT ∆a ≤
a=a

4 ln(T K/δ) .
∆a

Together with the fact that Inequality 0.1 holds with probability at least 1 − δ, we conclude the proof.

6.2 Linear Bandits: Handling Large Action Spaces
Let D ⊂ Rd be a compact (but otherwise arbitrary) set of decisions. On each round, we must choose a decision xt ∈ D. Each such choice results in a reward rt ∈ [−1, 1].
65

Algorithm 4 The Linear UCB algorithm

Input: λ, βt 1: for t = 0, 1 . . . do 2: Execute

xt

=

argmaxx∈D

max
µ∈BALLt

µ

·

x

and observe the reward rt. 3: Update BALLt+1 (as speciﬁed in Equation 0.2). 4: end for

We assume that, regardless of the history H of decisions and observed rewards, the conditional expectation of rt is a ﬁxed linear function, i.e. for all x ∈ D,
E[rt|xt = x] = µ · x ∈ [−1, 1], where x ∈ D is arbitrary. Here, observe that we have assumed the mean reward for any decision is bounded in [−1, 1]. Under these assumptions, the noise sequence,
ηt = rt − µ · xt
is a martingale difference sequence.

The is problem is essentially a bandit version of a fundamental geometric optimization problem, in which the agent’s feedback on each round t is only the observed reward rt and where the agent does not know µ apriori.

If x0, . . . xT −1 are the decisions made in the game, then deﬁne the cumulative regret by
T −1
RT = T µ · x − µ · xt
t=0
where x ∈ D is an optimal decision for µ , i.e.

x ∈ argmaxx∈D µ · x

which exists since D is compact. Observe that if the mean µ were known, then the optimal strategy would be to play

x every round. Since the expected loss for each decision x equals µ · x, the cumulative regret is just the difference

between the expected loss of the optimal algorithm and the expected loss for the actual decisions xt. By the Hoeffding-

Azuma inequality (see Lemma A.5), the observed reward

T −1 t=0

rt

will

be

close

to

their

(conditional)

expectations

T −1 t=0

µ

· xt.

Since the sequence of decisions x1, . . . , xT −1 may depend on the particular sequence of random noise encountered, RT is a random variable. Our goal in designing an algorithm is to keep RT as small as possible.

6.2.1 The LinUCB algorithm

LinUCB is based on “optimism in the face of uncertainty,” which is described in Algorithm 4. At episode t, we use all previous experience to deﬁne an uncertainty region (an ellipse) BALLt. The center of this region, µt, is the solution of the following regularized least squares problem:

t−1

µt

=

arg

min
µ

µ · xτ − rτ

2 2

+

λ

µ

2 2

τ =0

t−1
= Σ−t 1 rτ xτ ,

τ =0

66

where λ is a parameter and where

t−1
Σt = λI + xτ xτ , with Σ0 = λI.
τ =0

The shape of the region BALLt is deﬁned through the feature covariance Σt.

Precisely, the uncertainty region, or conﬁdence ball, is deﬁned as:

BALLt = µ|(µt − µ) Σt(µt − µ) ≤ βt ,

(0.2)

where βt is a parameter of the algorithm.

Computation. Suppose that we have an efﬁcient linear optimization oracle, i.e. that we can efﬁciently solve the problem:
max ν · x
x∈D
for any ν. Even with this, Step 2 of LinUCB may not be computationally tractable. For example, suppose that D is provided to us as a polytope, then the above oracle can be efﬁciently computed using linear programming, while LinUCB is an NP-hard optimization. Here, we can actually use a wider conﬁdence region, where we can keep track of 1 ball which contains BALLt. See Section 6.4 for further reading.

6.2.2 Upper and Lower Bounds

Our main result here is that we have sublinear regret with only a polynomial dependence on the dimension d and, importantly, no dependence on the cardinality of the decision space D, i.e. on |D|.

Theorem 6.3. Suppose that the expected costs are bounded, in magnitude, by 1, i.e. that |µ · x| ≤ 1 for all x ∈ D; that µ ≤ W and x ≤ B for all x ∈ D; and that the noise ηt is σ2 sub-Gaussian 1. Set

λ = σ2/W 2, βt := σ2 2 + 4d log

tB2W 2 1+
d

+ 8 log(4/δ) .

We have that with probability greater than 1 − δ, that (simultaneously) for all T ≥ 0,

√ RT ≤ cσ T

T B2W 2 d log 1 + dσ2

+ log(4/δ)

where c is an absolute constant. In other words, we have that RT is O˜(d√T ) with high probability.

The following shows that no algorithm can (uniformly) do better.

Theorem 6.4. (Lower bound) There exists a distribution over linear bandit problems (i.e. a distribution over µ) with

the rewards being bounded by 1, in magnitude, and σ2 ≤ 1, such that for every (randomized) algorithm, we have for

n ≥ max{256, d2/16},

Eµ

ERT

≥

1√ d T.
2500

where the inner expectation is with respect to randomness in the problem and the algorithm.

We will only prove the upper bound (See Section bib:bandits).
1Roughly, this assumes the tail probabilities of ηt decay no more slowly than a Gaussian distribution. See Deﬁnition A.2.
67

LinUCB and D-optimal design. Let us utilize the D-optimal design to improve the dependencies in Theorem 6.3. Let ΣD denote the D-optimal design matrix from Theorem 3.2. Consider the coordinate transformation:
x˜ = Σ−D1/2x, µ = Σ1D/2µ .
Observe that µ · x = µ · x, so we still have a linear expected reward function in this new coordinate systems. Also, observe that x˜ = x ΣD ≤ d, which is a property of the D-optimal design, and that

µ = µ ΣD = (µ ) ΣDµ = Ex∼ρ[(µ · x)2] ≤ 1,
where the last step uses our assumption that the rewards are bounded, in magnitude, by 1.
The following corollary shows that, wit√hout loss of generality, we we can remove the dependencies on B and W from the previous theorem, due to that B ≤ d and W ≤ 1 when working under this coordinate transform. Corollary 6.5. Suppose that the expected costs are bounded, in magnitude, by 1, i.e. that |µ · x| ≤ 1 for all x ∈ D and that the noise ηt is σ2 sub-Gaussian. Suppose linUCB is implemented in the x coordinate system, as described above, with the following settings of the parameters.

λ = σ2, βt := σ2 2 + 4d log (1 + t) + 8 log(4/δ) .

We have that with probability greater than 1 − δ, that (simultaneously) for all T ≥ 0,

√ RT ≤ cσ T

T d log 1 + σ2

+ log(4/δ)

where c is an absolute constant.

6.3 LinUCB Analysis

In establishing the upper bounds there are two main propositions from which the upper bounds follow. The ﬁrst is in showing that the conﬁdence region is appropriate. Proposition 6.6. (Conﬁdence) Let δ > 0. We have that
Pr(∀t, µ ∈ BALLt) ≥ 1 − δ.

Section 6.3.2 is devoted to establishing this conﬁdence bound. In essence, the proof seeks to understand the growth of the quantity (µt − µ ) Σt(µt − µ ).

The second main step in analyzing LinUCB is to show that as long as the aforementioned high-probability event holds, we have some control on the growth of the regret. Let us deﬁne

regrett = µ · x∗ − µ · xt

which denotes the instantaneous regret.

The following bounds the sum of the squares of instantaneous regret.

Proposition 6.7. (Sum of Squares Regret Bound) Suppose that x ≤ B for x ∈ D. Suppose βt is increasing and that βt ≥ 1. For LinUCB, if µ ∈ BALLt for all t, then

T −1
regret2t ≤ 8βT d log

T B2 1+
dλ

t=0

68

This is proven in Section 6.3.1. The idea of the proof involves a potential function argument on the log volume (i.e. the log determinant) of the “precision matrix” Σt (which tracks how accurate our estimates of µ are in each direction). The proof involves relating the growth of this volume to the regret.
Using these two results we are able to prove our upper bound as follows:
Proof:[Proof of Theorem 6.3] By Propositions 6.6 and 6.7 along with the Cauchy-Schwarz inequality, we have, with probability at least 1 − δ,

T −1
RT = regrett ≤
t=0

T −1
T regret2t ≤
t=0

T B2 8T βT d log 1 + dλ .

The remainder of the proof follows from using our chosen value of βT and algebraic manipulations (that 2ab ≤ a2 + b2).

We now provide the proofs of these two propositions.

6.3.1 Regret Analysis

In this section, we prove Proposition 6.7, which says that the sum of the squares of the instantaneous regrets of the algorithm is small, assuming the evolving conﬁdence balls always contain the true mean µ . An important observation is that on any round t in which µ ∈ BALLt, the instantaneous regret is at most the “width” of the ellipsoid in the direction of the chosen decision. Moreover, the algorithm’s choice of decisions forces the ellipsoids to shrink at a rate that ensures that the sum of the squares of the widths is small. We now formalize this.
Unless explicitly stated, all norms refer to the 2 norm.
Lemma 6.8. Let x ∈ D. If µ ∈ BALLt and x ∈ D. Then
|(µ − µt) x| ≤ βtx Σ−t 1x

Proof: By Cauchy-Schwarz, we have:

|(µ − µt) x| = |(µ − µt) Σ1t /2Σ−t 1/2x| = |(Σ1t /2(µ − µt)) Σ−t 1/2x| ≤ Σ1t /2(µ − µt) Σ−t 1/2x = Σ1t /2(µ − µt) x Σ−t 1x ≤ βtx Σ−t 1x

where the last inequality holds since µ ∈ BALLt.

Deﬁne

wt := xt Σ−t 1xt

which w√e interpret as the “normalized width” at time t in the direction of the chosen decision. We now see that the width, 2 βtwt, is an upper bound for the instantaneous regret.

Lemma 6.9. Fix t ≤ T . If µ ∈ BALLt and βt ≥ 1, then

regrett ≤ 2 min ( βtwt, 1) ≤ 2 βT min (wt, 1)

Proof: Let µ ∈ BALLt denote the vector which minimizes the dot product µ xt. By the choice of xt, we have

µ xt = max µ xt = max max µ x ≥ (µ ) x∗,

µ∈BALLt

x∈D µ∈BALLt

69

where the inequality used the hypothesis µ ∈ BALLt. Hence,

regrett = (µ ) x∗ − (µ ) xt ≤ (µ − µ ) xt = (µ − µt) xt + (µt − µ ) xt ≤ 2 βtwt

where the last step follows from Lemma 6.8 since µ and µ are in BALLt. Since rt ∈ [−1, 1], regrett is always at most 2 and the ﬁrst inequality follows. The ﬁnal inequality is due to that βt is increasing and larger than 1.
The following two lemmas prove useful in showing that we can treat the log determinant as a potential function, where can bound the sum of widths independently of the choices made by the algorithm.

Lemma 6.10. We have:

T −1
det ΣT = det Σ0 (1 + wt2).
t=0

Proof: By the deﬁnition of Σt+1, we have

det Σt+1 = det(Σt + xtxt ) = det(Σ1t /2(I + Σ−t 1/2xtxt Σ−t 1/2)Σ1t /2) = det(Σt) det(I + Σ−t 1/2xt(Σ−t 1/2xt) ) = det(Σt) det(I + vtvt ),

where vt := Σ−t 1/2xt. Now observe that vt vt = wt2 and (I + vtvt )vt = vt + vt(vt vt) = (1 + wt2)vt

Hence (1 + wt2) is an eigenvalue of I + vtvt . Since vtvt is a rank one matrix, all other eigenvalues of I + vtvt equal 1. Hence, det(I + vtvt ) is (1 + wt2), implying det Σt+1 = (1 + wt2) det Σt. The result follows by induction.
Lemma 6.11. (“Potential Function” Bound) For any sequence x0, . . . xT −1 such that, for t < T , xt 2 ≤ B, we have:

1 T −1

log

det ΣT −1/ det Σ0

= log det

I+ λ

xtxt

t=0

≤ d log

T B2 1+

.

dλ

Proof: Denote the eigenvalues of

T −1 t=0

xt

xt

as σ1, . . . σd, and note:

d

T −1

σi = Trace

xtxt

i=1

t=0

T −1

=

xt 2 ≤ T B2.

t=0

Using the AM-GM inequality,

1 T −1

log det I + λ

xtxt

t=0

d
= log (1 + σi/λ)
i=1

= d log

d

1/d

(1 + σi/λ) ≤ d log

1d d (1 + σi/λ)

≤ d log

T B2 1+
dλ

,

i=1

i=1

which concludes the proof.

Finally, we are ready to prove that if µ always stays within the evolving conﬁdence region, then our regret is under control.

70

Proof:[Proof of Proposition 6.7] Assume that µ ∈ BALLt for all t. We have that:

T −1

T −1

T −1

regret2t ≤ 4βt min(wt2, 1) ≤ 4βT min(wt2, 1)

t=0

t=0

t=0

T −1
≤ 8βT ln(1 + wt2) ≤ 8βT log det ΣT −1/ det Σ0
t=0

= 8βT d log

T B2 1+
dλ

where the ﬁrst inequality follow from By Lemma 6.9; the second from that βt is an increasing function of t; the third uses that for 0 ≤ y ≤ 1, ln(1 + y) ≥ y/2; the ﬁnal two inequalities follow by Lemmas 6.10 and 6.11.

6.3.2 Conﬁdence Analysis

Proof:[Proof of Proposition 6.6] Since rτ = xτ · µ + ητ , we have:

t−1

t−1

µt − µ = Σ−t 1 rτ xτ − µ = Σ−t 1 xτ (xτ · µ + ητ ) − µ

τ =0

τ =0

t−1

= Σ−t 1

xτ (xτ )

t−1

t−1

µ − µ + Σ−t 1 ητ xτ = λΣ−t 1µ + Σ−t 1 ητ xτ

τ =0

τ =0

τ =0

For any 0 < δt < 1, using Lemma A.9, it holds with probability at least 1 − δt,

(µt − µ ) Σt(µt − µ ) = (Σt)1/2(µt − µ )

≤ λΣ−t 1/2µ √
≤ λµ +

t−1

+ Σ−t 1/2

ητ xτ

τ =0

2σ2 log (det(Σt) det(Σ0)−1/δt).

where we have also used the triangle inequality and that Σ−t 1 ≤ 1/λ.
We seek to lower bound Pr(∀t, µ ∈ BALLt). Note that at t = 0, by our choice of λ, we have that BALL0 contains W , so Pr(µ ∈/ BALL0) = 0. For t ≥ 1, let us assign failure probability δt = (3/π2)/t2δ for the t-th event, which, using the above, gives us an upper bound on the sum failure probability as

∞

∞

1 − Pr(∀t, µ ∈ BALLt) = Pr(∃t, µ ∈/ BALLt) ≤ Pr(µ ∈/ BALLt) < (1/t2)(3/π2)δ = δ/2.

t=1

t=1

This along with Lemma 6.11 completes the proof.

6.4 Bibliographic Remarks and Further Readings
The orignal multi-armed bandit model goes to back to [Robbins, 1952]. The linear bandit model was ﬁrst introduced in [Abe and Long, 1999]. Our analysis of the LinUCB algorithm follows from the original proof in [Dani et al., 2008], with a simpliﬁed concentration analysis due to [Abbasi-Yadkori et al., 2011]. The ﬁrst sublinear regret bound here was due to [Auer et al., 2002], which used a more complicated algorithm. The lower bound we present is also due to [Dani et al., 2008], which also shows that LinUCB is minimax optimal.
71

72

Chapter 7
Strategic Exploration in Tabular MDPs

We now turn to how an agent acting in an unknown MDP can obtain a near-optimal reward over time. Compared with the previous setting with access to a generative model, we no longer have easy access to transitions at each state, but only have the ability to execute trajectories in the MDP. The main complexity this adds to the learning process is that the agent has to engage in exploration, that is, plan to reach new states where enough samples have not been seen yet, so that optimal behavior in those states can be learned.

We will work with ﬁnite-horizon MDPs with a ﬁxed start state s0 (see Section 1.2), and we will assume the agent learns in an episodic setting, as introduced in Chapter 2. Here, in in every episode k, the learner acts for H step starting from
a ﬁxed starting state s0 and, at the end of the H-length episode, the state is reset to s0. It is straightforward to extend this setting where the starting state is sampled from a distribution, i.e. s0 ∼ µ.

The goal of the agent is to minimize her expected cumulative regret over K episodes:

K−1 H−1

Regret := E KV (s0) −

r(skh, akh) ,

k=0 h=0

where the expectation is with respect to the randomness of the MDP environment and, possibly, any randomness of the agent’s strategy.

This chapter considers tabular MDPs, where S and A are discrete. We will now present a (nearly) sublinear regret algorithm, UCB-Value Iteration. This chapter follows the proof in√[Azar et al., 2017]. We ﬁrst give a simpliﬁed analysis of√UCB-VI that gives a regret bound in the order of H2S AK, followed by a more reﬁned analysis that gives a H2 SAK + H3S2A regret bound.

7.1 On The Need for Strategic Exploration
Consider the chain MDP of length H + 2 shown in Figure 0.1. The starting state of interest is state s0. We consider a uniformly random policy πh(s) that at any time step h and any state s, selects an action from all A many actions uniform randomly. It is easy to see that starting from s0, the probability of such uniform random policy hitting the rewarding state sH+1 is exponentially small, i.e., O AH , since it needs to select the right action that moves one step right at every time step h. This example demonstrates that we need to perform strategic exploration in order to avoid the exponential sample complexity.

73

a4

a4

a1

a3

a3

s0

a2

s1 · · ·

a2

sH

a1 sH+1

a1

a1

Figure 0.1: A deterministic, chain MDP of length H + 2. Rewards are 0 everywhere other than r(sH+1, a1) = 1. In this example we have A = 4.

Algorithm 5 UCBVI
Input: reward function r (assumed to be known), conﬁdence parameters 1: for k = 0 . . . K − 1 do 2: Compute Phk as the empirical estimates, for all h (Eq. 0.1) 3: Compute reward bonus bkh for all h (Eq. 0.2) 4: Run Value-Iteration on {Phk, r + bkh}Hh=−01 (Eq. 0.3) 5: Set πk as the returned policy of VI.
6: end for

7.2 The UCB-VI algorithm

If the learner then executes πk in the underlying MDP to generate a single trajectory τ k = {skh, akh}Hh=−01 with ah = πhk(skh) and skh+1 ∼ Ph(·|skh, akh). We ﬁrst deﬁne some notations below. Consider the very beginning of episode k. We use the history information up to the end of episode k − 1 (denoted as H<k) to form some statistics. Speciﬁcally,
we deﬁne:

k−1
Nhk(s, a, s ) = 1{(sih, aih, sih+1) = (s, a, s )},
i=0
k−1
Nhk(s, a) = 1{(sih, aih) = (s, a)}, ∀h, s, a.
i=0

Namely, we maintain counts of how many times s, a, s and s, a are visited at time step h from the beginning of the learning process to the end of the episode k − 1. We use these statistics to form an empirical model:

Phk (s

|s, a)

=

Nhk(s, a, s Nhk(s, a)

)

,

∀h,

s,

a,

s

.

(0.1)

We will also use the counts to deﬁne a reward bonus, denoted as bh(s, a) for all h, s, a. Denote L := ln (SAHK/δ) (δ as usual represents the failure probability which we will deﬁne later). We deﬁne reward bonus as follows:

bkh(s, a) = 2H

L Nhk(s, a) .

(0.2)

With reward bonus and the empirical model, the learner uses Value Iteration on the empirical transition Phk and the combined reward rh + bkh. Starting at H (note that H is a ﬁctitious extra step as an episode terminates at H − 1), we
74

perform dynamic programming all the way to h = 0:

VHk (s) = 0, ∀s,

Qkh(s, a) = min rh(s, a) + bkh(s, a) + Phk(·|s, a) · Vhk+1, H ,

Vhk (s)

=

max
a

Qkh(s,

a),

πhk (s)

=

argmaxa

Qkh(s,

a),

∀h,

s,

a.

(0.3)

Note that when using Vhk+1 to compute Qkh, we truncate the value by H. This is because we know that due to the assumption that r(s, a) ∈ [0, 1], no policy’s Q value will ever be larger than H.
Denote πk = {π0k, . . . , πHk −1}. Learner then executes πk in the MDP to get a new trajectory τ k.
UCBVI repeats the above procedure for K episodes.

7.3 Analysis

We will prove the following theorem.

Theorem 7.1 (Regret Bound of UCBVI). UCBVI achieves the following regret bound:

K −1

Regret := E

V (s0) − V πk (s0)

k=0

√ ≤ 10H2S AK · ln(SAH2K2) = O H2S AK

Remark While the above√regret is sub-optimal, the algorithm we presented here indeed achieves a sharper bound in the leading term O(H2 SAK) [Azar et al., 2017], which gives the√tight dependency bound on S, A, K. In Section 7.4, we will present a reﬁned analysis that indeed achieves H2 SAK regret. The dependency on H is not tight and tightening the dependency on H requires modiﬁcations to the reward bonus (use Bernstein inequality rather than Hoeffding’s inequality for reward bonus design).
We prove the above theorem in this section.
We start with bounding the error from the learned model Phk.
Lemma 7.2 (State-action wise model error). Fix δ ∈ (0, 1). For all k ∈ [0, . . . , K − 1], s ∈ S, a ∈ A, h ∈ [0, . . . , H − 1], with probability at least 1 − δ, we have that for any f : S → [0, H]:

Phk(·|s, a) − Ph (·|s, a)

f ≤ 8H

S ln(SAHK/δ) Nhk(s, a) .

Remark The proof of the above lemma requires a careful argument using Martingale difference sequence mainly because Nhk(s, a) itself is a random quantity. We give a proof the above lemma at the end of this section (Sec. 7.3.1). Using the above lemma will result a sub-optimal regret bound in terms of the dependence on S. In Section 7.4, we will give a different approach to bound Phk(·|s, a) − Ph (·|s, a) f which leads to an improved regret bound.
The following lemma is still about model error, but this time we consider an average model error with respect to V — a deterministic quantity.
75

Lemma 7.3 (State-action wise average model error under V ). Fix δ ∈ (0, 1). For all k ∈ [1, . . . , K − 1], s ∈ S, a ∈ A, h ∈ [0, . . . , H − 1], and consider Vh : S → [0, H], with probability at least 1 − δ, we have:

Phk(·|s, a) · Vh+1 − Ph (·|s, a) · Vh+1 ≤ 2H

ln(S AH N/δ) Nhk(s, a) .

Note that we can use Lemma 7.2 to bound Phk(·|s, a) · Vh+1 − Ph (·|s, a) · Vh+1 . However, the above lemma shows that by leveraging the fact that V is a deterministic quantity (i.e., independent of the data collected during learning), we can get a tighter upper bound which does not scale polynomially with respect to S.
Proof: Consider a ﬁxed tuple s, a, k, h ﬁrst. By the deﬁnition of Phk, we have:

Phk(·|s, a)

· Vh+1

=

1 Nhk(s, a)

k−1
1{(sih, aih)
i=0

=

(s, a)}Vh+1(sih+1).

Now denote Hh,i as the entire history from t = 0 to iteration t = i where in iteration i, Hh,i includes history from

time step 0 up to and including time step h. We deﬁne random variables: Xi = 1{(sih, aih) = (s, a)}Vh+1(sih+1) −

E 1{(sih, aih) = (s, a)}Vh+1(sh+1)|Hh,i for i = 0, . . . , K − 1. These random variables have the following proper-

ties. First |Xi| ≤ H if 1{(sih, aih) = (s, a)} = 1, else |Xi| = 0, which implies that

k i=0

|Xi|2

≤

Nhk(s, a).

Also

we have E [Xi|Hh,i] = 0 for all i. Thus, {Xi}i≥0 is a Martingale difference sequence. Using Azuma-Hoeffding’s

inequality, we get that for any ﬁxed k, with probability at least 1 − δ,

k−1

k−1

Xi =

1{(sih, aih) = (s, a)}Vh+1(sih+1) − Nhk(s, a)Es ∼Ph (s,a)V (s ) ≤ 2H Nhk(s, a) ln(1/δ).

i=0

i=0

Divide Nhk(s, a) on both sides and use the deﬁnition of Ph(·|s, a) Vh+1, we get:

Ph(·|s, a) Vh+1 − Ph (·|s, a) Vh+1 ≤ 2H ln(1/δ)/Nhk(s, a).

With a union bound over all s ∈ S, a ∈ A, k ∈ [N ], h ∈ [H], we conclude the proof. We denote the two inequalities in Lemma 7.2 and Lemma 7.3 as event Emodel. Note that the failure probability of Emodel is at most 2δ. Below we condition on Emodel being true (we deal with failure event at the very end). Now we study the effect of reward bonus. Similar to the idea in multi-armed bandits, we want to pick a policy πk, such that the value of πk in under the combined reward rh + bkh and the empirical model Phk is optimistic, i.e., we want V0k(s0) ≥ V0 (s0) for all s0. The following lemma shows that via reward bonus, we are able to achieve this optimism. Lemma 7.4 (Optimism). Assume Emodel is true. For all episode k, we have:
V0k(s0) ≥ V0 (s0), ∀s0 ∈ S;
where Vhk is computed based on VI in Eq. 0.3.
Proof: We prove via induction. At the additional time step H we have VHk (s) = VH (s) = 0 for all s. Starting at h + 1, and assuming we have Vhk+1(s) ≥ Vh+1(s) for all s, we move to h below.
76

Consider any s, a ∈ S × A. First, if Qkh(s, a) = H, then we have Qkh(s, a) ≥ Qh(s, a). Qkh(s, a) − Qh(s, a) = bkh(s, a) + Phk(·|s, a) · Vhk+1 − Ph (·|s, a) · Vh+1 ≥ bkh(s, a) + Phk(·|s, a) · Vh+1 − Ph (·|s, a) · Vh+1 = bkh(s, a) + Phk(·|s, a) − Ph (·|s, a) · Vh+1

≥ bkh(s, a) − 2H

ln(S AH K/δ) Nhk(s, a)

≥

0.

where the ﬁrst inequality is from the inductive hypothesis, and the last inequality uses Lemma 7.3 and the deﬁnition of bonus.

From Qkh, one can ﬁnish the proof by showing Vhk(s) ≥ Vh (s), ∀s.

Before we prove our ﬁnal result, one more technical lemma will be helpful:

Lemma 7.5. Consider arbitrary K sequence of trajectories τ k = {skh, akh}Hh=−01 for k = 0, . . . , K − 1. We have

K−1 H−1 k=0 h=0

1

√ ≤ 2H SAK.

Nhk(skh, akh)

Proof: We swap the order of the two summation above:

K−1 H−1 k=0 h=0

1

H−1 K−1

=

Nhk(skh, akh) h=0 k=0

1

H −1

=

NhK (s,a) √1

Nhk(skh, akh) h=0 s,a∈S×A i=1

i

H −1
≤2

H −1

NhK (s, a) ≤

SA

√ NhK (s, a) = H SAK,

h=0 s,a∈S×A

h=0

s,a

where in the ﬁrst inequality we use the fact that

N i=1

√ 1/ i

≤

√ 2 N,

and

in

the

second

inequality

we

use

CS

inequality.

The proof of our main theorem now follows:

Proof:[Proof of Theorem 7.1]

Let us consider episode k and denote H<k as the history up to the end of episode k − 1. We consider bounding V − V πk . Using optimism and the simulation lemma, we can get the following result:

H −1

V (s0) − V πk (s0) ≤ V0k(s0) − V0πk (s0) ≤

Esh,ah∼dπhk bkh(sh, ah) + Phk(·|sh, ah) − P (·|sh, ah) · Vhπ+k1

h=0

(0.4)

We prove the above two inequalities in the lecture. We leave the proof of the above inequality (Eq 0.4) as an exercise for readers. Note that this is slightly different from the usual simulation lemma, as here we truncate V by H during VI.

Under Emodel, we can bound Phk(·|sh, ah) − P (·|sh, ah) · Vhπ+k1 (recall Lemma 7.2) with a Holder’s inequality:

Phk(·|sh, ah) − P (·|sh, ah) · Vhπ+k1 ≤ Phk(·|sh, ah) − P (·|sh, ah) Vhπ+k1

1

∞

≤ 8H

SL Nhk(s, a) ,

77

where recall L := ln(SAKH/δ). Hence, for the per-episode regret V (s0) − V πk (s0), we obtain:

H −1

V (s0) − V πk (s0) ≤

Esh,ah∼dπhk bkh(sh, ah) + 8H

h=0

SL/Nhk(sh, ah)

H −1

≤

Esh ,ah ∼dπhk

h=0

10H

SL/Nhk(sh, ah)



√

H −1

= 10H SL · E 

h=0



1

H<k ,

Nhk(skh, akh)

where in the last term the expectation is taken with respect to the trajectory {skh, akh} (which is generated from πk) while conditioning on all history H<k up to and including the end of episode k − 1.
Now we sum all episodes together and take the failure event into consideration:

K −1

K−1

E

V (s0) − V πk (s0) = E 1{Emodel}

V (s0) − V πk (s0)

K −1

+ E 1{E model}

V (s0) − V πk (s0)

k=0

k=0

k=0

K−1

≤ E 1{Emodel}

V (s0) − V πk (s0) + 2δKH

(0.5)

k=0

≤ 10H


K−1 H−1
S ln(SAHK/δ)E 
k=0 h=0


1  + 2δKH,
Nhk(skh, akh)

where the last inequality uses the law of total expectation. We can bound the double summation term above using lemma 7.5. We can conclude that:

K−1

E

V (s0) − V πk (s0) ≤ 10H2S AK ln(SAHK/δ) + 2δKH.

k=0

Now set δ = 1/KH, we get:

K−1

E

V (s0) − V πk (s0) ≤ 10H2S AK ln(SAH2K2) + 2 = O H2S AK ln(SAH2K2) .

k=0

This concludes the proof of Theorem 7.1.

7.3.1 Proof of Lemma 7.2

Here we include a proof of Lemma 7.2. The proof consists of two steps, ﬁrst we using Martingale concentration to bound |(Phk(·|s, a) − Ph (·|s, a)) f | for all s, a, h, k, but for a ﬁxed f : S → [0, H]. Note that there are inﬁnitely many such f that maps from S to [0, H]. Thus, the second step is to use a covering argument (i.e., -net on all
{f : S → [0, H]}) to show that |(Phk(·|s, a) − Ph (·|s, a)) f | is bounded for all s, a, h, k, and all f : S → [0, H].

Proof:[Proof of Lemma 7.2] We consider a ﬁxed tuple s, a, k, h, f ﬁrst. Recall the deﬁnition of Phk(s, a), and we

have: Phk(·|s, a) f =

k−1 i=0

1{sih, aih

=

s, a}f (sih+1)/Nhk(s, a).

78

Deﬁne Hh,i as the history starting from the beginning of iteration 0 all the way up to and include time step h at iteration i. Deﬁne random variables Xi as Xi = 1{sih, aih = s, a}f (sih+1) − 1{sih, aih = s, a}Es ∼Ph (s,a)f (s ). We now show that {Xi} is a Martingale difference sequence. First we have E[Xi|Hh,i] = 0, since 1{sih, aih = s, a} is a deterministic quantity given Hh,i. Second, we have |Xi| = 0 for (sih, aih) = (s, a), and |Xi| ≤ H when (sih, aih) = (s, a). Thus,
we have shown that it is a Martingale difference sequence.

Now we can apply Azuma-Hoeffding’s inequality (Theorem A.5). With probability at least 1 − δ, we have:

k−1

k−1

Xi =

1{(sih, aih) = (s, a)}f (sih+1) − Nhk(s, a)Es ∼Ph (s,a)f (s ) ≤ 2H Nhk(s, a) ln(1/δ).

i=0

i=0

Now apply union bound over all s ∈ S, a ∈ A, h ∈ [0, . . . , H − 1], k ∈ [0, . . . , K − 1], we have that with probability at least 1 − δ, for all s, a, h, k, we have:

k−1
1{(sih, aih) = (s, a)}f (sih+1) − Nhk(s, a)Es ∼Ph (s,a)f (s ) ≤ 2H Nhk(s, a) ln(SAKH/δ).
i=0

(0.6)

Note that the above result only holds for the ﬁxed f that we speciﬁed at the beginning of the proof. We are left to
perform a covering argument over all functions that map from S to [0, H]. We use a standard -net argument for that. We abuse notation a bit and denote f as a vector in [0, H]S.
√ Note that f 2 ≤ H S for any f : S√→ [0, H]. A standard -net argument shows that we can construct an -net N over [0, H]S with |N | ≤ (1 + 2H S/ )S, such that for any f ∈ [0, H]S, there exists a point f ∈ N , such that f − f 2 ≤ . Note that the -net N is independent of the training data during the learning process.

Now using Eq. 0.6 and a union bound over all f in N , we have that with probability at least 1−δ, for all s, a, k, h, f ∈

N , we have:

k−1 i=0

1{(sih, aih) = (s, a)}f (sih+1) Nhk(s, a)

−

Es

∼Ph (s,a)f (s

)

≤ 2H

√

S ln(SAKH(1 + 2H S/ )/δ)

Nhk(s, a)

.

Finally, for any f ∈ [0, H]S, denote its closest point in N as f , we have:

k−1 i=0

1{(sih, aih) = (s, a)}f (sih+1) Nhk(s, a)

−

Es

∼Ph (s,a)f (s

)

≤

k−1 i=0

1{(sih, aih) = (s, a)}f Nhk(s, a)

(sih+1)

−

Es

∼Ph (s,a)f

(s

)

+

k−1 i=0

1{(sih,

aih)

=

(s, a)}(f (sih+1) − f

(sih+1))

Nhk(s, a)

+ Es ∼Ph (s,a)(f (s ) − f (s ))

√ S ln SAKH(1 + 2H S/ )/δ

≤ 2H

Nhk(s, a)

+2 ,

where in the last inequality we use the fact that if f − f

2 ≤ , then |f (s) − f (s)| ≤ , ∀s ∈ S.

Now we can set = 1/K and use the fact that Nhk(s, a) ≤ K, we have:

k−1 i=0

1{(sih, aih) = (s, Nhk(s, a)

a)}f (sih+1)

−

Es

∼Ph (s,a)f (s

)

≤ 2H

√ S ln SAKH(1 + 2HK S)/δ
Nhk(s, a)

2 +
K

≤ 4H

S

ln(4H 2 S 2 K 2 A/δ) Nhk(s, a)

≤

8H

S ln(HSKA/δ) Nhk(s, a) ,

79

where the last inequality uses the simple inequality ln(4H2S2K2A) ≤ 4 ln(HSKA). This concludes the proof.

7.4 An Improved Regret Bound

In this section, we show that UCB-VI also enjoys the regret bound O˜

√ H2 SAK + H3S2A .

Theorem 7.6 (Improved Regret Bound for UCB-VI). The total expected regret is bounded as follows:

K−1

E KV (s0) −

V πk (s0)

k=0

√

√

≤ 8eH2 SAK · ln(SAH2K2) + 2eH3S2A · ln(K) ln(SAH2K2) = O˜ H2 SAK + H3S2A .

Note that comparing to the regret bound we had in√the previous section, this new regret improves the S dependence in the leading order term (i.e., the term that contains K), but at the cost of introducing a lower order term H3S2A.

We ﬁrst give an improved version of the model error presented in Lemma 7.2. Lemma 7.7. With probability at least 1 − δ, for all h, k, s, a, and all f : S → [0, H], we have:

Phk(·|s, a) − Ph (·|s, a)

f

≤

Es

∼Ph (·|s,a)f (s H

)

+

3H 2 S L Nhk(s, a) .

Below we prove this lemma. Recall the deﬁnition of Phk(s |s, a) in Eq. 0.1. We can use Azume-Bernstein’s inequality to bound |Phk(s |s, a) − Ph (s |s, a)|.
Lemma 7.8. With probability at least 1 − δ, for all s, a, k, h, we have:

Phk(s |s, a) − Ph (s |s, a) ≤ where L := ln(SAKH/δ).

2Ph (s |s, a)L Nhk(s, a)

+

2L Nhk (s,

, a)

Proof: Consider a ﬁxed tuple (s, a, h, k). Deﬁne Hh,i as the history starting from the beginning of iteration 0 to time step h at iteration i (including step h, i.e., up to sih, aih). Deﬁne random variables {Xi}i≥0 as Xi = 1{sih, aih, sih+1 = s, a, s } − 1{sih, aih = s, a}Ph (s |s, a). We now show that {Xi} is a Martingale difference sequence. Note that E [Xi|Hh,i] = 0. We have |Xi| ≤ 1. To use Azuma-Bernstein’s inequality, we note that E[Xi2|Hh,i] is bounded as:

E[Xi2|Hh,i] = 1{sih, aih = s, a}Ph (s |s, a)(1 − Ph (s |s, a)),

where we use the fact that the variance of a Bernoulli with parameter p is p(1−p). This means that

k−1 i=0

E[Xi2|Hh,i]

=

Ph (s |s, a)(1 − Ph (s |s, a))Nhk(s, a). Now apply Bernstein’s inequality on the martingale difference sequence {Xi},

we have:

k−1
Xi ≤
i=0
≤

2Ph (s

|s,

a)(1 − Ph (s |s, Nhk(s, a)

a))

ln(1/δ)

+

2 ln(1/δ) Nhk(s, a)

2Ph

(s |s, a) ln(1/δ) Nhk(s, a)

+

2 ln(1/δ) Nhk(s, a)

80

Recall the deﬁnition of L := ln(SAHK/δ). Apply a union bound over all s ∈ S, a ∈ A, h ∈ [0, . . . , H − 1], k ∈ [0, . . . , K − 1], we conclude the proof.
Now we are ready to prove Lemma 7.7.
Proof:[Proof of Lemma 7.7] Let us condition on the event in Lemma 7.8 being true, of which the probability is at least 1 − δ.
Take any function f : S → [0, H], we have:

Phk(·|s, a) − Ph (·|s, a) f

≤

Phk(s |s, a) − Ph (·|s, a) f (s ) ≤

s ∈S

s ∈S

2LPh (s |s, a)f 2(s Nhk(s, a)

)

+

s

∈S

2Lf (s ) Nhk(s, a)

≤
s ∈S

2LPh (s |s, a)f 2(s Nhk(s, a)

)

+

2S H L Nhk(s, a)

≤

√ S

s

∈S

2LPh (s Nhk (s,

|s, a)

a)f 2(s

)

+

2S H L Nhk(s, a)

=

2H 2 S L Nhk(s, a)

·

s

∈S

Ph (s |s, a)f 2(s H2

)

+

2S H L Nhk(s, a)

≤

H2SL Nhk(s, a)

+

Es

∼Ph (·|s,a)f 2(s H2

)

+

2S H L Nhk(s, a)

≤

Es

∼Ph (·|s,a)f (s H

)

+

H2SL Nhk(s, a)

+

2S H L Nhk(s, a) ,

where the third inequality uses that condition that f ∞ ≤ H, the fourth inequality uses Cauchy-Schwarz inequality, the ﬁfth inequality uses the inequality that ab ≤ (a2 + b2)/2 for any real numbers a and b, and the last inequality uses the condition that f ∞ ≤ H again. This concludes the proof.
So far in Lemma 7.7, we have not specify what function f we will use. The following lemma instantiate f to be Vhk+1 − Vh+1. Recall the bonus deﬁnition, bkh(s, a) := 2H L/Nhk(s, a) where L := ln(SAHK/δ).

Lemma 7.9. Assume the events in Lemma 7.7 and Lemma 7.3 are true. For all s, a, k, h, set f := Vhk+1 − Vh+1, we have:

Phk(·|s, a) − Ph (·|s, a)

f

≤

ξhk (s,

a)

+

e H

E

H −1

ξτk(sτ , aτ ) + bkτ (sτ , aτ )

sh = s, ah = a, πk

τ =h+1

where

ξτi (s, a)

=

3H 2 S L Nτi (s,a)

,

for

any

τ

∈

[0, . . . , H

− 1], i

∈

[0, . . . , K

− 1],

and

the

expectation

is

with

respect

to

the

randomness from the event of following policy πk from (s, a) starting from time step h.

Proof: Using Lemma 7.7 and the fact that f := Vhk+1 − Vh+1, we have:

Phk(·|s, a) − Ph (·|s, a)

f

≤

Es

∼Ph (·|s,a)(Vhk+1(s H

) − Vh+1(s

))

+ ξhk(s, a).

81

(0.7)

Now consider Vhk+1(s ) − Vh+1(s )), we have:

Vhk+1(s ) − Vh+1(s ) ≤ Vhk+1(s ) − r(s , πk(s )) + Ph+1(·|s , πk(s )) Vh+2 ≤ r(s , πk(s )) + Phk+1(·|s , πk(s )) Vhk+2 − r(s , πk(s )) + Ph+1(·|s , πk(s )) Vh+2 = Phk+1(·|s , πk(s )) Vhk+2 − Ph+1(·|s , πk(s )) Vhk+2 + Ph+1(·|s , πk(s )) Vhk+2 − Vh+2

= Phk+1(·|s , πk(s )) − Ph+1(·|s , πk(s )) Vhk+2 − Vh+2

+ Phk+1(·|s , πk(s )) − Ph+1(·|s , πk(s )) + Ph+1(·|s , πk(s )) Vhk+2 − Vh+2

Vh+2

where the ﬁrst inequality uses the fact that Vh+1(s ) = maxa (r(s , a ) + Ph+1(·|s , a ) Vh+2), the second inequality uses the deﬁnition of Vhk+1 and πk. Note that the second term in the RHS of the above inequality can be upper bounded exactly by the bonus using Lemma 7.3, and the ﬁrst term can be further upper bounded using the same operation that
we had in Lemma 7.7, i.e.,

Phk+1(·|s , πk(s )) − Ph+1(·|s , πk(s ))

Vhk+2 − Vh+2

Ph+1(·|s , πk(s )) ≤
H

Vhk+2 − Vh+2

+ ξhk+1(s , πk(s )).

Combine the above inequalities together, we arrive at:

Vhk+1(s ) − Vh+1(s ) ≤ bkh+1(s , πk(s )) + ξhk+1(s , πk(s )) +

1 1+
H

Es ∼Ph+1(s ,πk(s )) Vhk+2(s ) − Vh+2(s ) .

We can recursively apply the same operation on Vhk+2(s ) − Vh+2(s ) till step H, we have:

H −1
Vhk+1(s ) − Vh+1(s ) ≤ E

1 1+
H

τ −h−1
ξτk(sτ , aτ ) + bkτ (sτ , aτ )

sh+1 = s , πk

τ =h+1

H −1

≤ eE

ξτk(sτ , aτ ) + bkτ (sτ , aτ ) sh+1 = s , πk ,

τ =h+1

where the last inequality uses the fact that (1 + 1/H)x ≤ e for any x ∈ N+. Substitute the above inequality into Eq. 0.7, we get:

Phk(·|s, a) − Ph (·|s, a)

f

≤

ξhk (s,

a)

+

e H

E

H −1

ξτk(sτ , aτ ) + bkτ (sτ , aτ )

sh = s, ah = a, πk

τ =h+1

Now we are ready to derive a reﬁned upper bound for per-episode regret. Lemma 7.10. Assume the events in Lemma 7.9 and Lemma 7.4 (optimism) hold. For all k ∈ [0, K − 1], we have:

H −1

V (s0) − V πk (s0) ≤ e

Esh,ah∼dπhk 2bkh(sh, ah) + ξhk(sh, ah) .

h=0

82

Proof: Via optimism in Lemma 7.4, we have:

H −1

V (s0) − V πk (s0) ≤ V0k(s0) − V0πk (s0) ≤

Esh,ah∼dπhk bkh(sh, ah) + Phk(·|sh, ah) − P (·|sh, ah) · Vhk+1

h=0

H −1

=

Esh,ah∼dπhk bkh(sh, ah) + Phk(·|sh, ah) − P (·|sh, ah)

h=0

Vhk+1 − Vh+1 + Phk(·|sh, ah) − P (·|sh, ah)

H −1

≤

Esh,ah∼dπhk 2bkh(sh, ah) + Phk(·|sh, ah) − P (·|sh, ah)

h=0

Vhk+1 − Vh+1

H −1

≤

Esh ,ah ∼dπhk

2bkh(sh,

ah)

+

ξhk (sh ,

ah)

+

e H

E

H −1

ξτk(sτ , aτ ) + bkτ (sτ , aτ ) πk

h=0

τ =h+1

H −1

≤e

Esh,ah∼dπhk 2bkh(sh, ah) + ξhk(sh, ah)

h=0

where the ﬁrst inequality uses optimism, and the fourth inequality uses Lemma 7.9, and the last inequality rearranges by grouping same terms together.

To conclude the ﬁnal regret bound, let us denote the event Emodel as the joint event in Lemma 7.3 and Lemma 7.8, which holds with probability at least 1 − 2δ.

Vh+1

Proof:[Proof of Theorem 7.6] Following the derivation we have in Eq. 0.5, we decompose the expected regret bound as follows:

K −1

K−1

E

V (s0) − V πk (s0) ≤ E 1{Emodel}

V (s0) − V πk (s0)

k=0

k=0

K−1 H−1

≤ eE

2bkh(skh, akh) + ξhk(skh, akh)

k=0 h=0

+ 2δKH + 2δKH,

where the last inequality uses Lemma 7.10. Using Lemma 7.5, we have that:

K−1 H−1

√

2bkh(skh, akh) ≤ 4H L

k=0 h=0

kh

1

√ ≤ 8H2 SAKL.

Nhk(skh, akh)

For k h ξhk(skh, akh), we have:

K−1 H−1
ξhk(skh, akh)
k=0 h=0

=

K−1 H−1
2H 2 S L
k=0 h=0

1 Nhk(sih, akh)

=

H −1
2H 2 S L
h=0

s,a

NhK (s,a) i=1

1 i

≤ 2H2SALHS ln(K) = 2H3S2AL ln(K),

where in the last inequality, we use the fact that NhK(s, a) ≤ K for all s, a, h, and the inequality

t i=1

1/i

≤

ln(t).

Putting things together, and setting δ = 1/(KH), we conclude the proof.

7.5 Phased Q-learning
to be added 83

7.6 Bibliographic Remarks and Further Readings
The ﬁrst provably correct PAC algorithm for reinforcement learning (which ﬁnds a near optimal policy) was due to Kearns and Singh [2002], which provided the E3 algorithm; it achieves polynomial sample complexity in tabular MDPs. Brafman and Tennenholtz [2002] presents the Rmax algorithm which provides a reﬁned PAC analysis over E3. Both are model based approaches [Kakade, 2003] improves the sample complexity to be O(S2A). Both E3 and Rmax uses the concept of absorbing MDPs to achieve optimism and balance exploration and exploitation. Jaksch et al. [2010] provides the ﬁrst O( (T )) regret bound, where T is the number of timesteps in the MDP (T is proportional to K in our setting); this dependence on T is optimal. Subsequently, Azar et al. [2017], Dann et al. [2017] provide algorithms that, asymptotically, achieve minimax regret bound in tabular MDPs. By this, we mean that for sufﬁciently large T (for T ≥ Ω(|S|2)), the results in Azar et al. [2017], Dann et al. [2017] obtain optimal dependencies on |S| and |A|. The requirement that T ≥ Ω(|S|2) before these bounds hold is essentially the requirement that nontrivial model accuracy is required. It is an opent question to remove this dependence. Lower bounds are provided in [Dann and Brunskill, 2015, Osband and Van Roy, 2016, Azar et al., 2017]. Further exploration strategies. Refs and discussion for Q-learning, reward free, and thompson sampling to be added...
84

Chapter 8
Linearly Parameterized MDPs

In this chapter, we consider learning and exploration in linearly parameterized MDPs—the linear MDP. Linear MDP generalizes tabular MDPs into MDPs with potentially inﬁnitely many state and action pairs.
This chapter largely follows the model and analysis ﬁrst provided in [Jin et al., 2020].

8.1 Setting

We consider episodic ﬁnite horizon MDP with horizon H, M = {S, A, {rh}h, {Ph}h, H, s0}, where s0 is a ﬁxed initial state, rh : S × A → [0, 1] and Ph : S × A → ∆(S) are time-dependent reward function and transition kernel. Note that for time-dependent ﬁnite horizon MDP, the optimal policy will be time-dependent as well. For simplicity, we overload notations a bit and denote π = {π0, . . . , πH−1}, where each πh : S → A. We also denote V π := V0π(s0), i.e., the expected total reward of π starting at h = 0 and s0.
We deﬁne the learning protocol below. Learning happens in an episodic setting. Every episode k, learner ﬁrst proposes a policy πk based on all the history information up to the end of episode k − 1. The learner then executes πk in the underlying MDP to generate a single trajectory τ k = {skh, akh}hH=−01 with ah = πhk(skh) and skh+1 ∼ Ph(·|skh, akh). The goal of the learner is to minimize the following cumulative regret over N episodes:

K −1

Regret := E

V − V πk ,

k=0

where the expectation is with respect to the randomness of the MDP environment and potentially the randomness of the learner (i.e., the learner might make decisions in a randomized fashion).

8.1.1 Low-Rank MDPs and Linear MDPs
Note that here we do not assume S and A are ﬁnite anymore. Indeed in this note, both of them could be continuous. Without any further structural assumption, the lower bounds we saw in the Generalization Lecture forbid us to get a polynomially regret bound. The structural assumption we make in this note is a linear structure in both reward and the transition. Deﬁnition 8.1 (Linear MDPs). Consider transition {Ph} and {rh}h. A linear MDP has the following structures on rh
85

and Ph:
rh(s, a) = θh · φ(s, a), Ph(·|s, a) = µhφ(s, a), ∀h
where φ is a known state-action feature map φ : S × A → Rd, and µh ∈ R|S|×d. Here φ, θh are known to the learner, while µ is u√nknown. We further assume the following norm bound on the parameters: (1) sups,a φ(s, a) 2 ≤ 1, (2) v µh 2 ≤ d for any v such that v ∞ ≤ 1, and all h, and (3) θh 2 ≤ W for all h. We assume rh(s, a) ∈ [0, 1] for all h and s, a.
The model essentially says that the transition matrix Ph ∈ R|S|×|S||A| has rank at most d, and Ph = µhΦ. where Φ ∈ Rd×|S||A| and each column of Φ corresponds to φ(s, a) for a pair s, a ∈ S × A.

Linear Algebra Notations For real-valued matrix A, we denote A 2 = supx: x 2=1 Ax 2 which denotes the

maximum singular value of A. We denote

A F as the Frobenius norm

A

2 F

=

i,j A2i,j where Ai,j denotes the

i, j’th entry of A. For any Positive Deﬁnite matrix Λ, we denote x Λx = x 2Λ. We denote det(A) as the determinant

of the matrix A. For a PD matrix Λ, we note that det(Λ) =

d i=1

σi

where

σi

is

the

eigenvalues

of

Λ.

For

notation

simplicity, during inequality derivation, we will use , to suppress all absolute constants. We will use O to suppress

all absolute constants and log terms.

8.2 Planning in Linear MDPs

We ﬁrst study how to do value iteration in linear MDP if µ is given.
We start from QH−1(s, a) = θH−1 · φ(s, a), and πH−1(s) = argmaxa QH−1(s, a) = argmaxa θH−1 · φ(s, a), and VH−1(s) = argmaxa QH−1(s, a).
Now we do dynamic programming from h + 1 to h:

Qh(s, a) = θh · φ(s, a) + Es∼Ph(·|s,a)Vh+1(s ) = θh · φ(s, a) + Ph(·|s, a) · Vh+1 = θh · φ(s, a) + (µhφ(s, a)) Vh+1 (0.1)

= φ(s, a) · θh + (µh) Vh+1 = φ(s, a) · wh,

(0.2)

where we denote wh := θh + (µh) Vh+1. Namely we see that Qh(s, a) is a linear function with respect to φ(s, a)! We can continue by deﬁning πh(s) = argmaxa Qh(s, a) and Vh (s) = maxa Qh(s, a).
At the end, we get a sequence of linear Q , i.e., Qh(s, a) = wh · φ(s, a), and the optimal policy is also simple, πh(s) = argmaxa wh · φ(s, a), for all h = 0, . . . , H − 1.
One key property of linear MDP is that a Bellman Backup of any function f : S → R is a linear function with respect to φ(s, a). We summarize the key property in the following claim.

Claim 8.2. Consider any arbitrary function f : S → [0, H]. At any time step h ∈ [0, . . . H − 1], there must exist a w ∈ Rd, such that, for all s, a ∈ S × A:

rh(s, a) + Ph(·|s, a) · f = w φ(s, a).

The proof of the above claim is essentially the Eq. 0.1. 86

8.3 Learning Transition using Ridge Linear Regression

In this section, we consider the following simple question: given a dataset of state-action-next state tuples, how can we learn the transition Ph for all h?
Note that µ ∈ R|S|×d. Hence explicitly writing down and storing the parameterization µ takes time at least |S|. We show that we can represent the model in a non-parametric way.

We consider a particular episode n. Similar to Tabular-UCBVI, we learn a model at the very beginning of the episode n using all data from the previous episodes (episode 1 to the end of the episode n − 1). We denote such dataset as:

Dhn =

sih, aih, sih+1

n−1 i=0

.

We maintain the following statistics using Dhn:

n−1
Λnh = φ(sih, aih)φ(sih, aih) + λI,
i=0

where λ ∈ R+ (it will be set to 1 eventually, but we keep it here for generality).
To get intuition of Λn, think about the tabular setting where φ(s, a) is a one-hot vector (zeros everywhere except that the entry corresponding to (s, a) is one). Then Λnh is a diagonal matrix and the diagonal entry contains N n(s, a)—the number of times (s, a) has been visited.

We consider the following multi-variate linear regression problem. Denote δ(s) as a one-hot vector that has zero

everywhere except that the entry corresponding to s is one. Denote

i h

=

P (·|sih, aih) − δ(sih+1).

Conditioned on

history Hhi (history Hhi denotes all information from the very beginning of the learning process up to and including

(sih, aih)), we have:

E ih|Hhi = 0,

simply because sih+1 is sampled from Ph(·|sih, aih) conditioned on (sih, aih). Also note that

i h

1 ≤ 2 for all h, i.

Since µhφ(sih, aih) = Ph(·|sih, aih), and δ(sih+1) is an unbiased estimate of Ph(·|sih, aih) conditioned on sih, aih, it is reasonable to learn µ via regression from φ(sih, aih) to δ(sih+1). This leads us to the following ridge linear regression:

n−1

µnh = argminµ∈R|S|×d

µφ(sih, aih) − δ(sih+1)

2 2

+

λ

µ

2 F

.

i=0

Ridge linear regression has the following closed-form solution:

n−1
µnh = δ(sih+1)φ(sih, aih) (Λnh)−1
i=0

(0.3)

Note that µnh ∈ R|S|×d, so we never want to explicitly store it. Note that we will always use µnh together with a speciﬁc s, a pair and a value function V (think about value iteration case), i.e., we care about Phn(·|s, a) · V := (µnhφ(s, a)) · V , which can be re-written as:

n−1

Phn(·|s, a) · V := (µnhφ(s, a)) · V = φ(s, a)

(Λnh)−1φ(sih, aih)V (sih+1),

i=0

87

where we use the fact that δ(s) V = V (s). Thus the operator Phn(·|s, a) · V simply requires storing all data and can be computed via simple linear algebra and the computation complexity is simply poly(d, n)—no poly dependency on
|S |.

Let us calculate the difference between µnh and µh.

Lemma 8.3 (Difference between µh and µh). For all n and h, we must have:

n−1

µnh − µh = −λµh (Λnh)−1 +

i h

φ(sih

,

aih)

(Λnh)−1 .

i=1

Proof: We start from the closed-form solution of µnh:

n−1

n−1

µnh =

δ(sih+1)φ(sih, aih) (Λnh)−1 =

(P (·|sih, aih) +

n h

)φ(sih,

aih

)

(Λnh )−1

i=0

i=0

n−1

n−1

n−1

= (µhφ(sih, aih) + ih)φ(sih, aih) (Λnh)−1 = µ∗hφ(sih, aih)φ(sih, aih) (Λnh)−1 +

ihφ(sih, aih) (Λnh)−1

i=0

i=0

i=0

n−1

n−1

= µhφ(sih, aih)φ(sih, aih) (Λnh)−1 +

ihφ(sih, aih) (Λnh)−1

i=0

i=0

n−1

n−1

= µh(Λnh − λI)(Λnh)−1 +

i h

φ(sih,

aih

)

(Λnh)−1 = µ∗h − λµh(Λnh)−1 +

ihφ(sih, aih) (Λnh)−1.

i=0

i=0

Rearrange terms, we conclude the proof.

Lemma 8.4. Fix V : S → [0, H]. For all n and s, a ∈ S × A, and h, with probability at least 1 − δ, we have:

n−1
φ(sih, aih)(V
i=0

ih)

≤ 3H

(Λnh )−1

ln H det(Λnh)1/2 det(λI)−1/2 . δ

Proof: We ﬁrst check the noise terms {V

i h

}h,i

.

Since V

is independent of the

data (it’s a pre-ﬁxed function), and

by linear property of expectation, we have:

EV

ih|Hhi = 0,

|V

ih| ≤

V∞

i h

1 ≤ 2H, ∀h, i.

Hence, this is a Martingale difference sequence. Using the Self-Normalized vector-valued Martingale Bound (Lemma A.9), we have that for all n, with probability at least 1 − δ:

n−1
φ(sih, aih)(V
i=0

ih)

≤ 3H

(Λnh )−1

ln det(Λnh)1/2 det(λI)−1/2 . δ

Apply union bound over all h ∈ [H], we get that with probability at least 1 − δ, for all n, h:

n−1
φ(sih, aih)(V
i=0

ih)

≤ 3H

(Λnh )−1

ln H det(Λnh)1/2 det(λI)−1/2 . δ

(0.4)

88

8.4 Uniform Convergence via Covering
Now we take a detour ﬁrst and consider how to achieve a uniform convergence result over a function class F that contains inﬁnitely many functions. Previously we know how to get uniform convergence if F is ﬁnite—we simply do a union bound. However, when F contains inﬁnitely many functions, we cannot simply apply a union bound. We will use the covering argument here. Consider the following ball with radius R: Θ = {θ ∈ Rd : θ 2 ≤ R ∈ R+}. Fix an . An -net N ⊂ Θ is a set such that for any θ ∈ Θ, there exists a θ ∈ N , such that θ − θ 2 ≤ . We call the smallest -net as -cover. Abuse notations a bit, we simply denote N as the -cover. The -covering number is the size of -cover N . We deﬁne the covering dimension as ln (|N |)
Lemma 8.5. The -covering number of the ball Θ = {θ ∈ Rd : θ 2 ≤ R ∈ R+} is upper bounded by (1 + 2R/ )d.

We can extend the above deﬁnition to a function class. Speciﬁcally, we look at the following function. For a triple of (w, β, Λ) where w ∈ Rd and w 2 ≤ L, β ∈ [0, B], and Λ such that σmin(Λ) ≥ λ, we deﬁne fw,β,Λ : S → [0, R] as
follows:

fw,β,Λ(s) = min

max
a

w φ(s, a) + β

φ(s, a) Λ−1φ(s, a)

,H

, ∀s ∈ S.

(0.5)

We denote the function class F as:

F = {fw,β,Λ : w 2 ≤ L, β ∈ [0, B], σmin(Λ) ≥ λ}.

(0.6)

Note that F contains inﬁnitely many functions as the parameters are continuous. However we will show that it has ﬁnite covering number that scales exponentially with respect to the number of parameters in (w, β, Λ).
Why do we look at F? As we will see later in this chapter F contains all possible Qh functions one could encounter during the learning process.

Lemma 8.6 ( -covering dimension of F). Consider F deﬁned in Eq. 0.6. Denote its -cover as N with the ∞ norm as the distance metric, i.e., d(f1, f2) = f1 − f2 ∞ for any f1, f2 ∈ F . We have that:

√

√

ln (|N |) ≤ d ln(1 + 6L/ ) + ln(1 + 6B/( λ )) + d2 ln(1 + 18B2 d/(λ 2)).

Note that the -covering dimension scales quadratically with respect to d. Proof: We start from building a net over the parameter space (w, β, Λ), and then we convert the net over parameter space to an -net over F under the ∞ distance metric.
89

We pick two functions that corresponding to parameters (w, β, Λ) and (wˆ, βˆ, Λ).

|f (s) − fˆ(s)| ≤ max w φ(s, a) + β φ(s, a) Λ−1φ(s, a) − max wˆ φ(s, a) + βˆ φ(s, a) Λˆ−1φ(s, a)

a

a

≤ max w φ(s, a) + β φ(s, a) Λ−1φ(s, a) − wˆ φ(s, a) + βˆ φ(s, a) Λˆ−1φ(s, a)
a

≤ max (w − wˆ) φ(s, a) + max (β − βˆ) φ(s, a) Λ−1φ(s, a)

a

a

+ max βˆ( φ(s, a) Λ−1φ(s, a) − φ(s, a) Λˆ −1φ(s, a))
a
√ ≤ w − wˆ 2 + |β − βˆ|/ λ + B φ(s, a) (Λ−1 − Λˆ −1)φ(s, a)

≤ w − wˆ 2 + |β − βˆ|/√λ + B Λ−1 − Λˆ −1 F

Note that Λ−1 is a PD matrix with σmax(Λ−1) ≤ 1/λ. √
Now we consider the /3-Net N /3,w over {√w : w 2 ≤ L}, λ /3-net N√λ /3,β over interval [0, B] for β, and 2/(9B2)-net N 2/(9B),Λ over {Λ : Λ F ≤ d/λ}. The product of these three nets provide a -cover for F , which
means that that size of the -net N for F is upper bounded as:

ln |N

|

≤

ln |N

/3,w |

+

ln |N√λ

/3,β |

+

ln |N √

2 /(9B 2 ),Λ |

√

≤ d ln(1 + 6L/ ) + ln(1 + 6B/( λ )) + d2 ln(1 + 18B2 d/(λ 2)).

Remark Covering gives a way to represent the complexity of function class (or hypothesis class). Relating to VC, covering number is upper bound roughly by exp(d) with d being the VC-dimension. However, there are cases where VC-dimensional is inﬁnite, but covering number if ﬁnite.

Now we can build a uniform convergence argument for all f ∈ F.

Lemma 8.7 (Uniform Convergence Results). Set λ = 1. Fix δ ∈ (0, 1). For all n, h, all s, a, and all f ∈ F, with probability at least 1 − δ, we have:

Phn(·|s, a) − P (·|s, a) · f H φ(s, a) (Λnh)−1

√

√

d ln(1 + 6L N ) + d ln(1 + 18B2 dN ) +

H ln

.

δ

Proof: Recall Lemma 8.4, we have with probability at least 1 − δ, for all n, h, for a pre-ﬁxed V (independent of the random process):

n−1
φ(sih, aih)(V
i=1

2

i h

)

≤ 9H2ln H det(Λnh)1/2 det(λI)−1/2 ≤ 9H2 δ

(Λnh )−1

where we have used the fact that φ 2 ≤ 1, λ = 1, and Λnh 2 ≤ N + 1.

H ln + d ln (1 + N )
δ

Denote the -cover of F as N . With an application of a union bound over all functions in N , we have that with probability at least 1 − δ, for all V ∈ N , all n, h, we have:

n−1
φ(sih, aih)(V
i=1

2

i h

)

≤ 9H2 ln H + ln (|N |) + d ln (1 + N ) . δ

(Λnh )−1

90

Recall Lemma 8.6, substitute the expression of ln |N | into the above inequality, we get:

n−1
φ(sih, aih)(V
i=1

2
ih)

≤ 9H2

ln

H

+ d ln(1 +

6L/

)

√ + d2 ln(1 + 18B2 d/

2) + d ln (1

+ N)

.

δ

(Λnh )−1

Now consider an arbitrary f ∈ F. By the deﬁnition of -cover, we know that for f , there exists a V ∈ N , such that f − V ∞ ≤ . Thus, we have:

n−1
φ(sih, aih)(f
i=1

2

n−1

2

n−1

2

i h

)

≤2

φ(sih, aih)(V ih)

+2

φ(sih, aih)((V − f )

i h

)

(Λnh )−1

i=1

(Λnh )−1

i=1

(Λnh )−1

n−1

2

≤2

φ(sih, aih)(V ih)

+ 8 2N

i=1

(Λnh )−1

≤ 9H2

ln

H

+

d ln(1

+ 6L/

) + d2 ln(1 +

√ 18B2 d/

2) + d ln (1 + N )

+ 8 2N,

δ

where in the second inequality we use the fact that

n−1 i=1

φ(sih,

aih)(V

− f)

i h

2 (Λnh )−1

≤

4

2N , which is from

n−1
φ(sih, aih)(V − f )
i=1

2

i h

≤

(Λnh )−1

n−1
φ(sih, aih)2
i=1

2 (Λnh )−1

≤

42 λ

n−1
φ(sih, aih) 2 ≤ 4 2N.
i=1

√ Set = 1/ N , we get:

n−1
φ(sih, aih)(f
i=1

2
ih)

≤ 9H2

ln

H

+ d ln(1 +

√

√

6L N ) + d2 ln(1 + 18B2 dN )

+ d ln (1 + N )

+8

δ

(Λnh )−1

H2

ln

H

+

d ln(1 +

√ 6L N )

+ d2

ln(1 +

√ 18B2 dN )

,

δ

where we recall ignores absolute constants. Now recall that we can express Phn(·|s, a) − P (·|s, a) · f = φ(s, a) (µnh − µh) f . Recall Lemma 8.3, we have:

n−1

|(µnhφ(s, a) − µhφ(s, a)) · f | ≤ λφ(s, a) (Λnh)−1 (µh) f +

φ(s, a) (Λnh)−1φ(sih, aih)( ih) f

i=1

√ H d φ(s, a) + (Λnh)−1 φ(s, a) (Λnh)−1

H2

ln

H

+

√ d ln(1 + 6L N )

+ d2

ln(1 +

√ 18B2 dN )

δ

H φ(s, a) (Λnh)−1

H ln +

√

√

d ln(1 + 6L N ) + d ln(1 + 18B2 dN ) .

δ

91

8.5 Algorithm

Our algorithm, Upper Conﬁdence Bound Value Iteration (UCB-VI) will use reward bonus to ensure optimism. Specifically, we will the following reward bonus, which is motivated from the reward bonus used in linear bandit:

bnh(s, a) = β φ(s, a) (Λnh)−1φ(s, a),

(0.7)

where β contains poly of H and d, and other constants and log terms. Again to gain intuition, please think about what this bonus would look like when we specialize linear MDP to tabular MDP.

Algorithm 6 UCBVI for Linear MDPs
1: Input: parameters β, λ 2: for n = 1 . . . N do 3: Compute Phn for all h (Eq. 0.3) 4: Compute reward bonus bnh for all h (Eq. 0.7) 5: Run Value-Iteration on {Phn, rh + bnh}Hh=−01 (Eq. 0.8) 6: Set πn as the returned policy of VI. 7: end for

With the above setup, now we describe the algorithm. Every episode n, we learn the model µnh via ridge linear regression. We then form the quadratic reward bonus as shown in Eq. 0.7. With that, we can perform the following
truncated Value Iteration (always truncate the Q function at H):

VHn(s) = 0, ∀s, Qnh(s, a) = θ · φ(s, a) + β φ(s, a) (Λnh)−1φ(s, a) + φ(s, a) (µnh) Vhn+1

= β φ(s, a) (Λnh)−1φ(s, a) + (θ + (µnh) Vhn+1) φ(s, a),

Vhn(s)

=

min{max
a

Qnh (s,

a),

H },

πhn(s) = argmaxa Qnh(s, a).

(0.8)

Note that above Qnh contains two components: a quadratic component and a linear component. And Vhn has the format of fw,β,Λ deﬁned in Eq. 0.5.

The following lemma bounds the norm of linear weights in Qnh.

Lemma 8.8. Assume β ∈ [0, B]. For all n, h, we have Vhn is in the form of Eq. 0.5, and Vhn falls into the following class:

V = {fw,β,Λ :

w

2

≤W

+

HN λ

,

β

∈

[0,

B],

σmin(Λ)

≥

λ}.

(0.9)

Proof: We just need to show that θ + (µnh) Vhn+1 has its 2 norm bounded. This is easy to show as we always have Vhn+1 ∞ ≤ H as we do truncation at Value Iteration:

θ + (µnh) Vhn+1 ≤ W + (µnh) Vhn+1 .

2

2

Now we use the closed-form of µnh from Eq. 0.3:

n−1

(µnh )

Vhn+1

=
2

Vhn+1(sih+1)φ(sih, aih) (Λnh)−1

n−1
≤ H (Λnh)−1 φ(sih, aih)

≤

Hn ,

λ

i=1

2

i=0

2

where we use the fact that Vhn+1 ∞ ≤ H, σmax(Λ−1) ≤ 1/λ, and sups,a φ(s, a) 2 ≤ 1.

92

