1
Image De-raining Using a Conditional Generative Adversarial Network
He Zhang, Member, IEEE, Vishwanath Sindagi, Student Member, IEEE Vishal M. Patel, Senior Member, IEEE

arXiv:1701.05957v4 [cs.CV] 2 Jun 2019

Abstract—Severe weather conditions such as rain and snow adversely affect the visual quality of images captured under such conditions thus rendering them useless for further usage and sharing. In addition, such degraded images drastically affect performance of vision systems. Hence, it is important to address the problem of single image de-raining. However, the inherent ill-posed nature of the problem presents several challenges. We attempt to leverage powerful generative modeling capabilities of the recently introduced Conditional Generative Adversarial Networks (CGAN) by enforcing an additional constraint that the de-rained image must be indistinguishable from its corresponding ground truth clean image. The adversarial loss from GAN provides additional regularization and helps to achieve superior results. In addition to presenting a new approach to de-rain images, we introduce a new reﬁned loss function and architectural novelties in the generator-discriminator pair for achieving improved results. The loss function is aimed at reducing artifacts introduced by GANs and ensure better visual quality. The generator sub-network is constructed using the recently introduced densely connected networks, whereas the discriminator is designed to leverage global and local information to decide if an image is real/fake. Based on this, we propose a novel single image de-raining method called Image De-raining Conditional Generative Adversarial Network (ID-CGAN), which considers quantitative, visual and also discriminative performance into the objective function. Experiments evaluated on synthetic and real images show that the proposed method outperforms many recent state-of-the-art single image de-raining methods in terms of quantitative and visual performance. Furthermore, experimental results evaluated on object detection datasets using FasterRCNN also demonstrate the effectiveness of proposed method in improving the detection performance on images degraded by rain.
Index Terms—Generative adversarial network, single image de-raining, de-snowing, perceptual loss
I. INTRODUCTION
It has been known that unpredictable impairments such as illumination, noise and severe weather conditions, such as rain, snow and haze, adversely inﬂuence the performance of many computer vision algorithms such as tracking, detection and segmentation. This is primarily due to the fact that most of these state-of-the-art algorithms are trained using images that are captured under well-controlled conditions. For example, it can be observed from Fig. 1, that the presence of heavy rain greatly degrade perceptual quality of the image, thus imposing larger challenge for face detection and veriﬁcation algorithms in such weather conditions. A possible method to address

Input

De-rained results

Fig. 1: Sample results of the proposed ID-CGAN method for single image de-raining.

this issue is to include images captured under unconstrained conditions during the training process of these algorithms. However, it may not be practical to collect such large scale datasets as training set. In addition, in this age of ubiquitous cellphone usage, images captured in the bad weather conditions by cellphone cameras undergo degradations that drastically affect the visual quality of images making the images useless for sharing and usage. In order to improve the overall quality of such degraded images for better visual appeal and to ensure enhanced performance of vision algorithms, it becomes essential to automatically remove these undesirable artifacts due to severe weather conditions discussed above. In this paper, we investigate the effectiveness of conditional generative adversarial networks (GANs) in addressing this issue, where a learned discriminator network is used as a guidance to synthesize images free from weather-based degradations. Speciﬁcally, we propose a single image-based deraining algorithm using a conditional GAN framework for visually enhancing images that have undergone degradations due to rain.

He Zhang is with Adobe, San Jose, CA email: he.zhang92@rutgers.edu Vishwanath Sindagi and Vishal M. Patel are with the Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA. Email: {vsindag1, vpatel36}@jhu.edu

Mathematically, a rainy image can be decomposed into two separate images: one corresponding to rain streaks and the other corresponding to the clean background image (see Fig.

2

(a)

(b)

(c)

Fig. 2: Rain streak removal from a single image. A rainy image (a) can be viewed as the superposition of a clean background image (b) and a rain streak image (c).

2). Hence, the input rainy image can be expressed as

x = y + w,

(1)

where y represents the clean background image and w represents the rain streaks. As a result, similar to image de-noising and image separation problems [1]–[5], image de-raining can be viewed as the problem of separating two components from a rainy image.
In the case of video-based de-raining, a common strategy to solve (1) is to leverage additional temporal information, such as methods proposed in [6]–[9]. However, temporal information is not available for cases of single image deraining problem. In such cases, previous works have designed appropriate prior in solving (1) such as sparsity prior [10]– [13], Gaussian Mixture Model (GMM) prior [14] and patchrank prior [15]. Most recently, with the large scale synthesized training samples released by de-raining researchers, Convolutional Neural Networks (CNNs) have been also successfully applied to solve single image de-raining problem [16]–[19]. By directly learning a non-linear mapping between the input rainy image and its corresponding ground truth using a CNN structure with some prior information enforced, CNN-based methods are able to achieve superior visual performance.
Even though tremendous improvements have been achieved, we note that these methods do not consider additional information into the optimization. Hence, to design a visually appealing de-raining algorithm, we must consider the following information into the optimization framework:
(a) The criterion that performance of vision algorithms such as detection and classiﬁcation should not be affected by the presence of rain streaks should be considered in the objective function. The inclusion of this discriminative information ensures that the reconstructed image is indistinguishable from its original counterpart.
(b) Rather than concentrating only on the characterization of rain-streaks, visual quality may also be considered into the optimization function. This can ensure that the de-rained image looks visually appealing without losing important details.
(c) Some of the existing methods adopt off-line additional image processing techniques to enhance the results [10], [17]. Instead, it would be better to use a more uniﬁed structure to deal with the problem without any additional processing.
In this work, these criteria are incorporated in a novel conditional GAN-based framework called Image De-raining Condi-

tional Generative Adversarial Network (ID-CGAN) to address the single image de-raining problem. We aim to leverage the generative modeling capabilities of the recently introduced CGANs. While existing CNN-based approaches minimize only L2 error, these methods need additional regularization due to the ill-posed nature of the problem. In this work, adversarial loss from CGANs is used as additional regularizer leading to superior results in terms of visual quality and quantitative performance. The use of discriminator for classifying between real/fake samples provides additional feedback, enabling the generator to produce results that are visually similar to the ground-truth clean samples (real samples). Inspired by the recent success of GANs for pixel-level vision tasks such as image generation [20], image inpainting [21] and image superresolution [22], our network consists of two sub-networks: densely-connected generator (G) and multi-scale discriminator (D). The generator acts as a mapping function to translate an input rainy image to de-rained image such that it fools the discriminator, which is trained to distinguish rainy images from images without rain. The discriminator is designed to capture hierarchical context information through multi-scale pooling. However, traditional GANs [20] are not stable to train and may introduce artifacts in the output image making it visually unpleasant and artiﬁcial. To address this issue, we introduce a new reﬁned perceptual loss to serve as an additional loss function to aid the proposed network in generating visually pleasing outputs. Furthermore, to leverage different scale information in determining whether the corresponding de-rained image is real or fake, a multi-scale discriminator is proposed. Sample results of the proposed ID-CGAN algorithm are shown in Fig. 1. In summary, this paper makes the following contributions:
1) A conditional GAN-based framework to address the challenging single image de-raining problem without the use of any additional post-processing.
2) A densely-connected generator sub-network that is specifically designed for the single image de-raining task.
3) A multi-scale discriminator is proposed to leverage both local and global information to determine whether the corresponding de-rained image is real or fake.
4) Extensive experiments are conducted on publicly available and synthesized datasets to demonstrate the effectiveness of the proposed method in terms of visual quality and quantitative performance. /
5) Lastly, effectiveness of the proposed method in improving high-level object detection task is demonstrated on VOC dataset [23]. The detections are performed using FasterRCNN [24].
This paper is organized as follows. A brief background on de-raining, GANs and perceptual loss is given in Section II. The details of the proposed ID-CGAN method are given in Section III. Experimental results on both synthetic and real images are presented in Section IV. Finally, Section V concludes the paper with a brief summary and discussion.
II. BACKGROUND
In this section, we brieﬂy review the literature for existing single image de-raining methods and conditional GANs.

3

A. Single Image De-raining
As discussed in Section I, single image de-raining is an extremely challenging task due to its ill-posed nature. In addition, the unavailability of temporal information, which could have been used as additional constraints, also pose challenges to solve the single image de-raining problem. Hence, in order to generate optimal solutions to this problem, different kinds of prior knowledge are enforced into the optimization framework. In the following, we discuss sparsity-based methods, low-rank method, gaussian mixture model methods and deep learning methods in solving image de-raining problem. Sparsity-based Methods: To overcome the issue of ill-posed nature of (1), authors in [10] employed two set of learned dictionary atoms (ﬁlters) to sparsely represent clean background image and rain-streak separately. The separation is performed in the high frequency part of the given rainy image, assuming that the low-frequencies correspond to the clean background image. An important assumption that is made in this approach is that rain streaks in an image usually have similar orientations. Similarly, Luo et al. in [11] proposed a discriminative approach that sparsely approximates the patches of clean background and rain-streak components by discriminative codes over two set of learned dictionary atoms with strong mutual exclusivity property. However, their method generates artifacts around the rain-streak components in the resulting images. Low-rank Representation-based Methods Inspired by the observation that rain streak components within an image share similar patterns and orientations, Chen et al. proposed a low patch-rank prior to capture these patterns. This is motivated by the use of patch-rank for characterizing the texture component in solving cartoon-texture image decomposition problem [25], [26]. Since the patch-rank may also capture some repetitive texture patterns, this method removes important texture details from the input image, due to which the results become blurred. To address this issue, Zhang et al. recently proposed a convolutional coding-based method [27] that uses a set of learned convolutional low-rank ﬁlters to capture the rain pixels. Gaussian Mixture Model-based Methods: Based on the assumptions that GMMs can accommodate multiple orientations and scales of rain streaks, Li et al. in [14] used the image decomposition framework to propose patch-based GMM priors to model background and rain streaks separately. Deep Learning-based Methods
The success of convolutional neural networks in several computer vision tasks [28]–[31] has inspired researchers to develop CNN-based approaches for image de-raining [16]– [18]. These methods attempt to learn a non-linear function to convert an input rainy image to a clean target image. Based on the observation that both rain streaks and object details remain only in the detail layer, Fu et al. [17] employed a twostep procedure, where the input rainy image is decomposed into a background-based layer and a detail layer separately. Then, a CNN-based non-linear mapping is learned to remove the rain streaks from the detail layer. Built on [17], Fu et al. extended the network structure using Res-block [28] in [16]. Yang et al. proposed a CNN structure that can jointly detect

and remove rain streaks. Most recently, several deep learning methods have been explored for single image de-raining task [32]–[35].
B. Generative Adversarial Networks
Generative Adversarial Networks [36](GANs) are a class of methods to model a data distributions and consist of two functions: the generator G, which translate a sample from a random uniform distribution to the data distribution; the discriminator D which measure the probability whether a given sample belongs to the data distribution or not. Based on a game theoretic min-max principles, the generator and discriminator are typically learned jointly by alternating the training of D and G. Although GANs are able to generate visual appealing images by preserving high frequency details, yet GANs still face many unsolved challenges: in general they are notoriously difﬁcult to train and GANs easily suffer from modal collapse. Recently, researchers have explored various aspects of GANs such as leveraging another conditional variable [37], training improvements [38] and use of task speciﬁc cost function [39]. Also, an alternative viewpoint for the discriminator function is explored by Zhao et al. [40] where they deviate from the traditional probabilistic interpretation of the discriminator model.
The success of GANs in synthesizing visually appealing images has inspired researchers to explore the use of GANs in other related such as text-to-image synthesis [41]–[43], single image super-resolution [22], domain adaption [44], face synthesis [45] and other related applications [46]–[51].
III. PROPOSED METHOD
Instead of solving (1) in a decomposition framework, we aim to directly learn a mapping from an input rainy image to a de-rained (background) image by constructing a conditional GAN-based deep network called ID-CGAN. The proposed network is composed of three important parts (generator, discriminator and perceptual loss function) that serve distinct purposes. Similar to traditional GANs [36], [52], the proposed method contains two sub-networks: a generator sub-network G and a discriminator sub-network D. The generator subnetwork G is a densely-connected symmetric deep CNN network with appropriate skip connections as shown in the top part in Fig. 3. Its primary goal is to synthesize a derained image from an image that is degraded by rain (input rainy image). The multi-scale discriminator sub-network D, as shown in the bottom part in Fig. 3, serves to distinguish ‘fake’ de-rained image (synthesized by the generator) from corresponding ground truth ‘real’ image. It can also be viewed as a guidance for the generator G. Since GANs are known to be unstable to train which results in artifacts in the output image synthesized by G, we deﬁne a reﬁned perceptual loss functions to address this issue. Additionally, this new reﬁned loss function ensures that the generated (de-rained) images are visually appealing. In what follows, we elaborate these modules in further detail, starting with the GAN objective function followed by details of the network architecture for the generator/discriminator and the overall loss function.

4

No addition pre(or post) processing

End-to-end mapping

Consider discriminative performance in the optimization

Consider visual performance in the optimization

Not Patch-based

Time efﬁciency

SPM [10]

√ PRM [15]

√ DSC [11]

√ CNN [17]

√ GMM [14]

√ CCR [27]

√ DDN [16]

√

√

JORDER [18]

√

√

√

ID-CGAN

√

√

√

√

√

√

√

√

√

√

√

√

TABLE I: Compared to the existing methods, our ID-CGAN has several desirable properties: 1. No additional image processing. 2. Include discriminative factor into optimization. 3. Consider visual performance into optimization.

Fig. 3: An overview of the proposed ID-CGAN method for single image de-raining. The network consists of two sub-networks: generator G and discriminator D.

A. GAN Objective Function

The objective of a generative adversarial network is based on the mini-max game, where the idea is to learn a generator G that synthesizes samples similar to the data distribution such that the discriminator is not able to distinguish between the synthesize samples and real samples. At the same time, the goal is also to learn a good discriminator D such that it is able to distinguish between synthesized and real samples. To achieve this, the proposed method alternatively updates G and D following the structure proposed in [36], [52]. Note that in our work, we use a conditional variant of GAN, where the generator G, learns to generate a mapping from a condition variable. Given an input rainy image x, conditional GAN learns a non-linear function to synthesize the output image y by conditioning on the input image x:

min max
GD

Ex∼pdata(x),[log(1 − D(x, G(x)))]+

(2)

Ex∼pdata(x,y) [log D(x, y))].

B. Generator with Symmetric Structure
As the goal of single image de-raining is to generate pixellevel de-rained image, the generator should be able to remove rain streaks as much as possible without loosing any detail information of the background image. So the key part lies in designing a good structure to generate de-rained image.
Existing methods for solving (1), such as sparse codingbased methods [3], [4], [10], [53], neural network-based methods [54] and CNN-based methods [1] have all adopted a symmetric (encoding-decoding) structure. For example, sparse coding-based methods use a learned or pre-deﬁned synthesis dictionaries to decode the input noisy image into sparse coefﬁcient map. Then another set of analysis dictionaries are used to transfer the coefﬁcients to desired clear output. Usually, the input rainy image is transferred to a speciﬁc domain for effective separation of background image and undesired component (rain-streak). After separation, the background image (in the

5

Architecture
Input, num c=3 3x3 Convolution, BN, ReLu, MaxP
num c=64 D(4 layers)+Td, num c=128 D(6 layers)+Td, num c=256 D(8 layers)+Tn, num c=512 D(8 layers)+Tn, num c=128 D(6 layers)+Tu, num c=120 D(4 layers)+Tu, num c=64 D(4 layers)+Tu, num c=64 D(4 layers)+Tn, num c=16 3x3 Conv, Tanh, num c=3
Ourput, num c=3
TABLE II: Network architecture for generator.

new domain) has to be transferred back to the original domain which requires the use of a symmetric process.
Following these methods, a symmetric structure is adopted to form the generator sub-network. The generator G directly learns an end-to-end mapping from input rainy image to its corresponding ground truth. In contrast to the existing adversarial networks for image-to-image translation that use U-Net [52], [55] or ResNet blocks [28], [56] in their generators, we use the recently introduced densely connected blocks [57]. These dense blocks enable strong gradient ﬂow and result in improved parameter efﬁciency. Furthermore, we introduce skip connections across the dense blocks to efﬁciently leverage features from different levels and guarantee better convergence. The jth dense block Dj is represented as:

Dj = cat[Dj,1, Dj,2, ..., Dj,6],

(3)

where Dj,i represents the features from the ith layer in dense block Dj and each layer in a dense block consists of three consecutive operations, batch normalization (BN), leaky rectiﬁed linear units (LReLU) and a 3×3 convolution.
Each dense block is followed by a transition block (T ), functioning as up-sampling (T u), down-sampling (T d) or no-sampling operation (T n). To make the network efﬁcient in training and have better convergence performance, symmetric skip connections are included into the proposed generator subnetwork, similar to [1]. The generator network is as follows: CBLP(64)-D(256)-Td(128)-D(512)-Td(256)-D(1024)Tn(512)-D(768)-Tn(128)-D(640)-Tu(120)-D(384)Tu(64)-D(192)-Tu(64)-D(32)-Tn(16)-C(3)-Tanh where, CBLP is a set of convolutional layers followed by batch normalization, leaky ReLU activation and pooling module, and the number inside braces indicates the number of channels for the output feature maps of each block. Details of the architecture is also shown in Table II.

C. Multi-scale Discriminator
From the point of view of a GAN framework, the goal of de-raining an input rainy image is not only to make the derained result visually appealing and quantitatively comparable to the ground truth, but also to ensure that the de-rained result is indistinguishable from the ground truth image. Therefore, a learned discriminator sub-network is designed to classify if each input image is real or fake. Previous methods [52] have demonstrated the effectiveness of leveraging an efﬁcient patchdiscriminator in generating high quality results. For example, Isola et al [52] adopt a 70× 70 patch discriminator, where 70 × 70 indicates the receptive ﬁeld of the discriminator. Though such a single scale (eg. 70× 70) patch-discriminator is able to achieve visually pleasing results, however, it is still not capable enough to capture the global context information, resulting in insufﬁcient estimation. As shown in the zoomedin part of the Fig. 6 (e), it can be observed that certain tiny details are still missing in the de-rained results using a single scale discriminator. For example, it can be observed from the second row of Fig. 6 that the front mirror of truck is largely being removed in the de-rained results. This is probably due to the fact that the receptive ﬁeld size in the discriminator is 70× 70 and no additional surrounding context is provided. Hence, we argue that it is important to leverage a more powerful discriminator that captures both local and global information to decide whether it is real or fake.
To effectively address this issue, a novel multi-scale discriminator is proposed in this paper. This is inspired by the usage of multi-scale features in objection detection [58] and semantic segmentation [59]. Similar to the structure that was proposed in [52], a convolutional layer with batch normalization and PReLU activation are used as a basis throughout the discriminator network. Then, a multi-scale pooling module, which pools features at different scales, is stacked at the end of the discriminator. The pooled features are then upsampled and concatenated, followed by a 1×1 convolution and a sigmoid function to produce a probability score normalized between [0,1]. By using features at different scales, we explicitly incorporate global hierarchical context into the discriminator. The proposed discriminator sub-network D is shown in the bottom part of Fig. 3. And details of the multi-scale discriminator is shown in Table III.
D. Reﬁned Perceptual Loss
As discussed earlier, GANs are known to be unstable to train and they may produce noisy or incomprehensible results via the guided generator. A probable reason is that the new input may not come from the same distribution of the training samples. As illustrated in Fig. 5(c), it can be clearly observed that there are some artifacts introduced by the normal GAN structure. This greatly inﬂuences the visual performance of the output image. A possible solution to address this issue is to introduce perceptual loss into the network. Recently, loss function measured on the difference of high-level feature representation, such as loss measured on certain layers in CNN [60], has demonstrated much better visual performance than the per-pixel loss used in traditional CNNs. However, in

6

Fig. 4: Sample images from real-world rainy dataset.

Architecture
Input, num c=6 3x3 Convolution, BN, ReLu, MaxP,
num c=64
3x3 Convolution, BN, ReLu, MaxP, num c=256
3x3 Convolution, BN, ReLu, MaxP, num c=512
3x3 Convolution, BN, ReLu, MaxP, num c=64
Four-level Poling Module, num c=72 Sigmoid
Output, num c=72
TABLE III: Network architecture for the discriminator.

many cases it fails to preserve color and texture information [60]. Also, it does not achieve good quantitative performance simultaneously. To ensure that the results have good visual and quantitative scores along with good discriminatory performance, we propose a new reﬁned loss function. Speciﬁcally, we combine pixel-to-pixel Euclidean loss, perceptual loss [60] and adversarial loss together with appropriate weights to form our new reﬁned loss function. The new loss function is then deﬁned as follows:

LRP = LE + λaLA + λpLP ,

(4)

where LA represents adversarial loss (loss from the discriminator D), LP is perceptual loss and LE is normal per-pixel loss function such as Euclidean loss. Here, λp and λa are pre-deﬁned weights for perceptual loss and adversarial loss, respectively. If we set both λp and λa to be 0, then the network reduces to a normal CNN conﬁguration, which aims to minimize only the Euclidean loss between output image and ground truth. If λp is set to 0, then the network reduces to a normal GAN. If λa set to 0, then the network reduces to the structure proposed in [60].
The three loss functions LP , LE and LA are deﬁned as follows. Given an image pair {x, yb} with C channels, width W and height H (i.e. C × W × H), where x is the input image and yb is the corresponding ground truth, the per-pixel Euclidean loss is deﬁned as:

1

CW

LE = CW H

H

φE (x)c,w,h − (yb)c,w,h

2 2

,

(5)

c=1 x=1 y=1

where φE is the learned network G for generating the derained output. Suppose the outputs of certain high-level layer are with size Ci × Wi × Hi. Similarly, the perceptual loss is deﬁned as

1

Ci Wi Hi

LP = CiWiHi c=1 w=1 h=1

V (φE (x))c,w,h − V (yb)c,w,h

2 2

,

(6)

where V represents a non-linear CNN transformation. Similar
to the idea proposed in [60], we aim to minimize the distance
between high-level features. In our method, we compute the feature loss at layer relu2 2 in VGG-16 model [31].1
Given a set of N de-rained images generated from the generator {φE(x)}Ni=1, the entropy loss from the discriminator to guide the generator is deﬁned as:

1N

LA

=

− N

log(D(φE (x))).

(7)

i=1

IV. EXPERIMENTS AND RESULTS
In this section, we present details of the experiments and quality measures used to evaluate the proposed ID-CGAN method. We also discuss the dataset and training details followed by comparison of the proposed method against a set of baseline methods and recent state-of-the-art approaches.

A. Experimental Details
1) Synthetic dataset: Due to the lack of availability of large size datasets for training and evaluation of single image de-raining, we synthesized a new set of training and testing samples in our experiments. The training set consists of a total of 700 images, where 500 images are randomly chosen from the ﬁrst 800 images in the UCID dataset [61] and 200 images are randomly chosen from the BSD-500’s training set [62]. The test set consists of a total of 100 images, where 50 images are randomly chosen from the last 500 images in the UCID dataset and 50 images are randomly chosen from the test-set of the BSD-500 dataset [62]. After the train and test sets are created, we add rain-streaks to these images by following the guidelines mentioned in [17] using Photoshop2. It is ensured that rain pixels of different intensities and orientations are added to generate a diverse training and test set. Note that the images with rain form the set of observed images and the corresponding clean images form the set of
1https://github.com/ruimashita/caffe-train/blob/master/vgg.train val.prototxt 2http://www.photoshopessentials.com/photo-effects/rain/

7

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Fig. 5: Qualitative comparisons for different baseline conﬁgurations of the proposed method. (a) Input image, (b) GEN, (c) GEN-CGAN-S, (d) GEN-P, (e) GEN-PS, (f) ID-CGAN and (g) Target image.

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Fig. 6: Qualitative comparisons for different baseline conﬁgurations of the proposed method. (a) Input image, (b) GEN, (c) GEN-CGAN-S, (d) GEN-P, (e) GEN-CGAN-PS, (f) ID-CGAN and (g) Target image.

ground truth images. All the training and test samples are resized to 256×256.
2) Real-world rainy images dataset: In order to demonstrate the effectiveness of the proposed method on real-world data, we created a dataset of 50 rainy images downloaded from the Internet. While creating this dataset, we took all possible care to ensure that the images collected were diverse in terms of content as well as intensity and orientation of the rain pixels. A few sample images from this dataset are shown in Fig. 4. This dataset is used for evaluation (test) purpose only.
3) Quality measures: The following measures are used to evaluate the performance of different methods: Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM) [63], Universal Quality Index (UQI) [64] and Visual Information Fidelity (VIF) [65]. Similar to previous methods [14], all of these quantitative measures are calculated using the luminance channel. Since we do not have ground truth reference images for the real dataset, the performance of the proposed and other methods on the real dataset is evaluated visually.
B. Model Details and Parameters
The entire network is trained on a Nvidia Titan-X GPU using the torch framework [66]. We used a batch size of 1 and number of training iterations of 100k. Adam algorithm [67] with a learning rate of 2 × 10−3 is used. During training,

GEN GEN-CGAN-S GEN-P GEN-CGAN-PS ID-CGAN

PSNR (dB) SSIM UQI VIF

24.40 0.8275 0.6506 0.3999

23.55 0.8290 0.6541 0.4003

23.77 0.8305 0.6557 0.4056

24.08 0.8376 0.6705 0.4133

24.34 0.8430 0.6741 0.4188

TABLE IV: Quantitative comparison baseline conﬁgurations.

we set λa = 6.6 × 10−3 and λp = 1. All the parameters are set via cross-validation. A low value for λa is used so as to ensure that the adversarial loss does not dominate the other losses.

C. Comparison with Baseline Conﬁgurations
In order to demonstrate the signiﬁcance of different modules in the proposed method, we compare the performance of following baseline conﬁgurations: • GEN: Generator G is trained using per-pixel Euclidean loss
by setting λa and λp to zero in (4). This amounts to a traditional CNN architecture with Euclidean loss. • GEN-CGAN-S: Generator G trained using per-pixel Euclidean loss and Adversarial loss from a single-scale discriminator D (no multi-scale pooling). λp is set to zero in (4).

8

SPM [10] PRM [15] DSC [11] CNN [17] GMM [14] CCR [27] DDN [16] JORDER [18] PAN [68] ID-CGAN

PSNR (dB) SSIM UQI VIF

18.88 0.7632 0.4149 0.2197

20.46 0.7297 0.5668 0.3441

18.56 0.5996 0.4804 0.3325

19.12 0.6013 0.4706 0.3307

22.27 0.7413 0.5751 0.4042

20.56 0.7332 0.5582 0.3607

23.28 0.8208 0.6533 0.4116

21.09 0.7525 0.5768 0.3785

23.35 0.8303 0.6644 0.4050

24.34 0.8430 0.6741 0.4188

TABLE V: Quantitative comparisons with state-of-the-art methods evaluated on using four different criterions.

• GEN-P: Generator G is trained using per-pixel Euclidean loss and perceptual loss. λa is set to zero in (4).
• GEN-CGAN-PS: Generator G is trained using per-pixel Euclidean loss, perceptual loss and adversarial loss from a single scale discriminator.
• ID-CGAN: Generator G is trained using per-pixel Euclidean loss, perceptual loss and adversarial loss from multi-scale discriminator D. All four conﬁgurations along with ID-CGAN are learned
using training images from the synthetic training dataset. Results of quantitative performance, using the measures discussed earlier on test images from the synthetic dataset, are shown in Table IV. Sample results for the above baseline conﬁgurations on test images from real dataset are shown in Fig. 5 and Fig. 6. It can be observed from Fig. 5(c), that the introduction of adversarial loss improves the visual quality over the traditional CNN architectures, however, it also introduces certain artifacts. The use of perceptual loss along with adversarial loss from a single scale discriminator reduces these artifacts while producing sharper results. However, part of the texture details are still missing in the de-rained results (Fig. 5(e)) such as the edge of the left back part of the car (shown in third row in Fig. 5) and the structure of truck’s front mirror (shown in second row in Fig. 6). Finally, the use of adversarial loss from multi-scale discriminator along with other loss functions (ID-CGAN) results in recovery of these texture details and achieve the best results. Quantitative results shown in Table V also demonstrate the effectiveness of the each module.
D. Comparison with State-of-the-art Methods
We compare the performance of the proposed ID-CGAN method with the following recent state-of-the-art methods for single image de-raining: • SPM: Sparse dictionary-based method [10] (TIP ’12) • DSC: Discriminative sparse coding-based method [11]
(ICCV ’15) • PRM: PRM prior-based method [14] (CVPR ’16) • GMM: GMM-based method [15] (ICCV ’13) • CNN: CNN-based method [17] (TIP ’17) • CCR: Convolutional-coding based method [27] (WACV ’17) • DDN: Deep Detail Network method [16] (CVPR ’17) • JORDER: CNN-based method [18] (CVPR ’17) • PAN: GAN-based method [68] (TIP ’18)
1) Evaluation on synthetic dataset: In the ﬁrst set of experiments, we evaluate the proposed method and compare its quantitative and qualitative performance against several stateof-the-art approaches on test images from the synthetic dataset. As the ground truth is available for the these test images, we

calculate the quantitative measures such as PSNR, SSIM, UQI and VIF. Table V shows the comparison of results based on these metrics. This table clear demonstrates that the proposed ID-CGAN method is able to achieve superior quantitative performance as compared to the recent methods in terms of all the metrics stated earlier.
Fig. 7 illustrates the qualitative improvements on two sample images from the synthetic dataset, achieved due to the use of the proposed method. Note that we selectively sample difﬁcult images to show that our method performs well in difﬁcult conditions. While PRM [15] is able to remove the rain-streaks, it produces blurred results which are not visually appealing. The other compared methods are able to either reduce the intensity of rain or remove the streaks in parts, however, they fail to completely remove the rain-streaks. In contrast to the other methods, the proposed method is able to successfully remove majority of the rain streaks while maintaining the details of the de-rained images.
2) Evaluation on Real Rainy Images: We also evaluated the performance of the proposed method and recent state-ofthe-art methods on real-world rainy test images. The de-rained results for all the methods on two sample input rainy images are shown in Fig. 8. For better visual comparison, we show zoomed versions of the two speciﬁc regions-of-interest below the de-rained results. By looking at these regions-of-interest, we can clearly observe that DSC [11] tends to add artifacts on the de-rained images. Even though the other methods GMM [14], CNN [17], CCR [27], DNN [16] and JORDER [18] are able to achieve good visual performance, rain drops are still visible in the zoomed regions-of-interest. In comparison, the proposed method is able to remove most of the rain drops while maintaining the details of the background image. One may observe that the proposed method leaves out a few rainstreaks in the output images. This is because the two image samples represent relatively difﬁcult cases for de-raining. However, the proposed method is able to achieve better results compared to state-of-the-art methods. Additional comparisons are provided in Fig. 9. It can be seen that the proposed method achieves better results among all the methods. In addition, more de-rained results on different rainy images, shown in Fig. 11, demonstrate that the proposed method successfully removes rain streaks.
3) Evaluation on Object Detection Results: Single image de-raining algorithms can be used as a pre-processing step to improve the performance of other high level vision tasks such as face recognition and object detection [10]. In order to demonstrate the performance improvement obtained after deraining using the proposed IDCGAN method, we evaluated Faster-RCNN [24] on VOC 2007 dataset [23]. First, the VOC

9

Input

PRM [15]

DSC [11]

CNN [17]

GMM [14]

CCR [27]

DDN [16]

JORDER [18]

ID-CGAN

Ground Truth

Input

PRM [15]

DSC [11]

CNN [17]

GMM [14]

CCR [27]

DDN [16]

JORDER [18]

ID-CGAN

Ground Truth

Fig. 7: Qualitative comparison of rain-streak removal on two sample images from synthetic dataset.

2007 dataset is artiﬁcially degraded with rain streaks similar to Section IV A. Due to the degradations, object detection performance using Faster-RCNN results in poor performance. Next, the degraded images are processed by ID-CGAN method to remove the rain streaks and the de-rained images are fed to the Faster-RCNN method. We present the mean average precision (mAP) for the entire VOC dataset in Table IV. It may be noted that Faster-RCNN on degraded images results in a low average precision, however, the performance is boosted by 78% when the images undergo de-raining using the proposed ID-CGAN method.
Sample detection results for Faster-RCNN on real-world rainy and de-rained images are shown in Fig. 10. The degradations result in total failure of Faster-RCNN on these images, however, after being processed by ID-CGAN, the same detection method is able successfully detect different objects in the scene.

4) Computation times: Table ?? compares the running time of several state-of-the-art methods. All baseline methods are implemented using MATLAB or MATLAB wrapper. Our method is implemented in Torch. It can be observed that all GPU-based CNN methods [16]–[18] are computationally more efﬁcient. The proposed ID-CGAN is able achieve the fastest time3 as compared to these methods. On an average, IDCGAN in GPU can process and image of size 500 × 500 in about 0.3s.
V. CONCLUSION
In this paper, we proposed a conditional GAN-based algorithm for the removal of rain streaks form a single image. In comparison to the existing approaches which attempt to solve the de-raining problem in an image decomposition framework by using prior information, we investigated the use
3ID-CGAN is running as fast as Fu et al [16].

10

Input

DSC [11]

CNN [17]

GMM [14]

CCR [27] Input CCR [27]

DDN [16] DSC [11]

JORDER [18]

ID-CGAN

CNN [17]

GMM [14]

DDN [16]

JORDER [18] (a)

ID-CGAN

Input

DSC [11]

CNN [17]

GMM [14]

CCR [27] Input

DDN [16] DSC [11]

JORODR [18] CNN [17]

ID-CGAN GMM [14]

CCR [27]

DDN [16]

JORDER [18] (b)

ID-CGAN

Fig. 8: Qualitative comparison of rain-streak removal on two sample real images.

11

Input

DSC [11]

CNN [17]

GMM [14]

CCR [27]

DDN [16]

JORDER [10]

ID-CGAN

Input

DSC [11]

CNN [17]

GMM [14]

CCR [27]

DDN [16]

JORDER [18]

Fig. 9: Qualitative comparison of rain-streak removal on two sample real images.

ID-CGAN

(a)

(b)

Fig. 10: Real-world examples of object detection (Faster-RCNN [24]) improvements obtained by the proposed ID-CGAN. Left: Detection results on rainy images; Right: Detection results on de-rained images. The detection performance is boosted when ID-CGAN is used as a pre-processing step.

12

REFERENCES

Fig. 11: Additional de-rained results using the proposed ID-CGAN method on real-world dataset. Left: Input; Right: Derained results.
of generative modeling for synthesizing de-rained image from a given input rainy image. For improved stability in training and reducing artifacts introduced by GANs in the output images, we proposed the use of a new reﬁned loss function in the GAN optimization framework. In addition, a multi-scale discriminator is proposed to leverage features from different scales to determine whether the de-rained image is real or fake. Extensive experiments are conducted on synthetic and real-world dataset to evaluate the performance of the proposed method. Comparison with several recent methods demonstrates that our approach achieves signiﬁcant improvements in terms of different metrics. Moreover, a detailed ablation study is conducted to clearly illustrate improvements obtained due to different modules in the proposed method. Furthermore, experimental results evaluated on objection detection using Faster-RCNN demonstrated signiﬁcant improvements in detection performance when ID-CGAN method is used as a preprocessing step.
ACKNOWLEDGMENT
This work was supported by an ARO grant W911NF-16-10126.
Condition mAP
With Rain 0.39 DDN [16] 0.65 JORDER [16] 0.63 Our De-rained 0.69
TABLE VI: Object detection performance using Faster-RCNN on VOC 2007 dataset.

[1] X.-J. Mao, C. Shen, and Y.-B. Yang, “Image denoising using very deep fully convolutional encoder-decoder networks with symmetric skip connections,” arXiv preprint arXiv:1603.09056, 2016.
[2] H. Zhang and V. M. Patel, “Convolutional sparse coding-based image decomposition,” in BMVC, 2016.
[3] J.-L. Starck, M. Elad, and D. L. Donoho, “Image decomposition via the combination of sparse representations and a variational approach,” IEEE TIP, vol. 14, no. 10, pp. 1570–1582, 2005.
[4] G. Peyre´, J. Fadili, and J.-L. Starck, “Learning the morphological diversity,” SIAM Journal on Imaging Sciences, vol. 3, no. 3, pp. 646– 669, 2010.
[5] J. Yang, F. Liu, H. Yue, X. Fu, C. Hou, and F. Wu, “Textured image demoire´ing via signal decomposition and guided ﬁltering,” IEEE Transactions on Image Processing, vol. 26, no. 7, pp. 3528–3541, 2017.
[6] K. Garg and S. K. Nayar, “Detection and removal of rain from videos,” in CVPR, vol. 1. IEEE, pp. I–528.
[7] ——, “Vision and rain,” IJCV, vol. 75, no. 1, pp. 3–27, 2007. [8] X. Zhang, H. Li, Y. Qi, W. K. Leow, and T. K. Ng, “Rain removal
in video by combining temporal and chromatic properties,” in IEEE International Conference on Multimedia and Expo. IEEE, 2006, pp. 461–464. [9] W. Wei, L. Yi, Q. Xie, Q. Zhao, D. Meng, and Z. Xu, “Should we encode rain streaks in video as deterministic or stochastic?” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 2516–2525. [10] L.-W. Kang, C.-W. Lin, and Y.-H. Fu, “Automatic single-image-based rain streaks removal via image decomposition,” IEEE TIP, vol. 21, no. 4, pp. 1742–1755, 2012. [11] Y. Luo, Y. Xu, and H. Ji, “Removing rain from a single image via discriminative sparse coding,” in ICCV, 2015, pp. 3397–3405. [12] D.-A. Huang, L.-W. Kang, Y.-C. F. Wang, and C.-W. Lin, “Self-learning based image decomposition with applications to single image denoising,” IEEE Transactions on multimedia, vol. 16, no. 1, pp. 83–93, 2014. [13] L. Zhu, C.-W. Fu, D. Lischinski, and P.-A. Heng, “Joint bi-layer optimization for single-image rain streak removal,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2526– 2534. [14] Y. Li, R. T. Tan, X. Guo, J. Lu, and M. S. Brown, “Rain streak removal using layer priors,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016, pp. 2736–2744. [15] Y.-L. Chen and C.-T. Hsu, “A generalized low-rank appearance model for spatio-temporally correlated rain streaks,” in IEEE ICCV, 2013, pp. 1968–1975. [16] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Paisley, “Removing rain from single images via a deep detail network,” in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017, pp. 1715–1723. [17] X. Fu, J. Huang, X. Ding, Y. Liao, and J. Paisley, “Clearing the skies: A deep network architecture for single-image rain removal,” IEEE Transactions on Image Processing, vol. 26, no. 6, pp. 2944–2956, 2017. [18] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint rain detection and removal from a single image,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1357–1366. [19] H. Zhang and V. M. Patel, “Density-aware single image de-raining using a multi-stream dense network,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 695–704. [20] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep convolutional generative adversarial networks,” arXiv preprint arXiv:1511.06434, 2015. [21] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, “Context encoders: Feature learning by inpainting,” arXiv preprint arXiv:1604.07379, 2016. [22] C. Ledig, L. Theis, F. Husza´r, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang et al., “Photo-realistic single image super-resolution using a generative adversarial network,” arXiv preprint arXiv:1609.04802, 2016. [23] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results,” http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html. [24] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks,” in Advances in neural information processing systems, 2015, pp. 91–99.

13

SPM [10] PRM [15] DSC [11] CNN (GPU) [17] GMM [14] CCR [27] DDN (GPU) [16] JORDER (GPU) [18] ID-CGAN (GPU)

250X250 400.5s

40.5s

1.3s

54.9s

169.6s

150.2s

0.2s

0.4s

0.2s

500X500 1455.2s

140.3s

2.8s

189.3

674.8

600.6s

0.3s

1.4s

0.3s

TABLE VII: Time complexity (in seconds) for different methods.

[25] S. Ono, T. Miyata, and I. Yamada, “Cartoon-texture image decomposition using blockwise low-rank texture characterization,” IEEE TIP, vol. 23, no. 3, pp. 1128–1142, 2014.
[26] H. Schaeffer and S. Osher, “A low patch-rank interpretation of texture,” SIAM Journal on Imaging Sciences, vol. 6, no. 1, pp. 226–262, 2013.
[27] H. Zhang and V. M. Patel, “Convolutional sparse and low-rank codingbased rain streak removal,” in 2017 IEEE WACV. IEEE, 2017, pp. 1–9.
[28] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.
[29] S. Xie and Z. Tu, “Holistically-nested edge detection,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1395– 1403.
[30] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten, “Densely connected convolutional networks,” arXiv preprint arXiv:1608.06993, 2016.
[31] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[32] Y.-T. Wang, X.-L. Zhao, T.-X. Jiang, L.-J. Deng, Y. Chang, and T.Z. Huang, “Rain Streak Removal for Single Image via Kernel Guided CNN,” ArXiv e-prints, Aug. 2018.
[33] W. Wei, D. Meng, Q. Zhao, and Z. Xu, “Semi-supervised cnn for single image rain removal,” arXiv preprint arXiv:1807.11078, 2018.
[34] X. Fu, B. Liang, Y. Huang, X. Ding, and J. Paisley, “Lightweight pyramid networks for image deraining,” arXiv preprint arXiv:1805.06173, 2018.
[35] Z. Fan, H. Wu, X. Fu, Y. Hunag, and X. Ding, “Residual-guide feature fusion network for single image deraining,” arXiv preprint arXiv:1804.07493, 2018.
[36] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in NIPS, 2014, pp. 2672–2680.
[37] M. Mirza and S. Osindero, “Conditional generative adversarial nets,” arXiv preprint arXiv:1411.1784, 2014.
[38] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved techniques for training gans,” in NIPS, 2016, pp. 2226–2234.
[39] A. Creswell and A. A. Bharath, “Task speciﬁc adversarial cost function,” arXiv preprint arXiv:1609.08661, 2016.
[40] J. Zhao, M. Mathieu, and Y. LeCun, “Energy-based generative adversarial network,” arXiv preprint arXiv:1609.03126, 2016.
[41] Z. Zhang, Y. Xie, and L. Yang, “Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network,” in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
[42] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He, “Attngan: Fine-grained text to image generation with attentional generative adversarial networks,” arXiv preprint arXiv:1711.10485, 2017.
[43] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas, “Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks,” in Proceedings of the IEEE international conference on computer vision, 2017.
[44] M.-Y. Liu and O. Tuzel, “Coupled generative adversarial networks,” in Advances in neural information processing systems, 2016, pp. 469–477.
[45] X. Di, H. Zhang, and V. M. Patel, “Polarimetric thermal to visible face veriﬁcation via attribute preserved synthesis,” arXiv preprint arXiv:1901.00889, 2019.
[46] Z. Zhang, L. Yang, and Y. Zheng, “Translating and segmenting multimodal medical volumes with cycle-and shapeconsistency generative adversarial network.”
[47] Y. Zhu, M. Elhoseiny, B. Liu, X. Peng, and A. Elgammal, “A generative adversarial approach for zero-shot learning from noisy texts,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2018.
[48] H. Zhang and V. M. Patel, “Densely connected pyramid dehazing network,” in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.

[49] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, “Generative image inpainting with contextual attention,” arXiv preprint arXiv:1801.07892, 2018.
[50] X. Peng, Z. Tang, Y. Fei, R. S. Feris, and D. Metaxas, “Jointly optimize data and network training: Adversarial data augmentation in human pose estimation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.
[51] X. Di, V. A. Sindagi, and V. M. Patel, “Gp-gan: gender preserving gan for synthesizing faces from landmarks,” arXiv preprint arXiv:1710.00962, 2017.
[52] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” CVPR, 2017.
[53] J. Bobin, J. L. Starck, J. M. Fadili, Y. Moudden, and D. L. Donoho, “Morphological component analysis: An adaptive thresholding strategy,” IEEE Transactions on Image Processing, vol. 16, no. 11, pp. 2675–2681, Nov 2007.
[54] J. Xie, L. Xu, and E. Chen, “Image denoising and inpainting with deep neural networks,” in NIPS, 2012, pp. 341–349.
[55] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2015, pp. 234–241.
[56] C. Ledig, L. Theis, F. Husza´r, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang et al., “Photo-realistic single image super-resolution using a generative adversarial network,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1–8.
[57] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[58] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep convolutional networks for visual recognition,” in European Conference on Computer Vision. Springer, 2014, pp. 346–361.
[59] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,” in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 1–8.
[60] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for realtime style transfer and super-resolution,” in European Conference on Computer Vision. Springer, 2016, pp. 694–711.
[61] G. Schaefer and M. Stich, “Ucid: an uncompressed color image database,” in Electronic Imaging 2004. International Society for Optics and Photonics, 2003, pp. 472–480.
[62] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and hierarchical image segmentation,” IEEE Trans. on PAMI, vol. 33, no. 5, pp. 898–916, 2011.
[63] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: from error visibility to structural similarity,” IEEE TIP, vol. 13, no. 4, pp. 600–612, 2004.
[64] Z. Wang and A. C. Bovik, “A universal image quality index,” IEEE Signal Processing Letters, vol. 9, no. 3, pp. 81–84, 2002.
[65] H. R. Sheikh and A. C. Bovik, “Image information and visual quality,” IEEE TIP, vol. 15, no. 2, pp. 430–444, 2006.
[66] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: A matlab-like environment for machine learning,” in BigLearn, NIPS Workshop, 2011.
[67] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.
[68] C. Wang, C. Xu, C. Wang, and D. Tao, “Perceptual adversarial networks for image-to-image transformation,” IEEE Transactions on Image Processing, vol. 27, no. 8, pp. 4066–4079, 2018.

14
He Zhang [S’14] received his Ph.D. degree in Electrical and Computer Engineering from Rutgers University, NJ, USA in 2018. He is currently a Research Scientist in Adobe, CA. His research interests include image restoration, image compositing, federate learning, generative adversarial network, deep learning and sparse and low-rank representation.
Vishwanath Sindagi [S’16] is a PhD student in the Dept. Of Electrical & Computer Engineering at The Johns Hopkins University. Prior to joining Johns Hopkins, he worked for Samsung R&D InstituteBangalore. He graduated from IIIT-Bangalore with a Master’s degree in Information Technology. His research interests include deep learning based crowd analytics, object detection, applications of generative modeling, domain adaptation and low-level vision. Attachments area
Vishal M. Patel [SM15] is an Assistant Professor in the Department of Electrical and Computer Engineering (ECE) at Johns Hopkins University. Prior to joining Hopkins, he was an A. Walter Tyson Assistant Professor in the Department of ECE at Rutgers University and a member of the research faculty at the University of Maryland Institute for Advanced Computer Studies (UMIACS). His current research interests include signal processing, computer vision, and pattern recognition with applications in biometrics and imaging. He has received a number of awards including the 2016 ONR Young Investigator Award, the 2016 Jimmy Lin Award for Invention, A. Walter Tyson Assistant Professorship Award, Best Paper Award at IEEE AVSS 2017, Best Paper Award at IEEE BTAS 2015, Honorable Mention Paper Award at IAPR ICB 2018, two Best Student Paper Awards at IAPR ICPR 2018, and Best Poster Awards at BTAS 2015 and 2016. He is an Associate Editor of the IEEE Signal Processing Magazine, IEEE Biometrics Compendium, Pattern Recognition Journal, and serves on the Information Forensics and Security Technical Committee of the IEEE Signal Processing Society. He is serving as the Vice President (Conferences) of the IEEE Biometrics Council. He is a member of Eta Kappa Nu, Pi Mu Epsilon, and Phi Beta Kappa.

