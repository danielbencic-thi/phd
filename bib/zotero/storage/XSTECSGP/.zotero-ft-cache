CRC 9008 FM.pdf 14/8/2007 14:39
AUTOMATION AND CONTROL ENGINEERING
A Series of Reference Books and Textbooks
Editor FRANK L. LEWIS, PH.D.
Professor Automation and Robotics Research Institute
The University of Texas at Arlington
Co-Editor SHUZHI SAM GE, PH.D. The National University of Singapore
1. Nonlinear Control of Electric Machinery, Darren M. Dawson, Jun Hu, and Timothy C. Burg
2. Computational Intelligence in Control Engineering, Robert E. King 3. Quantitative Feedback Theory: Fundamentals and Applications,
Constantine H. Houpis and Steven J. Rasmussen 4. Self-Learning Control of Finite Markov Chains, A. S. Poznyak, K. Najim,
and E. Gómez-Ramírez 5. Robust Control and Filtering for Time-Delay Systems,
Magdi S. Mahmoud 6. Classical Feedback Control: With MATLAB®, Boris J. Lurie
and Paul J. Enright 7. Optimal Control of Singularly Perturbed Linear Systems
and Applications: High-Accuracy Techniques, Zoran Gajif and Myo-Taeg Lim 8. Engineering System Dynamics: A Unified Graph-Centered Approach, Forbes T. Brown 9. Advanced Process Identification and Control, Enso Ikonen and Kaddour Najim 10. Modern Control Engineering, P. N. Paraskevopoulos 11 . Sliding Mode Control in Engineering, edited by Wilfrid Perruquetti and Jean-Pierre Barbot 12. Actuator Saturation Control, edited by Vikram Kapila and Karolos M. Grigoriadis 13. Nonlinear Control Systems, Zoran Vukiç, Ljubomir Kuljaãa, Dali Donlagiã, and Sejid Tesnjak 14. Linear Control System Analysis & Design: Fifth Edition, John D’Azzo, Constantine H. Houpis and Stuart Sheldon 15. Robot Manipulator Control: Theory & Practice, Second Edition, Frank L. Lewis, Darren M. Dawson, and Chaouki Abdallah 16. Robust Control System Design: Advanced State Space Techniques, Second Edition, Chia-Chi Tsui 17. Differentially Flat Systems, Hebertt Sira-Ramirez and Sunil Kumar Agrawal

CRC 9008 FM.pdf 14/8/2007 14:39
18. Chaos in Automatic Control, edited by Wilfrid Perruquetti and Jean-Pierre Barbot
19. Fuzzy Controller Design: Theory and Applications, Zdenko Kovacic and Stjepan Bogdan
20. Quantitative Feedback Theory: Fundamentals and Applications, Second Edition, Constantine H. Houpis, Steven J. Rasmussen, and Mario Garcia-Sanz
21. Neural Network Control of Nonlinear Discrete-Time Systems, Jagannathan Sarangapani
22. Autonomous Mobile Robots: Sensing, Control, Decision Making and Applications, edited by Shuzhi Sam Ge and Frank L. Lewis
23. Hard Disk Drive: Mechatronics and Control, Abdullah Al Mamun, GuoXiao Guo, and Chao Bi
24. Stochastic Hybrid Systems, edited by Christos G. Cassandras and John Lygeros
25. Wireless Ad Hoc and Sensor Networks: Protocols, Performance, and Control, Jagannathan Sarangapani
26. Optimal and Robust Estimation: With an Introduction to Stochastic Control Theory, Second Edition, Frank L. Lewis, Lihua Xie, and Dan Popa

This page intentionally left blank

CRC 9008 FM.pdf 14/8/2007 14:39
Optimal and Robust Estimation
With an Introduction to Stochastic Control Theory
SECOND EDITION

MATLAB® is a trademark of The MathWorks, Inc. and is used with permission. The MathWorks does not warrant the accuracy of the text or exercises in this book. This book’s use or discussion of MATLAB® software or related products does not constitute endorsement or sponsorship by The MathWorks of a particular pedagogical approach or particular use of the MATLAB® software.
CRC Press Taylor & Francis Group 6000 Broken Sound Parkway NW, Suite 300 Boca Raton, FL 33487-2742 © 2008 by Taylor & Francis Group, LLC CRC Press is an imprint of Taylor & Francis Group, an Informa business No claim to original U.S. Government works Version Date: 20110614 International Standard Book Number-13: 978-1-4200-0829-6 (eBook - PDF) This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been made to publish reliable data and information, but the author and publisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint. Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission from the publishers. For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged. Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for identification and explanation without intent to infringe. Visit the Taylor & Francis Web site at http://www.taylorandfrancis.com and the CRC Press Web site at http://www.crcpress.com

CRC 9008 FM.pdf 14/8/2007 14:39
Optimal and Robust Estimation
With an Introduction to Stochastic Control Theory
SECOND EDITION
FRANK L. LEWIS LIHUA XIE DAN POPA
Boca Raton London New York CRC Press is an imprint of the Taylor & Francis Group, an informa business

This page intentionally left blank

CRC 9008 FM.pdf 14/8/2007 14:39
To Galina, Roma, Chris. Frank L. Lewis
To my wife, Meiyun, and daughters, Jassica and Julie, for their constant support and enduring love. Lihua Xie
In memory of my late wife Priya Mancharam, for her constant support and inspiration during the revision of the book. Dan Popa

This page intentionally left blank

CRC 9008 FM.pdf 14/8/2007 14:39

Contents

Preface ............................................................................................. xv Authors ............................................................................................ xvii Subject ............................................................................................. xix Audience .......................................................................................... xxi

I Optimal Estimation

1

1 Classical Estimation Theory

3

1.1 Mean-Square Estimation ....................................................... 3

1.1.1 Mean-Square Estimation of a Random Variable X by a

Constant .................................................................... 4

1.1.2 Mean-Square Estimation of a Random Variable X Given

a Random Variable Z: General Case ............................ 5

1.1.3 The Orthogonality Principle ........................................ 13

1.1.4 Linear Mean-Square Estimation of a Random

Variable X Given a Random Variable Z ...................... 14

1.2 Maximum-Likelihood Estimation ........................................... 19

1.2.1 Nonlinear Maximum-Likelihood Estimation.................. 19

1.2.2 Linear Gaussian Measurements.................................... 20

1.3 The Cram´er–Rao Bound ....................................................... 25

1.4 Recursive Estimation ............................................................ 28

1.4.1 Sequential Processing of Measurements ........................ 28

1.4.2 Sequential Maximum-Likelihood Estimation ................. 31

1.4.3 Prewhitening of Data.................................................. 33

1.5 Wiener Filtering ................................................................... 34

1.5.1 The Linear Estimation Problem................................... 36

1.5.2 Solution of the Wiener–Hopf Equation ......................... 38

1.5.2.1 Inﬁnite-Delay Steady-State Smoothing ........... 38

1.5.2.2 Causal Steady-State Filtering ........................ 41

Problems ..................................................................................... 50

2 Discrete-Time Kalman Filter

59

2.1 Deterministic State Observer ................................................ 59

2.2 Linear Stochastic Systems ..................................................... 64

2.2.1 Propagation of Means and Covariances ........................ 65

2.2.2 Statistical Steady-State and Spectral Densities ............. 68

ix

CRC 9008 FM.pdf 14/8/2007 14:39

x

Contents

2.3 The Discrete-Time Kalman Filter .......................................... 70 2.3.1 Kalman Filter Formulations ........................................ 71
2.4 Discrete Measurements of Continuous-Time Systems .............. 84 2.4.1 Discretization of Continuous Stochastic Systems........... 85 2.4.2 Multiple Sampling Rates ............................................. 98 2.4.3 Discretization of Time-Varying Systems ....................... 101
2.5 Error Dynamics and Statistical Steady State ......................... 101 2.5.1 The Error System ....................................................... 101 2.5.2 The Innovations Sequence ........................................... 102 2.5.3 The Algebraic Riccati Equation................................... 103 2.5.4 Time-Varying Plant .................................................... 110
2.6 Frequency Domain Results .................................................... 112 2.6.1 A Spectral Factorization Result ................................... 112 2.6.2 The Innovations Representation................................... 114 2.6.3 Chang–Letov Design Procedure for the Kalman Filter ............................................................ 115 2.6.4 Deriving the Discrete Wiener Filter ............................. 118
2.7 Correlated Noise and Shaping Filters ..................................... 123 2.7.1 Colored Process Noise ................................................. 124 2.7.2 Correlated Measurement and Process Noise.................. 127 2.7.3 Colored Measurement Noise ........................................ 130
2.8 Optimal Smoothing .............................................................. 132 2.8.1 The Information Filter ................................................ 133 2.8.2 Optimal Smoothed Estimate ....................................... 136 2.8.3 Rauch–Tung–Striebel Smoother ................................... 138
Problems ..................................................................................... 140

3 Continuous-Time Kalman Filter

151

3.1 Derivation from Discrete Kalman Filter ................................. 151

3.2 Some Examples .................................................................... 157

3.3 Derivation from Wiener–Hope Equation ................................ 166

3.3.1 Introduction of a Shaping Filter .................................. 167

3.3.2 A Diﬀerential Equation for the Optimal Impulse

Response.................................................................... 168

3.3.3 A Diﬀerential Equation for the Estimate ...................... 169

3.3.4 A Diﬀerential Equation for the Error Covariance .......... 170

3.3.5 Discussion .................................................................. 172

3.4 Error Dynamics and Statistical Steady State ......................... 177

3.4.1 The Error System ....................................................... 177

3.4.2 The Innovations Sequence ........................................... 178

3.4.3 The Algebraic Riccati Equation................................... 179

3.4.4 Time-Varying Plant .................................................... 180

3.5 Frequency Domain Results .................................................... 180

3.5.1 Spectral Densities for Linear Stochastic Systems........... 181

3.5.2 A Spectral Factorization Result ................................... 181

3.5.3 Chang–Letov Design Procedure ................................... 184

CRC 9008 FM.pdf 14/8/2007 14:39

Contents

xi

3.6 Correlated Noise and Shaping Filters ..................................... 188 3.6.1 Colored Process Noise ................................................. 188 3.6.2 Correlated Measurement and Process Noise.................. 189 3.6.3 Colored Measurement Noise ........................................ 190
3.7 Discrete Measurements of Continuous-Time Systems .............. 193 3.8 Optimal Smoothing .............................................................. 197
3.8.1 The Information Filter ................................................ 200 3.8.2 Optimal Smoothed Estimate ....................................... 201 3.8.3 Rauch–Ting–Striebel Smoother ................................... 203 Problems ..................................................................................... 204

4 Kalman Filter Design and Implementation

213

4.1 Modeling Errors, Divergence, and Exponential

Data Weighting .................................................................... 213

4.1.1 Modeling Errors ......................................................... 213

4.1.2 Kalman Filter Divergence ........................................... 223

4.1.3 Fictitious Process Noise Injection ................................ 226

4.1.4 Exponential Data Weighting ....................................... 230

4.2 Reduced-Order Filters and Decoupling .................................. 236

4.2.1 Decoupling and Parallel Processing.............................. 236

4.2.2 Reduced-Order Filters................................................. 242

4.3 Using Suboptimal Gains ....................................................... 249

4.4 Scalar Measurement Updating .............................................. 253

Problems ..................................................................................... 254

5 Estimation for Nonlinear Systems

259

5.1 Update of the Hyperstate ..................................................... 259

5.1.1 Discrete Systems ........................................................ 259

5.1.2 Continuous Systems.................................................... 263

5.2 General Update of Mean and Covariance ............................... 265

5.2.1 Time Update.............................................................. 266

5.2.2 Measurement Update .................................................. 268

5.2.3 Linear Measurement Update ....................................... 269

5.3 Extended Kalman Filter ....................................................... 271

5.3.1 Approximate Time Update.......................................... 271

5.3.2 Approximate Measurement Update.............................. 272

5.3.3 The Extended Kalman Filter....................................... 273

5.4 Application to Adaptive Sampling ......................................... 283

5.4.1 Mobile Robot Localization in Sampling........................ 284

5.4.2 The Combined Adaptive Sampling Problem ................. 284

5.4.3 Closed-Form Estimation for a Linear Field without

Localization Uncertainty ............................................. 286

5.4.4 Closed-Form Estimation for a Linear Field

with Localization Uncertainty ..................................... 290

5.4.5 Adaptive Sampling Using the Extended

Kalman Filter ............................................................ 295

CRC 9008 FM.pdf 14/8/2007 14:39

xii

Contents

5.4.6 Simultaneous Localization and Sampling Using a Mobile Robot .......................................................... 297
Problems ..................................................................................... 305

II Robust Estimation

313

6 Robust Kalman Filter

315

6.1 Systems with Modeling Uncertainties .................................... 315

6.2 Robust Finite Horizon Kalman a Priori Filter ....................... 317

6.3 Robust Stationary Kalman a Priori Filter ............................. 321

6.4 Convergence Analysis ........................................................... 326

6.4.1 Feasibility and Convergence Analysis ........................... 326

6.4.2 ε–Switching Strategy .................................................. 329

6.5 Linear Matrix Inequality Approach ....................................... 331

6.5.1 Robust Filter for Systems with Norm-Bounded

Uncertainty ................................................................ 332

6.5.2 Robust Filtering for Systems with Polytopic

Uncertainty ................................................................ 335

6.6 Robust Kalman Filtering for Continuous-Time Systems .......... 341

Proofs of Theorems ...................................................................... 343

Problems ..................................................................................... 350

7 H∞ Filtering of Continuous-Time Systems

353

7.1 H∞ Filtering Problem .......................................................... 353

7.1.1 Relationship with Two-Person Zero-Sum Game ............ 356

7.2 Finite Horizon H∞ Linear Filter ........................................... 357 7.3 Characterization of All Finite Horizon H∞ Linear Filters ....... 361 7.4 Stationary H∞ Filter—Riccati Equation Approach ................ 365
7.4.1 Relationship between Guaranteed H∞ Norm and Actual H∞ Norm ................................................. 371

7.4.2 Characterization of All Linear

Time-Invariant H∞ Filters .......................................... 373

7.5 Relationship with the Kalman Filter ..................................... 373

7.6 Convergence Analysis ........................................................... 374

7.7 H∞ Filtering for a Special Class of Signal Models .................. 378 7.8 Stationary H∞ Filter—Linear Matrix Inequality Approach ..... 382

Problems ..................................................................................... 383

8 H∞ Filtering of Discrete-Time Systems

387

8.1 Discrete-Time H∞ Filtering Problem .................................... 387

8.2 H∞ a Priori Filter ............................................................... 390

8.2.1 Finite Horizon Case .................................................... 391

8.2.2 Stationary Case .......................................................... 397

8.3 H∞ a Posteriori Filter ......................................................... 400

8.3.1 Finite Horizon Case .................................................... 400

8.3.2 Stationary Case .......................................................... 405

CRC 9008 FM.pdf 14/8/2007 14:39

Contents

xiii

8.4 Polynomial Approach to H∞ Estimation ............................... 408 8.5 J-Spectral Factorization ........................................................ 410 8.6 Applications in Channel Equalization .................................... 414 Problems ..................................................................................... 419

III Optimal Stochastic Control

421

9 Stochastic Control for State Variable Systems

423

9.1 Dynamic Programming Approach .......................................... 423

9.1.1 Discrete-Time Systems................................................ 424

9.1.2 Continuous-Time Systems ........................................... 435

9.2 Continuous-Time Linear Quadratic Gaussian Problem ............ 443

9.2.1 Complete State Information ........................................ 445

9.2.2 Incomplete State Information and

the Separation Principle.............................................. 449

9.3 Discrete-Time Linear Quadratic Gaussian Problem ................ 453

9.3.1 Complete State Information ........................................ 454

9.3.2 Incomplete State Information ...................................... 455

Problems ..................................................................................... 457

10 Stochastic Control for Polynomial Systems

463

10.1 Polynomial Representation of Stochastic Systems ................... 463

10.2 Optimal Prediction ............................................................... 465

10.3 Minimum Variance Control ................................................... 469

10.4 Polynomial Linear Quadratic Gaussian Regulator .................. 473

Problems ..................................................................................... 481

Appendix: Review of Matrix Algebra

485

A.1 Basic Deﬁnitions and Facts ................................................... 485

A.2 Partitioned Matrices ............................................................. 486

A.3 Quadratic Forms and Deﬁniteness ......................................... 488

A.4 Matrix Calculus ................................................................... 490

References

493

Index

501

This page intentionally left blank

CRC 9008 FM.pdf 14/8/2007 14:39
Preface
This book is intended for use in a second graduate course in modern control theory. A background in both probability theory and the state variable representation of systems is assumed. One advantage of modern control theory is that it employs matrix algebra, which results in a simpliﬁcation in notation and mathematical manipulations when dealing with multivariable systems. The Appendix provides a short review of matrices for those who need it.
The book is also intended as a reference. Equations of recurring usefulness are displayed in tabular form for easy access. Many examples are used to illustrate the concepts and to impart intuition, and the reader is shown how to write simple software implementations of estimators using MATLAB . In this edition, the example problems have been revisited and updated. Sample code in MATLAB will be available from the publisher’s Web site for download.
Optimal control and optimal estimation are the dual theories that provide the foundation for the modern study of systems. Optimal control can be studied in a purely deterministic context in which the unrealistic assumption is made that perfect information about nature is available. The solutions to control problems are discovered by a backward progression through time.
Optimal estimation, however, is the problem of casting into a useful form information from an inherently noisy and substantially uncooperative environment; it is, therefore, intimately concerned with probabilistic notions. Solutions to estimation problems are discovered by a forward progression through time.
Stochastic control theory uses information reconstructed from noisy measurements to control a system so that it has a desired behavior; hence, it represents a marriage of optimal estimation and deterministic optimal control.
In Part I, we discuss the estimation problem, covering in detail a system that extracts information from measured data and which is known as the Kalman ﬁlter. Kalman ﬁlters are used, for example, in communication networks and in the navigation of ships, aircraft, and space vehicles. Classical estimation theory, including the Wiener ﬁlter, is reviewed in Chapter 1, and throughout the book we point out relationships between the Kalman ﬁlter and this older body of knowledge.
In Part II, we focus our discussions on robust estimation in which the dynamics of the underlying system are not exactly known. First, we are concerned with the design of a ﬁlter that would have an optimal guaranteed performance for systems with parametric uncertainties of a norm-bounded type or polytopic type. However, when little information about the statistics
xv

CRC 9008 FM.pdf 14/8/2007 14:39

xvi

Preface

of noises is given, an H∞ ﬁlter would be more desirable than a Kalman ﬁlter. In Chapters 7 and 8, we give a detailed account of H∞ ﬁltering.
In Part III, we introduce stochastic control theory, treating both state variable systems and polynomial systems. For a complete foundation in modern system theory and more insight on stochastic control, a course in deterministic optimal control is required.
Frank L. Lewis—This book is dedicated to my teachers, J.B. Pearson and E.W. Kamen, whose knowledge and wisdom I have imperfectly transcribed into this written volume wherein the insight and the better ideas are completely theirs. It is also dedicated to N.B. Nichols, whose achievements have been a source of inspiration and whose friendship a source of strength. Finally, it is dedicated to Brian Stevens, a true engineer, and a close friend who has given me more insight than he realizes in our years of working together.
Lihua Xie—The book is dedicated to C.E. de Souza, M. Fu, and U. Shaked for their willingness to share their extensive knowledge and their research collaborations on robust estimation. We are also indebted to Y.C. Soh and H. Zhang for many useful discussions and S. Sun and J. Xu for their careful reading of Part II of the book. The help from J. Xu in some simulation examples of Part II is greatly appreciated. The book is also dedicated to my wife, Meiyun, and daughters, Jassica and Julie, for their constant support and enduring love.
Dan Popa wishes to dedicate his contribution to this edition in the memory of his late wife Priya Mancharam, for her constant support and inspiration during the revision of the book. We also acknowledge the collaboration work with Professors Arthur Sanderson and Harry Stephanou, and the help of Chad Helm, Vadiraj Hombal, Koushil Sreenath, and Muhammad Mysorewala on the example of adaptive sampling with robots added to Chapter 5.
Finally, we thank Muhammad Mysorewala for his patience and diligence working through most of the examples, codes, and the diagrams in the book.

Frank L. Lewis Lihua Xie Dan Popa

CRC 9008 FM.pdf 14/8/2007 14:39
Authors
Frank L. Lewis, Fellow IEEE, Fellow U.K. Institute of Measurement and Control, PE Texas, U.K. Chartered Engineer, is distinguished scholar professor and Moncrief-O’Donnell chair at the University of Texas at Arlington’s Automation and Robotics Research Institute. He obtained his Bachelor’s degree in physics/EE and his MSEE degree at Rice University, his MS degree in aeronautical engineering from the University of West Florida, and his PhD at the Georgia Institute of Technology. He works in feedback control, intelligent systems, and sensor networks. He is the author of ﬁve U.S. patents, 174 journal papers, 286 conference papers, and 11 books. He received the Fulbright Research Award, NSF Research Initiation Grant, and ASEE Terman Award. He also received the Outstanding Service Award from the Dallas IEEE Section and was selected as engineer of the year by the Ft. Worth IEEE Section. He is listed in the Ft. Worth Business Press Top 200 Leaders in Manufacturing. He was appointed to the NAE Committee on Space Station in 1995. Dr. Lewis is an elected guest consulting professor at both South China University of Technology and Shanghai Jiao Tong University. He is the founding member of the Board of Governors of the Mediterranean Control Association. He helped win the IEEE Control Systems Society Best Chapter Award (as founding chairman of the DFW Chapter), the National Sigma Xi Award for Outstanding Chapter (as president of UTA Chapter), and the US SBA Tibbets Award in 1996 (as director of ARRI’s SBIR Program).
Lihua Xie received his BE and ME degrees in electrical engineering from Nanjing University of Science and Technology in 1983 and 1986, respectively, and his PhD degree in electrical engineering from the University of Newcastle, Australia, in 1992. Since 1992, he has been with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, where he is a professor. He held teaching appointments in the Department of Automatic Control, Nanjing University of Science and Technology from 1986 to 1989. He has also held visiting appointments with the University of Newcastle, the University of Melbourne, and Hong Kong Polytechnic University.
Dr Xie’s research interests include robust control and estimation, networked control systems, time delay systems, control of disk drive systems, and sensor networks. He has published over 120 journal papers and coauthored the books H-Inﬁnity Control and Filtering of Two-Dimensional Systems (with C. Du), Optimal and Robust Estimation (with F.L. Lewis and D. Popa), and Control and Estimation of Systems with Input/Output Delays (with H. Zhang). He is an associate editor of the IEEE Transactions on Automatic Control,
xvii

CRC 9008 FM.pdf 14/8/2007 14:39

xviii

Authors

Automatica, and IEEE Transactions on Circuits and Systems II. He is also a member of the editorial board of IEE Proceedings on Control Theory and Applications. He served as an associate editor of the International Journal of Control, Automation and Systems from 2004 to 2006 and an associate editor of the Conference Editorial Board, IEEE Control Systems Society from 2000 to 2005. He was the general chairman of the 9th International Conference on Control, Automation, Robotics and Vision. Dr. Xie is a fellow of IEEE and a fellow of Institution of Engineers, Singapore.

Dan Popa is presently an assistant professor of electrical engineering at the Automation and Robotics Research Institute at the University of Texas at Arlington. He received a BA degree in engineering, mathematics, and computer science and an MS degree in engineering, both from Dartmouth College where he was a Montgomery Scholar from 1990 to 1994. He received a PhD in EE from Rensselaer Polytechnic Institute in 1998, focusing on nonholonomic robotics. He then joined the Center for Automation Technologies at RPI, where he held the rank of senior research scientist until 2004. Dr. Popa led numerous projects sponsored by government agencies and industry in the areas of robotics, control, and microsystems assembly and packaging.
Dr. Popa has a broad experience base in control and robotics, including the design, characterization, modeling, and control of MEMS actuators; the design of parallel and multiscale assembly architectures; the deployment of mobile sensor networks; and the development of new processes for 3D wafer integration. His current research focuses on sensors, actuators, and integrated microsystems, and their deployment using robots. He is the recipient of several prestigious awards and the author of over 60 refereed papers, and a member of IEEE, ASME, SME, and IMAPS.

CRC 9008 FM.pdf 14/8/2007 14:39
Subject
The book covers estimation theory and design techniques that are important in navigation, communication systems and signal processing, aerospace systems, industry, seismology and earthquake prediction, and elsewhere. Estimators seek to combine the available measurements and knowledge of system dynamics to provide the most accurate knowledge possible of desired characteristics or quantities of interest. An example is in spacecraft navigation where dynamical information about the motion of the vehicle and also measurements from navigation systems and star positioning information are available. The Kalman ﬁlter combines all this information to provide the best possible estimate of the vehicle’s location. Another example is in communications, where one seeks to reconstruct the signals in the presence of noise and distortions. The book covers optimal ﬁltering including the Kalman ﬁlter, optimal smoothing, and nonlinear estimation. Three chapters on robust estimation and H∞ estimation show how to apply ﬁltering for modern-day high-performance systems with disturbances and uncertainty. Both continuous-time and discretetime systems are discussed. Extensive computer examples and case studies provide insight and design experience. MATLAB code is available on the Web site. Design equations and algorithms are collected in tables for ease of use and reference.
xix

This page intentionally left blank

CRC 9008 FM.pdf 14/8/2007 14:39
Audience
The book is intended for senior undergraduate or ﬁrst graduate courses on estimation. The ﬁrst edition has been used for both. It is also useful for researchers in academia and government labs. Industry engineers currently using the book will welcome the new material on robust and H-inﬁnity ﬁltering.
The material is important in navigation, communication systems and signal processing, aerospace systems, industry, seismology and earthquake prediction, biological systems, control systems, and elsewhere. As such it will market in the above IEEE societies and other corresponding societies nationally and internationally.
xxi

This page intentionally left blank

CRC 9008 S001.pdf 16/6/2007 10:56
Part I
Optimal Estimation

This page intentionally left blank

CRC 9008 C001.pdf 20/7/2007 12:35

1
Classical Estimation Theory

There are many ways to estimate an unknown quantity from available data. This chapter basically covers mean-square estimation and the Wiener ﬁlter. A discussion of recursive estimation is included, since it provides a good background for the Kalman ﬁlter. The techniques presented here can be described as classical techniques in that they represent the state of the art prior to 1960, when Kalman and Bucy developed their approach (Kalman 1960, 1963; Kalman and Bucy 1961).

1.1 Mean-Square Estimation

Suppose there is an unknown vector X and available data Z that is somehow

related to X, so that knowing Z we have some information about X. Mean-

square estimation implies a setting in which there is prior information about

both the unknown X ∈ Rn and the measurement Z ∈ Rp, so that both X

and Z are stochastic. That is, we know the joint statistics of X and Z. If Z is given, we would like to make an estimate Xˆ (Z) of the value of X so that

the estimation error

X˜ = X − Xˆ

(1.1)

is small. “Small” can be deﬁned in several ways, leading to diﬀerent methods
of estimation.
Deﬁne the mean-square error as the expected value of the Euclidean norm squared of X˜ ,

J = X˜ T X˜ = (X − Xˆ )T (X − Xˆ )

(1.2)

Since X˜ is a random variable, the expected value (overbar) is required as shown. (We use both overbars and E(·) for expected value.)
The mean-square estimation criterion can be stated as follows. Using all available information, select an estimate Xˆ to minimize Equation 1.2.
It is worth saying a few words about the “available information.” In the
next subsection, we consider the case where there is no information other than the statistics of X. Next, we consider the case where a random variable Z, the measurement, provides additional information on X. There will be no assumption about how Z derives from X, so this latter result will be

3

CRC 9008 C001.pdf 20/7/2007 12:35

4

Optimal and Robust Estimation

completely general. Finally, in the third subsection we make a useful restricting assumption on the form of the estimate. This presentation is modiﬁed from the one in Papoulis (1965).
Mean-square estimation is a special case of Bayesian estimation. The general Bayesian estimation problem is to ﬁnd the estimate that minimizes the Bayes risk

∞

J = E[C(X˜ )] =

C(x − Xˆ )fXZ (x, z)dx dz

−∞

(1.3)

where C(·) is a cost function. Selecting C(X˜ ) = X˜ T X˜ results in Equation 1.2. Another special case is maximum a posteriori estimation (C(X˜ ) = 0, if all components of X˜ have magnitude less than a given ε, C(X˜ ) = 1 otherwise).
It can be shown that as long as the cost is symmetric and convex, and the
conditional probability density function (PDF) fX/Z (x/z) is symmetric about its mean, then all Bayes estimates yield the same optimal estimate (Sorenson
1970a). As we shall discover, this is the conditional mean of the unknown,
given the data X/Z.

1.1.1 Mean-Square Estimation of a Random Variable X by a Constant
Suppose the measurement Z is not yet given, but we know only the a priori statistics of the unknown X. That is, the PDF fX (x) is known, or equivalently all moments of X are given. The problem is to choose a constant that will yield the best a a priori estimate Xˆ for X in the mean-square sense.
To solve this problem, work with the mean-square error (Equation 1.2)

J = (X − Xˆ )T (X − Xˆ ) = XT X − Xˆ T X¯ − X¯ T Xˆ + Xˆ T Xˆ
= XT X − 2Xˆ T X¯ + Xˆ T Xˆ
where we have used the fact that, since it is a constant, Xˆ does not depend on X and so can be taken out from under the expectation operator. Now, diﬀerentiating this, it is seen that

∂J ∂Xˆ

=

−2X¯

+ 2Xˆ

=

0

or

XˆMS = X¯

(1.4)

We have thus arrived at the possibly obvious and certainly intuitive conclusion that if only the statistics of X are known, then the best constant to estimate X is the mean of X.
To obtain a measure of conﬁdence for the estimate (Equation 1.4), let us ﬁnd the error covariance. Notice ﬁrst that

E(X˜ ) = E(X − X¯ ) = X¯ − X¯ = 0

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

5

so that the expected estimation error is equal to zero. This property is of suﬃcient importance to give it a name. Accordingly, deﬁne an estimate Xˆ to
be unbiased if

E(X˜ ) = 0

(1.5)

This is equivalent to

E(Xˆ ) = X¯

(1.6)

which says that the expected value of the estimate is equal to the mean of the unknown. (In the absence of data, the optimal mean-square estimate is the mean of the unknown, as we have just discovered.)
We can now write for the error covariance
PX˜ = (X˜ − X¯˜ )(X˜ − X¯˜ )T = X˜ X˜ T = (X − Xˆ )(X − Xˆ )T = (X − X¯ )(X − X¯ )T

but this is just equal to the covariance of X. The a priori error covariance is therefore just

PX˜ = PX

(1.7)

If the error covariance is small, then the associated estimate is a good one. We call the inverse of the error covariance, PX−˜ 1, the information matrix.

1.1.2 Mean-Square Estimation of a Random Variable X Given a Random Variable Z: General Case
Now, additional information is prescribed. Suppose that in addition to the statistics of X, the value of another related random variable Z is known
(presumably by some measurement process). We shall make no assumptions on how Z derives from X. It is assumed that only the joint density fXZ (x, z) of X and Z is given.
The optimal mean-square estimate depends on the (known) measurement, so write Xˆ (Z). We call this estimate after taking the measurement the a posteriori estimate for X. As the value of Z changes, so does Xˆ , but if Z has a ﬁxed value of Z = z, then Xˆ (Z) has the ﬁxed value Xˆ (z). This fact will be
required later. By deﬁnition, to ﬁnd Xˆ (Z), write Equation 1.2 as

∞

J=

[x − Xˆ (z)]T [x − Xˆ (z)]fXZ (x, z)dx dz

−∞

CRC 9008 C001.pdf 20/7/2007 12:35

6

Optimal and Robust Estimation

or, using Bayes’ rule,

∞

∞

J=

fZ (z) [x − Xˆ (z)]T [x − Xˆ (z)]fX/Z (x/z)dx dz

−∞

−∞

Since fZ(z) and the inner integral are nonnegative, J can be minimized by ﬁxing Z = z and minimizing for every such z the value of the conditional
mean-square error

∞
J = [x − Xˆ (z)]T [x − Xˆ (z)]fX/Z (x/z)dx
−∞

(1.8)

Now, repeat the derivation of Equation 1.4 using conditional means. Then,

J = [X − Xˆ (Z)]T [X − Xˆ (Z)]/Z

= XT X/Z − 2Xˆ (Z)T X/Z + Xˆ (Z)T Xˆ (Z)

(1.9)

where we have used our previous realization that Xˆ (Z) is ﬁxed if Z is ﬁxed, so that Xˆ (Z) comes out from under the conditional expectation operator.
Diﬀerentiate to ﬁnd that

∂J ∂Xˆ (Z)

=

−2X/Z

+

2Xˆ (Z)

=

0

or ﬁnally

XˆMS(Z) = X/Z

(1.10)

This is the not so obvious but equally intuitive counterpart to Equation 1.4. If a measurement Z = z is given, then we should take as the a posteriori estimate for X the conditional mean of X, given that Z = z. If X and Z are independent then fXZ = fX fZ so that

X/Z =

xf X/Z (x/z)dx =

x

fXZ (x, z) fZ (z)

dx

= xfX (x)dx = X¯

(1.11)

and the a priori and a posteriori estimates are the same. Then, knowledge of Z does not provide any information about X.
Equation 1.10 appears very simple, but the simplicity of its appearance belies the diﬃculty of its use, the computation of the conditional mean X/Z may be an intractable problem. Worse than this, to ﬁnd X/Z we require fXZ , which may not be known. In some cases, we may know only its ﬁrst and second moments.
Equation 1.10 is the mean-square estimate in the most general case. It is often called the conditional mean estimate of X and, under the general conditions we have mentioned, all Bayes estimation schemes lead to this same optimal estimate.

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

7

It is sometimes convenient to express the conditional mean X/Z in terms of the PDF fZ/X (z/x). To do this, note that by Bayes’ rule

X/Z = X/Z =
= X/Z =

∞

xfX/Z (x/z) dx
−∞

∞ −∞

x

fX/Z fZ

(x, (z)

z

)

dx

∞ −∞

xfXZ

(x,

z)dx

∞ −∞

fXZ

(x,

z

)dx

∞ −∞

xfZ/X

(z/x)fX

(x)dx

∞ −∞

fZ/X

(z/x)fX

(x)dx

(1.12) (1.13)

which is the desired result. The conditional mean estimate is unbiased since X¯ = E(X/Z):

E(X˜ ) = E(X˜ /Z) = E((X − Xˆ )/Z) = E((X − X/Z)/Z) = E(X/Z − X/Z/Z) = E(X/Z − X/Z) = 0

(Note that taking the same expected value twice is the same as taking it once.) A measure of conﬁdence in an estimate is given by the error covariance.
To ﬁnd the error covariance associated with the conditional mean estimate, write, since X¯˜ = 0,

PX˜ = X˜ X˜ T = (X − Xˆ )(X − Xˆ )T = E((X − X/Z)(X − X/Z)T /Z) = E(PX/Z )
or

PX˜ = PX/Z

(1.14)

Under some conditions PX˜ = PX/Z , so that the unconditional error covariance is the same as the conditional error covariance given a particular mea-
surement Z. In general, however, Equation 1.14 must be used since PX/Z may depend on Z.
It is worth noting that J as given by Equation 1.2 is equal to trace (PX˜ ) so that XˆMS minimizes the trace of the error covariance matrix.
The next example shows how to compute mean-square estimates. We shall use (X¯ , PX ) to denote a random variable with mean X¯ and covariance PX . If X is normal we write N (X¯ , PX ).

CRC 9008 C001.pdf 20/7/2007 12:35

8

Optimal and Robust Estimation

Example 1.1 Nonlinear Mean-Square Estimation (Melsa and Cohn 1978) Let unknown X be distributed uniformly according to

fX (x) =

1, 0,

0≤x≤1 otherwise

(1.15)

a. In the absence of other information, choose a constant to estimate X in the optimal mean-square sense (i.e., ﬁnd the a priori estimate). According to Equation 1.4, in this case the best estimate is

∞

XˆMS = Xˆap = X¯ =

xf X (x)dx

−∞

= 1 x dx = 1

0

2

(1.16)

b. Now, suppose a Z is measured, which is related to X by

Z = ln

1 X

+V

(1.17)

where V is noise with exponential distribution

fV (v) =

e−v , 0,

v≥0 v<0

(1.18)

and X and V are independent. Find the conditional mean estimate.
If X and V are independent, then (function of a random variable; Papoulis 1965)

fZ/X (z/x) = fV z − ln

1 x

=

e−(z−ln(1/x)), 0,

z

−

ln

1 x

≥

0

z

−

ln

1 x

<

0

=

1 x

e−z

,

0,

x ≥ e−z x < e−z

(1.19)

Now using Equation 1.13 we get

XˆMS(z) =

1 e−z

e−z dx

1 e−z

1 x

e−z

dx

=

xe−z |1e−z e−z ln x|1e−z

(1 − e−z)e−z

=

ze −z

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

9

1
XMS 0.8

0.6 Xap

0.4

XLMS

0.2 XML

0

1

2

3

4

5

6

7

z

FIGURE 1.1 Nonlinear mean-square estimate.

or

XˆMS

=

1

− e−Z Z

(1.20)

This nonlinear estimate is sketched in Figure 1.1 along with the a priori estimate Xap and some other estimates for the same problem, which we shall discuss in Examples 1.4 and 1.6.

Example 1.2 Gaussian Conditional Mean and Covariance

This is an extremely important example, which we shall constantly use in
subsequent sections. Let Y ∈ Rn+p be normal, symbolized by N (Y¯ , PY ), so that its PDF is

fY (y) =

1

= e−(1/2)(y−Y¯ )T PY−1(y−Y¯ )

(2π)n+p|PY |

(1.21)

Now, partition Y as Y = [XT ZT ]T , where X ∈ Rn and Z ∈ Rp are
themselves random vectors (RV). Suppose we perform an experiment where
the value z of Z is measured and that it is then desired to get some information about the unmeasured portion X of Y .
To ﬁnd an expression for the conditional PDF, note ﬁrst that X and Z are both normal, that is, X ∼ N (X¯ , PX ), Z ∼ N (Z¯, PZ ). The cross-covariance is

CRC 9008 C001.pdf 20/7/2007 12:35

10

Optimal and Robust Estimation

PXZ = E(X − X¯ )(Z − Z¯)T . Then

y=

x z

,

Y¯ =

X¯ Z¯

,

PY =

PX PZX

PXZ PZ

(1.22)

and so (Appendix)

PY−1 =

D−1 −PZ−1PZX D−1

−D−1PXZ PZ−1 PZ−1 + PZ−1PZX D−1PXZ PZ−1

where the Schur complement of PX is D = PX − PXZ PZ−1PZX . Noting that fY (y) and fXZ (x, z) are the same PDF

fX/Z (x/z)

=

fXZ (x, z) fZ (z)

=

fY (y) fZ (z)

we can write

(1.23)

fX/Z (x/z)

=

(2π)p|PZ | exp (2π)n+p|PY |

−

1 2

[(y

−

Y¯

)T

PY−1(y

−

Y¯

)

−

(z

−

Z¯)T

PZ−1(z

−

Z¯)]

(1.24)

(To write Equation 1.21 as fXZ (x, z), we can substitute from Equation 1.22.) By using Equations 1.22 and 1.23, write the exponent as

x − X¯ z − Z¯

T
PY−1

x − X¯ z − Z¯

− (z − Z¯)T PZ−1(z − Z¯)

= (x − X¯ )T D−1(x − X¯ ) − (x − X¯ )T D−1PXZ PZ−1(z − Z¯) − (z − Z¯)T PZ−1PZX D−1(x − X¯ ) + (z − Z¯)T (PZ−1 + PZ−1PZX D−1PXZ PZ−1)(z − Z¯) − (z − Z¯)T PZ−1(z − Z¯)

Deﬁne the conditional mean as

X/Z = X¯ + PXZ PZ−1(Z − Z¯)

and the conditional covariance as

(1.25) (1.26)

PX/Z = PX − PXZ PZ−1PZX

(1.27)

the Schur complement of PX in PY . Now, it takes only patience to show that Equation 1.25 can be written as

(x − X/Z = z)T PX−/1Z (x − X/Z = z)

(1.28)

Furthermore, it can be demonstrated (Appendix) that

|PY |/|PZ | = |PX/Z |

(1.29)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

11

We have thus proven that X/Z is a normal RV with PDF

fX/Z (x/z) =

1 exp
(2π)n|PX/Z |

− 1 (x − X/Z 2

= z)T PX−/1Z (x − X/Z

= z)

(1.30)

The conditional mean estimate in the Gaussian case is therefore given by Equation 1.26. Note that in the Gaussian case, PX/Z is independent of Z so that the error covariance

PX˜ = PX/Z

(1.31)

is given by Equation 1.27.
From Equation 1.27 it is clear that PX/Z ≤ PX . If in a particular case, PX/Z < PX , then, after measuring Z we are more certain of the value of X than we were before (i.e., the a posteriori RV X/Z is distributed more closely about its mean X/Z than is the a priori RV X about its mean X¯ ).

Example 1.3 Conditional Mean for Linear Gaussian Measurements

Here we ﬁnd the Gaussian conditional mean estimate and error covariance in the case where X and Z have a particular relation.
Suppose the unknown X and the data Z are related by the linear Gaussian measurement model

Z = HX + V

(1.32)

where X ∼ N (X¯ , PX ) and V ∼ N (0, R) is zero mean measurement noise, and measurement matrix H is known and deterministic (i.e., constant). Suppose
R > 0 and X and V are orthogonal so that XV T = 0. Let X ∈ Rn and V ∈ Rp, so that H is a p × n matrix and Z ∈ Rp.
The measurement Z is then a normal RV, and it is not diﬃcult to determine its mean Z¯ and variance PZ in terms of the known quantities X¯ , PX , R, and H. Thus

Z¯ = HX + V = HX¯ + V¯

by linearity, or Furthermore,

Z¯ = HX¯

(1.33)

PZ = (Z − Z¯)(Z − Z¯)T = (HX + V − HX¯ )(HX + V − HX¯ )T = H(X − X¯ )(X − X¯ )T HT + H(X − X¯ )V T + V (X − X¯ )T HT + VV T

= HP X HT + HXV T − HX¯ V¯ T + VX T HT − V¯ X¯ T HT + R

or PZ = HP X HT + R
Therefore, Z is N (HX¯ , HP X HT + R).

(1.34)

CRC 9008 C001.pdf 20/7/2007 12:35

12

Optimal and Robust Estimation

To ﬁnd the cross-covariance of X and Z, write

PXZ = (X − X¯ )(Z − Z¯)T = (X − X¯ )(HX + V − HX¯ )T = (X − X¯ )(X − X¯ )T HT + XV T − X¯ V¯ T

(1.35)

or

PZX = PXTZ = HP X

(1.36)

since PX is symmetric. The error covariance is obtained by using the above expressions for PZ,
PXZ , and PZX in Equation 1.27, which yields

PX˜ = PX/Z = PX − PXZ PZ−1PZX = PX − PX HT (HP X HT + R)−1HP X

(1.37)

By using the matrix inversion lemma (Appendix), this can be written as

PX˜ = PX/Z = (PX−1 + HT R−1H)−1

(1.38)

if |PX | = 0. Notice that PX/Z ≤ PX . If strict inequality holds, measuring RV Z decreases our uncertainty about the unknown RV X.
The conditional mean estimate is found by substituting for PXZ , PZ , and Z¯ in Equation 1.26

Xˆ = X/Z = X¯ + PXZ PZ−1(Z − Z¯) = X¯ + PX HT (HP X HT + R)−1(Z − HX¯ )

(1.39)

To ﬁnd a neater expression for X/Z, note that

PX HT (HP X HT + R)−1 = PX HT (HP X HT + R)−1[(HP X HT + R) − HP X HT ]R−1 = PX HT [I − (HP X HT + R)−1(HP X HT )]R−1 = [PX − PX HT (HPX HT + R)−1HP X ]HT R−1 = PX/Z HT R−1

Hence,

Xˆ = X/Z = X¯ + PX/Z HT R−1(Z − HX¯ )

(1.40)

The PDF fX/Z (x/z) is normal with conditional mean X/Z and conditional covariance PX/Z .
It is important to note that X/Z depends on the value of Z, but that covariance PX/Z is independent of the value of Z. Thus, PX/Z can be computed before we take any measurements.
It is not diﬃcult to redo this example for the case of nonorthogonal X and V. See the problems.

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

13

EXERCISE 1.1 Kalman Gain

In the case of linear Gaussian measurements, deﬁne the Kalman gain by

K = PX HT (HP X HT + R)−1

(1.41)

a. Express PX/Z and X/Z in terms of K. b. Prove that

PX/Z = (I − KH )PX (I − KH )T + KRK T = (I − KH )PX (1.42)

1.1.3 The Orthogonality Principle
The following orthogonality principle is basic to probabilistic estimation theory. Let RVs X and Z be jointly distributed. Then, for any function g(·),

g(Z)(X − X/Z)T = 0

(1.43)

That is, any function of Z is orthogonal to X once the conditional mean has been subtracted out. To show this, write

g(Z)(X − X/Z)T = g(Z)XT − g(Z)XT /Z

= g(Z)XT − g(Z)XT /Z = g(Z)XT − g(Z)XT = 0

We used the fact that

g(Z) · XT /Z = g(Z)XT /Z

(1.44)

for any function g(·), since g(Z) is deterministic if Z is ﬁxed.
The idea behind this can be illustrated as in Figure 1.2 where orthogonal
RVs are represented as being at right angles. All functions of Z are represented as being in the direction of Z. (In the ﬁgure, interpret XˆLMS as X/Z.) The orthogonality principle says that the RV

X˜ = X − X/Z

(1.45)

which is the estimation error, is orthogonal to all other RVs g(Z). It is emphasized that this ﬁgure is extremely heuristic, since if nonlinear functions g(·) are allowed, then RVs of the form g(Z) do not form a subspace. The ﬁgure should only be considered as a mnemonic device.
We can quickly show that the orthogonality principle has an immediate application to estimation theory. In fact, if g(·) is any function, then

E X − X/Z ≤ E X − g(Z)

(1.46)

where “ · ” denotes the Euclidean norm. Thus, no other function of Z provides a “closer approximation to X” in a probabilistic sense than does X/Z.
This is a partial statement of the stochastic projection theorem. To show it, a probabilistic formulation of Pythagoras’ theorem is used: if Y and Z are

CRC 9008 C001.pdf 20/7/2007 12:35

14

Optimal and Robust Estimation

X

~ XLMS

XLMS

z

FIGURE 1.2 The orthogonality principle.

orthogonal, then

E Y +Z 2 =E Y 2+E Z 2

(1.47)

Now use Equation 1.43 to write for any g(·)

E X − g(Z) 2 = E X − X/Z + X/Z − g(Z) 2 = E X − X/Z) 2 + E X/Z − g(Z) 2 ≥ E X − X/Z 2

with equality if and only if g(Z) = X/Z.

1.1.4 Linear Mean-Square Estimation of a Random Variable X Given a Random Variable Z
No assumptions are made here on the relation between Z and X; the results to be presented are valid for any possible nonlinear relation between measurement and unknown. In the realization that Equation 1.10 is in general too much trouble to compute, except for the Gaussian case, let us restrict the admissible estimate Xˆ (Z) to those that depend linearly on Z, that is,

Xˆ (Z) = AZ + b

(1.48)

for some constant matrix A ∈ Rn×p and vector b ∈ Rn. This linear estimate restriction should not be confused with the linear measurement model (Equation 1.32).
An additional motive for restricting an admissible estimate to those of the form in Equation 1.48 is that this form allows processing of the measurements Z by a linear system to obtain Xˆ .

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

15

Since the class of admissible estimate is restricted, the linear mean-square estimate will not, in general, be as good in the sense of minimizing Equation 1.2 as the general mean-square estimate (Equation 1.10).
The constants A and b depend on the measurement process and on the a priori information available before the measurement is made. They must be chosen to minimize the mean-square error (Equation 1.2). Recall that trace(MN ) = trace(NM ) for compatible matrices M and N and write

J = (X − Xˆ )T (X − Xˆ ) = trace(X − X)(X − X)T = trace(X − AZ − b)(X − AZ − b)T = trace[X − X¯ ) − (AZ + b − X¯ )][(X − X¯ ) − (AZ + b − X¯ )]T

After some straightforward work we arrive at

J = trace[PX + A(PZ + ZZ T )AT + (b − X¯ )(b − X¯ )T + 2AZ¯(b − X¯ )T − 2AP ZX ]

(1.49)

From Appendix, (d/dA) trace (ABAT ) = 2AB and (d/dA) trace(BAC ) = BT CT ; so for a minimum

∂J ∂b

= 2(b − X¯ ) + 2AZ¯

=0

and

∂J ∂A

=

2A(PZ

+

Z¯Z¯T ) −

2PXZ

+ 2(b −

X¯ )Z¯T

=

0

(Recall PZTX = PXZ .) From Equation 1.50a

b = X¯ − AZ¯

(1.50a) (1.50b)

and substituting this into Equation 1.50b

AP Z − PXZ = 0

or

A = PXZ PZ−1

Using this A and b in Equation 1.48 the optimal linear mean-square estimate is determined as

XˆLMS = X¯ + PXZ PZ−1(Z − Z¯)

(1.51)

This equation is typical of the form of many estimators. We can consider Z¯ as an estimate for the measurement Z. We also deﬁne the residual as

Z˜ = Z − Z¯

(1.52)

CRC 9008 C001.pdf 20/7/2007 12:35

16

Optimal and Robust Estimation

The mean X¯ is an a priori estimate for X and the second term is a correction based on the residual. If we measure exactly what we expect, so that Z = Z¯, then the a priori estimate X¯ is correct and it does not need to be modiﬁed; in that case Xˆ = X¯ . The weighting PXZ PZ−1 by which the measurement error (Z − Z¯) is incorporated into the estimate Xˆ is dependent on the second-order joint statistics. Thus, if PXZ is smaller, then X and Z depend on each other to a lesser degree and measurement of Z yields less information about X. In fact, if X and Z are independent, then PXZ = 0 and Z yields no information about X. In this case Xˆ = X¯ for any value of Z, as given by Equation 1.4. Or again, if PZ is very large, then we have little conﬁdence in Z and so (Z − Z¯) is weighted less heavily into Xˆ . If we have great conﬁdence in Z, then PZ is small and the residual has a greater role in determining Xˆ .
Compare Equation 1.51 to the general mean-square estimate XˆMS given by Equation 1.10. To ﬁnd XˆMS, all the joint statistics of X and Z are required since the PDF fZ/X (or equivalently fXZ and fZ ) is needed. To ﬁnd the best linear estimate XˆLMS, however, all that is required is the ﬁrst-order statistics (i.e., means) and second-order statistics (i.e., covariances).
To ﬁnd the error covariance associated with XˆLMS, ﬁrst verify that XˆLMS is unbiased by writing
X¯ˆLMS = X¯ + PXZ PZ−1(Z − Z¯) = X¯ + PXZ PZ−1(Z¯ − Z¯) = X¯

Therefore, X¯˜ = 0 and

PX˜ = (X˜ − X¯˜ )(X˜ − X¯˜ )T = X˜ X˜ T = (X − Xˆ )(X − Xˆ )T = [(X − X¯ ) − PXZ PZ−1(Z − Z¯)][(X − X¯ ) − PXZ PZ−1(Z − Z¯)]T = PX − PXZ PZ−1PZX − PXZ PZ−1PZX + PXZ PZ−1PZ PZ−1PZX
or

PX˜ = PX − PXZ PZ−1PZX

(1.53)

In this expression, PX represents the a priori error covariance and the second term represents the reduction in uncertainty because of the measurement. We see again that if X and Z are independent then PXZ = 0 and PX˜ = PX , therefore measuring Z provides no increase in our knowledge of X.
A very important point will now be made. Equations 1.51 and 1.53 are
identical on the right-hand sides to Equations 1.26 and 1.27. This means
that in the Gaussian case, the optimal mean-square estimate is linear, since XˆMS = X/Z and XˆLMS are the same.

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

17

The similarity of Equations 1.51 and 1.53 to Equations 1.26 and 1.27 implies another very important result which we shall need later. The expressions derived in Example 1.3 are the optimal linear mean-square estimate and error covariance for the case of linear measurements even if the measurement noise is not Gaussian.

EXERCISE 1.2 The Orthogonality Principle with Linear Estimates
If we restrict our attention to linear estimates, we have a particularly nice geometric formulation of the orthogonality principle, for we can talk in terms of vector spaces.
a. The linear mean-square estimate does not satisfy g(Z)(X − XˆLMS)T = 0 for all function g(·). Show that it does satisfy the orthogonality condition

Z(X − XˆLMS)T = 0

(1.54)

Thus, (X − XˆLMS) is orthogonal to all linear functions of Z of the form g(Z) = AZ for constant A. Since XˆLMS is unbiased show that, (X − XˆLMS) is orthogonal to all linear functions g(Z) = AZ + b for constants A and b. Show this.
b. Show that the set of all linear functions g(Z) = AZ is a vector space, with an inner product of two elements deﬁned as Z1T Z2 = trace Z2Z1T .
c. In terms of these constructions we can now use Figure 1.2, where the axis labeled “Z” represents the subspace of linear functions of Z. This should be contrasted to the situation in the previous subsection, where the orthogonality principle was ﬁrst discussed. There, we did not restrict g(·) to be linear, so we could not speak in terms of subspaces.
d. Show that if g(·) is any linear function, then

E X − XˆLMS ≤ E X − g(Z)

(1.55)

with equality only if g(Z) = XˆLMS. Thus, XˆLMS provides a “closer approximation” in a probabilistic sense to the unknown X than any other linear function g(Z). Does Equation 1.55 hold in general for nonlinear g(·)?
e. The orthogonality principle can be used to derive XˆLMS. Suppose we want to derive an optimal estimate of the form Xˆ = AZ + b.

First, assume that A is given. Then the best value for b is the optimal
estimate for the RV (X − AZ ) by a constant. Write it down. Now, to ﬁnd A, show that, assuming Xˆ is unbiased, the optimal A satisﬁes

[(X − X¯ ) − A(Z − Z¯)](Z − Z¯)T = 0

(1.56)

Hence, ﬁnd A. Is Xˆ unbiased?

CRC 9008 C001.pdf 20/7/2007 12:35

18

Optimal and Robust Estimation

Example 1.4 Linear Mean-Square Estimation

Find the optimal linear mean-square estimate for Example 1.1. To apply Equation 1.51 we need to compute the ﬁrst and second moments
of X and Z. Some of the intermediate steps will be left out

X¯ =

∞ −∞

xf

X (x)dx

=

1 2

fXZ (x, z) = fZ/X (z/x)fX (x)

=

1 x

e−z

;

e−z ≤ x ≤ 1;

0≤z

0,

otherwise

∞

fZ (z) =

fXZ (x, z)dx

−∞

ze−z; z ≥ 0

=

0;

z<0

∞

Z¯ =

zf Z (z)dz = 2

−∞

∞∞

XZ =

xzf XZ (x, z)dx dz

−∞ −∞

= ∞ 1 ze−zdx dz = 3

0

e−z

4

PXZ

=

XZ

− XZ

=

3 4

−1

=

−1 4

∞

Z2 =

z2fZ (z)dz = 6

−∞

PZ = Z2 − Z¯2 = 6 − 4 = 2

and ﬁnally, using Equation 1.51,

XˆLMS = X¯ + PXZ PZ−1(Z − Z¯) = 1 − 1 1 (Z − 2) 2 42 = 3 − 1Z 48

Note

that

XˆLMS

is

unbiased

since

X¯ˆLMS

=

3 4

−

1 8

Z¯

=

1 2

.

To compare this to XˆMS found previously, examine Figure 1.1. Note that

XˆLMS cannot generally be obtained simply by linearizing XˆMS.

This example has not presented linear mean-square estimation under the

most ﬂattering of circumstances. The computation of the moments is tedious, and ﬁnding XˆLMS is more work in this case than is ﬁnding XˆMS. However, it is
not always possible to ﬁnd the conditional mean, but given the ﬁrst and second

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

19

moments, XˆLMS can always be found. Besides this, the means and variances can often be determined from the measurement process without knowing the joint PDF fXZ itself. Linear mean-square estimation comes into its own as we proceed in our development.

Example 1.5 Quadratic Mean-Square Estimation
It is desired to ﬁnd the optimal estimate Xˆ that depends on Z quadratically, that is,

Xˆ (Z) = CZ 2 + AZ + b

(1.57)

where, for simplicity, it is supposed that X and Z are scalars. The mean-square criterion becomes
J = (X − Xˆ )T (X − Xˆ ) = (X − CZ 2 − AZ − b)2

and to ﬁnd the optimal C, A, b, write

∂J ∂b

= −2(X − CZ 2 − AZ

− b) = 0

∂J ∂A

=

−2Z (X

−

CZ 2

−

AZ

−

b)

=

0

∂J ∂C

=

−2Z 2 (X

− CZ 2 − AZ

− b)

=

0

which can be collected into the “normal equations”

 1

Z¯ Z2  b   X¯ 

 Z¯ Z2 Z3 A =  XZ 

Z2 Z3 Z4 C

XZ2

(1.58)

Several things are interesting about these equations. First, clearly if C = 0 they provide us with an alternative derivation of Equation 1.51. Second, we have seen that the optimal linear mean-square estimate depends on the joint statistics of X and Z through order 2. It is now apparent that the quadratic mean-square estimate depends on joint statistics through order 4.

1.2 Maximum-Likelihood Estimation
1.2.1 Nonlinear Maximum-Likelihood Estimation
The connotation of “maximum likelihood” is a setting in which nothing is known a priori about the unknown quantity, but there is prior information on the measurement process itself. Thus, X is deterministic and Z is stochastic.

CRC 9008 C001.pdf 20/7/2007 12:35

20

Optimal and Robust Estimation

The conditional PDF of Z given the unknown X, fZ/X (z/X), contains information about X, and if it can be computed, then X may be estimated ac-
cording to the maximum-likelihood estimation criterion, which can be stated as follows: Given a measurement Z, the maximum-likelihood estimate XˆML is the value of X which maximizes fZ/X , the likelihood that X resulted in the observed Z.
The PDF fZ/X is a likelihood function. Since ln(·) is a monotonically increasing function, the estimate Xˆ could equally well be chosen to maximize ln(fZ/X ), which is often more convenient. Thus, ln(fZ/X ) is also a likelihood function.
If the ﬁrst derivatives are continuous and fZ/X (z/X) has its maximum interior to its domain as a function of X, then the maximum-likelihood criterion
can be expressed as

∂fZ/X (z/X) ∂X

X =Xˆ

=

0

(1.59a)

or

∂ ln fZ/X (z/X) ∂X

X =Xˆ

=

0

(1.59b)

This is called the likelihood equation. Note that since X is deterministic it makes no sense here to talk about fX/Z .

Example 1.6 Nonlinear Maximum-Likelihood Estimation
Let measurement Z and unknown X be related as in Example 1.1b, where X is now deterministic (i.e., no a priori statistics are given on X).
The likelihood function is given by Equation 1.19 in Example 1.1 (where lowercase x’s should be replaced by uppercase since X is deterministic). To maximize the likelihood function, select X as small as possible; so the maximum-likelihood estimate is
XˆML = e−Z
See Melsa and Cohn (1978). Note that the estimate is given by XˆML = e−Z , while the value of the
estimate given the data Z = z is XˆML = e−z. See Figure 1.1.

1.2.2 Linear Gaussian Measurements

For our applications, assume now that Z depends linearly on X according to

Z = HX + V

(1.60)

Assume the measurement noise is normal so that

fV (v) =

1

e−(1/2)vT R−1v

(2π)p|R|

(1.61)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

21

Then the likelihood function is given by a shifted version of fV (v),

fZ/X (z/X) = fV (z − HX )

=

1

e−(1/2)(z−HX )T R−1(z−HX )

(2π)p|R|

(1.62)

To maximize fZ/X , we can equivalently maximize ln(fZ/X ), or minimize

J = 1 (z − HX )T R−1(z − HX ) 2

(1.63)

Diﬀerentiating,

∂J ∂X

=

HT R−1(z

− HX )

=

0

so that the optimal estimate is given by

XˆML = (HT R−1H)−1HT R−1Z

(1.64)

Evidently, to obtain the maximum-likelihood estimate, the measurement
matrix H must have full rank.
Note that the mean-square performance index (Equation 1.2) depends on the estimation error, while Equation 1.63 depends on the residual, since Zˆ = HXˆ is an estimate for Z so that the residual is

Z˜ = Z − HXˆ

(1.65)

To ﬁnd the error covariance, ﬁrst note that the estimation error is

X˜ = X − Xˆ = X − (HT − R−1H)−1HT R−1(HX + V ) = −(HT R−1H)−1HT R−1V

(1.66)

(Of course we cannot actually compute X˜ by this equation since noise V is unknown!) Now, taking expected values

X¯˜ = −(HT R−1H)−1HT R−1V¯ = 0

(1.67)

so that the maximum-likelihood estimate for the linear Gaussian case is unbiased.
Now the error covariance becomes
PX˜ = (X˜ − X¯˜ )(X˜ − X¯˜ )T = X˜ X˜ T = (HT R−1H)−1HT R−1V V T R−1H(HT R−1H)−1

or PX˜ = (HT R−1H)−1
In terms of PX˜ the optimal estimate can be expressed as XˆML = PX˜ HT R−1Z

(1.68) (1.69)

CRC 9008 C001.pdf 20/7/2007 12:35

22

Optimal and Robust Estimation

Note that if H = I then the error covariance PX˜ is equal to R, the noise covariance, which makes sense intuitively.
An estimator will be taken to mean an estimate plus its error covariance.
The estimator described by Equations 1.68 and 1.69 is called the Gauss–
Markov estimator. The absence of a priori information on X can be expressed as

X¯ = 0, PX → ∞

(1.70)

which models complete ignorance. Using these values in Equations 1.38 and 1.40 we obtain Equations 1.68 and 1.69. The maximum-likelihood estimate is, therefore, in the linear Gaussian case, a limiting case of the mean-square estimate.
The error covariance PX˜ is independent of the measurement Z, so that once we have designed an experiment described by Z = HX + V, PX˜ can be computed oﬀ-line to see if the accuracy of the experiment is acceptable before we build the equipment and take any measurements. This is an important result.

Example 1.7 Experiment Design

To estimate a deterministic voltage X, there are two alternatives for designing an experiment. Two expensive meters can be used, each of which add N (0, 2) Gaussian noise to X, or four inexpensive meters can be used, each of which add N (0, 3.5) Gaussian noise to X. Which design would result in the more reliable estimate for X?
Design number one is described by

Z=

1 1

X +V

where

V ∼ N 0,

20 02

This gives an error covariance of

PX˜ = (HT R−1H)−1 =

11

1
2
0

0
1 2

1 1

−1
=1

Design number two is described by  1
Z = 11 X + V,
1

V ∼ N (0, 3.5I)

where I is the identity matrix, so

PX˜

=

(HT R−1H)−1

=

7 8

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

23

Design number two using four inexpensive meters is therefore a more reliable scheme since it has a lower error covariance.
The noise covariance matrix R must be determined before maximumlikelihood methods can be used in the linear Gaussian case. The eﬀects of inaccurately known noise covariance are explored in the problems. The accurate determination of the noise parameters is a crucial part of the estimation process.
The next examples provide further insight into maximum-likelihood estimation.

Example 1.8 Independent Measurement of a Scalar Unknown

If the measurements are independent, then R is diagonal. Suppose an experi-

ment is described by three scalar measurements (one vector measurement)

 1
Z = 1 X + V,
1

 σ12

 0

V ∼ N 0,  σ22 

0

σ32

with X deterministic. Then the error covariance is given by

PX˜ = (HT R−1H)−1 =

+ + 1

1

1 −1

σ12

σ22

σ32

(1.71)

and the estimate is

XˆML = PX˜ HT R−1Z

=

(Z1/σ12 + Z2/σ22 + Z3/σ32) (1/σ12 + 1/σ22 + 1/σ32)

(1.72)

where Z = [Z1 Z2 Z3]T is the measurement vector, with Zi being scalar
random variables. This is a very natural and appealing result, as can be seen
by noticing a few points. First, suppose that σ32 → ∞, indicating that we have no conﬁdence in Z3.
Then, 1/σ32 → 0 and the eﬀects of Z3 simply drop out, leaving a twomeasurement experiment. Now, suppose that σ32 → 0, indicating that we have great conﬁdence in Z3. Then PX˜ → σ32 and XˆML → Z3, so that the maximumlikelihood procedure would say to disregard Z1 and Z2 in this situation. What we have here, then, is an estimator that weights Zi into XˆML, depending on our measure of conﬁdence σi2 in Zi.
The generalization to an arbitrary number of measurements Zi is immediate. If all σi are equal to σ, then Equation 1.72 becomes

XˆML

=

Z1

+

Z2 3

+

Z3

the sample mean, and Equation 1.71 becomes

PX˜

=

σ2 3

CRC 9008 C001.pdf 20/7/2007 12:35

24

Optimal and Robust Estimation

V2

I2
R2 R1 V1

R in=∞ +

I1

I=0

R3

I3

V R

− V3

FIGURE 1.3 An electric circuit analog to maximum-likelihood estimation.

Example 1.9 An Electric Circuit Analog

Examine Figure 1.3. Let voltages Vi be applied between one end of resistance Ri and ground. The other ends of the resistances are connected to a common point at potential V . Find the voltage V if Rin = ∞.
This problem can be solved by using Kirchhoﬀ’s current law I1 +I2 +I3 = I, where Rin = ∞ means that I = 0. Now apply Ohm’s law to each Ii to obtain

I1

+

I2

+

I3

=

V1 − V R1

+

V2 − R2

V

+

V3 − R3

V

=0

or

V1 R1

+

V2 R2

+

V3 R3

=

V

111 R1 + R2 + R3

The solution is V=

1

1

1 −1

R1 + R2 + R3

V1 R1

+

V2 R2

+

V3 R3

(1.73)

which corresponds exactly to Equation 1.72! The “eﬀective resistance seen by V ” is

R=

1

1

1 −1

R1 + R2 + R3

(1.74)

This corresponds to the error covariance PX˜ in Equation 1.71. In electric circuit theory, R is written as the parallel combination of Ri, that is, R = R1//R2//R3, which is just a shorthand notation for “the reciprocal of

the sums of the reciprocals.” With this notation, Equation 1.71 can be writ-

ten as

PX˜ = σ12//σ22//σ32

(1.75)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

25

so that the error covariance is just the parallel combination of the individual noise variances. As such, it is smaller than any individual σi2, so our conﬁdence in Xˆ after the experiment is greater than our conﬁdence in any one measurement.
Equation 1.73 is called Millman’s theorem and it is an exact analog to maximum-likelihood estimation of a scalar unknown with independent measurements.

1.3 The Cram´er–Rao Bound

To examine the performance of the maximum-likelihood estimate, we can use the following lower bound on the variance of any unbiased estimate. If Xˆ is
any unbiased estimate of a deterministic variable X based on measurement Z, then the covariance of the estimation error X˜ = X − Xˆ is bounded by

PX˜ ≥ JF−1

(1.76)

where the Fisher information matrix is given by

JF = e

∂ ln fZ/X (z/X) ∂X

∂ ln fZ/X (z/X) T ∂X

(1.77)

= −E

∂2 ln fZ/X (z/X) ∂X2

(1.78)

Equality holds in Equation 1.76 if and only if

∂ ln fZ/X (z/X) = k(X)(X − Xˆ ) ∂X

(1.79)

It is assumed that ∂fZ/X /∂X and ∂2fZ/X /∂X2 exist and are absolutely integrable.

Equation 1.79 is a limitation on the form of the derivative: it must be
proportional to X˜ with constant of proportionality at most a function of the unknown X. An estimate Xˆ is eﬃcient if it satisﬁes the Cram´er–Rao

bound (Equation 1.76) with equality, that is, if Equation 1.79 holds. Proof

for the Cram´er–Rao bound can be found in Van Trees (1968) or Sorenson

(1970a,b). In general, XˆML is not necessarily eﬃcient. However, it is not hard to show
that if Equation 1.79 is ever satisﬁed so that there exists eﬃcient estimate, then this estimate is given by XˆML. The Cram´er–Rao bound thus demon-
strates the importance of the maximum-likelihood estimate.

To gain some insight into the Fisher information matrix, consider the linear

Gaussian measurement model. From Equation 1.62 we obtain

∂

ln fZ/X (z/X) ∂X

=

HT R−1(z

−

HX )

(1.80)

CRC 9008 C001.pdf 20/7/2007 12:35

26

Optimal and Robust Estimation

and diﬀerentiating again we ﬁnd

JF = HT R−1H = PX−˜ 1

(1.81)

the inverse of the error covariance matrix. According to Equation 1.71, we have for Example 1.8 that JF = PX−˜ 1 =
1/σ12 + 1/σ22 + 1/σ32, which can be phrased in the intuitively appealing statement that in the scalar case of independent measurements, the information JF at the end of an experiment is the sum of the information provided by each measurement. An electrical analog is provided by the conductance 1/R,
which adds according to Example 1.9 Equation 1.74 when resistances are put
in parallel.
The maximum-likelihood estimate has several nice properties as the number of independent measurements N goes to inﬁnity. To discuss them we need the concept of stochastic convergence. The sequence Y1, Y2, . . . of RVs converges in probability to RV Y if for all ε > 0

P ( YN − Y > ε) → 0 for N → ∞

(1.82)

Under fairly general conditions, the maximum-likelihood estimate XˆML has the following limiting properties:

1. XˆML converges in probability to the correct value of X as N → ∞. (Any estimate with this property is said to be consistent.)
2. XˆML becomes eﬃcient as N → ∞. 3. As N → ∞, XˆML becomes Gaussian N (X, PX˜ ).

For a stochastic unknown X, a result similar to the Cram´er–Rao bound can
be stated as follows: If Xˆ is any estimate of a stochastic variable X based on measurement Z,
then the covariance of the estimation error X˜ = X − Xˆ is bounded by

PX˜ ≥ L−1 where the information matrix L is given by

(1.83)

L=E

∂ ln fXZ (x, z) ∂x

∂ ln fXZ (x, z) T ∂x

= −E

∂2 ln fXZ (x, z) ∂x2

(1.84) (1.85)

Equality holds in Equation 1.83 if and only if

∂

ln fXZ (x, z) ∂x

=

k(x

−

Xˆ )

(1.86)

where k is a constant. It is assumed that ∂fXZ /∂x and ∂2fXZ /∂x2 exist and are absolutely integrable with respect to both variables, and that

lim
x→∞

B(x)fX

(x)

=

0

(1.87a)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

27

and where

lim
x→−∞

B(x)fX

(x)

=

0

B(x)

∞
(x − Xˆ )fZ/X (z/x)dz
−∞

(1.87b) (1.88)

An estimate Xˆ for which the bound is satisﬁed with equality (i.e., condition Equation 1.86 on the form of fXZ holds) is said to be eﬃcient. The proof is similar to that of the Cram´er–Rao bound, but expectations are now taken with respect to Z and X (Van Trees 1968; Sorenson 1970a,b). Several points should be noted.

1. This bound depends on the joint PDF fXZ while the Cram´er–Rao bound (for deterministic unknown X) depends on the likelihood function fZ/X .
2. The Cram´er–Rao bound deals with unbiased estimates Xˆ ; this bound substitutes requirements 1.87a or 1.87b for unbiasedness of Xˆ .
3. The eﬃciency condition (Equation 1.86) in terms of the constant k should be contrasted with the eﬃciency condition of the Cram´er–Rao bound in terms of the function k(X) of X.

4. Diﬀerentiating Equation 1.86 yields the equivalent condition for eﬃciency

∂2

ln fXZ (x, z) ∂x2

=

k

or (since fZ (z) is independent of X)

∂2

ln fX/Z (x, z) ∂x2

=

k

Integrating twice and taking the antilog yields

(1.89) (1.90)

fX/Z (x/z) = e−(k/2)xT x+kXˆ T x+c

(1.91)

so that the a posteriori PDF must be Gaussian for an eﬃcient estimate to
exist. It is interesting to relate information matrix L to the Fisher information
matrix JF . Since fXZ = fZ/X fX , it follows that

L = JF − E

∂2 ln fX (x) ∂x2

(1.92)

so that the stochastic description of X enters independent of the noise statistics.
We can gain a little more insight by considering the linear Gaussian measurement model. Thus, let measurement Z and unknown X be related by Equation 1.60 with V = N (0, R). Then, by Equation 1.81 we know that JF = HT R−1H. To ﬁnd L, note that

fX (x) =

1

e−(1/2)(x−X¯ )T PX−1(x−X¯ )

(2π)n|PX |

(1.93)

CRC 9008 C001.pdf 20/7/2007 12:35

28

Optimal and Robust Estimation

so that

∂ ln fX (x) ∂x

=

−PX−1(x

− X¯ )

(1.94)

Then, by Equation 1.92

L = HT R−1H + PX−1

(1.95)

The total information available has been increased by a priori information on X. See Equation 1.38.

1.4 Recursive Estimation
1.4.1 Sequential Processing of Measurements
Up to this point, we have considered batch processing of the measurements in which all information is incorporated in one single step into the estimate. This leads to the necessity for inverting p × p matrices, where p is the number of measurements. The next examples demonstrate a more sensible approach when p is large.

Example 1.10 Recursive Computation of Sample Mean and Variance Given k measurements, Zi the sample mean is

Xˆk

=

1 k

k

Zi

i=1

(1.96)

Suppose that Xˆk has been computed based on measurements Zi for i =

1, . . . , k. Now, one more measurement Zk+1 is made. The new sample mean

could be computed as

Xˆk+1

=

k

1 +1

k+1
Zi
i=1

(1.97)

but this clearly involves duplication of some of the work we have already done. To avoid this, Xˆk+1 can be computed in terms of Xˆk and Zk+1 by proceeding
as follows:

Xˆk+1

=

k

1 +

1

1 k

k

Zi

i=1

+

k

1 +

1 Zk+1

=

k

1 +

1 Xˆk

+

k

1 +

1 Zk+1

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

29

Zk +1 −

1 k + 1

Xˆk +1

Xˆk

Z −1

FIGURE 1.4 Linear system for recursive generation of sample mean.

To make this look like some of our previous results, write it as

Xˆk+1

=

Xˆk

+

k

1 +

1 (Zk+1

−

Xˆ k )

(1.98)

This recursive equation can be represented as a time-varying linear system, as in Figure 1.4. The quantity Z˜k+1 = Zk+1 − Xˆk can be interpreted as a residual. If the Zi are measurements of a constant unknown X, then Xˆk is the estimate of X based on the ﬁrst k measurements and Equation 1.98 is the estimate update equation that adds in the eﬀects of the next measurement Zk+1. Compare this equation to Equation 1.40. Conceptually, the two are identical;
they both express the current estimate as a linear combination of the previous estimate and the residual. Note that as k increases, the measurements Zk+1 have less and less eﬀect as more weighting is given to Xˆk.
A recursive method can also be found to compute the sample variance. To wit, after k measurement we have

σk2

=

1 k

k
(Zi − Xˆk)2

i=1

Introducing one more measurement Zk+1, there results

σk2+1

=

k

1 +

1

k+1
(Zi

−

Xˆk+1)2

i=1

=

k

1 +

1

k+1
[(Zi

−

Xˆ k )

+

(Xˆk

−

Xˆk+1)]2

i=1

After some manipulation, we achieve

(1.99)

σk2+1

=

k

k +

1

σk2

+

k

1 +

1 (Zk+1

−

Xˆ k )2

− (Xˆk+1

− Xˆk)2

(1.100)

Now, using Equation 1.98 write

(Xˆk+1

− Xˆk)2

=

(k

1 +

1)2

(Zk+1

−

Xˆ k )2

CRC 9008 C001.pdf 20/7/2007 12:35

30

Optimal and Robust Estimation

so that

σk2+1

=

k

k +

1 σk2

+

(k

k +

1)2

(Zk+1

−

Xˆ k )2

σk2+1

=

σk2

+

k

1 +

1

k

k +

1

(Zk+1

−

Xˆ k )2

−

σk2

(1.101)

Schemes such as the above in which the current estimate depends on the previous estimate and the current measurement are called recursive or sequential estimation schemes. Such schemes rely on sequential processing of data, where the measurements are processed in stages.

Example 1.11
Suppose there are four measurements of a scalar unknown X given by Z1 = 10, Z2 = 12, Z3 = 11.5, and Z4 = 11. Find the sample mean and variance recursively.
Using Equation 1.98 several times with Xˆ0 = 0,

Xˆ1 = Xˆ0 + (Z1 − Xˆ0) = Z1 = 10

Xˆ2

=

10

+

1 (12
2

−

10)

=

11

Xˆ3

=

11

+

1 (11.5 3

−

11)

=

1 11
6

Xˆ4

=

1 11
6

+

1 4

11 − 11 1 6

1 = 11
8

Thus, after each such measurement Zk, Xˆk provides an up-to-date estimate of X. These estimates are up to date in the maximum-likelihood sense, since
there is no a priori information about X. To ﬁnd σk2 recursively, let σ02 = 0. Then, applying Equation 1.101,

σ12 = 0

σ22

=

1 4

(Z2

−

Xˆ1)2

=

1

σ32

=

1

+

1 3

2 (11.5 − 11)2 − 1 3

= 0.722

σ42

=

0.722

+

1 4

3 4

11 − 11 1 6

2
− 0.722

= 0.547

The sample variance σk2 yields a measure of conﬁdence in Xˆk after each measurement Zk. Sequence σk2 is decreasing, indicating that each measurement increases the reliability of Xˆk.

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

31

1.4.2 Sequential Maximum-Likelihood Estimation

Let us generalize our results by deriving a sequential maximum-likelihood

estimator. Let there be given a linear Gaussian measurement Z of an unknown

constant X:

Z = HX + V

(1.102)

where X ∈ Rn, Z ∈ Rp, V ∈ Rp, V ∼ N (0, R), and |R| = 0. Recall that

“maximum likelihood” implies ignorance of any statistics of X.

Write Equation 1.102 explicitly in terms of the components of vectors as

 z1
z...2

=

 hh...T1T2

 X

+

 v1
v...2

zp

hTp

vp

where hTi is the ith row of H and suppose that R = diag{σi2}. This assumption says that the measurements are independent and is suﬃcient to guarantee
that a recursive estimation procedure exists. (Note that in this subsection,

lowercase letters are used to deﬁne components of RV, which are themselves

random variables.) Deﬁne

 z1
Zk = z...2 , zk

 Hk = hh...T1T2  ,
hTk

 v1
Vk = v...2 vk

as the ﬁrst k components of Z, H, and V , respectively. Deﬁne Rk = diag{σ12, σ22, . . . , σk2}.
Considering the ﬁrst k components of Z, we have

Zk = HkX + Vk, Vk ∼ N (0, Rk)

Based on this information, the error covariance Pk is given by Equation

1.68, or

Pk = (HkT Rk−1Hk)−1

(1.103)

and the estimate Xˆk is given by Equation 1.69, or

Xˆk = PkHkT Rk−1Zk

(1.104)

Now, it is necessary to include the eﬀects of measurement component Zk+1 to update Xˆk to Xˆk+1. To do this, write

Zk+1 = Hk+1X + Vk+1,

Vk+1 ∼ N (0, Rk+1)

CRC 9008 C001.pdf 20/7/2007 12:35

32

Optimal and Robust Estimation

or

Zk+1 =

Hk hTk+1

X + Vk+1,

Vk+1 ∼ N

0,

Rk 0

0 σk2+1

The error covariance Pk+1 based on the ﬁrst k + 1 measurement components is

Pk+1 = (HkT+1Rk−+11Hk+1)−1

=

Hk T Rk

0 −1 Hk

−1

hTk+1

0 σk2+1

hTk+1

=

HkT Rk−1Hk

+

hk+1hTk+1 σk2+1

−1

or

Pk+1 =

Pk−1

+

hk+1hTk+1 σk2+1

−1

(1.105)

This is the error covariance update equation. To ﬁnd an alternative form for Equation 1.105, use the matrix-inversion lemma to write

Pk+1 = Pk − Pkhk+1(hTk+1Pkhk+1 + σk2+1)−1hTk+1Pk

(1.106)

To ﬁnd the estimate update equation, we have

Xˆk+1 = Pk+1HkT+1Rk−+11Zk+1

= Pk+1

Hk hTk+1

T

Rk 0

0 −1 Zk

σk2+1

zk+1

= Pk+1

HkT Rk−1Zk

+

hk+1 σk2+1

zk+1

By Equation 1.104, this becomes

Xˆk+1 = Pk+1

Pk−1Xˆk

+

hk+1 σk2+1

zk+1

(1.107)

This is the estimate update. To ﬁnd a more familiar form for this update, note that

Pk+1Pk−1 = Pk+1

Pk−+11

−

hk+1hTk+1 σk2+1

=

I

−

Pk+1

hk+1hTk+1 σk2+1

hence Equation 1.107 is equivalent to

Xˆk+1

=

Xˆk

+

Pk+1

hk+1 σk2+1

zk+1 − hTk+1Xˆk

Note that z˜k+1 = zk+1 − hTk+1Xˆk is the residual.

(1.108)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

Zk +1

~ Zk +1

Pk +1hk + 1
2 k+1

Xˆk+1

Z −1 Xˆk

33
Xˆk+1

hkT+1
FIGURE 1.5 Linear system for sequential maximum-likelihood estimation.

The recursions (Equations 1.105 and 1.108) should be started using P0−1 = 0, Xˆ0 = 0, which models complete ignorance of the a priori statistics of X.
To summarize our results then, if the measurements are independent, the
maximum-likelihood (batch) estimator (Equations 1.68 and 1.69) is equivalent
to the sequential maximum-likelihood estimator given by Equations 1.106
and 1.108, which requires no matrix inversions but only scalar divisions. The
sequential estimator can be implemented as in Figure 1.5. Note that owing
to the unity feedback, it has the structure of a summer or integrator. It is a time-varying system where σk2+1 and hk+1 are given directly by the structure of the measurement process and the Pk+1 are computed oﬀ-line (i.e., before we begin to take measurements) from the knowledge of the measurement process.
Compare with Figure 1.4. If a priori statistics X¯ and PX on X are available, then we should use
Xˆ0 = X¯ , P0 = PX to initialize Equations 1.106 and 1.108, which then amount to a recursive mean-square estimator.

1.4.3 Prewhitening of Data

Now, suppose that the assumptions required to derive Equations 1.105 and
1.108 all hold except that R is nonsingular but arbitrary. For sequential processing of Z, one component at a time, we require a diagonal R, that is, the
measurement components must be independent. The data can be preﬁltered to make R diagonal as follows. Let S be any square root of R−1 so that R−1 = ST S. Now premultiply Equation 1.102 by S to obtain

SZ = SHX + SV

and deﬁne new quantities by

Zω = SZ Hω = SH V ω = SV

(1.109)

CRC 9008 C001.pdf 20/7/2007 12:35

34

Optimal and Robust Estimation

so that

Zω = HωX = V ω

The covariance of the preﬁltered noise V ω is

(1.110)

Rω = SV (SV )T = SVV T ST = SRS T = S(ST S)−1ST = I

so that the components of V ω are independent and V ω is a white noise vector.
Hence the superscripts. To implement Equation 1.109, we deﬁne a new measurement matrix Hω and process all our measurements Z through a linear prewhitening ﬁlter S. We can now proceed as above to derive Equations 1.105 and 1.108 using Zω, Hω, and Rω = I instead of Z, H, and R.

1.5 Wiener Filtering
Wiener developed the ideas we are about to discuss in the early 1940s for application to antiaircraft ﬁre control systems. His monograph in 1949 brought stochastic notions into control and communication theory, and was the basis for a great deal of subsequent work. Wiener dealt with the continuous-time case; Kolmogorov (1939) applied mean-square theory to discrete stochastic processes. See Van Trees (1968) and the excellent survey by Kailath (1974).
Recall (Papoulis 1965) that if x(t) is a stationary stochastic process with autocorrelation function

RX (τ ) = x(t + τ )xT (t)

then its spectral density ΦX (ω) is

∞

ΦX (s) =

RX (t)e−st dt

−∞

evaluated at s = jω. In the discrete case, we have

(1.111) (1.112)

RX (k) = xi+kxTi

and the spectral density is

∞

ΦX (z) =

RX (k)z−k

k=−∞

evaluated at z = ejω.

(1.113) (1.114)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

35

If u(t) and y(t) are the input and output of a linear system with impulse response h(t) and transfer function H(s), then the autocorrelation functions and spectral densities are related by

RYU (t) = h(t) ∗ RU (t) RY (t) = h(t) ∗ RU (t) ∗ hT (−t)
ΦYU (s) = H(s)ΦU (s) ΦY (s) = H(s)ΦU (s)HT (−s)

(1.115) (1.116) (1.117) (1.118)

where * represents convolution. In the discrete counterparts, −s is replaced by z−1.
We shall have occasion to use the spectral factorization theorem, which can be stated as follows (˚Astr¨om 1970; Anderson and Moore 1979):
Let |ΦY (s)| = 0 for almost every s (such a process is said to be of full rank). Then, ΦY (s) can be factored uniquely as

ΦY (s) = H(s)ΩHT (−s)

(1.119)

where H(s) is a square, real, rational transfer function with all poles in the open left-half plane; lims→∞ H(s) = I; H−1(s) has all poles in the left-half plane or on the jω axis; and Ω = ΩT > 0 is a real matrix.
If |ΦY (s)| = 0 for all s satisfying Re(s) = 0, then H−1(s) has all poles in the open left-half plane.
For a square multivariable transfer function H(s) as described above, the zeros of H(s) are the poles of H−1(s). A transfer function with all poles
and zeros stable is said to be of minimum phase. A similar result holds for discrete systems, with −s replaced by z−1 and using the discrete version of
stability (i.e., with respect to the unit circle). According to Equation 1.118
and the spectral factorization theorem, the process y(t) can be regarded as the output of the stable system H(s) when the input is white noise with spectral
density Ω. This is the representation theorem.

Example 1.12 Discrete Spectral Factorization The rational (in cos ω) spectral density

Φ(ω)

=

1.25 + cos ω 1.0625 + 0.5 cos ω

=

(ejω + 0.5)(e−jω (ejω + 0.25)(e−jω

+ 0.5) + 0.25)

can be factored in four ways:

H1(z)

=

z + 0.5 z + 0.25

H2(z)

=

z + 0.5 1 + 0.25z

CRC 9008 C001.pdf 20/7/2007 12:35

36

Optimal and Robust Estimation

H3(z)

=

1 z

+ +

0.5z 0.25

H4(z)

=

1 + 0.5z 1 + 0.25z

Only H1(z) has no poles and zeros outside the unit circle. Thus, if the stable system H1(z) has a white noise input with unit spectral density, the output will have spectral density given by Φ(ω).

Example 1.13 Continuous Spectral Factorization The rational function of ω

Φ(ω)

=

ω2 ω2

+4 +1

=

(jω (jω

+ +

2)(−jω 1)(−jω

+ +

2) 1)

can be factored in four ways, one of which has

H (s)

=

s s

+ +

2 1

the minimum phase. The system with this transfer function will have output with spectral density Φ(ω) when driven by continuous-time white noise with unit spectral density.

1.5.1 The Linear Estimation Problem
Suppose that x(t) ∈ Rn is an unknown stochastic process. A related process z(t) ∈ Rp is measured, and based on this data it is desired to reconstruct x(t). We want to use a linear ﬁlter, in general time-varying, for this purpose; so suppose that the estimate for x(t) is

T
xˆ(t) = h(t, τ )z(τ )dτ
t0

(1.120)

The interval [t0, T ] is the time interval over which the data are available. It is desired to ﬁnd the optimal ﬁlter impulse response h(t, τ ), which is called the time-varying Wiener ﬁlter.
If T < t, the determination of h(t, τ ) is called the linear prediction problem; if T = t, it is the linear ﬁltering problem; and if T > t, it is called linear smoothing or interpolation. The general problem is known as linear estimation.
To ﬁnd the optimal linear after h(t, τ ), which reconstructs unknown x(t) given the data z(τ ) for t0 ≤ τ ≤ T , minimize the mean-square error at time t

J(t) = E[x˜(t)T x˜(t)] = tracePX˜ (t)

(1.121)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

37

where the estimation error is x˜(t) = x(t) − xˆ(t), fE[x˜(t)] = 0. The error covariance is

PX˜

(t)

=

[x(t) 

−

xˆ(t)][x(t)

−

xˆ(t)]T



T

= E x(t) − t0 h(t, τ )z(τ )dτ



T

T

x(t) − h(t, τ )z(τ )dτ
t0



(1.122)

According to the orthogonality principle, J(t) is minimized when x˜(t) is orthogonal to the data over the interval (t0, T ), that is,

x˜(t)z(u)T = E = 0;

T
x(t) − h(t, τ )z(τ )dτ z(u)T
t0
t0 < u < T

(1.123)

This results in the nonstationary Wiener–Hopf equation

T
x(t)z(u)T = h(t, τ )z(τ )z(u)T dτ ;
t0

t0 < u < T

In terms of correlation functions, this can be written as

(1.124)

T
RXZ (t, u) = h(t, τ )RZ (τ, u)dτ ;
t0

t0 < u < T

(1.125)

It is important to note that the optimal linear ﬁlter is found by knowing only the ﬁrst- and second-order joint statistics of x(t) and z(t).
If the statistics are Gaussian, then Equation 1.125 provides the optimal ﬁlter for reconstructing x(t) from z(t). For arbitrary statistics, it provides the optimal linear ﬁlter. (In the Gaussian case, the optimal ﬁlter is linear.)
With impulse response given by the Wiener–Hopf equation, the error covariance is equal to (use Equation 1.122 and the orthogonality of x˜(t) and z(τ ))

T
PX˜ (t) = E x(t) − h(t, τ )z(τ )dτ x(t)T
t0

or

T
PX˜ (t) = RX (t, t) − h(t, τ )RZX (τ, t)dτ
t0

(1.126)

If x(t) and z(t) are stationary and t0 = −∞ so that there is an inﬁnite amount of data, then the optimal ﬁlter turns out to be time-invariant, h(t, τ ) = h(t − τ ). In this steady-state case, Equation 1.125 becomes

T

RXZ (t − u) =

h(t − τ )RZ (τ − u)dτ ;

−∞

−∞ < u < T

CRC 9008 C001.pdf 20/7/2007 12:35

38

Optimal and Robust Estimation

or by changing variables

∞

RXZ (τ ) =

h(u)RZ (τ − u)du;

t−T

t−T <τ <∞

(1.127)

This stationary version is the equation Wiener actually worked with, and it is the one he and Hopf provided an elegant frequency domain solution for in 1931 in another context. Wiener was quite delighted in 1942 to discover that the linear estimation problem depended on the solution of this equation he had studied 11 years earlier!
In the steady-state case, the error covariance is a constant given by

PX˜ = RX (0) − = RX (0) −

∞
h(τ )RZX (−τ )dτ
t−T ∞
h(τ )RXT Z (τ )dτ
t−T

(1.128)

1.5.2 Solution of the Wiener–Hopf Equation

1.5.2.1 Inﬁnite-Delay Steady-State Smoothing

Let us begin by solving Equation 1.127 with T = ∞. This means that we must wait until all the data z(t), −∞ ≤ t ≤ ∞, are received before we begin processing and so it amounts to non-real-time data processing. It corresponds to the inﬁnite-delay smoothing problem.
In this case, the Wiener-Hopf equation 1.127 becomes

∞

RXZ (t) =

h(τ )RZ (t − τ )dτ,

−∞

−∞ < t < ∞

(1.129)

The solution is trivial, for according to Fourier transform theory we can write this convolution in terms of spectral densities as

ΦXZ (ω) = H(jω)ΦZ (ω)

(1.130)

The optimal linear ﬁlter is therefore given by

H(jω) = ΦXZ (ω)Φ−Z1(ω)

(1.131)

We can quickly ﬁnd an expression for the error covariance (Equation 1.128) in terms of spectral densities, for, if we deﬁne an auxiliary function by

∞

f (t) = RX (t) −

h(τ )RZX (t − τ )dτ

−∞

then PX˜ = f (0). But, using the convolution property of Fourier transforms:

F (ω) = ΦX (ω) − H(jω)ΦZX (ω)

(1.132)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

39

Substituting for the optimal H(jω) and taking the inverse transform, there results

PX˜

=

1 2π

∞
[ΦX (ω) − ΦXZ (ω)Φ−Z1(ω)ΦZX (ω)]dω
−∞

(1.133)

Compare this to the time-domain expression (Equation 1.53).

Example 1.14 Inﬁnite-Delay Smoothing for Unknown Markov Process
Suppose the unknown process x(t) is ﬁrst-order Markov. Then it can be modeled as being generated by the stable shaping ﬁlter

x˙ (t) = −ax (t) + w(t)

(1.134)

where a > 0 and the process noise w(t) is white, with spectral density 2aσw2 . For simplicity, we consider the scale case. In the steady-state case, x(t) has
spectral density (Equation 1.118) and autocorrelation (Equation 1.116) given
by (prove this)

ΦX (ω)

=

2aσw2 ω2 + a2

RX (t) = σw2 e−a|t|

(1.135) (1.136)

To ﬁnd ΦZ and ΦXZ , let us assume the special case of linear measurements with additive noise. Hence, let

z(t) = cx (t) + v(t)

(1.137)

where c ∈ R and the measurement noise v(t) is white with spectral density r. Suppose here that x(t) and v(t) are orthogonal. Then,

RXZ (t) = x(τ + t)z(τ ) = cRX (t)

(1.138)

and

RZ (t) = z(τ + t)z(τ ) = [cx (τ + t) + v(τ + t)][cx (τ ) + v(τ )] = c2RX (t) + rδ(t)
with δ(t) the Kronecker delta. Hence,

(1.139)

ΦXZ (ω) = cΦX (ω)

(1.140)

and

ΦZ (ω) = c2ΦX (ω) + r

(1.141)

CRC 9008 C001.pdf 20/7/2007 12:35

40

Optimal and Robust Estimation

In this linear measurement case, then the optimal linear ﬁlter (Equation 1.131) is

H (j ω)

=

ΦXZ (ω) ΦZ (ω)

=

cΦX (ω) c2ΦX (ω) + r

2acγ = ω2 + (a2 + 2ac2γ)

(1.142)

where the signal-to-noise ratio γ = σw2 /r. In this problem it is convenient to deﬁne another parameter (Van Trees 1968)

λ

=

2c2γ a

(1.143)

which can also be thought of as a signal-to-noise ratio. Note that λ > 0. Then,

H (j ω)

=

ω2

2acγ + a2(1

+

λ)

(1.144)

so the impulse response of the optimal linear ﬁlter is

h(t) = √ cγ

√
e−a 1+λ|t|

1+λ

(1.145)

This impulse response is sketched in Figure 1.6. The error variance (Equation 1.133) becomes

PX˜

=

1 2π

∞ −∞

rΦX (ω) c2ΦX (ω) +

r dω

1 = 2π

∞ −∞

ω2

+

2aσω2 (a2 + 2ac2

γ)

dω

c 1+

h(t)

0 t
FIGURE 1.6 Optimal inﬁnite-delay smoother.

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

41

√ Letting α = a 1 + λ, we get

PX˜

=

2aσw2 πα

∞ 0

ω2

α +

α2

dω

=

aσw2 /α

or

PX˜

=

√ σw2 1+

λ

(1.146)

If γ = 0, so that the measurements provide no information, then h(t) = 0

so that the estimate is xˆ(t) = 0, the expected value of x(t). The error variance

is then

PX˜ = σw2 = RX (0)

(1.147)

which is independent of the measurement noise. If γ = ∞, then the measurements are perfect. Examine Equation 1.142 to
see that in this case

h(t)

=

1 c

δ(t)

(1.148)

so that

xˆ(t)

=

1 c

z(t)

(1.149)

Then the error variance is zero. If

γ

=

a 2c2

(1.150)

(i.e., λ = 1), then the optimal inﬁnite-delay smoother is

h(t)

=

√c 2r

σw2

√
e− 2a|t|

(1.151)

with error variance

PX˜

=

√σw2 2

=

R√X (0) 2

This example points out a problem with the simple-minded approach of letting T = ∞. Although this assumption leads to an easy solution, the resulting impulse response is noncausal and so unrealizable.

1.5.2.2 Causal Steady-State Filtering
To ﬁnd a stable causal h(t) that can be used for real-time processing of the data, we present the solution of Wiener and Hopf in a modiﬁcation owing

CRC 9008 C001.pdf 20/7/2007 12:35

42

Optimal and Robust Estimation

to Bode and Shannon (1950). Accordingly, consider the steady-state Wiener– Hopf equation 1.127 with T = t, which corresponds to the ﬁltering problem,

∞
RXZ (t) = h(τ )RZ (t − τ )dτ ; 0 < t < ∞
0

(1.152)

Recall that the steady-state case means that x(t) and z(t) are stationary and a data signal z(t) with inﬁnite past history (i.e., t0 = −∞) is available.
Because of the constraint t > 0, the Fourier transform theorems can no longer be directly used to provide a simple solution h(t) to the Wiener–Hopf equation. However, if the data signal has a rational spectral density ΦZ(ω), we can proceed as follows. For simplicity, we initially consider the scalar case.
Our presentation is modiﬁed from that of Van Trees (1968). Note ﬁrst that if the data z(t) are white then Equation 1.152 has a trivial
solution, for then RZ (t) = rδ(t) and

h(t) =

1 r

RXZ

(t),

t>0

0,

t<0

(1.153)

This will not happen except in pathological cases, but we can always use spectral factorization to prewhiten the data!
To wit, we know that the rational spectral density of z(t) can always be factored as

ΦZ (ω) = W −1(jω)W −1(−jω)

(1.154)

where W −1(s) is stable with zeros in the left-half plane. The representation theorem says that z(t) can be considered as the output of the linear system W −1(s) driven by a white noise process v(t). It is clear then that the “inverse” of this theorem says that W (s) describes a stable linear system which results in a white noise output v(t) when the input is the data z(t). Thus, W (s) is a whitening ﬁlter for z(t). See Figure 1.7.
The fact that there exists a realizable whitening ﬁlter with a realizable
inverse for any process with rational spectral density is called the whitening
property. It can be considered as the “inverse” of the representation theorem. Since v(t) is white, it is uncorrelated with itself from instant to instant.
The information contained in the data z(t) is present also in v(t), but the uncorrelatedness of v(t) means that each value brings us new information, unlike the values of z(t), which are correlated with each other. The information

z (t )

v (t )

xˆ (t)′

W (s )

H ′(s)

Prewhitening filter

FIGURE 1.7 Solution of the Wiener–Hopf equation by prewhitening of the data.

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

43

in z(t) is therefore available in the whitened process v(t) in a particularly
convenient form; this process is called the innovations or new information
process. See Kailath (1981). The prewhitening of the data z(t) by spectral factorization of Φ−Z1(ω) should
be compared to the prewhitening we did in the random variable case in Section 1.4. There we factored the inverse of the noise covariance as R−1 = ST S.
There is now only one step left in our solution of Equation 1.152. From
Figure 1.7 we see that it is necessary to design an optimal ﬁlter h (t) which reconstructs x(t) from the innovations v(t). Thus, h (t) satisﬁes

∞
RXv (t) = h (τ )Rv(t − τ )dτ ;
0

0<t<∞

(1.155)

Since Rv(t) = δ(t) (note that v(t) has spectral density equal to 1), the optimal solution is given by

h (t) = RXv (t), t > 0

0,

t<0

(1.156)

It is not hard to ﬁnd ΦXv (ω) in terms of ΦXZ (ω) and W (jω) and so provide a frequency-domain solution. In fact, using a modiﬁcation of Equation 1.117,

ΦXv (ω) = ΦXZ (ω)W (−jω)

(1.157)

Since h (t) is causal, we retain only the portion of RXv (t) for t ≥ 0. This means that we must do a partial fraction expansion of ΦXv (s) and retain the realizable portion [ΦXv (s)]+, which has no poles in the open right-half plane,

ΦXv (s) = [ΦXv (s)]+ + [ΦXv (s)]−

(1.158)

The second term has poles in the right-half plane and corresponds to the anticausal (i.e., t < 0) portion of RXv (t). Any constant terms in ΦXv (s) are included in the realizable portion. In this notation,

H (jω) = [ΦXZ (ω)W (−jω)]+

(1.159)

The overall Wiener ﬁlter h(t) that solves Equation 1.152 is given by the cascade of the whitening ﬁlter and H (s), or

H(jω) = [ΦXZ (ω)W (−jω)]+W (jω)

(1.160)

where

Φ−Z1(ω) = W (−jω)W (jω)

(1.161)

This should be compared with our solution (Equation 1.131) to the inﬁnitedelay smoothing problem. The transfer function H(s) manufactured by Equation 1.160 represents a realizable system and allows real-time processing of the data z(t) to ﬁnd the optimal estimate xˆ(t) of the unknown process x(t).

CRC 9008 C001.pdf 20/7/2007 12:35

44

Optimal and Robust Estimation

To ﬁnd an expression for the error covariance (i.e., Equation 1.128 with T = t), substitute Equation 1.160 into Equation 1.132 to get (scalars!)

F (ω) = ΦX (ω) − [ΦXZ (ω)W (−jω)]+W (jω)ΦXZ (−ω) = ΦX (ω) − [ΦXv (ω)]+ΦXv (−ω)

(1.162)

where Equation 1.157 was used. If the cross-correlation of unknown x(t) and innovations v(t) is RXv (t), we can retain only the causal portion by noting that

[RXv (t)u−1(t)] = [ΦXv (ω)]+.

(1.163)

where [·] denotes Fourier transform and u−1(t) is the unit step. Hence, Equa-

tion 1.162 can be transformed to obtain (recall the transform of ΦXv (−ω) is

RXv (−t))

∞

f (t) = RX (t) −

RXv (τ )u−1RXv (τ − t)dτ

(1.164)

−∞

so that PX˜ = f (0) or

∞

PX˜ = RX (0) −

RX2 v (τ )dτ

0

(1.165)

We can also use these constructions to obtain a time-domain expression for the optimal error covariance obtained by using the unrealizable ﬁlter (Equation 1.131). Considering the scalar case and using superscript u for clarity, there results, from Equations 1.131, 1.132, and 1.161,

F u(ω) = ΦX (ω) − ΦXZ (ω)W (−jω)W (jω)φXZ (−ω)

(1.166)

Now use Equation 1.157 to write

F u(ω) = ΦX (ω) − ΦXv (ω)φXv (−ω)

(1.167)

Transforming this, we get

∞

f u(t) = RX (t) −

RXv(τ )RXv (τ − t)dτ

−∞

(1.168)

so that

∞

PXu˜ = RX (0) −

RX2 v (τ )dτ
−∞

(1.169)

Therefore, the (unattainable!) error using the unrealizable ﬁlter is in general less than the error (Equation 1.165) using the realizable Wiener ﬁlter.

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

45

Example 1.15 Causal Steady-State Filter for Unknown Markov Process

Let us ﬁnd the realizable Wiener ﬁlter (Equation 1.160) for Example 1.14. From that example,

ΦX (ω)

=

2aσw2 ω2 + a2

ΦXZ (ω)

=

cΦX (ω)

=

2acσw2 ω2 + a2

ΦZ (ω)

=

c2ΦX (ω)

+

r

=

r[ω2

+ a2(1 + ω2 + a2

λ)]

(1.170) (1.171) (1.172)

To ﬁnd the whitening ﬁlter, factor the inverse of the spectral density of z(t) to get

Φ−Z1(ω) = W (−jω)W (jω)

= √ (−jω +√a)

· √ (jω +√a)

r(−jω + a 1 + λ) r(jω + a 1 + λ)

(1.173)

Therefore,

ΦXv(ω) = ΦXZ (ω)W (−jω)

=

2a√cσw2 r

(−jω

+

(−√jω + a) a 1 + λ)(ω2

+

a2)

(1.174)

To facilitate things, convert this to a function of s = σ + jω. Then a partial fraction expansion yields

ΦXv (s)

=

−2√acσw2 r

(s

−

√ a1

s−a + λ)(s −

a)(s

+

a)

= √ 2cσ√w2 r(1 + 1 + λ)

s

1 +

a

+

s

−

−√1 a1

+

λ

(1.175) (1.176)

The causal portion of RXv (t) is given by the ﬁrst term, which contains the
left-half plane pole. (Note that the region of convergence of Equation 1.176 contains the imaginary axis.) Therefore, by Equation 1.160 (recall γ = σw2 /r)

H(s) = [ΦXv (s)]+W (s)

= 2√cγ ·

√1

1+ 1+λ s+a 1+λ

(1.177)

so that

h(t)

=

1

2√cγ + 1+

√

e−a λ

1+λtu−1(t)

(1.178)

This is the impulse response of the Wiener ﬁlter for reconstructing x(t) from the data z(t). It is causal and stable, and should be compared to the unrealizable ﬁlter (Equation 1.145) found in Example 1.14. Note that we cannot obtain Equation 1.178 by taking the t ≥ 0 portion of the unrealizable ﬁlter!

CRC 9008 C001.pdf 20/7/2007 12:35

46

Optimal and Robust Estimation

2c

2 w

r (1+ 1+ )

Rxv(t )

0 t
FIGURE 1.8 Cross-correlation of unknown with innovations.

From Equation 1.177 it is clear that the Wiener ﬁlter is a low-pass ﬁlter

whose bandwidth depends on a and λ.

The optimal error covariance is found by using RXv (t), which by Equa-

tion 1.176 is



RXv (t)

=



√ 2cσ√w2 r(1+ 1+λ)

e−at

,



√ 2cσ√w2 r(1+ 1+λ)

√
ea 1+λt

,

t≥0 t<0

(1.179)

This cross-correlation is sketched in Figure 1.8. Now, by Equation 1.166,

PX˜ = σw2 −

∞

4c√2σw2 γ

e−2atdt

0 (1 + 1 + λ)2

=

σw2

[(1 + (1

√ 1√+
+1

λ)2 − + λ)2

λ]

=

2√σw2

1+ 1+λ

(1.180) (1.181) (1.182)

This error covariance and PXu˜ given by Equation 1.146 in Example 1.14 are both sketched in Figure 1.9 as a function of λ.

Example 1.16 The Wiener Filter for Linear Measurements
Here we derive expressions for the Wiener ﬁlter and error covariance for the special case of scalar linear measurements

z(t) = cx (t) + v(t)

(1.183)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

47

2 w

Px~

P

u x~

FIGURE 1.9 Error variances for realizable and unrealizable optimal linear ﬁlters.

where c ∈ R and measurement noise v(t) is white with spectral height r. We assume v(t) is uncorrelated with the unknown x(t).
From Example 1.14,

ΦXZ (ω) = cΦX (ω)

(1.184)

and

ΦZ (ω) = c2ΦX (ω) + r

(1.185)

To ﬁnd the Wiener ﬁlter, we factor ΦZ(ω) into unrealizable and realizable spectral factors as

ΦZ (ω) = W −1(jω)W −1(−jω)

(1.186)

where W −1(s) has poles in the open left-half plane (i.e., not on the jω axis). From the spectral factorization theorem we know that W −1(s) can only be
guaranteed to have zeros in the left-half plane, including possible zeroes on
the jω axis. Thus, if we write

Φ−Z1(ω) = W (−jω)W (jω)

(1.187)

we cannot in general guarantee that W (−jω) has all poles in the s plane with real parts strictly greater than zero (i.e., unstable). However, it is easy to show that if ΦZ(ω) has the form of Equation 1.185 with r = 0, then W (−jω) does have all poles in the open right-half plane. (We shall soon see why we want W (−jω) to be completely unstable!) For

Φ−Z 1 (s)

=

1/r 1 + (c2/r)ΦX (s)

(1.188)

CRC 9008 C001.pdf 20/7/2007 12:35

48

Optimal and Robust Estimation

is in the exact canonical form for the root-locus theory. Hence, as the “feedback gain” c2/r increases, the poles of Φ−Z1(s) migrate from the poles of ΦX (s) to the zeros of ΦX (s). Since ΦX (s) corresponds to a spectral density, it may have
zeros but not poles on the imaginary axis. It does not have poles or zeros in the right-half plane. Hence, if r = 0, the poles of Φ−Z1(s) do not occur on the jω axis and the spectral factors given by

Φ−Z1(ω) = W (−jω)W (jω)

(1.189)

are such that W (s) has all poles in the open left-half plane and W (s∗) has all poles in the open right-half plane (where * denotes complex conjugate).
We can use this fact to ﬁnd an easy expression for the realizable part of W (−jω), which we will need shortly. Let

ΦX (s)

=

n(s) d(s)

(1.190)

for polynomials n(s) and d(s) in s2. If x(t) is band-limited, the degree of d(s) in s2 is at least one greater than the degree of n(s) (why?). Therefore, Φ−Z1(s) has a relative degree of zero since

Φ−Z 1 (s)

=

(1/r)d(s) d(s) + (c2/r)n(s)

(1.191)

In Equation 1.189, a multiplier of 1/√r is assigned to each of the spectral factors, which also have relative degrees of zero. W (−jω) thus has the form

W (−jω) = √1r + unrealizable part

(1.192)

and so

[W (−jω)]+ = √1r

(1.193)

This can be used to ﬁnd a very simple expression for the Wiener ﬁlter in the case of linear measurements. From Equation 1.160, the Wiener ﬁlter is

H (j ω)

=

1 c

[c2ΦX

(ω)W

(−jω)]+W

(j

ω)

(1.194)

(note that the “realizable part” operator is linear), or

H (j ω)

=

1 c

[(c2ΦX

(ω)

+

r)W (−jω)

−

rW

(−jω)]+W (jω)

=

1 c

[W

−1(j

ω)

−

rW

(−jω)]+W (jω)

=

1 c

[W

−1(j

ω)]+

W

(jω)

−

r c

[W

(−jω)]+W

(j

ω)

(1.195)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

49

Now, since W −1(s) is completely realizable (all poles in the open left-half plane), the ﬁrst term is 1/c. Therefore (see Equation 1.193),

H (j ω)

=

1 c [1

−

√ rW (jω)]

(1.196)

which is the desired expression for the Wiener ﬁlter in the linear measurement case.
It is not too diﬃcult to show (Yovits and Jackson 1955) that the error covariance is given by

PX˜

=

r 2πc2

∞
ln
−∞

1

+

c2ΦX (ω) r

dω

(1.197)

Using Equations 1.196 and 1.197, the results of Example 1.15 can be obtained at somewhat less expense!
A useful identity when using Equation 1.197 is

1∞

ω2 + α2

2π

ln
−∞

ω2 + β2

dω = α − β

(1.198)

It should be noted that although the whitening ﬁlter W (s) is the central concept in our derivation of the Wiener ﬁlter, only the latter is actually im-
plemented in an application.
The Wiener ﬁlter can be generalized to multivariable processes and a counterpart also exists in the discrete case. In fact, if the unknown xk and the data zk are discrete vector random processes, then Equations 1.160 and 1.161 become

H(z) = [ΦXZ (z)W T (z−1)]+W (z)

(1.199)

where

Φ−Z1(z) = W T (z−1)W (z)

(1.200)

In this case, the realizable part operator means to retain the portion with poles inside or on the unit circle. If |ΦZ(z)| = 0 on |z| = 1, then W (z) is minimum phase and the realizable poles are strictly within the unit circle.
In the multivariable case with linear measurements,

z(t) = x(t) + v(t)

(1.201)

where v(t) ∼ (0, R), the results of Example 1.16 can be generalized. The Wiener ﬁlter is given in this case by the simple formula

H(s) = I − R1/2W (s)

(1.202)

where W (s) is the stable spectral factor of φ−Z1(s) and R = R1/2RT/2 with R1/2 the square root of R (Kailath 1981). We derive the discrete counterpart to this result in Section 2.6.

CRC 9008 C001.pdf 20/7/2007 12:35

50

Optimal and Robust Estimation

Problems
Section 1.1
1. Least-Squares Solution of Linear Equations. The connotation of least-squares estimation is a setting in which both the unknown X and the data Z are deterministic, so that no a priori information is available on either one.
Let

Z = HX

(1.203)

with H of full column rank. Minimize the performance index

J = 1 (Z − HX )T R−1(Z − HX ) 2

(1.204)

where R−1 is a weighting matrix, to show that the (weighted) least-squares estimate is

Xˆ = (HT R−1H)−1HT R−1Z

(1.205)

Note that the least-squares performance index is expressed in terms of the residual.

2. Nonlinear LS Estimation. Let the data Z and the unknown X be related by

Z = h(X) + e

(1.206)

where e is a residual and h(·) is a (possibly nonlinear) function.

(a) Show that the estimate minimizing the least-squares error

J = 1 e, e = 1 Z − h(X), Z − h(X)

2

2

(1.207)

where ·, · represents inner product is the value of X satisfying the least-squares normal equations

∂h ∂X

,

h(X

)

=

∂h ∂X

,

Z

(1.208)

(b) Expand h(X) in a Taylor series about a given estimate Xˆ to show that

H, HX = H, HXˆ + H, Z − h(Xˆ )

(1.209)

where H = ∂h/∂X is the Jacobian. This provides an iterative method
for solving Equation 1.208, for it shows how to determine a more accurate updated estimate for X from a previous estimate Xˆ .

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

51

3. Continuous Estimate Updating. Let the inner product in Problem 2

be deﬁned as

J=1

T
eT R−1e dt

20

(1.210)

where R > 0.

(a) Write the linearized normal Equations 1.209. (b) Note that H = −∂e/∂X and deﬁne

T
M = HT R−1H dt
0

(1.211)

Using your result in part (a), let T become small to show that the

estimate satisﬁes

X˙ = −M −1

∂e ∂X

T
R−1e

(1.212)

where e = z − h(x).

Note that

M˙ = HT R−1H

(1.213)

Equations 1.212 and 1.213 provide a method for continuously updating the least-squares estimate X if data are taken continuously over a time interval.

4. (a) Suppose we have three measurements Zi of a constant nonrandom volt-
age X. The Zi are corrupted by additive noises V1 ∼ (0, 1), V2 ∼ (0, 2), and V3 ∼ (0, 4), respectively. If we observe Z1 = 4V , Z2 = 3V , and Z3 = 5V , ﬁnd the mean-square estimate for X.
(b) Find the minimum error variance σX2˜ .
5. Estimate for Linear Function of an Unknown. If Xˆ is the linear mean-square estimate for unknown X, show that the linear mean-square estimate for the linear function f (X) = CX + d is given by fˆ = CXˆ + d.

6. Fill in the steps in showing Equation 1.30.

7. A stochastic unknown X and the data Z are jointly distributed accord-

ing to

fXZ (x, z)

=

6 (x 7

+

z)2;

0≤x≤1

and

0≤z≤1

(a) Show that

fX (x)

=

6 7

x2 + x + 1 3

fX/Z (x/z)

=

x2 + 2xz + z2

1 3

+

z

+

z2

;

; 0≤x≤1 0≤x≤1

CRC 9008 C001.pdf 20/7/2007 12:35

52

Optimal and Robust Estimation

(b) Find the best estimate for X and its error covariance in the absence of data.
(c) Find the best mean-square estimate for X given the data Z.
(d) Find the best linear mean-square estimate for X and its error covariance given the data Z.

8. Multiplicative Noise. An unknown X is measured in the presence of multiplicative noise so that
Z = (1 + W )X
where X ∼ (0, σX2 ) and the noise is W ∼ (0, σW 2 ). The noise and unknown are uncorrelated.
(a) Find the optimal linear estimate Xˆ = aZ . (b) Find the error variance.

9. Linear Measurements with Correlated Noise and Unknown. Repeat Example 1.3 if XV T = PXV = 0.

10. Experiment Design. An unknown voltage S is distributed accord-

ing to

fS (s)

=

√1 e−s2/8 2 2π

Consider two ways to measure S.

(a) Use two expensive meters that add to S noise, which is (0, 2) so that

Z1 = S + V1, Z2 = S + V2,

V1 ∼ (0, 2) V2 ∼ (0, 2)

with V1 and V2 uncorrelated with each other and with S. Find linear mean-square estimate and error covariance.
(b) Use four inexpensive meters that add to S noise, which is (0, 3). Find linear mean-square estimate and error covariance.
(c) For the best estimate, should we use scheme (a) or scheme (b)? (d) Find the numerical values for SˆLMS: in (a) if Z1 = 1 and Z2 = 2; in
(b) if Z1 = 1, Z2 = 2, Z3 = 0.5, and Z4 = 1.2.

Section 1.2

11. Inaccurately Known Noise Covariance. To examine the eﬀects of inaccurately known noise parameters, suppose we have

Z=

1 1

X + v,

v∼N

0,

σ2 0

0 1

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

53

(a) Compute XˆML and PX˜ in terms of σ2 if the measurement is

Z=

5 8

Sketch PX˜ and XˆML versus σ2.

(b)

Evaluate

XˆML

and

PX˜

for

σ2

=

1 2

,

1,

and

2.

12. Let

Z=

1 0

2 −1

X + V,

X ∼N

1 −1

,

1 2

2 5

V ∼N

0 0

,

1 1

1 3

and let the measured data be

z=

2 0.5

(a) Ignore statistics and use matrix inverse to ﬁnd Xˆ (i.e., Xˆ = H−1Z).
(b) Ignore statistics and use weighted least-squares estimation (Problem 1) to ﬁnd Xˆ . Use weighting matrix R equal to the covariance of V . Can you ﬁnd an error covariance?
(c) Include statistics on V but not on X. Find maximum-likelihood estimate. Find PX˜ .
(d) Include statistics on X and V . Find linear mean-square estimate. Find PX˜ .
(e) What is your estimation for X before Z is measured? Find error covariance PX˜ in this case.

13. Consider two measurements of a scalar unknown random variable X

Z1 = X + V1, Z2 = X + V2,

V1 ∼ (0, σ12) V2 ∼ (0, σ22)

with noises V1 and V2 uncorrelated.
(a) X is known a priori to be (X¯ , σX2 ). Find the linear mean-square estimate Xˆ and error covariance PX˜ . Compare with Example 1.8.
(b) Find linear mean-square estimate if no statistics are available on X.

14. Let the data Z be related to an unknown X by Z = ln X + V

(1.214)

CRC 9008 C001.pdf 20/7/2007 12:35

54

Optimal and Robust Estimation

with scalar random variables X and V independent. Suppose X is uniformly distributed between 0 and b, and noise V is Laplacian with a = 1 [i.e., fV (v) = (a/2)e−a|v|].
(a) Find the optimal a priori estimate and error covariance. (b) Find the maximum-likelihood estimate. (c) Find the conditional mean estimate.

15. Arbitrary Linear Combination of Data. Let a constant scalar unknown X be measured with two meters corrupted by additive noise so that

Z1 = X + V1 Z2 = X + V2

(1.215)

where V1 ∼ N (0, σ12), and V2 ∼ N (0, σ22), and X, V1, and V2 are independent. We can estimate X by an arbitrary linear combination of the data,

Xˆ = aZ 1 + bZ 2

(1.216)

(a) Show that for this estimate to be unbiased, we require

b=1−a

(1.217)

(b) In the unbiased case, ﬁnd the error covariance PX˜ . For what value of a is this estimate equal to XˆML?
(c) Is it possible to ﬁnd a biased estimate (Equation 1.216) with smaller error covariance than that in (b)?

16. Optimal Combination of Two Independent Estimates. Suppose two estimates Xˆ1 and Xˆ2 of an unknown random variable X are obtained by independent means. The associated error covariances are P1 and P2. It is desired to ﬁnd the overall optimal linear estimate of X, that is, the optimal
estimate of the form

Xˆ = A1Xˆ1 + A2Xˆ2

(1.218)

where A1 and A2 are constant matrices.
(a) Show that if Xˆ1 and Xˆ2 are unbiased (i.e., E(Xˆi) = X¯ ), then for Xˆ to be unbiased also it is suﬃcient that

A2 = I − A1

(1.219)

(b) Deﬁne estimation errors as X˜1 = X −Xˆ1, X˜2 = X −Xˆ2, and X˜ = X −Xˆ . Show that, using Equation 1.219,

X˜ = A1X˜1 + (I − A1)X˜2

(1.220)

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

55

(c) Let the error covariance of Xˆ be P . Diﬀerentiate trace (P ) to ﬁnd that

the optimal A1 is

A1 = P2(P1 + P2)−1

(1.221)

and hence

A2 = P1(P1 + P2)−1

(1.222)

(d) Show that the overall error covariance P is given by P −1 = P1−1 + P2−1

(1.223)

(e) Show that

P −1Xˆ = P1−1Xˆ1 + P2−1Xˆ2

(1.224)

(This is a generalized Millman’s theorem. See Examples 1.8 and 1.9.)

17. Alternate Form of Estimate Update with Linear Measurements. Let unknown X be distributed as (X¯ , PX ) and measurements be taken so that

Z = HX + V

(1.225)

with noise V ∼ (0, R) uncorrelated with X. Show that the linear mean-square estimate Xˆ is given by

P −1Xˆ = PX−1X¯ + HT R−1Z

(1.226)

with error covariance

P −1 = PX−1 + HT R−1H

(1.227)

(If X and V are normal, then Equations 1.226 and 1.227 yield the optimal

mean-square estimate.)

Section 1.4
18. Derive Equation 1.101.
19. Sequential Estimation with Block Diagonal R. Derive Equations 1.105, 1.106, and 1.108 for the case X ∈ Rn and R block diagonal, so that R = diag {Si}, where Si is pi ×pi and nonsingular. Thus the components of Z are sequentially processed in groups of size pi.

Section 1.5

20. An Alternative Design Method for Optimal Filters. A measurement z(t) is taken of a signal x(t) corrupted by additive noise n(t). The signal and noise are uncorrelated, and

ΦX (ω)

=

2σ2 1 + ω2

ΦN (ω)

=

rω2 1 + ω2

(1.228) (1.229)

CRC 9008 C001.pdf 20/7/2007 12:35

56

Optimal and Robust Estimation

An estimate xˆ(t) is formed by passing z(t) through the ideal low-pass ﬁlter

H(jω) = 1, |ω| < ωc 0, otherwise

(1.230)

(a) Sketch ΦX (ω) and ΦN (ω). Show that ΦX (ω) = ΦN (ω) at

ω = ω0 =

2σ2 r

(1.231)

(b) If x˜(t) = x(t) − xˆ(t), show that X˜ (s) = [I − H(s)]X(s) − H(s)N (s)

(1.232)

and ΦX˜ (ω) = |I − H(jω)|2ΦX (ω) + |H(jω)|2ΦN (ω)
Sketch |H(jω)|2 and |I − H(jω)|2.

(1.233)

(c) Find the mean-square error PX˜ = x˜2(t). (Recall that PX˜ = RX˜ (0).) (d) Diﬀerentiate PX˜ with respect to ωc to ﬁnd the optimal value of ωc for
the ﬁlter H(ω).

(e) For r = 1 and σ2 = 2, evaluate the optimal ωc and error covariance PX˜ .

21. Assume measurement z(t) is derived from unknown x(t) as in the previous problem.
(a) Find the Wiener ﬁlter H(jω) to estimate x(t). Sketch |H(jω)|. Note that its cutoﬀ frequency is the same as the cutoﬀ frequency in the previous problem.
(b) Find the error covariance PX˜ . (c) For r = 1 and σ2 = 2, ﬁnd H(jω) and PX˜ .

22. Wiener Filter with Prediction. If it is desired to estimate an unknown scalar signal x(t) at time t + d using data z(t) up through time t, the Wiener–Hopf equation becomes

∞
RXZ (t + d) = h(τ )RZ (t − τ )dτ ; 0 < t < ∞
0
Show that in this case the Wiener ﬁlter is given by

(1.234)

H(s) = [φXZ (s)esd W (−s)]+W (s)

(1.235)

with W (s), the stable factor, given by

φ−Z1(s) = W (−s)W (s)

(1.236)

(Kailath 1981).

CRC 9008 C001.pdf 20/7/2007 12:35

Classical Estimation Theory

57

23. Wiener Filter for Second-Order Markov Process. The unknown x(t) is a second-order Markov process satisfying

x˙ = x2 + w x˙ 2 = −2x − 2x2

(1.237a) (1.237b)

with white process noise w(t) ∼ (0, 2). The data are generated using measure-

ments given by

z(t) = x(t) + v(t)

(1.238)

with white measurement noise v(t) ∼ (0, 1) uncorrelated with x(t).

(a) Find spectral densities φX (s), φXZ (s), and φZ (s).
(b) Find optimal noncausal ﬁlter for reconstructing x(t). Compute and sketch the impulse response.

Answer:

H (s)

=

2(−s2 + 4) s4 − 2s2 + 12

(c) Find a whitening ﬁlter for the data z(t).

Answer:

W (s)

=

s2

s2 + 2s + 2 + 2.988s + 3.4641

(d) Find the Wiener ﬁlter. Compute and sketch impulse response.

This page intentionally left blank

CRC 9008 C002.pdf 20/7/2007 12:42

2
Discrete-Time Kalman Filter

The object of this chapter is to estimate the state of a discrete linear system developing dynamically through time. We shall see that the optimal estimator, known as the Kalman ﬁlter, is itself a linear dynamical system and it is a natural extension of the estimators discussed in the previous chapter.
To introduce the basic ideas used in the discrete Kalman ﬁlter, we ﬁrst review the deterministic state observer.

2.1 Deterministic State Observer

Suppose a dynamical plant is prescribed

xk+1 = Ax k + Buk zk = Hx k

(2.1a) (2.1b)

where state xk ∈ Rn, control input uk ∈ Rm, output zk ∈ Rp, and A, B, and H are known constant matrices of appropriate dimension. All variables are deterministic, so that if initial state x0 is known then Equation 2.1 can be solved exactly for xk, zk for k ≥ 0.
The deterministic asymptotic estimation problem is as follows. Design an
estimator whose output xˆk converges with k to the actual state xk of Equation 2.1 when the initial state x0 is unknown, but uk and zk are given exactly.
An estimator or observer that solves this problem has the form

xˆk+1 = Axˆk + L(zk − Hxˆk) + Buk

(2.2)

which is shown in Figure 2.1. A, B, and H are given, so it is only necessary to ﬁnd the constant observer matrix L. Note that the observer consists of two parts, a replica (A, B, H) of the dynamics of the plant whose state is to be estimated and an error-correcting portion described by L(zk − zˆk) = Lz˜k, where the output estimate is zˆk = Hxˆk and z˜k = zk − zˆk is the residual. The observer should be compared with the recursive estimators shown in
Figures 1.4 and 1.5. Note, however, that here the gain L that multiplies the residual is a constant. Since we built the observer, its state xˆk is available to us. (Plant state xk is not available except through measurement zk; therein lies the problem.)

59

CRC 9008 C002.pdf 20/7/2007 12:42

60

Optimal and Robust Estimation

uk

B

B

FIGURE 2.1 State observer.

z−1 xk
A Plant z −1 xk

H
Estimate
H

Measurement zk
xk

A zk

L

Residual

Observer

To choose L so that the estimation error x˜k = xk − xˆk goes to zero with k for all x0, it is necessary to examine the dynamics of xˆk. Write

x˜k+1 = xk+1 − xˆk+1 = Ax k + Buk − [Axˆk + L(zk − Hxˆk) + Buk] = A(xk − xˆk) − L(Hx k − Hxˆk)

so

x˜k+1 = (A − LH )x˜k

(2.3)

describes the dynamics of the estimation error. System 2.3 is called the error
system. It does not actually exist, but is a mathematical ﬁction that allows
us to study the convergence properties of the observer. The initial condition of Equation 2.3 is x˜0 = x0 − xˆ0, the unknown initial error.
It is now apparent that for x˜k to go to zero with k for any x˜0, observer gain L must be selected so that (A − LH ) is stable. Thus the output injection problem is the key to deterministic observer design. L can be chosen so that x˜k → 0 if and only if (A, H) is detectable (i.e., the unstable poles can be arbitrarily assigned), for then (A − LH ) is stable for some L. If we want the error x˜k not simply to go to zero with k, but to die out as fast as we desire, then all the poles of (A − LH ) must be arbitrarily assignable, so that (A, H)
must be observable. There are many ways to compute L. In the single-output case, Ackermann’s
formula

L = ∆d(A)Vn−1en

(2.4)

CRC 9008 C002.pdf 20/7/2007 12:42

Discrete-Time Kalman Filter

61

can be used, where en is the nth column of the n × n identity matrix, ∆d(s)

the desired error system characteristic polynomial, and

C 

Vn = 

CA ...



(2.5)

CAn−1

the observability matrix. Alternatively, we can use the next result, which applies for multioutput systems.

THEOREM 2.1 Let G and D be any matrices such that (A, G) is reachable and rank ([H, D]) = rank (D). Then (A, H) is detectable if and only if there exists a positive deﬁnite solution P+ to the algebraic Riccati equation

P = APAT − APH T (HPH T + DD T )−1HPAT + GGT

(2.6)

In this event L = AP +HT (HPH T + DD T )−1
stabilizes (A − LH ).

(2.7)

The proof of this result is fairly easy; it is part of a theorem we will prove
in Section 2.5. There is as yet no basis for selecting D and G; any matrices that satisfy the
hypotheses will do. Diﬀerent D and G will result in diﬀerent locations for the error system poles. (One choice that satisﬁes the hypotheses is D = I, G = I.)

Example 2.1 Riccati Equation Design of Scalar Deterministic Observer

We can gain some insights on observer design using Equation 2.6 by considering the scalar case. Let the given plant be

xk+1 = ax k + buk

(2.8)

with measurement

zk = hx k

(2.9)

where uk, xk, and zk are scalars. If we let q = g2 and r = d2 then Equation 2.6 becomes

p

=

a2p

−

a2h2p2 h2p + r

+

q

(2.10)

or

p

=

a2rp h2p +

r

+

q

(2.11)

CRC 9008 C002.pdf 20/7/2007 12:42

62

Optimal and Robust Estimation

which can be written as h2p2 + [(1 − a2)r − h2q]p − qr = 0
This equation has two solutions given by

(2.12)

p

=

q 2λ

±

(1

−

λ)2

+

(1

4λ − a2)

−

(1

−

λ)

(2.13)

where

λ

=

(1

h2q − a2)r

(2.14)

(Compare λ to Equation 1.143 in Example 1.14.) We must now consider two cases.

a. Original system stable If |a| < 1, then (1 − a2) > 0 and λ > 0. In this case the positive (deﬁnite) solution to Equation 2.12 is

p+

=

q 2λ

(1

−

λ)2

+

(1

4λ − a2)

−

(1

−

λ)

(2.15)

and the observer gain (Equation 2.7) is selected as

l

=

ahp+ h2p+ +

r

Then the error system matrix is

acl

=

a

−

lh

=

1

+

a (h2/r)p+

or

acl =

1

+

1−a2 2

a

(1

−

λ)2

+

4λ (1−a2 )

−

(1

−

λ)

(2.16) (2.17) (2.18)

(Superscript “cl” means “closed loop.”) Since the denominator is greater than or equal to 1, we have |acl| ≤ |a| < 1,
so the observer error system is stable.
b. Original system unstable If |a| > 1, then (1 − a2) < 0 and λ < 0. In this case the positive solution to Equation 2.12 is

p+

=

−q 2λ

(1

−

λ)2

+

(1

4λ − a2)

+

(1

−

λ)

(2.19)

CRC 9008 C002.pdf 20/7/2007 12:42

Discrete-Time Kalman Filter

63

acl

0.5 0.4 0.3 0.2 0.1
0 −0.1 −0.2 −0.3 −0.4 −0.5
−25 −20 −15 −10 −5 −10 1 5 a
FIGURE 2.2 Closed-loop versus open-loop plant matrix.

10 15 20 25

Then the error system is given by

acl =

1

−

1−a2 2

a

(1

−

λ)2

+

4λ (1−a2 )

+

(1

−

λ)

(2.20)

It is not quite so easy this time to show that |acl| < 1 (although it is if

λ = 0). Note, however, that if |a| 1 then λ ≈ 0 and it is not diﬃcult to

show that

acl

≈

1 a

(2.21)

This is certainly stable if |a| 1! A plot of acl versus a for the case h2q/r = 1 is shown in Figure 2.2. Note
that acl is always stable. Now consider the situation shown in Figure 2.3. Process noise wk has been
added to the plant and measurement noise vk to the measurement. Under these more realistic circumstances, the observer designed deterministically as above cannot be expected to make x˜k → 0. It is somewhat surprising, however, that by using a structure virtually identical to Figure 2.1 the error
can be forced to zero even in this noisy case. What is required in general is to ﬁnd a time-varying observer gain Lk based on stochastic considerations such as those of the previous chapter. The resulting state estimator depends on a
time-varying version of the Riccati Equation 2.6 and is known as the Kalman
ﬁlter.

CRC 9008 C002.pdf 20/7/2007 12:42

64
uk

Process noise wk

B

z −1

xk

Optimal and Robust Estimation
Measurement noise vk

Measurement

H

zk

A Plant

Measurement model

FIGURE 2.3 Plant and measurement with additive noise.

2.2 Linear Stochastic Systems
To describe a system with noise like the one shown in Figure 2.3, write the discrete stochastic dynamical equation

xk+1 = Ax k + Buk + Gw k zk = Hx k + vk

(2.22a) (2.22b)

where xk ∈ Rn, uk ∈ Rm, zk ∈ Rp, wk ∈ Rl, vk ∈ Rp. Let process noise wk have mean w¯k = 0 and covariance wkwkT = Q, which is symbolized as wk ∼ (0, Q). The overbar denotes expected value. Let measurement noise vk be (0, R). Both {wk} and {vk} are assumed to be stationary white noise processes, that is, their covariance functions satisfy

Pwk1 wk2 = wk1 wkT2 = Qδ(k1 − k2)

(2.23)

where δ(k) is the Kronecker delta, and the corresponding version for vk. Suppose that initial state x0 is unknown, but that there is available a priori
knowledge that x0 is (x¯0, Px0 ). We assume that x0, wk, and vk are mutually uncorrelated, so that wkvjT = 0 for all j and k, and so on. Of course, Px0 , Q, and R are symmetric and positive semideﬁnite. Signal uk is a deterministic
control input.
Under these circumstances, state xk is a random variable with mean x¯k and covariance Pxk . Similarly, output zk is random (z¯k, Pzk ). Notice that {xk} and {zk} are stochastic processes, in general nonstationary.

EXERCISE 2.1 Discrete Markov Process
A discrete ﬁrst-order Markov process xk can be represented as the state of the linear system (Equation 2.22a). Using this system, we can manufacture

CRC 9008 C002.pdf 20/7/2007 12:42

Discrete-Time Kalman Filter

65

stationary or nonstationary Markov processes. Let xk be a scalar for simplicity. Let uk = 0, g = 1. (We use lowercase letters for scalar system parameters.)

a. Stationary Markov process
Let the initial condition at k = −∞ equal zero and wk ∼ (0, (1 − a2)σw2 ). Show that xk is a stationary Markov process with spectral density

ΦX (ω)

=

1

(1 − a2)σw2 + a2 − 2a cos ω

(2.24)

and autocorrelation function

RX (k) = σw2 a|k|

(2.25)

In this case the system must be stable.

b. Nonstationary Markov process
Let wk = 0 and initial condition x0 ∼ (0, σX2 ). Show that xk is a nonstationary Markov process with autocorrelation function

RX (j, k) = σX2 a(j+k)

(2.26)

This result is valid for a stable or unstable system.

Note: If wk is Gaussian, then so is xk. In that case the statistics of xk are completely described by the autocorrelation function.

2.2.1 Propagation of Means and Covariances
It is not diﬃcult to ﬁnd how the means and covariances of xk and zk propagate through time under the inﬂuence of the dynamics (Equation 2.22a).
For the mean of the state, simply write

x¯k+1 = Ax k + Buk + Gw k = Ax¯k + Bu¯k + Gw¯k

or

x¯k+1 = Ax¯k + Bu¯k

(2.27)

The initial condition of Equation 2.27 is x¯0, which is given. Therefore, the mean propagates exactly according to the deterministic dynamics (2.1a)!
To ﬁnd how the state covariance propagates, write

Pxk+1 = (xk+1 − x¯k+1)(xk+1 − x¯k+1)T = [A(xk − x¯k) + Gw k][A(xk − x¯k) + Gwk]T = A(xk − x¯k)(xk − x¯k)T AT + Gwk(xk − x¯k)T AT + A(xk − x¯k)wkT GT + GwkwkT GT

CRC 9008 C002.pdf 20/7/2007 12:42

66

Optimal and Robust Estimation

or

Pxk+1 = AP xk AT + GP wkxk AT + AP xkwk GT + GQG T Hence,

Pxk+1 = AP xk AT + GQG T

(2.28)

The last step follows from the uncorrelatedness of x0 and wk, and the whiteness of wk through the following argument. State xk depends on x0 and the process noise wj for j ≤ k − 1. However wk is uncorrelated with x0 and all previous wj, hence Pwkxk = 0. Equation 2.28 is a Lyapunov equation for Pxk . The initial condition is Px0 , which is given. (A Lyapunov equation is a linear matrix equation.)
Next, we want to ﬁnd the mean and covariance of the output. Taking ex-
pectations in (Equation 2.22b) yields

z¯k = Hx¯k

(2.29)

The cross covariance between state and output is

Pxkzk = (xk − x¯k)(zk − z¯k)T = (xk − x¯k)[H(xk − x¯k) + vk]T

or

Pxkzk = Pxk H T

(2.30)

due to the uncorrelatedness of xk and vk (why are they uncorrelated?). For the covariance of the output,

Pzk = (zk − z¯k)(zk − z¯k)T = [H(xk − x¯k) + vk][H(xk − x¯k) + vk]T

so

Pzk = HP xk HT + R

(2.31)

where we again used the uncorrelatedness assumptions.
Now note, and this is the most important point of this section, that xk and zk are jointly distributed RV with moments given by

xk zk

∼

x¯k z¯k

,

Pxk Pzk xk

Pxk zk Pzk

=

x¯k H x¯k

,

Pxk HP xk

Pxk H T HP xk HT + R

(2.32)

where x¯k and Pxk are determined recursively by Equations 2.27 and 2.28. If the RV of sequence zk is measured and the related RV xk is unknown, then we can apply the results of the previous chapter to ﬁnd an estimate xˆk for the state given the data zk. (At this point, x0, wk, and vk are assumed to have
arbitrary statistics and only the ﬁrst- and second-joint moments are given

CRC 9008 C002.pdf 20/7/2007 12:42

Discrete-Time Kalman Filter

67

above, so the linear mean-square approach would be used.) In the next section,
we pursue this to ﬁnd the optimal linear mean-square estimate of xk given the measurement sequence zk.
The quantities x¯k and Pxk found using Equations 2.27 and 2.28 should actually be considered as the conditional mean and covariance given x¯0, Px0 , and inputs u0, u1, . . . , uk−1.
All of these derivations have been for the case of general statistics. If it is as-
sumed that x0, wk, and vk are Gaussian, then since the system (Equation 2.22) is linear, xk and zk are also Gaussian for all k. Furthermore, xk and zk are
jointly Gaussian, with moments given by Equation 2.32. In this case the ﬁrst

and second moments totally specify the joint PDF, so that complete informa-

tion on the interdependence of state and output is available in Equation 2.32. If all statistics are Gaussian, then xk is described by the PDF

fxk (x, k) =

1 exp
(2π)n|Pxk |

−

1 2

(x

−

x¯k

)T

Px−k1

(x

−

x¯k

)

(2.33)

Note that this is actually a conditional PDF, given x¯0, Px0 , and inputs u0, u1, . . . , uk−1. Deﬁne the hyperstate as the conditional PDF of the state. Then in the Gaussian case, if no measurements are taken the hyperstate is com-
pletely described by the pair

(x¯k, Pxk )

(2.34)

the conditional mean and covariance, and so Equations 2.27 and 2.28 provide an updating algorithm for the entire hyperstate in the absence of measurements.

Example 2.2 Propagation of Scalar Mean and Covariance

Let x0 ∼ N (0, 1) (i.e., normal) and xk, zk be given by Equation 2.22, where

all

quantities

are

scalars

and

wk

∼

N (0,

1),

vk

∼

N (0,

1 2

).

Suppose

that

all

of

our assumptions hold. Let uk = u−1(k) be the unit step. Take b = 1, g = 1

for simplicity.

Then the mean x¯k is given by Equation 2.27, so

x¯k+1 = ax¯k + uk

which has the solution

k−1
x¯k = akx¯0 + ak−1−i

i=0

k−1
= ak−1
i=0

1i a

= ak−1

1 − (1/a)k 1 − 1/a

x¯k

=

ak − 1 a−1

(2.35)

CRC 9008 C002.pdf 20/7/2007 12:42

68

Optimal and Robust Estimation

This has a limiting value x¯∞ if and only if |a| < 1, that is, if Equation 2.35 is stable. It is

x¯∞

=

1 1−

a

(2.36)

To ﬁnd the covariance of the state use Equation 2.28,

Pxk+1 = a2Pxk + 1,

Px0 = 1

Thus

k−1
Pxk = (a2)k + (a2)k−1−i

i=0

=

a2k

+

a2k − 1 a2 − 1

a2(k+1) − 1 = a2 − 1

(2.37)

There is a limiting solution Px∞ if and only if Equation 2.35 is stable, and then

Px∞

=

1

1 − a2

(2.38)

Since xk, zk are normal, the conditional PDF fxk (x, k) is given by Equation 2.33 and we can sketch it as a function of k. This is done in Figure 2.4 for a = 1/2 and a = 2. It is evident that the eﬀect of the process noise is to “ﬂatten” the PDF for xk as k increases, thus decreasing our conﬁdence that xk is near x¯k. There are two cases. If the system is stable, a limiting PDF fxk (x) is reached and subsequent propagation does not inject further uncertainty.
This limiting case is known as statistical steady state. In this case we can say
that limk→∞ xk is near x¯∞ to some tolerance described by Px∞. However, if Equation 2.35 is unstable, there is no limiting PDF and as k increases both the value of x¯k and the probable spread of xk about x¯k go to inﬁnity.
The analysis of the time-varying stochastic system

xk+1 = Akxk + Bkuk + Gkwk zk = Hkxk + vk
where x0 ∼ (x¯0, Px0 ), wk ∼ (0, Qk), vk ∼ (0, Rk) is identical to the above. The results diﬀer only in that subscripts k must be added to A, B, G, H, Q, and R.

2.2.2 Statistical Steady-State and Spectral Densities
In this subsection, we are interested in examining the state and output of a discrete system when they are stationary random processes. We shall set the deterministic input uk to zero.
The solution Pk to Equation 2.28 is in general time-varying. However, if A is asymptotically stable, then the Lyapunov equation has a steady-state

CRC 9008 C002.pdf 20/7/2007 12:42

Discrete-Time Kalman Filter

69

a =1/2

k=0

k =1 k =2 k =3

k=∞

fxk (x, k)

0 0
k=0

1

1.5 1.75 2

x

(a)

a=2

f xk (x,k)

k=1 k=2

k=3 k=4

0

01 3

7

15 x

(b)

FIGURE 2.4 Propagation of the hyperstate. (a) Stable plant. (b) Unstable plant.

solution P Pk+1 = Pk for large k. This steady-state covariance satisﬁes the algebraic Lyapunov equation

P = APAT + GQGT

(2.39)

Under these circumstances, as k increases the state xk tends to a stationary Markov process with mean x¯k = 0 and covariance P . This limiting case is called statistical steady state. If A is unstable, then in general the covariance
Pxk increases without bound.

CRC 9008 C002.pdf 20/7/2007 12:42

70

Optimal and Robust Estimation

If the system has reached statistical steady state, then it is possible to discuss spectral densities. Using Equation 2.22a and the identities at the beginning of Section 1.5, the spectral density of the state xk is given by

ΦX (z) = (zI − A)−1GQGT (z−1I − A)−T

(2.40)

where superscript −T denotes the transpose of the inverse. (Note that (zI − A)−1G is the transfer function from wk to xk and Φw(z) = Q.)
Likewise, the spectral density of the output zk in statistical steady state is

ΦZ (z) = H(zI − A)−1GQGT (z−1I − A)−T HT + R

(2.41)

(If we add two uncorrelated processes, their spectral densities add.) This can be written as

ΦZ (z) = H(z)HT (z−1) + R

(2.42)

where

H(z) = H(zI − A)−1G Q

(2.43)

√ is the transfer function from wk to zk, where wk = Qwk and wk is white noise with unit spectral density. H(z) will be very useful to us later.

2.3 The Discrete-Time Kalman Filter

Consider the discrete-time stochastic dynamical system (Equation 2.22) with the attendant assumptions. All matrices describing the system, and x¯0 and all covariances are given, with Q ≥ 0, Px0 ≥ 0. We shall assume that R > 0. The statistics are not necessarily Gaussian, but arbitrary.
According to our results from Section 1.1, in the absence of measurements the best mean-square estimate for the unknown state xk is just the conditional mean x¯k of the state given x¯0 and u0, u1, . . . , uk−1, which propagates according to the deterministic version of the system (Equation 2.27). Then the error
covariance is just the (conditional) covariance of the state and it propagates
according to the Lyapunov equation (Equation 2.28). To derive these results,
we used only the state equation (Equation 2.22a).
However, according to Equations 1.51 and 1.53, and Example 1.3, given the
linear measurement model (Equation 2.22b), the best linear estimator for the RV xk, given measurement zk and a priori statistics x¯k, Pxk on xk is given by

Px˜k = Px−k1 + H T R−1H −1 xˆk = x¯k + Px˜k HT R−1(zk − Hx¯k)

(2.44) (2.45)

where the estimation error is x˜k = (xk − xˆk). In the case of Gaussian statistics this is the conditional mean estimator, which is the optimal estimator.

CRC 9008 C002.pdf 20/7/2007 12:42

Discrete-Time Kalman Filter

71

In this section we must reconcile the time update (Equations 2.27 and 2.28), which describes the eﬀects of the system dynamics (Equation 2.22a) with the measurement update (Equations 2.44 and 2.45) that describes the eﬀects of measuring the output zk according to Equation 2.22b. Since measurements zk occur at each time k and the system dynamics describe the transition between times, we can do this as follows.

2.3.1 Kalman Filter Formulations

Deﬁne at each time k the a priori (before including measurement zk) estimate

xˆ−k zk )

and error estimate

covariance Pk−, and the a posteriori (after xˆk and error covariance Pk. The relation

including between

measurement Pk− and Pk is

illustrated in Figure 2.5. Then the optimal estimator for the state of Equa-

tion 2.22 with stated assumptions and given measurements zk for all k ≥ 1 is realized as follows, assuming that there is no measurement z0 at time zero.

Discrete Kalman Filter Algorithm:
Set xˆ0 = x¯0, P0 = Px0 . Given xˆk−1 and Pk−1, for each value of k ≥ 1 apply time update (Equations 2.27 and 2.45) to obtain xˆ−k , Pk−. Then apply measurement update (Equations 2.44 and 2.45) to obtain xˆk, Pk.
The resulting estimator is called the discrete-time Kalman ﬁlter and it is the best linear estimator if x0, wk, vk have arbitrary statistics. If x0, wk, vk are normal, then it is the optimal estimator. For convenient reference, the discretetime Kalman ﬁlter is summarized in Table 2.1. The time-varying case, where all system matrices and noise covariances are functions of k, is presented for the sake of generality. Note that even for time-invariant systems and statistics, the Kalman ﬁlter itself is still time-varying since Pk+1 depends on k!
The time update portion of the algorithm gives a prediction xˆ−k+1 of the state at time k + 1, along with the associated error covariance Pk−+1. The measurement update provides a correction based on the measurement zk+1 at

Error covariance
a priori error covariance

P

− 1

P

− 2

P

− 3

Time update (TU) Measurement update
(MU) TU
MU TU
MU

a posteriori

error covariance

P0

P1

P2

P3

0

1

2

3

Time k

FIGURE 2.5 Error covariance update timing diagram.

CRC 9008 C002.pdf 20/7/2007 12:42

72

Optimal and Robust Estimation

TABLE 2.1 Discrete-Time Kalman Filter

System model and measurement model

xk+1 = Akxk + Bkuk + Gkwk zk = Hkxk + vk x0 ∼ (x¯0, Px0 ), wk ∼ (0, Qk),
Assumptions

vk ∼ (0, Rk)

(2.46a) (2.46b)

{wk} and {vk} are white noise processes uncorrelated with x0 and with each other. Rk > 0.

Initialization

P0 = Px0 ,

xˆ0 = x¯0

Time update (eﬀect of system dynamics)

Error covariance Estimate

Pk−+1 = AkPkATk + GkQkGTk xˆ−k+1 = Akxˆk + Bkuk

(2.47) (2.48)

Measurement update (eﬀect of measurement zk)

Error covariance

Pk+1 = (Pk−+1)−1 + HkT+1Rk−+11Hk+1 −1

(2.49)

Estimate

xˆk+1 = xˆ−k+1 + Pk+1HkT+1Rk−+11(zk+1 − Hk+1xˆ−k+1) (2.50)

time k + 1 to yield the net a posteriori estimate xˆk+1 and its error covariance Pk+1. For this reason the equations in Table 2.1 are called the predictor– corrector formulation of the discrete Kalman ﬁlter.
The correction term (Equation 2.50) depends on the residual

z˜k = zk − Hkxˆ−k

(2.51)

The residual weighting coeﬃcient is called the Kalman gain

Kk = PkHkT Rk−1

(2.52)

It is often useful to have an expression for Kk in terms of the a priori error covariance Pk−. To derive this expression, write
Kk = PkHkT Rk−1 = PkHkT Rk−1 Rk + HkPk−HkT Rk + HkPk−HkT −1 = Pk HkT + HkT Rk−1HkPk−HkT Rk + HkPk−HkT −1 = Pk Pk− −1 + HkT Rk−1Hk Pk−HkT Rk + HkPk−HkT −1 = PkPk−1Pk−HkT Rk + HkPk−HkT −1

so that

Kk = Pk−HkT HkPk−HkT + Rk −1

(2.53)

CRC 9008 C002.pdf 20/7/2007 12:42

Discrete-Time Kalman Filter

73

There are many equivalent formulations for Equations 2.47 and 2.49. Using the matrix inversion lemma, the latter can be written as
Pk+1 = Pk−+1 − Pk−+1HkT+1 Hk+1Pk−+1HkT+1 + Rk+1 −1Hk+1Pk−+1 (2.54)
The advantage of Equation 2.54 over Equation 2.49 is that inversion of only one p × p matrix is required. Equation 2.49 requires the inversion of two n × n matrices and the number of measurements p is usually less than n. If |Pk−+1| = 0 then Equation 2.54 must be used in any event.
It is often convenient to replace the measurement update equations 2.49 and 2.50 by the alternative equations in Table 2.2.

TABLE 2.2 Alternative Measurement Update Equations
Kk+1 = Pk−+1HkT+1 Hk+1Pk−+1HkT+1 + Rk+1 −1 Pk+1 = (I − Kk+1Hk+1)Pk−+1 xˆk+1 = xˆ−k+1 + Kk+1(zk+1 − Hk+1xˆ−k+1)

(2.55) (2.56) (2.57)

Equation 2.56 is equivalent (See the problems) to the Joseph Stabilized version.
Pk+1 = (I − Kk+1Hk+1)Pk−+1(I − Kk+1Hk+1)T + Kk+1Rk+1KkT+1 (2.58)
This symmetric version guarantees the positive semideﬁniteness of Pk+1 in the presence of roundoﬀ error and is often used for actual software implementation of the ﬁlter.
Note that the Kalman gain is the weighting which determines the inﬂuence of the residual in updating the estimate. Thus it plays the same role as the 1/(k + 1) in the recursive sample mean equation in Example 1.10.
It is worth noting that although we demanded |Rk| = 0, this is not required since we only need |HkPk−HkT + Rk| = 0.
An important point is that error covariance update equations 2.47 and 2.49 do not depend on the measurements. Thus, if Ak, Gk, Hk, Qk, and Rk are known beforehand for all k, these equations can be used to ﬁnd Pk oﬀ-line before the experiment is performed.
The time and measurement updates can be combined. The recursion for the a posteriori error covariance Pk involves a somewhat lengthy equation, so it is customary to use the recursion for the a priori error covariance Pk−. To derive this, substitute Equation 2.54 into Equation 2.47. If we also use Equation 2.50 in Equation 2.48, there results the alternative ﬁlter formulation in Table 2.3, which manufactures the a priori estimate and error covariance. We have suppressed the time index on the plant matrices, as we shall usually do henceforth for convenience.

CRC 9008 C002.pdf 20/7/2007 12:42

74

Optimal and Robust Estimation

TABLE 2.3 Kalman Filter A Priori Recursive Formulation

Kalman Filter

xˆ−k+1 = A(I − KkH)xˆ−k + Buk + AK kzk

Kk = Pk−HT

HP

− k

H

T

+

R

−1

Error covariance

Pk−+1 = A Pk− − Pk−HT

HP

− k

H

T

+

R

−1HP

− k

AT

+ GQGT

(2.59) (2.60)
(2.61)

The a priori ﬁlter formulation is identical in structure to the deterministic observer in Section 2.1, and Figure 2.1 provides a block diagram if L is replaced by the time-varying gain AK k. Even if the plant (Equation 2.46) is timeinvariant, the Kalman ﬁlter is time-varying since the gain Kk depends on time. Some authors deﬁne AK k as the Kalman gain. Equation 2.61 is a Riccati equation, which is a matrix quadratic equation. As we promised at the end
of Section 2.1, it is just a time-varying version of Equation 2.6, which can be
used to design the deterministic state observer!
In terms of the a posteriori estimate, we can write the recursion

xˆk+1 = (I − Kk+1H)Axˆk + (I − Kk+1H)Buk + Kk+1zk+1

(2.62)

It is worth remarking that if the unknown is constant, then (Equation 2.46a) becomes

xk+1 = xk

(2.63)

so the time update is simply xˆ−k+1 = xˆk, Pk−+1 = Pk. Then the error covariance measurement update in Table 2.1 is

Pk−+11 = Pk−1 + HkT+1Rk−+11Hk+1

(2.64)

which has the simple solution for the information matrix

k

Pk−1 = P0−1 +

HiT Ri−1Hi

i=1

The Kalman ﬁlter is

(2.65)

xˆk+1 = xˆk + Kk+1(zk+1 − Hk+1xˆk)

(2.66)

To get a maximum-likelihood estimate, we would set P0−1 = 0, xˆ0 = 0 to model a complete lack of a priori information. Use the matrix inversion lemma to compare Equations 2.64 and 2.66 to Equations 1.106 and 1.108.
The software for implementing the Kalman ﬁlter eﬃciently may be found in Bierman (1977). The number of computations required per iteration by the

CRC 9008 C002.pdf 20/7/2007 12:42

Discrete-Time Kalman Filter

75

ﬁlter is on the order of n3. As we shall see in subsequent examples, it is often possible in a particular application to do some preliminary analysis to simplify the equations that must actually be solved. Some short computer programs showing implementations of presimpliﬁed ﬁlters are included in the examples.
To gain some insight into the Kalman ﬁlter, let us consider a few examples.

Example 2.3 Ship Navigational Fixes

Suppose a ship is moving east at 10 mph. This velocity is assumed to be
constant except for the eﬀects of wind gusts and wave action. An estimate of the easterly position d and velocity s = d˙ is required every hour. The navigator guesses at time zero that d0 = 0, s0 = 10, and his “guesses” can be modeled as having independent Gaussian distributions with variances σd20 = 2, σs20 = 3. Hence the initial estimates are dˆ0 = 0, sˆ0 = 10. The north-south position is of no interest. If dk, sk indicate position and velocity at hour k, we are required to ﬁnd estimates dˆk, sˆk.
The ﬁrst task is to model the system dynamics. During hour k the ship moves with velocity sk mph, so its position will change according to

dk+1 = dk + sk

(2.67)

The quantity sk+1 should ideally be equal to sk since velocity is constant at 10 mph; however, to model the unknown eﬀects of wind and waves add a noise term wk so that

sk+1 = sk + wk

(2.68)

To apply the Kalman ﬁlter, suppose wk is white and independent of x0 for all k. If, for example, there is a steady easterly current then wk is not white. (We shall see later how to handle nonwhite noise. Basically, in this case a
model of the noise process itself must be included in the system dynamics.) Deﬁne the system state at time k as

xk

dk sk

and write Equations 2.67 and 2.68 as one state equation

xk+1 =

1 0

1 1

xk +

0 1

wk

(2.69)

There is no deterministic input uk. Suppose that previous observations of ships in the region have shown that wk has on the average no eﬀect, but that its eﬀect on velocity can be statistically described by a Gaussian PDF
with covariance 1. Then wk ∼ N (0, Q) = N (0, 1). From the above, the initial condition is

x0 ∼ N (x¯0, P0) = N

0 10

,

2 0

0 3

This completes our modeling of the dynamics (Equation 2.46a).

CRC 9008 C002.pdf 20/7/2007 12:42

76

Optimal and Robust Estimation

Now, suppose that navigational ﬁxes are taken at times k = 1, 2, 3. These ﬁxes determine position to within an error covariance of 2, but yield directly
no information about velocity. The ﬁxes yield easterly positions of 9, 19.5,
and 29 miles, respectively. Before we can incorporate this new information into the estimates for dk
and sk we need to model the measurement process. The situation described above corresponds to the output equation.

zk = [1 0]xk + vk, vk ∼ N (0, 2)

(2.70)

where only the position is observed and it has been assumed that the ﬁxes are corrupted by additive zero mean Gaussian noise with covariance of 2. Equation 2.70 corresponds to Equation 2.46b. We are now ready to estimate the state using the Kalman ﬁlter.
To incorporate the measurements z1 = 9, z2 = 19.5, z3 = 29 into the estimate we use Table 2.1 to proceed as follows:

k=0

Initial estimate:

xˆ0 = x¯0 =

0 10

P0 =

2 0

0 3

k=1

Time update: propagate estimate to k = 1 using system dynamics

xˆ−1 = Axˆ0 + Bu0 =

10 10

,

a prior estimate at k = 1

P1− = AP 0AT + GQGT =

5 3

3 4

Measurement update: include the eﬀects of z1

P1 =

(P1−)−1 + HT R−1H −1 =

1.429 0.857

0.857 2.714

xˆ1 = xˆ−1 + P1HT R−1(z1 − Hxˆ−1 )

=

9.286 9.571

,

a posteriori estimate at k = 1

k=2

Time update: eﬀects of system dynamics

xˆ−2 = Axˆ1 + Bu1 =

18.857 9.571

,

a priori estimate at time k = 2

P2− = AP 1AT + GQGT =

5.857 3.571

3.571 3.714

