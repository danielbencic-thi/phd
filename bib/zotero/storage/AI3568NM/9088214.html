<!DOCTYPE html> <html class="js postmessage history draganddrop borderimage borderradius boxshadow textshadow cssgradients csstransforms csstransforms3d csstransitions generatedcontent localstorage sessionstorage" style lang=en-US><!--
 Page saved with SingleFile 
 url: https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214?arnumber=9088214 
 saved date: Thu Oct 13 2022 13:16:12 GMT+0200 (CEST)
--><meta charset=utf-8><style>:root{--sf-img-3: url("data:image/jpeg;base64,/9j/4RCRRXhpZgAATU0AKgAAAAgABwESAAMAAAABAAEAAAEaAAUAAAABAAAAYgEbAAUAAAABAAAAagEoAAMAAAABAAIAAAExAAIAAAAiAAAAcgEyAAIAAAAUAAAAlIdpAAQAAAABAAAAqAAAANQALcbAAAAnEAAtxsAAACcQQWRvYmUgUGhvdG9zaG9wIENDIDIwMTkgKFdpbmRvd3MpADIwMjA6MDU6MDQgMTE6MjM6MzEAAAOgAQADAAAAAf//AACgAgAEAAAAAQAAApSgAwAEAAAAAQAAAScAAAAAAAAABgEDAAMAAAABAAYAAAEaAAUAAAABAAABIgEbAAUAAAABAAABKgEoAAMAAAABAAIAAAIBAAQAAAABAAABMgICAAQAAAABAAAPVwAAAAAAAABIAAAAAQAAAEgAAAAB/9j/7QAMQWRvYmVfQ00AAv/uAA5BZG9iZQBkgAAAAAH/2wCEAAwICAgJCAwJCQwRCwoLERUPDAwPFRgTExUTExgRDAwMDAwMEQwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwBDQsLDQ4NEA4OEBQODg4UFA4ODg4UEQwMDAwMEREMDAwMDAwRDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDP/AABEIAEcAngMBIgACEQEDEQH/3QAEAAr/xAE/AAABBQEBAQEBAQAAAAAAAAADAAECBAUGBwgJCgsBAAEFAQEBAQEBAAAAAAAAAAEAAgMEBQYHCAkKCxAAAQQBAwIEAgUHBggFAwwzAQACEQMEIRIxBUFRYRMicYEyBhSRobFCIyQVUsFiMzRygtFDByWSU/Dh8WNzNRaisoMmRJNUZEXCo3Q2F9JV4mXys4TD03Xj80YnlKSFtJXE1OT0pbXF1eX1VmZ2hpamtsbW5vY3R1dnd4eXp7fH1+f3EQACAgECBAQDBAUGBwcGBTUBAAIRAyExEgRBUWFxIhMFMoGRFKGxQiPBUtHwMyRi4XKCkkNTFWNzNPElBhaisoMHJjXC0kSTVKMXZEVVNnRl4vKzhMPTdePzRpSkhbSVxNTk9KW1xdXl9VZmdoaWprbG1ub2JzdHV2d3h5ent8f/2gAMAwEAAhEDEQA/APVUkkklKQslu7GtbtL5Y4bWnaTI+i18t2/5yKhZLd2Na3aHyxw2uO0GR9FzodtSU+Kdf+v31swuv9RxcbObVj4ubkMoaMfHdtDbLGfSfRv3bPz/AKb1QP8AjG+ubNpZ1Bo2SG/quMI3au2/q/58e5Z31trsb9Z+rucC1r8/K2FwIB/Sv+i4/SWZWS6ZEtA1I5Hmkp6If4xfroGgfb2Bhb6YH2bGA2R/N/0f+b/kLrv8Wf1p671/r+ThdYyGZeMMJzww00s9zLaWM3Opqrc5rG32+z6HvXlzwJLgPZrtErvv8TTHD6z5DnQCcC0bQOALsVK11abPsRx8ckk1MJcQ5xLRqWxscf6m32qsx1VjG2NwnFryLAYq1Me2z+c+ntV1VcTJxxiUg2sBFbfzh4DzRGx0tYd93N6jkPx7HPrwci0V+m4Y+PtDwbTketkN9Iu3Pd6ez9I9n0/5z9JYgHqNLK2W19N6m5gNzRsZ72+ntL/0fqbtt7v5v/TPqWjkMZmZZbUyiw11tLrHyTD3WbWfo/3fSVBuV0t1j6zdgtsreaXNeXMIe3dvr/SOb7mNregUondSrZuB6T1M1NHpM2VkktiXs9Lc3ZT+Z+5YpN6jRZYWHpvU9jy2oudU4At091ge7+YZ6jv/AAX9F/pDtf057/TZd091hJbsFkmR9Ju3f+bCgb+mBu4X4DgCAdry6C7btnY9377UlIrOp17yB0vqjx6kWvNZH0N2y1nuPrfQ3Vf+lfSqsLTeb7R+qZeNsy6mn19PUBbv9Vm87Hen6f6Xa7f6Vfps/wBAnF3TXEBt/TzJDQfU0LjEMad/0/cz2qxZiGi3HLqKXF9zGtNZc1zT/Oeo1x/daz6P+EZ+jSU66SSSSlJJJJKf/9D1VJJJJSkLL2/Zbt2zb6bp9T6EQf53/g/30VCyjGNadwZDHHc4S0aH3ObLdzf7SSnxP60fU/61ZPXc++jpV7qbczIsrsrhwfXZYXNf9P2bmLNP1O+tgYGv6Pe1gBA0DBJ83OXvP210mKHkAuaDNYnaSz86z+SqvUhdm0tqbU5gDtx3FhnTbyLf5SbKMuE0LI2QZaaPiDfqX9bRRZSej5Ja5wcxxYJb2d+d+c1dj/it6D1/pv1guu6lgXYlH2J9TbLBALzbjua3d/UY9ekNy7A0B1DyQNSDWAT8PWcl9sf/ANx3/wCdX/6VThD+K7jNU2lXw2MOJR7R/Ns7fyQp0X+sH+wsLDtIdB5DX/mOf++q+LjOOLSfXtE1t0Bb4D+QnAaHpst6hhlH08smh9jLDW0WMrqDxtDrPSdx7fc61UXdNxHWOtdjNdY8y95wqy4mfU9ztvu/Se/+utD08nGyXWsi9tlbGzY8NcCw2O7V/neqq/UsWzqVVdV9YYKrG3MdXftcHsn03bvSP0XO3JqWozo/Tqywsw2g1Gaz9iYS0k+p7HvDnt9/uTHo3TSSThMJJn3YTDrG327mnZ/YTVdDsqeXNvyjJBLTnPI0DR7v0fv+h9Oz32f4RQ/5vP3Me7Iy3vrLjW92c4lu4Frtn6L2/S/1YkpKOkdPHGGwTrpg1/8AkVatc+yzHGQ97gLmFjbavTYXg+2bNr/c3+cq/wCGZX+kS6biXdNZayoOtbdYbT62SbNpP0gzdX+c73/10XJdkXHHNjaa2VX1vJdZvB19JrWs2M/Sb7G+l7v53/ttJTpJJJJKUkkkkp//0fVUkkklKQ8gkUWEOLSGOIc0biNPpNZDt/8AV2oiFkkjGtI3Ahjo2QXcfmbvbv8A3UlPM9SzKqs6zHdnZuIbDLhi0OsazS/bZ676r2Ob7P5vH/w/85/wdK/OrfkMvZ1nqFOPcS6usYlpIcWs+k4M27Nrv5n0fp+/G9Ndh9jAnbba0El20OEDcS4/m/ylhfXDOzuk9PqswMqxmTbaGjcGvloa9zhD2OZ9L01JCByTEYnWR6rZS4YmR6JT9ZOkmn7QTeyo2ikTj27tzmeuP0QZ6vpbP8J6f0/0ayL+qPccm6nqnUcdj7vbQ/CL21tc9tZ2na21tXvf7W315Vf+j9T06LOtroFtbbWX3FrwHNMxofJzApfZD/p7f84f+RTSlBh15NYsrFosczY19trfe9wqqDrX+ka6tz/+DYiYgyvstMPrj02R7Xfuj+WjU0NpDoc5xedzi4yeAz/qWoGJkEYtI9GwxW3WB+6P5SW4NeCGh1WvJL7thpblPrpFFt1TnVgNtsOS3i33+g7/AD/TVH1+s7i0YnS4DW+8+qGlxL/U2/oHP9rWs9jmM/nP5+1bm6/Iy3sFluOyutjg0Bmpc61rid7bf9E1ZeT1yrEtya8p/UKxiug2jHDqnAhr2vZkNx/R2fpNm+19bK3/AM4mlcidb1JzAWY3TmWCzVrm2Fpr2H6L2Uud/Pub+bX+jp3+n+n9OhVW9T9Sv18bpbawHersNxJJn0/Ta7Fb9H27kd3WNr9pb1SDW21jhjAzub6r2Q3HLq7am/zldux/qfo696Y9aY3bI6rLw+GjDJMsO3b/AEb2+p/g/wDBpKazLesbmOsx+lABsPY31jLjH6Rrzi/m/wCj2/8AXP0n6KzjOtLcb7SzHrym5QO/FreWio7mM3766nMc/f6P7jE9HWDe/Hr29TrdkODSX4wDWEjd+ks+z7Nv8tivZTb6HYxGTc9tl7GPaW1ukGXcMp3fSb73/mVpKdFJJJJSkkkklP8A/9L1VJJJJSkLJBOPaAHEljoDDtcdPzH7mbXf20VCyW7sa1u3fLHDbJbMj6O8fRSU+Xdf6m+nql2RBsc2++kgXWVPa6vJudW5r63OZt9P0P8ABKt1r7ccV4xH3sqJDbq25llrXyRZTYPXYzb6Dm72en6fqf8AWkH619Fvb1e+zfc5uRk5lz3YrmZAqDbPfZfVX6f2RlFbsN+T9ots/nLf6N6P6UXQ7OrVsyOo5le3p1lTz9nyW2VsvpbGTsxr2t/SW1Mq3NbY/wDTb/S/SfpkMmWEIHJEXMbWd/XtEcTLKyAAR0c05t1LsfMuz7gbbHkxc+4ER7vVc5/+Bt2b/wDSer/1xdp/i7ycy/6wWnIvvtZ9je2kXOc6a2WUNY5u/wCk1j3XV1vd+l/sLlerZuXeL+v1ENbc99DGWUwKyax6dNV/rXejtx7m+l+jx6bf0nprs/8AF3h5DMwZeZfblZluK42W7mmnY99FtXpN2tt7fzn/ABn/AAaaMwnGJlpInhMR+jP5uD/qbORMYpR9mA4RxGcSZTGntkylxy+f/NvfqniZmGMWkG+sEVtBBe3wHmraFh/0Sj/i2f8AUhSCqLSO7n51+LZZZcPSdXj1NdbfucTDnPDWRjy5230/+mgNysIh0ZWJtYdrv1h8CRvj6e36Ct9QbW+99Z/SerU1t1LqDewsDn+nuHtb7nG1Z93S8G8RZiMPuLy5uG+t25073l9Ftb/duQSmbkYbmB7crELHRDhkvgy41j8//SDZ/XUTl4QcWHKxZb9L9YsgaOf7zu2t9rPzlD9l9P8A0n6lVtt2+oz7C4sdsLn1ufT6npOsY6yz9Ls9VS/Z2FBH2KmHDa4fs/lv7h930dUlMvtOGXvr+1YhcwAvH2h+gPE+9TYKyMTJrbXdVdawVW0WOcRMu3s9X2bdrXsu/P8AR9X/AItBb03BbG3Bobt+jHT4jn6Pu/lvRmVsZ9lxmsNVNdzDTRVQcdoIJdG/dtaxrfUtcz/DfzX+E9OxKdpJJJJSkkkklP8A/9P1VJfKqSSn6qQczb9kv37Cz037hZoyNpn1NHfo/wB9fLaSSn2H639Px8+zFt6n1PErYy67KwWZdb8X9FvrfkYdjXVb7r/dRW/+ayf+69r/AKEbcPrtVVj+odSwDeMbfF1dtjXYxhu9jfstFldjX1V/q9frfpvT9Sj1Lf0nkCSqz9ngHuXueDj4L/rcH6PCzQ47PDw/L6t//RuL+4+qVdEazB6o3H6lhfsm22p+TVVbcaa2l3sr+1Nx7Wu/ROr9f/B+n/PV0f0hbn1Lw8vAz325Obj5dDazj0swhc8EPfTZTcavQ27Gs9luT611Vf0PU/nl4eknY/Z4jw3xcQu9+Ph/6XBws36/gnXDwcJ4+Ht/WfqIZtJIG23V5r/mbeWhzif5v+b9n89/Mv8A3/0larUnFLam1Pytj5FftuAhnt9znV+xv+idZ/Pf4LevmZJWRfRpGur9HZJptFzd2X6WTjMJe0PZbWGue7fGSyv0rXes30a/5/8Anf0P6J6pfYAGe3M6uGtYbAWmsu2FzbLGt2V77LPUp99Pvy2b/S9P9LVUvn5JI31SH6GvwrHlzPtfVq7K6667XVmoucDY+yu13p1ne9+59L/s/wDNY/8AOel/OKP2DLa94Gb1UsfuqeHBhc2ywVelbjWV7GN9Jv03uZkYnvt/mL67dnz2kgp+hGdPsFxd9s6w6xx9J0msTtDzqPTDa6/Y7Ze306rH/wCHssv/AEtitza8Xp2I717GV3sDcjN0Lywub7rGNd+k3/0beytmTs/RW/4RfOaSSn6qSXyqkkp+qkl8qpJKf//Z/+0YYFBob3Rvc2hvcCAzLjAAOEJJTQQlAAAAAAAQAAAAAAAAAAAAAAAAAAAAADhCSU0EOgAAAAAA5QAAABAAAAABAAAAAAALcHJpbnRPdXRwdXQAAAAFAAAAAFBzdFNib29sAQAAAABJbnRlZW51bQAAAABJbnRlAAAAAENscm0AAAAPcHJpbnRTaXh0ZWVuQml0Ym9vbAAAAAALcHJpbnRlck5hbWVURVhUAAAAAQAAAAAAD3ByaW50UHJvb2ZTZXR1cE9iamMAAAAMAFAAcgBvAG8AZgAgAFMAZQB0AHUAcAAAAAAACnByb29mU2V0dXAAAAABAAAAAEJsdG5lbnVtAAAADGJ1aWx0aW5Qcm9vZgAAAAlwcm9vZkNNWUsAOEJJTQQ7AAAAAAItAAAAEAAAAAEAAAAAABJwcmludE91dHB1dE9wdGlvbnMAAAAXAAAAAENwdG5ib29sAAAAAABDbGJyYm9vbAAAAAAAUmdzTWJvb2wAAAAAAENybkNib29sAAAAAABDbnRDYm9vbAAAAAAATGJsc2Jvb2wAAAAAAE5ndHZib29sAAAAAABFbWxEYm9vbAAAAAAASW50cmJvb2wAAAAAAEJja2dPYmpjAAAAAQAAAAAAAFJHQkMAAAADAAAAAFJkICBkb3ViQG/gAAAAAAAAAAAAR3JuIGRvdWJAb+AAAAAAAAAAAABCbCAgZG91YkBv4AAAAAAAAAAAAEJyZFRVbnRGI1JsdAAAAAAAAAAAAAAAAEJsZCBVbnRGI1JsdAAAAAAAAAAAAAAAAFJzbHRVbnRGI1B4bEBywAAAAAAAAAAACnZlY3RvckRhdGFib29sAQAAAABQZ1BzZW51bQAAAABQZ1BzAAAAAFBnUEMAAAAATGVmdFVudEYjUmx0AAAAAAAAAAAAAAAAVG9wIFVudEYjUmx0AAAAAAAAAAAAAAAAU2NsIFVudEYjUHJjQFkAAAAAAAAAAAAQY3JvcFdoZW5QcmludGluZ2Jvb2wAAAAADmNyb3BSZWN0Qm90dG9tbG9uZwAAAAAAAAAMY3JvcFJlY3RMZWZ0bG9uZwAAAAAAAAANY3JvcFJlY3RSaWdodGxvbmcAAAAAAAAAC2Nyb3BSZWN0VG9wbG9uZwAAAAAAOEJJTQPtAAAAAAAQASwAAAABAAEBLAAAAAEAAThCSU0EJgAAAAAADgAAAAAAAAAAAAA/gAAAOEJJTQQNAAAAAAAEAAAAWjhCSU0EGQAAAAAABAAAAB44QklNA/MAAAAAAAkAAAAAAAAAAAEAOEJJTScQAAAAAAAKAAEAAAAAAAAAAThCSU0D9QAAAAAASAAvZmYAAQBsZmYABgAAAAAAAQAvZmYAAQChmZoABgAAAAAAAQAyAAAAAQBaAAAABgAAAAAAAQA1AAAAAQAtAAAABgAAAAAAAThCSU0D+AAAAAAAcAAA/////////////////////////////wPoAAAAAP////////////////////////////8D6AAAAAD/////////////////////////////A+gAAAAA/////////////////////////////wPoAAA4QklNBAgAAAAAABAAAAABAAACQAAAAkAAAAAAOEJJTQQeAAAAAAAEAAAAADhCSU0EGgAAAAADSQAAAAYAAAAAAAAAAAAAAScAAAKUAAAACgBVAG4AdABpAHQAbABlAGQALQAxAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAKUAAABJwAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAABAAAAABAAAAAAAAbnVsbAAAAAIAAAAGYm91bmRzT2JqYwAAAAEAAAAAAABSY3QxAAAABAAAAABUb3AgbG9uZwAAAAAAAAAATGVmdGxvbmcAAAAAAAAAAEJ0b21sb25nAAABJwAAAABSZ2h0bG9uZwAAApQAAAAGc2xpY2VzVmxMcwAAAAFPYmpjAAAAAQAAAAAABXNsaWNlAAAAEgAAAAdzbGljZUlEbG9uZwAAAAAAAAAHZ3JvdXBJRGxvbmcAAAAAAAAABm9yaWdpbmVudW0AAAAMRVNsaWNlT3JpZ2luAAAADWF1dG9HZW5lcmF0ZWQAAAAAVHlwZWVudW0AAAAKRVNsaWNlVHlwZQAAAABJbWcgAAAABmJvdW5kc09iamMAAAABAAAAAAAAUmN0MQAAAAQAAAAAVG9wIGxvbmcAAAAAAAAAAExlZnRsb25nAAAAAAAAAABCdG9tbG9uZwAAAScAAAAAUmdodGxvbmcAAAKUAAAAA3VybFRFWFQAAAABAAAAAAAAbnVsbFRFWFQAAAABAAAAAAAATXNnZVRFWFQAAAABAAAAAAAGYWx0VGFnVEVYVAAAAAEAAAAAAA5jZWxsVGV4dElzSFRNTGJvb2wBAAAACGNlbGxUZXh0VEVYVAAAAAEAAAAAAAlob3J6QWxpZ25lbnVtAAAAD0VTbGljZUhvcnpBbGlnbgAAAAdkZWZhdWx0AAAACXZlcnRBbGlnbmVudW0AAAAPRVNsaWNlVmVydEFsaWduAAAAB2RlZmF1bHQAAAALYmdDb2xvclR5cGVlbnVtAAAAEUVTbGljZUJHQ29sb3JUeXBlAAAAAE5vbmUAAAAJdG9wT3V0c2V0bG9uZwAAAAAAAAAKbGVmdE91dHNldGxvbmcAAAAAAAAADGJvdHRvbU91dHNldGxvbmcAAAAAAAAAC3JpZ2h0T3V0c2V0bG9uZwAAAAAAOEJJTQQoAAAAAAAMAAAAAj/wAAAAAAAAOEJJTQQRAAAAAAABAQA4QklNBBQAAAAAAAQAAAACOEJJTQQMAAAAAA9zAAAAAQAAAJ4AAABHAAAB3AAAhAQAAA9XABgAAf/Y/+0ADEFkb2JlX0NNAAL/7gAOQWRvYmUAZIAAAAAB/9sAhAAMCAgICQgMCQkMEQsKCxEVDwwMDxUYExMVExMYEQwMDAwMDBEMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMAQ0LCw0ODRAODhAUDg4OFBQODg4OFBEMDAwMDBERDAwMDAwMEQwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAz/wAARCABHAJ4DASIAAhEBAxEB/90ABAAK/8QBPwAAAQUBAQEBAQEAAAAAAAAAAwABAgQFBgcICQoLAQABBQEBAQEBAQAAAAAAAAABAAIDBAUGBwgJCgsQAAEEAQMCBAIFBwYIBQMMMwEAAhEDBCESMQVBUWETInGBMgYUkaGxQiMkFVLBYjM0coLRQwclklPw4fFjczUWorKDJkSTVGRFwqN0NhfSVeJl8rOEw9N14/NGJ5SkhbSVxNTk9KW1xdXl9VZmdoaWprbG1ub2N0dXZ3eHl6e3x9fn9xEAAgIBAgQEAwQFBgcHBgU1AQACEQMhMRIEQVFhcSITBTKBkRShsUIjwVLR8DMkYuFygpJDUxVjczTxJQYWorKDByY1wtJEk1SjF2RFVTZ0ZeLys4TD03Xj80aUpIW0lcTU5PSltcXV5fVWZnaGlqa2xtbm9ic3R1dnd4eXp7fH/9oADAMBAAIRAxEAPwD1VJJJJSkLJbuxrW7S+WOG1p2kyPotfLdv+cioWS3djWt2h8scNrjtBkfRc6HbUlPinX/r99bMLr/UcXGzm1Y+Lm5DKGjHx3bQ2yxn0n0b92z8/wCm9UD/AIxvrmzaWdQaNkhv6rjCN2rtv6v+fHuWd9ba7G/Wfq7nAta/PythcCAf0r/ouP0lmVkumRLQNSOR5pKeiH+MX66BoH29gYW+mB9mxgNkfzf9H/m/5C67/Fn9aeu9f6/k4XWMhmXjDCc8MNNLPcy2ljNzqaq3Oaxt9vs+h715c8CS4D2a7RK77/E0xw+s+Q50AnAtG0DgC7FStdWmz7EcfHJJNTCXEOcS0alsbHH+pt9qrMdVYxtjcJxa8iwGKtTHts/nPp7VdVXEyccYlINrARW384eA80RsdLWHfdzeo5D8exz68HItFfpuGPj7Q8G05HrZDfSLtz3ens/SPZ9P+c/SWIB6jSytltfTepuYDc0bGe9vp7S/9H6m7be7+b/0z6lo5DGZmWW1MosNdbS6x8kw91m1n6P930lQbldLdY+s3YLbK3mlzXlzCHt3b6/0jm+5ja3oFKJ3Uq2bgek9TNTR6TNlZJLYl7PS3N2U/mfuWKTeo0WWFh6b1PY8tqLnVOALdPdYHu/mGeo7/wAF/Rf6Q7X9Oe/02XdPdYSW7BZJkfSbt3/mwoG/pgbuF+A4AgHa8ugu27Z2Pd++1JSKzqde8gdL6o8epFrzWR9DdstZ7j630N1X/pX0qrC03m+0fqmXjbMupp9fT1AW7/VZvOx3p+n+l2u3+lX6bP8AQJxd01xAbf08yQ0H1NC4xDGnf9P3M9qsWYhotxy6ilxfcxrTWXNc0/znqNcf3Ws+j/hGfo0lOukkkkpSSSSSn//Q9VSSSSUpCy9v2W7ds2+m6fU+hEH+d/4P99FQsoxjWncGQxx3OEtGh9zmy3c3+0kp8T+tH1P+tWT13Pvo6Ve6m3MyLK7K4cH12WFzX/T9m5izT9TvrYGBr+j3tYAQNAwSfNzl7z9tdJih5ALmgzWJ2ks/Os/kqr1IXZtLam1OYA7cdxYZ028i3+UmyjLhNCyNkGWmj4g36l/W0UWUno+SWucHMcWCW9nfnfnNXY/4reg9f6b9YLrupYF2JR9ifU2ywQC8247mt3f1GPXpDcuwNAdQ8kDUg1gE/D1nJfbH/wDcd/8AnV/+lU4Q/iu4zVNpV8NjDiUe0fzbO38kKdF/rB/sLCw7SHQeQ1/5jn/vqvi4zji0n17RNbdAW+A/kJwGh6bLeoYZR9PLJofYyw1tFjK6g8bQ6z0nce33OtVF3TcR1jrXYzXWPMvecKsuJn1Pc7b7v0nv/rrQ9PJxsl1rIvbZWxs2PDXAsNju1f53qqv1LFs6lVXVfWGCqxtzHV37XB7J9N270j9FztyalqM6P06ssLMNoNRms/YmEtJPqex7w57ff7kx6N00kk4TCSZ92Ew6xt9u5p2f2E1XQ7Knlzb8oyQS05zyNA0e79H7/ofTs99n+EUP+bz9zHuyMt76y41vdnOJbuBa7Z+i9v0v9WJKSjpHTxxhsE66YNf/AJFWrXPssxxkPe4C5hY22r02F4Ptmza/3N/nKv8AhmV/pEum4l3TWWsqDrW3WG0+tkmzaT9IM3V/nO9/9dFyXZFxxzY2mtlV9byXWbwdfSa1rNjP0m+xvpe7+d/7bSU6SSSSSlJJJJKf/9H1VJJJJSkPIJFFhDi0hjiHNG4jT6TWQ7f/AFdqIhZJIxrSNwIY6NkF3H5m727/AN1JTzPUsyqrOsx3Z2biGwy4YtDrGs0v22eu+q9jm+z+bx/8P/Of8HSvzq35DL2dZ6hTj3EurrGJaSHFrPpODNuza7+Z9H6fvxvTXYfYwJ222tBJdtDhA3EuP5v8pYX1wzs7pPT6rMDKsZk22ho3Br5aGvc4Q9jmfS9NSQgckxGJ1keq2UuGJkeiU/WTpJp+0E3sqNopE49u7c5nrj9EGer6Wz/Cen9P9Gsi/qj3HJup6p1HHY+720Pwi9tbXPbWdp2ttbV73+1t9eVX/o/U9Oizra6BbW21l9xa8BzTMaHycwKX2Q/6e3/OH/kU0pQYdeTWLKxaLHM2Nfba33vcKqg61/pGurc//g2ImIMr7LTD649Nke137o/lo1NDaQ6HOcXnc4uMngM/6lqBiZBGLSPRsMVt1gfuj+UluDXghodVryS+7YaW5T66RRbdU51YDbbDkt4t9/oO/wA/01R9frO4tGJ0uA1vvPqhpcS/1Nv6Bz/a1rPY5jP5z+ftW5uvyMt7BZbjsrrY4NAZqXOta4ne23/RNWXk9cqxLcmvKf1CsYroNoxw6pwIa9r2ZDcf0dn6TZvtfWyt/wDOJpXInW9ScwFmN05lgs1a5thaa9h+i9lLnfz7m/m1/o6d/p/p/ToVVvU/Ur9fG6W2sB3q7DcSSZ9P02uxW/R9u5Hd1ja/aW9Ug1ttY4YwM7m+q9kNxy6u2pv85Xbsf6n6OvemPWmN2yOqy8PhowyTLDt2/wBG9vqf4P8AwaSmsy3rG5jrMfpQAbD2N9Yy4x+ka84v5v8Ao9v/AFz9J+is4zrS3G+0sx68puUDvxa3loqO5jN++upzHP3+j+4xPR1g3vx69vU63ZDg0l+MA1hI3fpLPs+zb/LYr2U2+h2MRk3PbZexj2ltbpBl3DKd30m+9/5laSnRSSSSUpJJJJT/AP/S9VSSSSUpCyQTj2gBxJY6Aw7XHT8x+5m139tFQslu7Gtbt3yxw2yWzI+jvH0UlPl3X+pvp6pdkQbHNvvpIF1lT2urybnVua+tzmbfT9D/AASrda+3HFeMR97KiQ26tuZZa18kWU2D12M2+g5u9np+n6n/AFpB+tfRb29Xvs33ObkZOZc92K5mQKg2z32X1V+n9kZRW7Dfk/aLbP5y3+jej+lF0Ozq1bMjqOZXt6dZU8/Z8ltlbL6Wxk7Ma9rf0ltTKtzW2P8A02/0v0n6ZDJlhCByRFzG1nf17RHEyysgAEdHNObdS7HzLs+4G2x5MXPuBEe71XOf/gbdm/8A0nq/9cXaf4u8nMv+sFpyL77WfY3tpFznOmtllDWObv8ApNY911db3fpf7C5Xq2bl3i/r9RDW3PfQxllMCsmsenTVf613o7ce5vpfo8em39J6a7P/ABd4eQzMGXmX25WZbiuNlu5pp2PfRbV6Tdrbe385/wAZ/wAGmjMJxiZaSJ4TEfoz+bg/6mzkTGKUfZgOEcRnEmUxp7ZMpccvn/zb36p4mZhjFpBvrBFbQQXt8B5q2hYf9Eo/4tn/AFIUgqi0ju5+dfi2WWXD0nV49TXW37nEw5zw1kY8udt9P/poDcrCIdGVibWHa79YfAkb4+nt+grfUG1vvfWf0nq1NbdS6g3sLA5/p7h7W+5xtWfd0vBvEWYjD7i8ubhvrdudO95fRbW/3bkEpm5GG5ge3KxCx0Q4ZL4MuNY/P/0g2f11E5eEHFhysWW/S/WLIGjn+87trfaz85Q/ZfT/ANJ+pVbbdvqM+wuLHbC59bn0+p6TrGOss/S7PVUv2dhQR9iphw2uH7P5b+4fd9HVJTL7Thl76/tWIXMALx9ofoDxPvU2CsjEya213VXWsFVtFjnETLt7PV9m3a17Lvz/AEfV/wCLQW9NwWxtwaG7fox0+I5+j7v5b0ZlbGfZcZrDVTXcw00VUHHaCCXRv3bWsa31LXM/w381/hPTsSnaSSSSUpJJJJT/AP/T9VSXyqkkp+qkHM2/ZL9+ws9N+4WaMjaZ9TR36P8AfXy2kkp9h+t/T8fPsxbep9TxK2MuuysFmXW/F/Rb635GHY11W+6/3UVv/msn/uva/wChG3D67VVY/qHUsA3jG3xdXbY12MYbvY37LRZXY19Vf6vX636b0/Uo9S39J5Akqs/Z4B7l7ng4+C/63B+jws0OOzw8Py+rf/0bi/uPqlXRGsweqNx+pYX7Jttqfk1VW3Gmtpd7K/tTce1rv0Tq/X/wfp/z1dH9IW59S8PLwM99uTm4+XQ2s49LMIXPBD302U3Gr0NuxrPZbk+tdVX9D1P55eHpJ2P2eI8N8XELvfj4f+lwcLN+v4J1w8HCePh7f1n6iGbSSBtt1ea/5m3loc4n+b/m/Z/PfzL/AN/9JWq1JxS2ptT8rY+RX7bgIZ7fc51fsb/onWfz3+C3r5mSVkX0aRrq/R2SabRc3dl+lk4zCXtD2W1hrnu3xksr9K13rN9Gv+f/AJ39D+ieqX2ABntzOrhrWGwFprLthc2yxrdle+yz1KffT78tm/0vT/S1VL5+SSN9Uh+hr8Kx5cz7X1auyuuuu11ZqLnA2Psrtd6dZ3vfufS/7P8AzWP/ADnpfzij9gy2veBm9VLH7qnhwYXNssFXpW41lexjfSb9N7mZGJ77f5i+u3Z89pIKfoRnT7BcXfbOsOscfSdJrE7Q86j0w2uv2O2Xt9Oqx/8Ah7LL/wBLYrc2vF6diO9exld7A3IzdC8sLm+6xjXfpN/9G3srZk7P0Vv+EXzmkkp+qkl8qpJKfqpJfKqSSn//2QA4QklNBCEAAAAAAF0AAAABAQAAAA8AQQBkAG8AYgBlACAAUABoAG8AdABvAHMAaABvAHAAAAAXAEEAZABvAGIAZQAgAFAAaABvAHQAbwBzAGgAbwBwACAAQwBDACAAMgAwADEAOQAAAAEAOEJJTQQGAAAAAAAH//8AAAABAQD/4Qz9aHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLwA8P3hwYWNrZXQgYmVnaW49Iu+7vyIgaWQ9Ilc1TTBNcENlaGlIenJlU3pOVGN6a2M5ZCI/PiA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJBZG9iZSBYTVAgQ29yZSA1LjYtYzE0NSA3OS4xNjM0OTksIDIwMTgvMDgvMTMtMTY6NDA6MjIgICAgICAgICI+IDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+IDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RFdnQ9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZUV2ZW50IyIgeG1sbnM6cGhvdG9zaG9wPSJodHRwOi8vbnMuYWRvYmUuY29tL3Bob3Rvc2hvcC8xLjAvIiB4bXA6Q3JlYXRvclRvb2w9IkFkb2JlIFBob3Rvc2hvcCBDQyAyMDE5IChXaW5kb3dzKSIgeG1wOkNyZWF0ZURhdGU9IjIwMjAtMDUtMDRUMTE6MjM6MzEtMDQ6MDAiIHhtcDpNZXRhZGF0YURhdGU9IjIwMjAtMDUtMDRUMTE6MjM6MzEtMDQ6MDAiIHhtcDpNb2RpZnlEYXRlPSIyMDIwLTA1LTA0VDExOjIzOjMxLTA0OjAwIiBkYzpmb3JtYXQ9ImltYWdlL2pwZWciIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6YjY5YjkxMGItMzBmNy1jODQ4LWI0MjktZDM2ZGNiMDcwNDM2IiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOmI2OWI5MTBiLTMwZjctYzg0OC1iNDI5LWQzNmRjYjA3MDQzNiIgeG1wTU06T3JpZ2luYWxEb2N1bWVudElEPSJ4bXAuZGlkOmI2OWI5MTBiLTMwZjctYzg0OC1iNDI5LWQzNmRjYjA3MDQzNiIgcGhvdG9zaG9wOkNvbG9yTW9kZT0iMyI+IDx4bXBNTTpIaXN0b3J5PiA8cmRmOlNlcT4gPHJkZjpsaSBzdEV2dDphY3Rpb249ImNyZWF0ZWQiIHN0RXZ0Omluc3RhbmNlSUQ9InhtcC5paWQ6YjY5YjkxMGItMzBmNy1jODQ4LWI0MjktZDM2ZGNiMDcwNDM2IiBzdEV2dDp3aGVuPSIyMDIwLTA1LTA0VDExOjIzOjMxLTA0OjAwIiBzdEV2dDpzb2Z0d2FyZUFnZW50PSJBZG9iZSBQaG90b3Nob3AgQ0MgMjAxOSAoV2luZG93cykiLz4gPC9yZGY6U2VxPiA8L3htcE1NOkhpc3Rvcnk+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw/eHBhY2tldCBlbmQ9InciPz7/7gAOQWRvYmUAZIAAAAAB/9sAhAASDg4OEA4VEBAVHhMREx4jGhUVGiMiFxcXFxciEQwMDAwMDBEMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMARQTExYZFhsXFxsUDg4OFBQODg4OFBEMDAwMDBERDAwMDAwMEQwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDAz/wAARCAEnApQDASIAAhEBAxEB/90ABAAq/8QBPwAAAQUBAQEBAQEAAAAAAAAAAwABAgQFBgcICQoLAQABBQEBAQEBAQAAAAAAAAABAAIDBAUGBwgJCgsQAAEEAQMCBAIFBwYIBQMMMwEAAhEDBCESMQVBUWETInGBMgYUkaGxQiMkFVLBYjM0coLRQwclklPw4fFjczUWorKDJkSTVGRFwqN0NhfSVeJl8rOEw9N14/NGJ5SkhbSVxNTk9KW1xdXl9VZmdoaWprbG1ub2N0dXZ3eHl6e3x9fn9xEAAgIBAgQEAwQFBgcHBgU1AQACEQMhMRIEQVFhcSITBTKBkRShsUIjwVLR8DMkYuFygpJDUxVjczTxJQYWorKDByY1wtJEk1SjF2RFVTZ0ZeLys4TD03Xj80aUpIW0lcTU5PSltcXV5fVWZnaGlqa2xtbm9ic3R1dnd4eXp7fH/9oADAMBAAIRAxEAPwDuEkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSU0KLK8nqGS19Td+E9rK7PztttVOS/wD8+p7Oj9LsyPtNmNW68nd6hHu3D89Hp9P179tZY7c3e+P507K9tn/W6/0K4D6223N69Y1lj2N9NmjXFo4/kpKfQvQp9YX7B6wG0Pj3bf3FVHR+ljI+0jFr9ed3qR7t3768mORkxpfZPcb3f+SS+0ZUfz9k+G93/kklPsYopFxvDALXDa58e4tH5qq1dH6XTf8Aaasatl4JIsAh0n85eTjIy+PXs/z3f+STfaMv/TWH+27/AMkkp9ibj0Ne+wMaH26WOjV4/lqvj9I6ZjXevj41dVuvvaIOv0l5IcrLHN1n+e7/AMkl9ryf9NZ/nu/8kkp9jGLjt9SK2j1tbdPpz/pEDF6V07DsNuLjspsIguaIMLyL7Vlf6az/AD3f+SS+1ZX+ms/z3f8AkklPsX2PFFdlXpN9O4k2NjR5P0t6HidM6fhuc7Fx2UueIcWiJC8h+1ZX+ms/z3f+SS+1ZP8AprP893/kklPsJwcM47sY0sND5Lq49pJ/kqOL03AxGvbjUMqFmjw0Ru/rLyJuVkazbYT2O92n/SS+1ZTT/PWOHb3u/wDJJJrS315/T8KzG+yvpY7H59Ij2/vfRTY/TsHGqfTRQyuqz6bGjR39deRnLyCyPVsDp53ugj/OVvpOVkftXDm15BuYCC53dzUkPqV3T8G/HbjW0MfQz6NZHtbH7qVXTsGnHdjVUMZQ/wClWB7TKspJKauR07ByqmVZFDLa6/oNcJDfzfYnZ0/CZjfZWUsGPz6YHtn6SspJKauV03AzA0ZVDLhXozcJ2/1VIYOGKG44pb6DCCyuPa0j6O1WEklNTL6Z0/Nc12Vjsuc0Q0uEkBE+x4vp11+k3ZSQamxowj6Ppo6SSmnldJ6bmWC3Jx2XWAQHOEmEc4uOTWTW0+j/ADWn0P8Ai0VJJTSyOkdMybvXvxq7LdPe4e7T6KJkfYadmRkbKzV7WWPhu2fzGOcrKzeqNa7IwGuAc036giR/N3JKa1g+q92R9osdiuvJDvULm7tw/O+krZzejG4XnIo9Vo2h+9m4N/d+mrf2XG/0LP8ANb/5FN9kxf8AQ1/5jf8AyKSnKj6rnJ+07sX7Ru3epubu3fv/AElb+29GFxvGRR6rhtL97Nxb+79NW/suN/oWf5rf/IpvsmL/AKGv/Mb/AORSU5VY+q9eR9orditvBLg8OaHbj+d9JWxm9FbY+0ZFAssEPdvZLgP3/crf2XF/0LP81v8A5FN9kxf9DX/mN/8AIpKcvHH1Yx7/AF8d2LXdr72uaDr9L85WRmdEb6hF+OPW/nfez3/m/pPcrn2XF/0LP81v/kU32TF/0Nf+Y3/yKSnMxD9WcW024r8aqwiC5rmgx/nI32roTa7K/Wx/TuJNrd7IeT9Lf7le+y4v+hr/AM1v/kU32TF/0Nf+a3/yKSnNw3fVvEc5+I/Gpc4Q4tc0SP8AORDkdA9B2Obcf0HyX172bXE/2lf+y4p/wNf+a3/yKb7Ji/6Gv/Nb/wCRSU52JZ9XMUPGLZjVCzR+17Ru/re9Ssv+r7sb7K+3GOPz6Zewtn/OV/7Jin/A1/5rf/IpfZMX/Q1/5rf/ACKSnPxbPq7jVPqxrMauuz6bWvaA7+v7krrvq9bjtxrrcZ9Dfo1l7C0R+77lofZMX/Q1/wCa3/yKX2TF/wBDX/mt/wDIpKc+i36vU47semzGZQ/6VYewNdP9tLJt+ruRUynIsxrKq/oMc5pDfzfZ7lofZMX/AENf+a3/AMil9lxRxTX/AJrf/IpKaFd31eZjfZq7cZuOdfTD2bZ/zlHLt+rmU1jcqzGtbXowOc07f6vuWj9kxf8AQ1/5rf8AyKX2XFH+Br/zW/8AkUlNFt/QBjtxxbjihmra97NoI930d6VjOhdVsa1/oZVjB7RIc4N/k7Ve+yYv+hr/AM1v/kVRyaqq+qdP9NjWE+sCWgN0DN35qSmx0u/HvwmWY1fpUy9jGeHpvsx3f9OpXFU6bXi1YjWYh3UB1haT+8bLX3/+D+qraSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkp/9DuEkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSU08XKsuzM2hwG3GexrCPB9VOS7d/1y1cB9cDH1gf/wAXX+RejVGbbh6eyHD3/wCl9jP0n/W/5n/ra83+uX/L9n/F1/kSU4DtHEeBSDh3Cd/0vioJJZgsnkhSlnZ2qEFKAlSmbp2wSCPJCiD5KUBMQkhjoklBSA1SUu0TzoEnNI857qeye+iawwA1K15jQ1WA0TgTomDgkT8klCqWVzpX/KmH/wAfX/1bFT1J01V7pX/KuFH+nr/6pqSK0L7AkkkktUkkkkpSHdfTQzfc8MbMSfEoipdSANVciR6rOUYiyAgmgy/aeD/pm/il+08D/TN/H/yKs7Gfuj7k+xn7o+5H09pfar1dx9jV/aeB/pm/j/5FL9p4H+mb+KtbGfuj7ktjP3R9yXp7S+1Xq7hq/tPA/wBM38f/ACKpZ2ZjW5GCa7A7bfJif3LK/wDv619jP3R9yzuotaMnp8AD9P4f8Hal6e0vtV6u4R9SyMs52N0/Gt+zm5r7XX7RY4CnZ+g9Kz2fpfUWZifWZ7bnYtgPUcl7i6sYjYrbWB/MfrLqX+tVt9SxbHUenXZFtWVi3fZ8ykOayxzfUZ6b/wCerfT/AGVRZ0LMqy8fKry2g1vdbe01iLbbh6WY+va79DXbT/g/8GmpTWfWPDaCW1Wv2uaHtDfexjvY7Kuqc7fXTVd+hVurqTLs+3Drqe4UtDn3gD0ZcN7KWWbv51YuX06/Azbs1j7MhuYx2OKWV+pYXWD9E/Lyt3sqqt/w3+i/RrY6L08dO6bTi/ntE2az+kd77tv8jekpy6/rU4Cx1/T72ht5x2Fm102f4OizdZXsyP8AwP8A4RW3fWGhjHbqLhe1pcaCALdzfddUxm/3+lT+n9T+a9NVcn6v577LfRzW102ZP2trDVuc26dzf0vqfzSJ1DoOXlZjcunLFNmxzHbq/U/nG+jc2nc9vp0bP8Gkps0/WHptrHWF5qrbWLWvsGz1K/5t76d3+hu/QWf8IqGT1nOysrHwKKLsA5Eu+0ObW8hjPfuqr32Vej/prER3QM3ZjluVWbKaDjWCykPqtpnfW19HqKdXQsunKxbmZYNWM0tNLmSNtv8ASqqLN++qj2/q9f8AgElMz9YsZlb3+lbYxrSabGgRlNr/AKXbie7/ALT/AM5aot+s1BO12LkMsfV61FbmjdfX9L9D6dj/AMz/AEqG76uXio01ZXp1Ute3CGwOdSy/25rL3bv1nfW706v9GoDoPVg5rx1Fm5lH2Vp9AfzP/b389/wiSm/i9fwMlu9m9rDX6rC5pHqNb/SPQb/hPsz/AOdQB9YRZmYdFGO+yvKLm2Pkfont9z6vb7P0P85ke/8Am0HH+r/Uarccuzwa8as0NDKgx/ou27/0m+z9L+jr/SoVH1b6ljWMfRnsYKn2WVs9EFoNw9O7/C/uJKbdXWg37U57brLW3+jViFrG2bmt37MVzP52v0/1n9KnP1lxBY2sVXOdYwuq2tn1H1/0vEZ7v5/E/wAMgt6D1H1LMizOa/I3+tS4VBra7tv2Z7n173+pT9m/Ren/ANcU6Og5FGZXczKBpa2wPqLBJdkfpM2yq3d+j9S/9JX/AKP+bSU38nqtFPTv2jW12RjwHTVBOz8+737P5r/CKn/zmwfULBXaQa23VO2aW0n+dyKv+Dxv8L6qTei5jehu6UMpsmWNu9P/AAB/wPp7/wCc/wCFVZ3QOrOc5x6gyX0fZT+gH8z/ACP0v86kpv5PX8KpxrrPr2+nvrYwj9I5w9Wmhn8u2r9L/wAWsyrr2Sb8LKt3tozabCMIMD3+rV9B+Na39J+s/wDCqzjfV6ym+q91zLbRX6V9jqxvLWt+z4rsJ279UfXV/O/6VRw+g9Qx78R781tlWG11TGeltJpf/OM9X1f532V/pElNu3r+Gxtb6g7Ia9otsNYB9Cn6P2rJ/wCDYqXXOtW11tHTnOc+t9ZttY1r6WttLfToyvU9/wCmrs9T9Emq+rWRVT6bMse5potJYPdhkuf9mb+5k/pLP1lFzfq7bYLK8LJ+zU3iv1mOZ6pLqNjcZ9Vjns9P+ar9RJTo0dTZkZj8emt1ldctfktg0i1v85iu93qeqoZunU+nH+VcPvrQundKvx8qzLyLmvteIIpZ6Fdh/Ovy6d9nr5H/AAqLm/8AKnTvjd/57SUm6ZlMy8Nt7Gem1zrAGf8AF2W4+7/rnpeoraq9ObitxGjFBbTusgGQd3qW+v8AS/4f1VaSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklP/0e4SSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJTSxMi6zNzqn/zdFlbatPzX00X2f8Agti8/wDrn/y9Z/xdf5F6TWbDZaHMDWgjY4f4QbWbnu/qWfol5t9dP+XrP+LZ+RJTgE8FRSnRNKKlwpSoSpSgpdPGk9lGVNh7JHZTD4JkQhzjAEBTLWtbIAkIWpE12hEweyhM6nlSJLuRr4poRVaycAlS/N0HHKQcIjxSUvtcG8RHfxVrpQ/yrh/8fX/1bFVLiQASYHAVrpX/ACrh/wDH1/8AVsSS+wpJJJIUkkkkpSp9R/m6/wDjWK4qfUf5uv8A41idH5giWxbiSSSalSSSSSlLP6nVkudjXY9frOot3uZuDCWltlXtc/8A4xaCSSnO+29T/wDK53/btaX23qf/AJXO/wC3a1opJKc37b1P/wArj/29Wl9t6n/5XO/7drWkkkpzvtvU/wDyud/27Wl9t6n/AOVzv+3a1opJKc77b1P/AMrnf9u1pfbep/8Alc7/ALdrWikkpzvtvU//ACud/wBu1pfbep/+Vzv+3a1opJKc77b1P/yud/27Wl9t6n/5XO/7drWikkpzvtvU/wDyud/27Wl9t6n/AOVzv+3a1opJKc77b1P/AMrnf9u1pfbep/8Alc7/ALdrWikkpzvtvU//ACud/wBu1pfbep/+Vzv+3a1opJKc77b1Lv093/btaX23qfbp7v8At2taKSSnN+29T/8AK53/AG7Whj7dk9QxrbcU49eP6hc5z2vkvb6e1ralrJJKafS8w5uE3ILQzc6xu0f8HZdjf9P0lcVbB9D7MPQrNVe58MI2677PVdt/4W7farKSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkp//0u4SSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJTRxLb35ueyyfSrsrFMjTa6miy3Z/15y4D65gnr1gH+jZ+Rek1m71Ld4AYCPSI5LdrN/qf9e9Rec/W9j3fWCzY0uIrZx8ElPNuZtGp18E3KM+rJrmxzC0eKG5w3B30j3B4SSw7pypl7S2NgafEKGqSFSnYdUwlFFRIklAlTIOTPfDfNRLdvOqQcPBCkgI5EeaW4ooe0/m6qJDD2gp1p4fFjuMfHlRTkFSYzckil6q951+4LQ6dTWzqmFsJk31zP9ZirVk1zHJET4K509wb1DBAEk3sk/2mJtm11aPrKSSScsUkkkkpSp9R/m6/+NYrip9R1rr/AONYnR+YIlsW4kkkmpUkkkkpSzeq73uxKG2PrbddteazscWhltm31P7C0lndS/pPT/8Aj/8A0Xckpb9jt/7mZf8A28f/ACKX7Hb/ANzMsf8AXj/5FaSSSnN/Y7f+5mX/ANvH/wAil+x2/wDczL/7eP8A5FLqHUsjEy8XHrxTe3KdsFgeGbHD3v3Mc3/QfpVXPW7m5OXRZjNqGLtDLLLWsbc+z+i1N3M/R+vV+kSU2P2O3/uZl/8Abx/8il+xx/3My/8At4/+RRcvqvT8IH7Xeyp4bvNZcPUj/g6vp2Ig6jg+lTd67BXkEClxcALC78yvd+ekprfsdv8A3My/+3j/AORS/Y7f+5mX/wBvH/yKt35uHjOazIvrpc+S0Pc1hcB+7vchs6n06xxazKpeWt3uDXtMMH+E+kkpB+x2/wDczL/7eP8A5FL9jt/7mZf/AG8f/Iq07PwmeoXX1t9GPVlzf0e/+a9X3ez1EVl1Vhc1j2vLI3BpB27hvZv/AK7ElND9jt/7mZf/AG8f/Ipfsdv/AHMy/wDt4/8AkVfFtbrHVtcC+uC9oPuaHfQ3t/lquzqnTbHtZXlUue87WtFjS4u/ca3ckpB+x2/9zMv/ALeP/kUv2O3/ALmZf/bx/wDIq03Ow3VusbfWa2O2PcHDa187PSf/AMIql3VvQ6k7DuqFdYpN7MhzwGEM2ttbb/ofppKX/Y7f+5mX/wBvH/yKX7Hb/wBzMv8A7eP/AJFTZ1XEbVW7Lupx7bGh/pm1jtHfQcyz2+qxTy+q9Pw6vVvva1gcKyQd3vd7tns/kfpP+LSUh/Y4/wC5mX/28f8AyKX7Hb/3My/+3j/5FHbnMfmfZ6yx7BX6j3h7dzJ2+l+r/T9K2v3+spV9QwbBW5mRW4XEtqIcP0jm/wA4yr9/Ykprfsdv/czL/wC3j/5FL9jt/wC5mX/28f8AyK0K7a7WCytwex3Dmnc0/wBpqkkpzf2O3/uZl/8Abx/8ig/ZzhdRxG133Pbf6jXtsebAdrfVY73rYWdm/wDKnTvjd/57SUk6TmWZuCzIsAa5zrGwOIrtuxmf9CpXUDDLTjgtqNA3P/RkRHvs9/8A17+e/wCuI6SlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkp//T7hJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklNHDOR9tzxZu9IWV+hPG30aPV9L/r/qLivrVa5vXbQNP0Vf/fl37Bd6lm8g1kj0gOQ3a31PU/696i4D601sd163d/oq/wAjkDsoOJda59D2zIiVQDHOAIWhke2ktqGrtD8Fm68apBJZOY5mju/CZrSRKlW3e8NcYHKsVH2AwIbwkTSmDMUmC921pHKiQ9v0vocB3ijGHe1p3iJjwUGh2yH+4D6I8Cm33VWiO0gxHzQxrMdkd7HOku9r+w8UDUfFOGyU2MAXEd+QnvqIh7R5EIdVkXNJ0HBWn6YMt8pTJHhla4Cw5PCJU4TtPfhK5zbHAt+aZo1Hgn9Fo3bO3hXemgftHD/45n/VNVNp0VnptjR1PDE/4dn/AFTU1l0p9YSSST2BSSSSSlKj1VgfRW0ktm1mo5V5U+o/zdf/ABrE6PzBEtir9n/8Pb/nJfs//h7f85XEkuI91cIaf2D/AIe3/OS+wf8AD2/5yuJJcR7q4Q0/2f8A8Pb/AJyp5eP6OXgH1Hv/AExEOM8ss/8AILYWX1e2umzButO2tl8udBIbLLme7ahxFVB1Elnft3pP/cpn4/8AkUv270j/ALlM/H/yKCWj9YMa/LycGkYtmRjMs9S57HCvaIdQ1u7cy32ep6yyL+nZlQ6liY/TbLKMl1foOdY14DqfpZG+9/rfpP8ABLpf270j/uUz8f8AyKX7d6R/3KZ+P/kUlOG7AzcxnUCcN7PXrcWtydlj23kbaq+m5Ff83R/xv/W1F2Jn2sFjunuDH44w2UOLD9ns/wDLRjfoMp93+D/Tre/bvSP+5TPx/wDIpft3pH/cpn4/+RSU85lNa3Msx8uh2SzEqpFx3VtGWWN3+r+vfpvY/wD7h2fpf8Mp4ODbf03Kezp3oWPudfjlwa2w1WfQqxv3Lqdv81f+rrXyc76t5T2PyX03Pr+g5zdxb+d7fYrI650g/wDaln4/+RSU8qzB6jtcbekvfacU44eX1ndaS/8AXMhm73/T/wCtrQ6NRm43UMb/ACfbj0Nx/QucXscDaD6v2q7Y/wDS/wCjW0eudJHOSz8f/Ipft3pH/cpn4/8AkUlOGOi9Q+1Z9Zfd6mSyw0ZAcPswbY5r2UW/9qvtH+D/ANHWqYoGX1DIYzFZi5eMzHsZQdnqWGhzrr/s76f5t99ez/wP1V1H7d6R/wBymfj/AORVZud9W25JymvpGQZm0N9+vtd+k2JKcd+H1HIp6ht6e+j17BfVLmbt230P0Vbf0frb/wBL+k/8+p2N6r6uO+3pltgpxHYtgc+t3qOfs/Se9/8ANfo10H7d6R/3KZ+P/kUv270j/uUz8f8AyKSnl/sWd9lxandFNl+O1gfc51Z3mk7sar6Tv1T/ALkpZXTeqb7qz043NsFTgGvZ6NT2ObkZLcX1f0my1n6D/wBRLqP270j/ALlM/H/yKX7d6R/3KZ+P/kUlOFl0dSz8tzqunPwzbjmg3Ocxu2HMyP0noO/mtlX2dBy+n5d5F37Md6N1tR+ztcxj8duP7clzNns/X/8Agl0f7c6R/wBymfil+3ekf9ymfj/5FJSDpFOTjZedS+l1eO631cd8j0tjw39BVU3+a2LYWd+3ekf9ymfj/wCRS/bvSP8AuUz8f/IpKdFZ2b/yn049913/AJ7S/bvSP+5TPx/8iqzs7EzOq4IxrRaa/Vc7aDoCzY3ckpudHyrsvAZfdHqOfa0wI0rtvx6//A6leQcVz3Ugvq9F0u/R+Huftf7f9N/PIySlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkp//U7hJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklNHDZkNzc91k+k+ys0zxtFNFdvp/wDX2rhfrc8t6/ZB/wAHX+ReiVtsFlpc8OYSPTaOWDazex3/AFz9IuT6/wDV7Nz+qPyaqw6ssY0HcBq36SEjQ7+SQHjC97h7BB8SVXsreyHO03LpLvqn1dsenRvJ772iP+kqx+qvXPzsQujiLGf+TQib6FJHi4bC0NJn3nQHwCds7tug8fAraP1W66SA3D2gdt7P/JqH/NPr8/0b/ps/9KJy1yRInaY8wnFrm+2drTytdv1U6+3/ALSyPDez/wAmpH6r9Z/wlAqb/Kc1wP8A22mmhuusV2cd9pfLfpD95J+oG0bRHJWv/wA1up9tg+aR+q3X3NAONuA497P/ACaAo7IsdwXChpVuq8NreXk+rEM8P7S0h9VOugaYv/TZ/wCTUh9VuvgQcUOH9dn/AJNEi0g04jW+EEeKYuDRAGq2j9VvrATP2WB4b2f+lEnfVXrzv+0v/TZ/5NFVuOH7h4K30v8A5Vwj/wAPX/1TVbb9VOvj/tL/ANNn/pRXenfVvrVXUcW27G2112tc929phrTu/Nekm9NX0ZJJJFYpJJJJSlT6j/N1/wDGsVxU+o/zVf8AxrE6PzBEti3Ekkk1KkkkklKSIB0IlJUuoZWRR6FeO1htyLPTBsnY32vt3fo/f/g0lNza3wH3JbW+A+5Z89d8MX77Up674Yv32/8AkUlOhtb4D7ktrfAfcs+eu+GL99v/AJFKeu+GL99v/kUlOhtb4D7ktjfAfcs+eu+GL99v/kUp674Yv32/+RSU6Gxn7o+5La3wH3LPnrvhi/fb/wCRSnrvhi/fakp0NjfAfcltb4D7lnz13wxfvt/8ilPXfDF++3/yKSnQ2t8B9yW1vgPuWfPXfDF++3/yKU9d8MX77f8AyKSnQ2t8B9yW1vgPuWfPXfDF++3/AMilPXfDF++3/wAikp0NrfAfcltb4D7ln/5d8MX77Up674Yv32pKdDYz90fcltb4D7ln/wCXf+6v/gqX+XfDF++1JTobW+A+5La3wH3LPnrvhi/fb/5FKeu+GL99v/kUlOhtb4D7kg1oMgAHyCz/APLvhi/fao15XUas2jHyxSWZAftNW+WurHqe71UlJOjX35HT2W5BJtL7QZEaMuvpq/8AAq1fQsY3OpBvYK7JdLW6iNz/AEv8+r3oqSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkp/9XuEkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSU0sSi+vNzrbP5u6yt1WvZtNFFn/gtauoVbSLbibN4LhDP9F7Wfo/+ufz3/XEVJSkkkklKSSSSUpUeo8M+JV5Ueo8M+JTJ/KUS2c88Lao/mWfALFPC2qP5lnwCZi3K2G6RJJJTL1JJJJKUkkkkpSSSSSlJJJJKUqfUf5qv/jWK4qPUy4UMLGh7xY3a0naHH+snR+YIlsW8kqP2nqf/cJv/bzf/SSX2nqX/cIf9vN/9JJcJ/q/48FcQ8f8WTeSVH7T1L/uEP8At5v/AKSS+09S/wC4Q/7eb/6SS4T/AFf8eCuIeP8Aiybyzupf0np//hj/ANF3qf2nqX/cJv8A283/ANJKpk25b8vAF9Apb6xIIeLDOyzb9FrEjE/1f8aKrHj/AIrspJJJqVJJJJKavUM1uBivyn1vtZXq8VgFwb+db73M9larYvWqr7mU20XYjrhNBvaGi7Te5lPp2W/pPTT9ee5vSMlrK32vtYa2srbvdusHp/Rb+YsHHovxmMtLcrJ+0VbMF9g32YV7mupdVdS7+j/+Hf8ARJKeqty8ak1tttaw3O2Vgke9/wDo2JmZIdkPp2xsA2vJbDyfp11t3+r+h/wi40YVjMei7LpuzLMXKHrtFRHpt2u9f7HW3+mUXZPp225H+FQ81lORn25NfT8uv1baXtubW5j2Nb/yi+v/AEdlySnvN7YmRB0mdExsYNwLh7dXaj2/11xmXj54bfi1C6nExsg2AtYbR6FjW/ZfsNX85fdTk/pv5z9B/OI/UcYPttLMa+4tx6/ttm0sOayav0FW3/tYz+et/wCKsoSU9Z6jNfcNBuOo+j++nDmngg6Tp4H85cj1FmL+0sW0YWScYYxba2qt0WNeG/ZMHJa38yn/AAv/AFtU+n/aqGYtWPj5deU6u2q6xzCGB7x/k5nq2f8AaTFs/wC2klPaXZmPUWNdY31LZFLJG61zR/N07kLp3UKs/HF7GOq9zmmuwBr2urPp2+33LlK8R1LMPJyabst2LkkXAVFrqoZ9HFob/P4v2z9P9o/wliliNyKqxnNoyRf9tebAWneMfI9TZ6FLvZ6G/wBH7T/wqSntA9riWgglujgDJH9ZUqep029QuwNrm2UMFheY9N7He3cx7Hf+fFzmNhdTLnMxWvx81lFjM29/tbkZDxuwrqrvf9o9P/Tf4JVoZVZe2jFysMXY7KrLmUOsm9jt+Tvq/wALVkfzfq/4RJT3LHte0OYQ5p4IMhOqHR6nVYFbXY7cQ6n0mfR1O71Nn+C9X/Q/4JX0lKSSSSUpZ2b/AMp9O/rXfd6a0VnZv/KnTvjd/wCe0lL9FfkP6cx2SXG4vtDt30oF17av/AfTWghY/r+kPtG31ZdO36O3c70f/AfTRUlKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJT//W7hJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklNLExbaszNufGzJfW6seTKqcZ+7/rlSuoNIZ615bZvcXN3Mn+aOyv8AR/8AXGfpv+uKgKX3X3zdYwMdADToiBaCXVSWb9iP/cm7/OS+xO/7k2/5yNDuq/B0klm/Yj/3Jt/zkvsTv+5N3+clQ7qvwdFzQ5paeCs6/CtbrW42N/dJ9wS+xO/7k2/5yb7Ef+5Fv+cmyxg7lB16IG03PO1rDPmIWhj4hrIe95c4cNn2hVvsX/di3XnVP9iP/ci3/OTY4ojW7QBXR0klm/Ynf9ybv85L7E7/ALk3f5ykod11+DpJLN+xO/7k3f5yX2N3/cm7/OSod1X4Okks37G7/uTd/nJfYnf9ybf85Kh3Vfg6SSzfsTv+5Fv+cl9id/3Ju/zkqHdV+DpJLHyaH01CxuRaTuAgu7ErYHCBFKBUqfUf5qv/AI1iuKn1H+br/wCNYjH5gqWxbiSSSalSSSSSlLM6rZXXfgPscGMF+rnGBrXd+ctNQsqqtbttY2xvMOAcP+mkpH9twv8AuRV/nt/8kl9tw/8AuRV/nt/8kofs3p3/AHFp/wC22f8AkEv2b07/ALi0/wDbbP8AyCSmf23C/wC5FX+e3/ySX23D/wC5FX+e3/ySh+zenf8AcWn/ALbZ/wCQS/ZvTv8AuLT/ANts/wDIJKZ/bsL/ALkVf57f/JJhnYR4yKv89v8A5JR/ZvTv+4tP/bbP/IJz07APONUf+ts/8gkpf7dhf9yKv89v/kk/23C/7kVf57f/ACSh+zenf9xaf+22f+QS/ZvTv+4tP/bbP/IJKZfbsL/uRV/nt/8AJJ/tuH/3Iq/z2/8AklD9m9O/7i0/9ts/8gl+zenf9xaf+22f+QSUy+3YX/cir/Pb/wCSS+3YX/cir/Pb/wCSUf2b07/uLT/22z/yCX7N6d/3Fp/7bZ/5BJTP7bh/9yKv89v/AJJL7dhf9yKv89v/AJJQ/ZvTv+4tP/bbP/IJfs3p3/cWn/ttn/kElM/tuH/3Iq/z2/8Akk327C/7kVf57f8AySj+zenf9xaf+22f+QS/ZvTv+4tP/bbP/IJKZ/bsL/uRV/nt/wDJJvt2F/3Iq/z2/wDklH9m9O/7i0/9ts/8gl+zenf9xaf+22f+QSUz+3YX/cir/Pb/AOSS+3YX/cir/Pb/AOSUP2b07/uLT/22z/yCX7M6d/3Fp/7bZ/5BJTP7dhf9yKv89v8A5JUMjIx7uqYDarWWOabXEMcHabNn5iufs3p3/cWn/ttn/kESvExanbqqWVu4lrWtP/Qakpq9Fbkt6cwZO71t9u7f9KPWv9H/AMB9NaCFji5tQF7g+yXS5ogRud6X/gSKkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKf/9fuEkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSU1MXEfTl5l7iC3KexzQPzRXVTje7+3UhY/8AP5P9f+Cs0eh6+R6by6zc31Wz9B3p1em3/tn07FXxdcjJHi+PwRHVB6J0lk9CGS5+eL8mzIFOQ6hgsIhrK/f6n/GfpEN/1hYGG2vFdZU8OOI/cB9p9H+mez6eN6O3/CfzqNqp2klh5P1jFJGzDdc37OzKJbY0Ftdu1v6Wt3/CPReqZhs6Pk2UXfZszGYLLK6nh9lT/pfZshzULVTrpLGHVDVl3+sy13o41VjWgh1Vvqf9qfT2/qz/AFv0dtv+i/SKjb1rNszcO4U2044tNFgqe2+jJc5u+qvH2fz/APx/+DStVPTpLFr63kX9SxsanF/QWh4uc5w3120nZlbP5GI//t//AAa2kVKSSSSQpJJIlwBLRucAS1sxucPos3JKUkuewuvWx6LsO99tgtso32NeXuqc5uTjvtb/AEemj+br9RXMTr2PkhjvQtrFtRtpkbnW+n/S6amM/wBD/pP8Mkl1UlkN6/jvwxkV1l1m81OrLgyqt7ff+n6g/wDQV/ov/BP0SEPrLjsZW7Lx34nqmWh7gSaj+j+1Vtb/ADv6X/B/6P8ASpWp08/+i/22/lWiOFndQEY0fy2/lWiOAkdgobqVPqP81X/xrFcVPqP83X/xrEo/MFS2LcSSSTUqSSSSUpV8vMqxGsLw55sdsYytu97nRu9rP7KsLO6l/Sen/wDH/wDou9JS/wC1mf8AcbK/7Zel+1q/+42V/wBsvWgkkpz/ANrV/wDcbK/7Zf8A+RS/azP+42V/2y9aCSSnP/azP+4uV/2y5L9rM/7i5X/bL1oJJKc/9rV/9xsr/tl//kUv2sz/ALi5R/6y5aCSSnP/AGtX/wBxsr/tl6X7Wr/7jZX/AGy9aCSSnP8A2tX/ANxsr/tl6X7Wr/7jZX/bL1oJJKc/9rV/9xsr/tl6X7Wr/wC42V/2y/8A8itBJJTn/tav/uNlf9svS/a1f/cbK/7ZetBJJTn/ALWr/wC42V/2y9L9rV/9xsr/ALZetBJJTnHqzP8AuLlf9suSHVmf9xcr/tly0UklOf8AtZn/AHGyv+2XqVPVKbchmOarqX2Alnqs2B236e1XlnZn/KnTvjd/57SUy6NTkU9PZXkgi0PtLpMmHXX21f8AgT1fQcVtjaQLLPWdLveO4Ln7Ge3/AETP0SMkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKf/0O4SSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJTVxsX0cvLv3bvtL2O2/ubKqsbb/4EhY2l+Uf5en3I9Dsc5GSKj+la9ovH8r06nVf+y/pKvj/z+V298k/JEdUHo5tPQLqrN37Sucx132iyva1vqWfnb7Ge/ZZ/o1Yx+jY9GS27e59FJc7GxiIZjut/pDq3/wCF9Td/hFB/1i6WwB5LzXv2OsaA5rGu/mMq3Z/2nyP8EgZHW3MysS2t5bh2PfRkYzmfpm3NDrKPo/pPUyP0fpJKT19BxaxSBY4mm43boE2NJ3Nwrv8AutUrGf02nLxrqGEYpySPWtra3fY0f4O1RPWeniumwvcG3yIjWot9ljcr9x9d36D/AIxDxuu4GRY2oCyqwvNTha3b6Vo/m6ch37+R/gElOfl/V7OLH20dRttyNjWFpa1gspqO9uN7Pz//AD5/hU2L03qWVfVbZddiY2I4PpqtZU2z1fz/AE2YrfS+y+m9ajes4ZbdZtsFFMBuQQBVe+fT9HCs3fpLPV/RoTvrF0/eWNbdY70zZXtbu9YN/pFWM7/S43+GSUhP1ee3KOTj9QtoO5762BrXNr9c+rlMbv8A9I9bfh3Pc+P8pZ2P1zpmQ0vZYWMFfqh7xtDmD23+lu/PxrP0dyF/zh6dvrZFpNoOza3cPVb/ANoP0f8A2rSU6ySyqOt05HUmYVNNjq7K/UbkR7ZH09//AAVf8z6n+nVnN6ljYFlDckPDMg7W3NG6tr/zarvz/wBIipuJiCWuAO1xBAcNdpP+E/sLNx+u9PvsFUWVWeoanixu30rP8FXe7/uz/gEHN+seHj1PfQx2U+t7WFjYjYT6f2n27/0fqfoKv+HSUir+rVtRDq+p3Bwa9k7GH23H1slvu/0tv6RSp+rrq30F3ULn14zTWysNaz9E7+co9Wv3+/anp6yaM3Lxs+0vDH1mohm001XN3/rjmfo9lNlldCu/tbDOX9l90F3pjJj9X9ce77F63/chBTjZPQMvFAsxcm3Io9Q2XYrWs3OLx6fqUVW/q9uz/hv+MVrH6Zl5ddeRlPOPk0nbTa9jH3ux53Ox86j+i1v3/wA1ZSnd1t7usY9dLnN6c9tosse0Nps9EOe/IxMv+c9j/wCcWnh5rM1jrGV2VsBhj7AA25p+hfjOa5/qUpKX6iZx5/lN/Kifacof9pXf5zULP/ov9tv5VojgI3oNLR13an2rK/7iu/zmqrnX5Lq65xy39I2JIK1VT6j/ADdf/GsRiRY0CpA0dV/tOV/3Fd/nNS+1ZX/cV3+c1W0k2x2CaPctT7Vlf9xXf5zUvtWV/wBxXf5zVbSSsdgqj3an2rK/7iu/zmqnl23WZWALKjWPWJkkHXZZ+6tdZ3Uv6T0//j//AEXclY7BVHu3Mmll+PZTZOyxpa6CWmD+69i4TEreenuyKqbaMvCrblNybLLHMyWsc71q/T3el6T6q13mRS2+h9Ly5rbAWksO10H9yxqz29Bwm9Od00Ot+zOMxvO4D/RNf/of+CQS57frBm0241GXSx12SG2tbjh9n6Cz2M+l/wBqKrP53/A+mlk9c6mXh+FVQcZ97sas3FzX+rXv9Sy30/0fofobPTWl+xMQVY7GvtY7F0rta8i3ZO77Nbf/AITG/wCBWd1To2T9vx8vptbSWW+rdW+wspLo2ezF/m/Wu/wl6SmVXXsktxsi2lv2W/Fsvfsk2NfR7rms3O2el/o0On6wdQv+zPpxWvryXOqBO6txsDfXZk1er/2hrr/n1d/5t9NOQ28+pLSS2refRYH/AM7j14/0Ps9n+EpTj6u4Qvrv9XILqHb6QbXFtf8AwdNf5lX+D/4tJSBnWszKFFGFUwZdrXusNxcKAKH/AGTJ9F1P6Wz9P/Nf8Go/tjqZH2L0aR1f1P5kl3o/Z/pfb/U/c/8ABf8Ag1ZP1ewA15q9RjzYbmubYWOa53vsors/weLc/wDnKk/TsDI+2XdSzmMZlXNbW2th3itlf7l7v9OkpFkdZZk11Y+AG3XZbrKmmwPrqHot35XqfRyPofzPprN6Xn52N07HxamtsvY26+42lxb9nptfRZTjWN9/rf6H1F0eb0/Hzawy3c0sO6uys+nbWfzvRuZ7696oV/VrptVYrYbQA/fu9R26D/O4+/8A7i3/AOGp/wAIkpov+sPUjVk3V41ZrprryWPLjH2e33tx7v8Au/s/60rf7ayxXkizG9PIprZcxhMtbVb7Wfav+Hpeyz1aqv8Araez6r9LsfY79Mxt0epWyxzay0fzdXpN/wAFV/gq1Or6tdOrtNpN1j3N2E2Wuf7f8H9L/Qf4D/RpKc/I6rmZVfT8zDoqcLRY5nrF7Cy2ptnru/RO/SUWVVWelvV/pvWb8q+mrIx/Q+00+vSAdxDWnZb6/wDxu/8AVv8AwRDH1T6WGtZuv2sksb6roaX/AM76f/G/4RWqehYVLsV9ZsDsLcKSXknY/wBzqLf9JR/waSmjm9Z6pjX51Lcep4xKxey2XBnon/A5H/duz07PS9NSf9YLaMe+3Lx/Scypl9bR74Zcfs9bcr/hPtH+j/wKPkfVvp+TfdfY+7dkfzoFrmte3/QuZ/oWf6NNV9Wum12eoTdYdnpxZY542f4Nnv8A9D/gf9Gkpzzn5/Ur20vaK8DIxbHvrO6u6az6X2mv/C1/rHp/Zv8AgE/S+t3Y3TqPtdLvTGKbq3Tvte2j23WXfyLvUq9BaVv1fwrm1Cyy8vpY6sW+q4Wured76b7f8KxJv1e6c1mPWRY5uLu9MOeXeyz+dxrf9Jjf8Ekpq/t7IobZVmVM+1RW+kVE+k5uS70MT1nWfpGPrt/n1QxcvN6Y3PFrW+vZktY2N9mLQ6xjbvWud/SKcX3req6JgV0W0lrrG3fSdY4vsaB/M103O99VeP8A9p/9EqzfqzgsLnNtyZsI9X9M79LH/cn/AEqSnWx3WPordYWmxzQXGszXMf4Fzv8ABqlm/wDKfTh/KuP/AIGr9dddVba62hlbBDWtEBoH5rVQzP8AlTp5/wCOH/QSUy6PjXYvT2UX/wA419pMGdLLb76//ArVfQMRobQALfXG5/6QmZ97/Z/1n+Y/62jpKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSn/0e4SSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJTVx8ZlWVl3NfudkPY57f3CyurHa3/ADK/UQccA3ZQPBdB+YR8e3Hfk5TKhFtb2i8/vONdVlX/ALLvrVWq6ll+SHvDSX6Apw6oPRx/+bmXVV9nw7serHbY20F1ZddYaz6tDM63dsu9N6sO6LnDqYzKsmoVeuMg1OYS7eWNxshvqf8AF/zK1vtON/pW/el9pxv9K370qVbh3fV3Ksxn47ctm0XuuoDmSGB7/tH6T9/J9REHRepGixr8uo3uyW5bLBXDfUHstZaz89n+jWx9pxv9K370vtWN/pW/elSrcv8AYT3YVvTbLmuwdHYzYIsrsDvX/WX/AOFo9ZVx0Tq7bG2V5OLTspdjsrZU4VsrsO+19Xv/AJ/etz7Tjf6Vv3pHKxQJNzQPilSrcJn1ez3CmnIy6nY1NLsYtZWWvdS/b6nv/wBP+jrRcbomfRkYr23YzMfDc4tqrrLd4sHpXWWvc53616P+FWt9swv9Oz71L7Tjf6Vv3pcJ7FFuLi9F6tiZDbKM2k11tdWxj6yT6L7PtTmO9387vV7q2BmZrsY4t1dLcaz1ttjS/dY3+a/O/m1c+043+lb96X2nG/0rfvSpNuEeh9ZIcPttEvyBlk+kZ9Yfm/S/mFP/AJuOZRk0UWVVi5zLK7dh9bex7cl7cp/+Ex/V/mltfacb/St+9L7Tjf6Vv3pUq3GPROo2tzvtGVS52cGGWMLdllJZ6H0nO/Q/ov0iLV0S0WMF17X4ws+1WMa3a92ZG2x9b/8AuH/wS1PtON/pW/el9pxv9K370qVbiu6F1Bj6W42VSMbFNnoVW1l523/0inL936X6a0el4D8HHdW+wPL3btjJFFX/AAeFU/c+qtWftWN/pW/el9pxv9K370qRaPP/AKL/AG2/lWiOAsrOyKHY+1tgLi9ugPmtUcBI7BIUqfUf5qv/AI1iuKn1H+br/wCNYlH5gqWxbiSSSalSSSSSlKtm4NeW2sOe+t1Tt7H1na5rocz+V++rKi+xlbdz3BjfFxgf9JJTn/sq7/ywyv8AOr/950/7Lt/7n5X+cz/0grn2rG/0zP8AOb/5JL7Vjf6Vn+c1JTT/AGXd/wCWGV/nV/8AvOm/ZVv/AHPyv89n/pBXftWN/pWf5zUvtWN/pWf5zUlNL9lXf+WGV/nV/wDvOl+yrf8Auflf5zP/AHnV37Vjf6Zn+c1L7Vjf6Vn+c1JTS/ZV3fqGUf7VY/8AddL9lW/9z8r/ADmf+kFd+1Y3+lZ/nNS+1Y3+lZ/nNSU0/wBl2/8Ac/K/zmf+86b9lWd8/KP9tg/6ihXftWN/pmf5zf8AySX2rG/0rP8AOakppfsqztn5Q/tsP/VUJfsu3/uflf51f/vOrv2rG/0rP85qX2rG/wBMz/Oakpp/su7/ALn5X+dX/wC86b9l2/8Alhlf51f/ALzq79qxv9Kz/Oal9qxv9Kz/ADmpKaX7Kt/8sMr/ADq//edL9l2/+WGV/nV/+86u/asb/Ss/zmpfasb/AErP85qSml+yrf8Aywyv86v/AN50v2Vb/wCWGV/nV/8AvOrv2rG/0rP85qX2rG/0rP8AOakppfsq3/uflf51f/vOn/Zdv/c/K/zq/wD3nVz7Vjf6Vn+c1L7Vjf6Zn+c1JTS/ZVv/AJYZX+dX/wC86nT0tteSzIsyLr31AhgtcHNG/wBr/axlatfasb/TM/zmp2X0WHbXYx58GuDj/wBFJTW6TiWYWCzHtIL2vtcSOIstuyGf+B2q6q+CKRjD0bDdXufDyd2u+z1Gbv8Agrf0SsJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSn//0u4SSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJTWx6KK8nKtrdusuew2t/cc2uqmtv/AGyyuxFdj0OJLq2knkkBCx76bMnKqY3bZS9jbXfvl1dV1bv+2rPTVlJSL7Ljf6Jn+aEvsuN/omf5oRUkrVSL7Ljf6Jn+aEvsuN/omf5oRUkrVSL7Ljf6Jn+aFhfWSqpjKNjA33HgQuiWB9Zv5uj+sfyKXCf1kVmT5S844DaV2uDj0OwqC6thOwctHguLd9EruOn/ANBo/qD8in5n5Y/3mPFufJJ9lxv9Ez/NCX2XG/0TP80IqSp2z0i+y43+iZ/mhL7Ljf6Jn+aEVJK1Ui+y43+iZ/mhL7Ljf6Jn+aEVJK1Ui+zY3+iZ/mhL7Ljf6Jn+aEVJK1IhjY44qZ/mhFSSSUpU+o/zdf8AxrFcVHqjmsprc87Wi1slOj8wRLYt5JVf2jg/6Zv3pftHB/07PvQ4Zdiqx3DaSVX9o4P+nZ96X7Rwf9Oz70uGXYqsdw2lmdWrZZdg12ND63X+5rhLT+ju+k1Wf2jg/wCnZ96o52Xi2ZGCWWBwbfJj+pZX/wB/S4Zdiqx3Dc/ZPTP+4lP/AG23/wAil+yel/8AcSn/ALbZ/wCRVxJBLT/ZPS/+4lP/AG2z/wAil+yel/8AcSn/ALbZ/wCRU+oOyG4dz8Z7WXMaXMc8bme33+9qxOn/AFnxz08W5znHJaGuua2ot2Ms/m7/AE/+4jP+5CSnY/ZPS/8AuJT/ANts/wDIpfsnpf8A3Ep/7bZ/5FCHWunuptubYS2mJAHufv8A5h9Ff+FryP8AApsjrmDRUy2X3Nez1Ipb6hbUPp33bfoVsSUm/ZPS/wDuJT/22z/yKX7J6X/3Ep/7bZ/5FCxet4GS6prHOY64SwPGwz+bU/d/hbWfpaf9JWqfWus242TXg47bG3WNNhuFRvAYz3PZVQ1zPW/4T/QJKdH9k9L/AO4lP/bbf/Ipfsnpf/cSn/ttn/kVXp65g2OxmhzgMqt1tdrm7a4r3ev6j/8AB2V+mg5XW6vsbrqvVoabW1MufVvZ7j7chrd/6TEt/wBOkpvfsnpf/cSn/ttn/kUv2T0v/uJT/wBts/8AIoNvW8Kq6yl29xrB97W7q32N+nhY9n+Fzf8AuunwutdPzXsZRYS6xu5u4Fsx/O0N3f8Aaij/AA1SSkv7J6X/ANxKf+22f+RS/ZPS/wDuJT/22z/yKpO+svTGB/qGxjqrPSua5h3VT9G+9v5mN/wqsjrOC6h94eS2t2wtiHkn+b9Op3+mZ+lq/wCCSUk/ZPS/+4lP/bbP/Ipfsnpf/cSn/ttn/kVk5v1ge/7ZTgAssxKhkNve3fTawe6xjf6/+BtV/H63g24zrjZHpsa5+hG7eP8AtLv/AJ5j7f0H/Hfo0lJ/2T0v/uJT/wBts/8AIpfsnpf/AHEp/wC22f8AkUEdaw3YpvG9rg70/Qc39ZFp+hT9j+n6r/53/i1Q6b1556f62aHW3m59TK6mfpntrP8AOfYt2/8AR/4f/RpKdX9kdL/7iU/9tt/8il+yel/9xKf+22f+RVtp3NB8ROvKdJTT/ZPS/wDuJT/223/yKqXYuNj9UwDRUyou9UOLGhsgM3e7YtdZ2b/yp0743f8AntJSXpeG7Cwm4znB5a6x24cfpLLsn/0arirdPOMcVpxXF9O58OOvu9S31/pf8P6qspKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSn//T7hJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklNbHrxm5OU+ozbY9hvHg4V1Mq/9l/TVlVcfJZblZdLWbXY72Nc79/fXVktf/wCC+mqNudlDq92G1wFYoD6xEn1PckoB2ElxGV1j6x42O3KdbW+l7i0BtYL2kf6RiIzrH1i+yjIcWuY/RjgxvP8AKYkdBdhdwF7NJclR17q4pDLqC6530HhvA/4atqC/6x9UqBFm0OHfZ4oXrQFp4DVvY2B5Y4VkNeR7SRIBXKdXPVBtbnNDq2H22sHsP9f/AEarj60Z52jfXLdTDdbB/JV6r6xEAtyQLQ9pdtgDb/wb1LjmYEHh4v8ApMc8ZI3pxSQdBqTwByV1HSR1dzazdtqxmCAwj9I4fm/8WuPHWOo0WWXUPZUbDLGmtruf9Hu/m1Yf9ausvraxj2ssq/nLNgPqE/yHfQU+SZkABEa/pT/QWQxkak/4r6CkvPP+cvXXvkXtYPA1t0SP1l6/W/bZkM1Eg+m3hV/bPgyPoaS4bB6/1PIba67MFbq49GttTCb3H97d9BFo+tGdTkbeoeypgmGsG+2f3E0iiR1C8QkY8X6G3F+i9okuU6n9ZLLMFmR0uwVvFgbayxg37T/hGMf+YhYXWuq/baKL7m2tse0OLWNAh39VNsJhjMiRcY0OL1PYJJJIsakkkklKVPqUelXIn9KzlXFT6j/NV/8AGsTo/MES2La9Kv8Acb9wS9Kr9xv3BSSTbSx9Kr9xv3BL0qv3G/cFJJK1MfSq/cb9wWd1FrBk4ENA/T+H/B2rTVHqONk3fZ7MbYbMez1NtkhrhtfVt3M/4xJTeSWd6vXP9Bj/APbr/wD3nS9Xrn+gx/8At1//ALzpKbGfVkXYd1OM5rbbGljXPksG72P+h/IXOf8AN/rfoXUnIxiMiqul7tj9za6BsqbX/X/wq2/V63/3Hx/+3X/+86Xq9b/7j4//AG6//wB50lOa3ovV33X2X3Y4bkUtqcytjgP0X9HZ+k+hV7/03/gSyL8PqXTBj0ZjqH4lDT9nsFdtjRdu9XZczF/T/wDbv6vZ/wAYup9Xrf8A3Hx/+3X/APvMm9Trn+hx/wDtx/8A6RSU52L0y3J6hR1h9LGPtDTcy7cX1msenTf07Z/N+sz/ALk/zau5eD1C3q+PmV2VDGoa5hrcHeoW2/0n3t/qV+iiCzrnejH/AO3H/wDpFP6vW/8AuPj/APbr/wD3mSU5J6D1Wt1FVFuO/FxPUFLLmvc6xl4231Zuz+v/AIJFd0XqY6UcOm6pj32NcWHc6iljD6rMfB377/5xlf8APLR9Xrf/AHHx/wDt1/8A7zper1z/AEGP/wBuv/8AedJTn/sPqAO1t1Qa2w5dZ2ncM1385u/81/6Sz/hksXonUKG9OJspNmC+zfAdtfVf/OO/8NLQ9Xrf/cfH/wC3X/8AvMl6vW/+4+P/ANuv/wDeZJTk5PQus3OzB6+MGZtjbHkseX7ai37PR/xf6Kv1E131f6lluyHZlmO71SyxrGNe1nq0N9HHrt/wv2X0v51a/q9b/wC4+P8A9uv/APedL1et/wDcfH/7df8A+86SnLyeh9Te2xmNZj015OMzHurLX7a/T3/0L/g/0v8AhVK3oObe6rfbU1ooZVaGNP8AOY7/ALTh2Y37lPqen660vV63/oMf/t1//vOl6vW/+4+P/wBuv/8AedJTR/YeU8HMstYOqG1t4IDvs2+pv2Wqr0/5/wBP7P8Azn/CKszoPVi8etbjz6z7hkVte3Iodad2R9jd/Nf9vrX9Xrf/AHHx/wDt1/8A7zJer1v/ALj4/wD26/8A950lOg0Q0AmSBEnkp1ner1v/ALj4/wD26/8A95kvV65/3Hx/+3X/APvMkp0Vm5v/ACn04edx/wDA0/q9c/0GP/26/wD951BlHU7s7Hvym0srxw/StznOJsHp/wCEaxJTZ6ZiNw8NuO1/qBrrHbx/wlluR/4H6vpq2qvTn4tmI12ICKC6zbP7wstbf/7MeqrSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkp//9TuEkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSU16G44yMk1CLXPb658XenV6X/sv6Swsm3Z9a6wfovqaJ/wA9beNl+tlZdG0N+zPY3d+/vqqyd3/gq5frtpq+sFT2nUMZP3uRAux4Jju1cjJZiZbqnl1bW2OLngbpP5jHVotd2BkQftewmZaW7Wh376l1aoO6leIkPa14+YWK9mx0RtnmEvbBiDco2yiXk9FRjWOuDq8ttriIdttjf/JVj9nZFbHGTdLSGMcRZtP7rf31ydVQstDGkgeI0Wien2MAItewt10cdEvZP732xQZDs6GVj1XYgquwhj2gANv2bdjv/M1zox7Rm/ZMo+mXfzb/AM137vvWo8dTez0vtj3sI1Y73Aqpl1ZYxttoD2t+ifzm/wBVPEZxHTX91ilIE6U1MnHcwisPa81kGQZhRfsdW86tfI0/f/lKux7wx1YGo13IgfbY5jXEFjiASPzf6ycCRv8AopAJIH71RTYtNl73AcNG5xP/AEWqllC+u+b2w8j2gajb/IWkca8BtFJ9Nsy947lT6hb6WIx8j1WkNY4iSf8ASbUwZDxbXfpi3pclwYpSkTHNi9Upf5H/AGeNDh1uZSXvJ3HWAjbfWpA3FzmS5hOrh+/WnouxzWNrhr9KeycWVttDsZ0emPeCOVGZGyabsYYjhGOMozx8HB6Jfpy/TawLri08t+7/ADldwWt/aWM5hlvqtGniCqNlo+0FhOyt3uds1Gq1MGqqvqeC0atNgIj/AKCMtKoU0By54Mp4ok4LHD/c/TfQEkkkmkpJJJJSlT6j/NV/8axXFT6j/N1/8axOj8wRLYtxJJJNSpJJJJSkklmdWY22zCpfJrsvh7QS3cAy5/u2JKdNJZ37C6V/3HH+c/8A9KJfsLpX+gH+c/8A9KJKdFJZ37C6V/oB/nP/APSiX7C6V/oB/nP/APSiSnRSWd+wulf6Af5z/wD0ol+w+lf6Af5z/wD0okp0UlnfsPpX+gH+c/8A9KJfsLpX/ccf5z//AEokp0UlnfsLpX/ccf5z/wD0ol+wulf6Af5z/wD0okp0UlnfsLpX+gH+c/8A9KJfsLpX+gH+c/8A9KJKdFJZ37C6V/oB/nP/APSiX7C6V/oB/nP/APSiSnRSWd+wulf6Af5z/wD0ol+wulf6Af5z/wD0okp0UlnfsLpX/ccf5z//AEol+wulf9xx/nP/APSiSnRSWd+wulf9xx/nP/8ASiX7C6V/oB/nP/8ASiSnRSWd+w+lf6Af5z//AEoq7sPGwup4X2ZprFvqteA5xDoZ6jPa9zklN7puPTjYbaaX+rWHWEP832W3WN9v+jss9NW1U6bdj34jbMZnp0l1gDeNW2W1XO/t3MssVtJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSU//V7hJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklIKfR9fI2MLbNzfVdH847ZX6b/7FP6Jcd9Z3R1c+Pps1/zl1uLlvuy8yhwAbivY1pHcWVU5Pu/t2rjvrUY6u6ePTZ/35PhukbpOoXNc/DyJ/naQwkfvMWTe5hJLSQ88ol2QH9Mx3SJpsc35OVI5DSS2RrqnQj6T/eK8tmhwD2nX4hbQu3sBHuMa/Jc/RbDwRBjgHhaDchrgSDtjQkcT+6lILSdW7XAdJOnJhNaxxJGRo060zpW5v/GKoX+4CSBz8Vt9L9DOw3YmQ0WMk+08j+U1Qc3lMMUZDi4eMRy8HzcKyrJHX955/Irsr0dU0g8beCEFu/Gu3Nr2NAIcD7h7lfybMJljsLp1dlr93py4l0u/4JaP/NvpuLQcrJNr3Vt32tD/AGuIG5/sVc54xiOLi/W6RhXrn/gJiD3+Vy8YNy8S+zHIddjt3uqJguaPzmLDvtGS8XvO7QBtbeGhaGRd0+97bun1vxLxLTWfoWVH+WqZw6yNAa3n6Lh9Cf3VZxaWTceL5Yz+bGzZOZy5AIzkZRj+i1w1oHt7awr2+7JYGU1F174Hs7x+8rRwsCjp32i20vt3AO0gtP51Kt0Mw24t1dLnUW3Q5ljTqwD81OMrF8MpGOi2GScYyETwjIOGQaGLW2vF9SoF2VvLMgOEtrj8xWunWZDOq4tkD+cDT/Vd7Paj2ZthxxVVsrBgXENh9hH+FsclhNsOdi9/0rePilGNgykPVZXe9kjHgEv1dVKNfvPepJJIMKkkkklKVHqbmtpY5xhrbGklXlS6j/NV/wDGtTo/MES2LL9pYP8ApR9x/wDIpftPB/0o+53/AJFWtjP3R9yWxn7o+5L09pfar1dx9jV/aeD/AKUfc7/yKX7Twf8ASj7nf+RVrYz90fclsZ+6PuS9PaX2q9XcfY1f2ng/6Ufcf/IqnmZWPfl4AqeHEXExB4DLf3v6y1tjP3R9yzuogDJ6fAA/Tn/z3ch6ex+1Wvg6Sw7+p9Xb1K/Epx6bWUMbdG5wtspcdnp0t/m/tXss/wCCW4ufux+uDq+RlY1VIrurbQy1z/dW1pdZ9rdRt/S/zv8AMIJdOnqmFbTTc2yGXzs3CDLP56uz/R+ltWfidfZkm/LBYzpuPLC5wf8AaPUH/A7dnpW/4P8AwyrZf1efW7FfjM+1Nx27HUOsNDXv3/a/tlljN/qfp/8AtMmp6V1U4266utmTj5RzKGNdNdxeXvsxrX/4H+c/R2pKdp3VcBmKzLdcPQs+g4AuLv8ArLG+t7P8J7P0aCPrB0c0euMlrqg7aXAOMH+WxrPUZX/wv80sLOx8rCrpda1rBlXW35Fu81NxbLQ308SnqFTXPrqs/wCL/WFXvzA99JFFOPhsq2Cw2GjHvsndZjU5lVf69hP/ANBZWkp3f+c3SvtDQMuoUFsGW2CzeT+je39H6X2b/hVojqWC6my5tzTVSS17hrDh+a3/AEn/AAfp/wA4uazMXqvU2+th4lVeNZjfZ2tsdsdt3Nv9SljG/wAx+j/V/wDgv0iXU8XqOIXZDMar7NezHbcA4g4nofTfS9n83XX/ANy/8Ckp6DG6z03KFpx7vU9Bu+2GvBY3+U17P5Kizr3SHiotymReCazqJ2fTa7/R2f8AB2Kl0PLFlORVj0sdZXLhdW821XvI9n2jqD2Msfmf6f8ARrLq6R1pldDHYWM4U5Dsp/6T+ee/fsY/9H/gPU/8CrSU9Ld1bp9OKzLsuAotjY4Au3T/AMGxvqf+k1W611mvp+Ebq31uucN1THbnNsb+d/Mf+B/4NZWP0vrVTjlGin12uu20h52vZln1bnOt2+z7Lt/66pW9Bz8THdVhbMk34wxbfVPp+nt3ubfV9P1P53+bSU71WdXaGVgtblWVC0VmYG4fR3/11Rwuu1/s/wC1dRLMdzbnY7i3c6vex3p/S93sVLE6Nls6hVm5FLXvLQ4htpFePbXX9lr9Oj/tR9o2/wA5/glAdM6yeiZeC6moXZFznj3+3Za717vzf8H/ADaSna/bXS/UfX9pZvrc1jh52fzW39+v/hf5tTzOqYeGy11z9aWeo5g1dH5uz+Wubv6V1q12Q4YOMz18duLWBZ/MMbu91X6P/CI9XQcz7Q++2pptyKXDcLD6eNcavse2vG/7U+r/AKZJTcq+sDHZLHWFleBfR69NriW2y1zKbaLqf6/+iWlf1LBx7KarrQ1+QYrGpBn6LnPb7K2f8YsHG6X1d+TjOysahtePjOxQ8P3lpLdjcxlez/X1LEOnofVW4poeysfaqmY2QQ6fRro3NqzKf9NZcx/80kp1OsdbGDfj0Ulll1tjGWVO3btlh2b67GfovU/41aFefi2Zb8Nj919Q3PbBgD/jdvpLBzekdUYbKsNld9Vt9eV6lrtjmvq9Nv2f09v5/ofzivdFws7GtudawY2PY4uGKH+uPUed9uRXkObX6Nf/AHWSU7Kzs3/lTpw87j/4GtFZubp1Ppx/lXD760lJ+m041OG2vFdvpDrCHc6ustsub/YvfYraqdNyasrDbdSz0mOdYAzzZZbRY72/6Syv1FbSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklP/1u4SSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSGotN14Fewhzdz4/nfZX+k/63/M/9bXDfW7d+2NrQXOfWzawck+5dliZNtuZnUvjZj2MbX8H00ZD/APwWxc31J7G/W0PeJZXjgu8vpuQM+CMpVxcMflX4xcgP3nm7em9UqxC+2v8AQE7i2fc1VPS+0EMpkuAmeJWjl9ZOTmWWX1GzF1bVQZYA39/2/wCEUbq8WjIqtdNFDmgtrpdvs1/0j3J0J5Kude4f3Pl/usvoBrX2/wBP97hc2upwvZVc7ZWT73eAV3OvdsY6na3FB2sYPpaf4a1XLx0a8Oux3P8AVY2DU8e2w/6RY+Rc2wtio1ECB/V/NSBMiCQRw/orCIa1f9SM/mStyt7AC73DzWp0LNczJeJgcgrLoNbKosoZbOu4kg/9BauL0bMe0X0UDHLhoNxduH9pR8xMe3KE/RGekZSKw0BZkP7vq4nrsfIx9Xsra15+k4ASUHrHUaqOnXPeJDm7Q397d7VzzXZ+O4DKsGMCYA5cUO52BlujJufe1muktaP81ZkeW9cZGRnjB4vT6/lZoYZSiCDAA7XJzn3010MuY9rp0bX+c3+uoDq7aw4MqDp1hx0BWk3p/QHVF4LgJjVx3BVb+iY0bse/XlrHdwtOOTHsRP8AwkHlMlfoz8pJLeosrxnwwWOyWjc1xDmsP79Cy23GtujjKbNa9ha1zAwDu3hLExHZJJJOxuhjkqaBAiTei2URxcMB/VbmDi9Qz7P0LhXX+dY8w1beN0PqOD1LCecpl7DaC9rTDgP3tj1LpBpwhY66slwaBU3lpP8ALWhTkYtd2CckzmZNv6MN/Mb/AMIqs82QzqI/V3VMnsgC5X/6E9Skkkp2spJJJJSlT6j/ADVf/GsVxU+o/wA3X/xrE6PzBEti3Ekkk1KkkkklKWZ1Ymt+Hdse9lV25/ptL3NaWW179jP6y00klOb+28T9y+fD0Lf/AEkl+28T9y//ALYt/wDSS0kklOb+28P9y/8A7Yt/9JJftvD/AHL/APti3/0ktJJJTmO6xgPaWuruc08g0Wkf9KlRPVenFoY6m0sHDTj2bR/Z9FaqSSnNHWsIaBl8f8Rb/wCkUj1rCIgsuIPINFv/AKRWkkkpy2dXwKxtrqua3wbj2gf+eVL9t4f7l/8A2xb/AOkVpJJKc39t4f7l/wD2xb/6SS/beH+5f/2xb/6SWkkkpzf21hfu3/8AbF3/AKRS/beH+5f/ANsW/wDpFaSSSnN/beH+5f8A9sW/+kUv21hfu3/9sXf+kVpJJKc39t4f7l//AGxb/wCkUv23h/uX/wDbFv8A6RWkkkpzf21ifuX/APbFv/pJL9tYf7l//bFv/pFaSSSnN/beH+5f/wBsW/8ApFA+1NzOp4ZqrtDafUc9z63VtG5nps91rWrZSSU1enMxWYjW4k+gHWbZ/eNlrr/pf92PVVpVOmZYzMNuQGemHOsbtH/B2W4+7/rnpeoraSlJJJJKUkkkkpSSSSSlJJJJKUkkkkpSSSSSlJJJJKUkkkkp/9fuEkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUirLjbcDXsAcNr/9J7We/wD63/MqjkPwac1z3UtdkPaA6wiSWj6DUbEvuszc6qz+bpsrFWn5rqaLrP8AwZ6p5/TL8nNNrLRWzaBESdEzIajvw6pG7X63ZjX9GywGND2MlugmR+4uFbhX2Y9WTTUXVuGx8SZf++u46vXj9L6PkWOa699jfSnwL/bv/kMYuJwOu9S6fj+jjWNFZMgOaHQf5KOIyMTR4tf0l2iaptlbX1ZFZqeGn0w4bXEfvfy1Wx6cnIosftNrWCQPIfylfqy+rfWHOox7Hs3Vtcd8bQ1kfpLH7fprpuj4uCcCsMreWVBzLHEbfWj/AAn8upNz5jijf6U5Rj/VimNE67a8Tz2N0Dq+QKy2prA9u8bjoI/MsWxR1bKdd9hsq+zX1+2yfouA/OpsWnl2ZFdPoOf6PtJrdV9INH7yyKnjqXTms6iTjDGeHssH079v738u1U55PdFzA9JqNfocf9T9NUsXFCwPl/SHzf4aD60GlteNWbI9Qy559zmt/wBJuWCx7WtcGv1GgMfo3M/0ly2s7Gdl035nTqSKqhDse0e6f8LbVW5Y1WS3Eqs2tgXNDH0vHuP7+z9xWMAAxiI9Rjp/hM2OMRHhlWg4pcf6PE6H1exse9+TvAdYwiAeNh/dS6zjPwLWXsJONbIA59N4/M/tqHSczGr6zjsoaa67W+lY137x/m/+muwsxGXNdTY0OH0oOokKtnyyxcxxH1Yskf5v+r8jICOHhEvlPzvnzcguHv8Aex3IK0+jmna7HjY+dzT++1D+sGNiVZFdmDAF2j2M+huH5zUPGqDb6NoIe38/tCucQljEhceMfL/dVjJ49RfDp/ju9ZVxqdfDhXMWmm27H9VgJrsD2k8tcPoquLN4GnA0VjDf+s1A6HeBqoATYbeSA9uQr9EvTpJJK44ykkkklKVPqP8AN1/8axXFT6j/ADdf/GsTo/MES2LcSSSTUqSSSSUpUOpX5DDjVY7xU/It2F5bv2t22W/zbv8Ai1fWd1L+k9P/AOP/APRdySlvsnVv/LAf9sM/8ml9k6t/5YD/ALYZ/wCTWks9nWcN3Uj0z3tygCQHNIa4Ab99dn7iSmP2Tq3/AJYD50M/8ml9k6v/AOWA/wC2Gf8Ak07es4bupO6aze7JZ9OGEsZp6m6y1aJMCUlOb9k6t/5YD/thn/k0vsfVv/LAf9sM/wDJqxgdQoz6nW0bgGPNb2vbsc17PpscxyNVcLH2NDHN9N20lwgO0376f9JWkpo/ZOrf+WA/7YZ/5NL7J1f/AMsB/wBsM/8ASi0lVz86nAxnZN4camkBxY3eWz+e5v8Ao0lNf7J1b/ywH/bDP/JpfZOr/wDlgP8Athn/AJNaLHte0PaZa4Ag+RTpKc37J1f/AMsB/wBsM/8ASiX2Tq//AJYD/thn/pRaSHVcLS8Bjm+m4sO4bd0f4Sr9+pJTR+ydX/8ALAf9sM/9KJfZOrf+WA/7YZ/6UWkkkpzfsnVv/LAf9sM/9KJfZOr/APlgP+2Gf+lFpJnOaxpe4hrWiS46AAfvJKc77J1b/wAsB/2wz/0ol9k6v/5YD/thv/pRGr6ni2Zxwmlxt9MXNdH6N9Z2/pKLvoWfTVxJTm/ZOr/+WA/7Yb/6US+ydW/8sB/2wz/0oj53UcfBFRvDg254qa5rdzWvd/N+r+4raSnN+ydW/wDLAf8AbDP/AEooNf1DHz8am/IbfXkB4I9MVlprb6n5jlqrNzf+U+nH+VcP/A0lNnp4xhitGMw107nw0iDu9S31/pf8P6isqn0rMdm4TMlzQwudY2Bx+jtuxm/+elcSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklP/0O4SSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJSOs2my0PaGsBHpuHLxtZvc/8A677FiZ/VLsXqrqfaKdjTLuQTuWjhvyHZue2yfSZZWKZ42mmh9vp/9f8AUWX1XopyeonLaTJY1u06t9qhzzhGFzNR4uFfAEnSv8JFldbrFTnsa7Le3ilrSWH/AI1cfYy/HyH5DscCrIlzW2NhjSfdtq/qLrhgdQx2l1Vm09mgcrK6m23NZXV1PNGIaT7WmtxD/wDhWuqUWDPjMqib/e/fXyhKuKrEf3S1ujssqru6w0NN2tGNjghnql422/5itYHUsgMHTK222h7HB7Ii3G/4PeqWPR0s3VYmIbLzO599jSwbm/Q+x1/4FVRb1JnUbr2mw5QJB9Np9/5v7qmnAT4r/vQ4/wDmLB3dbFzM7CNlsevaRs95J2sb9FWcTq9JNeHWxt9lj9xtsgMpcfpMc3/z2szp4ytRk1WiokkvIO9jv3bP32Kzl9NxrAC+RWf8KwEPB/4tV5QgJVL/ABofK2iBkjxQ9Mv04N/qnVcTfZSHGrPrcADXJZs/0j/zFS6lj4oZXXi1jMyHg2Pd+6PzlSq6dfjueMe0XOsEDc0y5qu5V2VkYdVDQ3CfT7XWgGX6bdntahwxjKPCbjfql8v/ADVohPYwJ/wv0f3XB+wfpGWVXta8+8T+Y5p+juXX4XWDk4t2a9waKWem+s6F1kfSr/rrBq6T9mqL3ze78wQdpcVbbi+rjkZVD8cNEusAPI/cranZhDJVnj4Jemf6X9ZdDGRZPDDjHy/M52PiMpe6u1tji4bmBuu4u/N/kIjX0YdIc+ybAYdWdXNaf8EpM6g0XtpxKrG0PBaXvaTa54/O/wCDUeq4TXVi6trtzxujaZ3NUw+cCfp41RIMScf6Lab1BtlUYx0A1J5b/JU+nhz+p4jrS41+oDzrv/MWHgvLJ3NfJj80re6fsdnY0Bzf0rY9pg6oECJod2aMjkxkk/oy2e9SSSU7mKSSSSUpV8vHdkVhjXmshwduHIj91WEkQa1URbQ+w5nbOt/za/8A0kn+xZn/AHOs/wAyr/0krySPEf6v+LBHCPH/ABpNH7Fm/wDc6z/Mq/8ASSX2LN/7nWf5lX/pJXkkuI/1f8WCuEeP+NJo/Ys3/udZ/mVf+klRzcXIZfhtflWPL7oDiGDadljtzdjFuLN6mQMnp5JAHr8nT/B3JcR8P8WCuEeP+NJ0WgtaATuIEFx5K5LPyAfrBbdQ2w5GMyprXhhLW1tc+zqrWfmWfq1v/pNdZvZ+8PvS3t/eH3pqXjabCM+4WMuGblY7hc5jCN7t/rYrt30Gf5N2er/6WQ8XMF+Ni/tH1XYOK57bdrLGgXB3+StvpfrVtP2f/wBSrtt7P3h96W9n7w+9JTxNeW4HI6jS+79HnNeWbHBrMW79Hfc3H/wv2itn/W0Gp2C/qbBQ7JqZblOG8C0MOLY3+Y/SfQquy/8ArlVa7zez94felvZ+8PvSU8RZkdRdVQz1nY9eHa+l9r63P9OwP29P2en78j9U/R+rZ+i/0i6P6xWivomSHS51rPTbtaXEuf7fotWp6jP3h96XqM/eH3pKeUw7cyzqm/1XV020n7NTsItupbX7G+q/9WxrKcn/AE36VZQyKnX11Z32gsoyCCGi0NGMWuda270vfZb9u/nv/Av0a9A9Rn7w+9Lez94feElPD4eR6FLcyp+Qa6M0sLHh7vSwbd/tbVZ77fW21/8AC1IeJ9if1Kuul2VVXbfaxzotDXYzw37JjfpP5uq23/ri73ez94fem3s/eH3pKeGOaMmurAyn2jIx2vFrXMs9jWv9Oi/G+x/p7cyvH/mrrf1daXU68Cn6shuI+y0Ag49jS99vrk7vUt2fQf8Azn6Nbed0/Ezdpse6uxn0banmqzaf8F61f+C/4NHxqMbFpbTSGsrb56k/nWWO/Psekp4e7LxxdlWU3ZTpprdSSyyLs1n/AGptbt/R+l+j/R/zCtWuy8++/wBex3oZGO51FGxzbMhrK93p72fosf0s39J/pbV2fq1nh7fvCW9n7w+9JTw+K/GybcHFpN9ba8N9bmhr2bsj+f8As1l7v+FZ6v8A22pHJysuqq671XZRqY3pztrmRms9uf6ra/0f0/T/AKT+i9JdtvZ+8PvT72+I+9JTxXV3bcku6gyx2XXkVWY+wOez7Iz03Xe2r9X/AJ37R/wy1OhPF+fk5EHILp/XYfUNst9LAtwr/T/S0s/w1Na6He394fem3s/eH3pKZLOzf+VOnfG7/wA9rQ3s/eH3rNy3sd1Tp+1wJBt0Bkwa0lNvBNRxh6VRoZufFZEEHfZ6j/8Ar1n6ZWFR6Rl2ZmAzItAD3PtaQOIrtux6/wDwOpXklKSSSSUpJJJJSkkkklKSSSSUpJJJJSkkkklKSSSSUpJJJJT/AP/R7hJeHJJKfcUl4ckkp9xSXhySSn3FJeHJJKfcUl4ckkp9xSXhySSn3FJeHJJKfcUl4ckkp9mvybMYWWW1vtr3NFTaGOts2lrd/qVVf8N6iVGey6h94pvYK+WWVPZY7/iqHt32rxlJJT7NkZ7KKGXGm54s4ZXU+yxv/G01t31Jm59NmMck02tDTHpvqc27+xjPb6q8aSSU+yZOfTjMY803W+pwKanWub/xrKm/olJudWcYZIquAJj0zW4XDXb78bb6q8ZSSU+y5nUK8UtDqb7t4n9DU+4D/jfSb7FL7ZX6Fd3pWxaQAz03eo2f9PRt9SleMJJKfY8vqVWLYGGi+4kTNNL7m/8AblLUZ2VWG1ONdhFxAA2Olk/9yW/9p/8Arq8WSSU+xZXVKsa30nY+RadDuqpfbXr/AMLU3YrLshrX1sLHn1eCGOLWf+GH/wCA/wCuLxRJJT7Dd1Wmm/0Dj5DyCBvroe+vX/h627FaN7Bc2nY8l4Lg8NPpiPzbLvoVvXiiSSn2B3VaW5H2f7PkE7tvqNoeav6/2jZs9P8A4RW/XZ6/obHTG7ftPp/1fX/m/UXiiSSn2JvVa3ZHofZ8kHdt9Q0WCr+t9o2bPT/4RWm3g3Pq2PBYAS4tIrM/6K76Fj14mkkp9ip6pXdeKRj5LCSRvsosZXp/w9jNistyGufYzZYPS5JY4Nd/4Xf/AIf/AK2vFEklPseN1OvJu9JuPk1mCd1tNlTNP+FtZsRxlNLbXenYBSSCCx0vj/uM3b+sf9aXiqSSn2TE6kzKsNbaMiogTuupfS3/ALcuai/bG+jZd6dsVkgs9N3qOj/QUbfUuXiySSn2XE6gzKLg2m+rYJ/TVPpn/i/Wb+kUzmsGMcn0ri0GPTFbzd+7/RtvqrxdJJT7Ni57Mlj3im+r0+RbU+pzv+JZa39Kq3UszpowG359TnUucNtb6z6m/wDN/QO/SLyJJJT6VRf9Ur6X3ekysV8ssHp2O/4qix/6VK+76pU0MvNVbw/hlY32N/42it++peapJKfSqrvqlZjOyBVW0N09N423H+pjOf6j02Vf9Usatj3VV2+pw2oeq5v/ABzK3/ol5skkp9Lbd9UnYwyfTrAJj0yIu/d/o2/1VHMv+qWLs3VMu3iR6I9aP+M9J/6NebJJKfTPU+qXoMv9OqLCAGf4Vs/6ajf6lSHmZP1RxXhjqWXEiZpb6rR/WfU9ebpJKfTS76pBlT9lMXEACRuZP/clm/8AV/8AriDlZf1RxrfSNDbTAO6lvqs1/wCFqfsXnCSSn093/NMOqbtoPrcEEEM/8MO3/oP+uKrfmfVGi80mgWER762GyvX/AIat2xedJJKfUC36qC5lUUEvEhwcPTb/AMbb6mypVX5n1SZk+gaA4h231GsLqv63r7/T9NecpJKfUSPqr6/obaJ27t8t9P8Aq/aN/p+oqYzfqicn7P6AB3bfULD6X9f7Ru2en/wi86SSU+oBv1TNzqYoBaJ37gKzP5td3qem96qU5n1RtyBQKAwkkb3tLKtP+He/YvOkklPp7R9Uy+xm2gGrUkkbXf8AEP8AU/Tf9bVbGzPqjkXekKG1nX32tNVen/DWO2LzlJJT6c0/VMi0htI9HmSPfH/cb3/rH/W1PpGb0C/L2YNBqyA0lrnVlnt/P9N715ckkp9pwbGW44exgraXPGwcS19lb3e3/SPb6isLw5JJT7ikvDkklPuKS8OSSU+4pLw5JJT7ikvDkklPuKS8OSSU+4pLw5JJT7ikvDkklPuKS8OSSU//2Q==");--sf-img-5: url("data:image/gif;base64,R0lGODlhGAAUAPcAAP//////zP//mf//Zv//M///AP/M///MzP/Mmf/MZv/MM//MAP+Z//+ZzP+Zmf+ZZv+ZM/+ZAP9m//9mzP9mmf9mZv9mM/9mAP8z//8zzP8zmf8zZv8zM/8zAP8A//8AzP8Amf8AZv8AM/8AAMz//8z/zMz/mcz/Zsz/M8z/AMzM/8zMzMzMmczMZszMM8zMAMyZ/8yZzMyZmcyZZsyZM8yZAMxm/8xmzMxmmcxmZsxmM8xmAMwz/8wzzMwzmcwzZswzM8wzAMwA/8wAzMwAmcwAZswAM8wAAJn//5n/zJn/mZn/Zpn/M5n/AJnM/5nMzJnMmZnMZpnMM5nMAJmZ/5mZzJmZmZmZZpmZM5mZAJlm/5lmzJlmmZlmZplmM5lmAJkz/5kzzJkzmZkzZpkzM5kzAJkA/5kAzJkAmZkAZpkAM5kAAGb//2b/zGb/mWb/Zmb/M2b/AGbM/2bMzGbMmWbMZmbMM2bMAGaZ/2aZzGaZmWaZZmaZM2aZAGZm/2ZmzGZmmWZmZmZmM2ZmAGYz/2YzzGYzmWYzZmYzM2YzAGYA/2YAzGYAmWYAZmYAM2YAADP//zP/zDP/mTP/ZjP/MzP/ADPM/zPMzDPMmTPMZjPMMzPMADOZ/zOZzDOZmTOZZjOZMzOZADNm/zNmzDNmmTNmZjNmMzNmADMz/zMzzDMzmTMzZjMzMzMzADMA/zMAzDMAmTMAZjMAMzMAAAD//wD/zAD/mQD/ZgD/MwD/AADM/wDMzADMmQDMZgDMMwDMAACZ/wCZzACZmQCZZgCZMwCZAABm/wBmzABmmQBmZgBmMwBmAAAz/wAzzAAzmQAzZgAzMwAzAAAA/wAAzAAAmQAAZgAAMwAAAP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAEAANgALAAAAAAYABQAAAhLALEJHEiwoMGDCBMqXMiw4IqHKxo6hEhRIjaIBDE2fGiQI0ONHjUqpOjxYsmEJAdWXJjSpMiRIF/CDClzZkuLLiPi3Mmzp8+fOwMCADs=")}</style>
<meta name=Description id=meta-description content="Images captured under poor illumination conditions often exhibit characteristics such as low brightness, low contrast, a narrow gray range, and color distortion">
<link rel=canonical href="https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/?arnumber=9088214">
<meta name=viewport content="width=device-width, initial-scale=1.0">
<meta name=customerIndustry content=NA>
<title>An Experiment-Based Review of Low-Light Image Enhancement Methods | IEEE Journals &amp; Magazine | IEEE Xplore</title>
<meta property=twitter:title content="An Experiment-Based Review of Low-Light Image Enhancement Methods">
<meta property=og:title content="An Experiment-Based Review of Low-Light Image Enhancement Methods">
<meta property=twitter:description content="Images captured under poor illumination conditions often exhibit characteristics such as low brightness, low contrast, a narrow gray range, and color distortion, as well as considerable noise, which seriously affect the subjective visual effect on human eyes and greatly limit the performance of various machine vision systems. The role of low-light image enhancement is to improve the visual effect of such images for the benefit of subsequent processing. This paper reviews the main techniques of low-light image enhancement developed over the past decades. First, we present a new classification of these algorithms, dividing them into seven categories: gray transformation methods, histogram equalization methods, Retinex methods, frequency-domain methods, image fusion methods, defogging model methods and machine learning methods. Then, all the categories of methods, including subcategories, are introduced in accordance with their principles and characteristics. In addition, various quality evaluation methods for enhanced images are detailed, and comparisons of different algorithms are discussed. Finally, the current research progress is summarized, and future research directions are suggested.">
<meta property=og:description content="Images captured under poor illumination conditions often exhibit characteristics such as low brightness, low contrast, a narrow gray range, and color distortion, as well as considerable noise, which seriously affect the subjective visual effect on human eyes and greatly limit the performance of various machine vision systems. The role of low-light image enhancement is to improve the visual effect of such images for the benefit of subsequent processing. This paper reviews the main techniques of low-light image enhancement developed over the past decades. First, we present a new classification of these algorithms, dividing them into seven categories: gray transformation methods, histogram equalization methods, Retinex methods, frequency-domain methods, image fusion methods, defogging model methods and machine learning methods. Then, all the categories of methods, including subcategories, are introduced in accordance with their principles and characteristics. In addition, various quality evaluation methods for enhanced images are detailed, and comparisons of different algorithms are discussed. Finally, the current research progress is summarized, and future research directions are suggested.">
<meta name=twitter:card content=summary_large_image>
<meta property=og:image content=https://ieeexplore-ieee-org.thi.idm.oclc.org/ielx7/6287639/8948470/9088214/graphical_abstract/access-gagraphic-2992749.jpg>
<meta property=twitter:image content=https://ieeexplore-ieee-org.thi.idm.oclc.org/ielx7/6287639/8948470/9088214/graphical_abstract/access-gagraphic-2992749.jpg>
<meta http-equiv=X-UA-Compatible content="IE=Edge">
<style>.MathJax_Preview{color:#888}#MathJax_Message{position:fixed;left:1px;bottom:2px;background-color:#E6E6E6;border:1px solid #959595;margin:0px;padding:2px 8px;z-index:102;color:black;font-size:80%;width:auto;white-space:nowrap}</style><meta http-equiv=origin-trial content="A+cA2PUOfIOKAdSDJOW5CP9ZlxONy1yu+hqAq72zUtKw4rLdihqRp6Nui/jUyCyegr+BUtH+C+Elv0ufn05yBQEAAACFeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="A+zsdH3aNZT/bkjT8U/o5ACzyaeNYzTvtoVmwf/KOilfv39pxY2AIsOwhQJv+YnXp98i3TqrQibIVtMWs5UHjgoAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="AxceVEhIegcDEHqLXFQ2+vPKqzCppoJYsRCZ/BdfVnbM/sUUF2BXV8lwNosyYjvoxnTh2FC8cOlAnA5uULr/zAUAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><style>.global-ng-wrapper[_ngcontent-dps-c455]{max-width:1680px;margin:0 auto;min-height:80vh;box-sizing:border-box}@media only screen and (min-width:768px){.global-ng-wrapper[_ngcontent-dps-c455]{padding-right:15px;padding-left:15px}}.ng2-xplore-meta-nav #global-header-cart-count{padding-right:.5rem;border-right:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav #global-header-cart-count{padding-right:0rem}}.ng2-xplore-meta-nav .xplore-meta-nav{display:flex;width:100%;background-color:#17445a;box-sizing:border-box;padding:.35rem 15px;max-width:1680px}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links{width:auto;max-width:none}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links .meta-nav-menu{-webkit-padding-start:0;padding-inline-start:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-ieee-links .meta-nav-item:last-child{padding-right:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links{width:auto;max-width:none;margin-left:auto}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .nav-right{width:100%;padding-right:1rem;padding-left:1rem}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .cart-container{margin-left:auto;display:flex}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .icons-panel{padding-right:0;padding-left:0}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links{flex:none;max-width:none;margin-left:0;width:100%;box-sizing:border-box}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item{list-style:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item{padding-right:0}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-user-links .meta-nav-item:last-child{padding-right:0}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{list-style:none;font-size:14px;padding-right:.75rem}@media only screen and (max-width:991px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{padding-right:.5rem}}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item{padding-right:0rem}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:last-child){border-right:1px solid white}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:last-child){border-right:none}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:first-child){padding-left:.75rem}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item:not(:first-child){padding-left:0rem}}.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item>a{text-decoration:none}@media only screen and (max-width:767px){.ng2-xplore-meta-nav .xplore-meta-nav .meta-nav-item>a{padding-left:.75rem}}.ng2-xplore-meta-nav .xplore-meta-nav a{color:#fff}.ng2-xplore-meta-nav .xplore-meta-nav .ieee-xplore{color:#e4a42c}.ng2-xplore-meta-nav .xplore-meta-nav .personal-signin-container{position:relative;z-index:1049}.main-header[_ngcontent-dps-c449]{position:relative;display:flex;flex-direction:column;background-color:#14303e}.search-bar-container[_ngcontent-dps-c449]{z-index:20}.fill-background[_ngcontent-dps-c449]{display:flex;justify-content:center;align-items:center;position:relative;background-color:#14303e;top:auto;left:auto;transform:translate(0);width:100%;min-height:115px}.menu-link[_ngcontent-dps-c459]{padding:.25rem .5rem;white-space:nowrap}.primary-menu[_ngcontent-dps-c459]{flex-grow:1;min-width:301px;max-width:400px;margin-top:5px}.primary-menu[_ngcontent-dps-c459] ul[_ngcontent-dps-c459]{display:flex;list-style-type:none;flex-grow:1;justify-content:space-around;margin:0;padding:0;padding-left:.75rem;padding-right:1.5rem}.primary-menu[_ngcontent-dps-c459] a[_ngcontent-dps-c459]{color:#fff}.hamburger-menu[_ngcontent-dps-c459]{width:33%}.hamburger-menu[_ngcontent-dps-c459] a[_ngcontent-dps-c459]{font-size:2rem;display:flex;color:#fff}.institution-container[_ngcontent-dps-c459]{flex-shrink:0}.institution-container.inst-logged-in[_ngcontent-dps-c459]{margin-top:0}.right-side-container[_ngcontent-dps-c459]{flex-basis:50%;flex-shrink:1;display:flex;flex-direction:row-reverse}.right-side-container.inst-logged-in[_ngcontent-dps-c459]{width:auto;padding-left:1rem;margin-left:auto}@media only screen and (max-width:767px){.right-side-container[_ngcontent-dps-c459]{width:33%}}.inst-logged-in[_ngcontent-dps-c459] .left-side-container[_ngcontent-dps-c459]{min-width:465px}.left-side-container[_ngcontent-dps-c459]{flex-basis:50%;flex-shrink:1}@media only screen and (max-width:767px){.left-side-container[_ngcontent-dps-c459]{width:33%}}.left-side-content[_ngcontent-dps-c459]{display:flex;justify-content:flex-start}@media only screen and (max-width:767px){.left-side-content[_ngcontent-dps-c459]{justify-content:center}}.left-side-content[_ngcontent-dps-c459] .xplore-logo-wrapper[_ngcontent-dps-c459]{margin-top:-5px}.navbar-container[_ngcontent-dps-c459]{box-sizing:border-box;display:flex;flex-direction:column;flex-grow:1;z-index:1004;width:100%;max-width:1680px;padding:18px 15px}@media only screen and (max-width:767px){.navbar-container[_ngcontent-dps-c459]{padding-top:0}}.inst-details-container[_ngcontent-dps-c459]{flex-grow:1}.top-navbar[_ngcontent-dps-c459]{display:flex}@media only screen and (max-width:767px){.top-navbar[_ngcontent-dps-c459]{padding:1rem 1rem .5rem}}.bottom-navbar[_ngcontent-dps-c459]{display:flex;justify-content:center;padding-top:13px}.navbar-container.not-homepage[_ngcontent-dps-c459]{position:relative;margin:0 auto;padding-bottom:1.5rem}@media screen and (-ms-high-contrast:none){.navbar-container[_ngcontent-dps-c459]{max-width:none;width:1460px}}[_nghost-dps-c462]{width:100%}.not-homepage [_nghost-dps-c462] .search-bar-wrapper[_ngcontent-dps-c462]{margin-bottom:0}.not-homepage [_nghost-dps-c462] .search-bar[_ngcontent-dps-c462] .below-search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462]{justify-content:unset;margin-left:auto;margin-right:0}.not-homepage [_nghost-dps-c462] .search-bar[_ngcontent-dps-c462] .below-search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462] .advanced-search-div[_ngcontent-dps-c462]{background-color:transparent;padding:0;margin:0;min-width:0}@media only screen and (max-width:767px){.not-homepage [_nghost-dps-c462] .search-bar[_ngcontent-dps-c462] .below-search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462] .advanced-search-div[_ngcontent-dps-c462]{padding-right:.5rem}}.not-homepage [_nghost-dps-c462] .search-bar[_ngcontent-dps-c462] .below-search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462] .advanced-search-div[_ngcontent-dps-c462] a[_ngcontent-dps-c462]{font-size:12px}@media only screen and (max-width:767px){.not-homepage [_nghost-dps-c462] .search-bar[_ngcontent-dps-c462] .below-search-bar[_ngcontent-dps-c462]{padding-left:.5rem}}.search-bar[_ngcontent-dps-c462]{max-width:960px;margin:0 auto}.search-bar[_ngcontent-dps-c462] .below-search-bar[_ngcontent-dps-c462]{display:flex;justify-content:center;max-width:780px;flex-grow:1;margin:0 auto}.search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462]{display:flex;margin:0 32%}@media only screen and (max-width:767px){.search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462]{flex-direction:column;margin:0 auto;margin-top:.5rem;max-width:230px}}.search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462]>div[_ngcontent-dps-c462]{text-align:center;max-width:200px;border-radius:.2rem}@media only screen and (max-width:767px){.search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462]>div[_ngcontent-dps-c462]{margin:1% 4%;margin-top:1rem}}.search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462]>div[_ngcontent-dps-c462]>a[_ngcontent-dps-c462]{color:#fff;font-family:Lato-Bold,Arial,sans-serif;font-weight:700}.search-bar[_ngcontent-dps-c462] .advanced-search-wrapper[_ngcontent-dps-c462]>div[_ngcontent-dps-c462]>a[_ngcontent-dps-c462]:hover{text-decoration:none}.search-bar-wrapper[_ngcontent-dps-c462]{display:flex;max-width:780px;background-color:#0000001a;padding:.5em;margin:0 auto 1%;justify-content:space-between}.search-bar-wrapper[_ngcontent-dps-c462]>.drop-down[_ngcontent-dps-c462]{flex-grow:.08}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-dps-c462]>.drop-down[_ngcontent-dps-c462]{flex-grow:1;max-width:4rem}}.search-bar-wrapper[_ngcontent-dps-c462]>.drop-down[_ngcontent-dps-c462]>label[_ngcontent-dps-c462]{position:relative}.search-bar-wrapper[_ngcontent-dps-c462]>.drop-down[_ngcontent-dps-c462]>label[_ngcontent-dps-c462]:after{content:"";font-family:"Font Awesome 5 Pro";font-size:22px;font-weight:900;color:#e4a42c;right:4px;top:-6px;padding:0 2%;position:absolute;pointer-events:none;height:19px;width:15px}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-dps-c462]>.drop-down[_ngcontent-dps-c462]>label[_ngcontent-dps-c462]:after{font-size:20px;right:5px;top:-4px;text-align:center}}.search-bar-wrapper[_ngcontent-dps-c462]>.drop-down[_ngcontent-dps-c462]>label[_ngcontent-dps-c462]:before{content:"";right:4px;top:0;background:#fff;position:absolute;pointer-events:none;display:block}.search-bar-wrapper[_ngcontent-dps-c462]>.drop-down[_ngcontent-dps-c462]>label[_ngcontent-dps-c462]>select[_ngcontent-dps-c462]{padding:.35em;padding-left:.5rem;width:100%;height:100%;margin:0;color:#000;border:none;font-weight:700;background-color:#ddd;border-radius:0;-webkit-appearance:none;-moz-appearance:none}.search-bar-wrapper[_ngcontent-dps-c462] .search-field[_ngcontent-dps-c462]{display:flex;justify-content:space-evenly;flex-grow:1}.search-bar-wrapper[_ngcontent-dps-c462] .search-field[_ngcontent-dps-c462]>div[_ngcontent-dps-c462]{flex-grow:1;padding-right:5%}.search-bar-wrapper[_ngcontent-dps-c462] .search-field[_ngcontent-dps-c462]>div[_ngcontent-dps-c462]:last-child{padding-right:0}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-dps-c462] .search-field[_ngcontent-dps-c462]>div[_ngcontent-dps-c462]{padding-right:0}}.search-bar-wrapper[_ngcontent-dps-c462] .search-field[_ngcontent-dps-c462] .global-search-bar[_ngcontent-dps-c462]{flex-grow:1}.search-bar-wrapper[_ngcontent-dps-c462] .search-field-icon-container[_ngcontent-dps-c462]{display:flex}.search-bar-wrapper[_ngcontent-dps-c462] .search-icon[_ngcontent-dps-c462]{background-color:#e4a42c;width:3.5625rem;display:flex;align-items:center;justify-content:center;cursor:pointer}@media only screen and (max-width:767px){.search-bar-wrapper[_ngcontent-dps-c462] .search-icon[_ngcontent-dps-c462]{order:1}}.search-bar-wrapper[_ngcontent-dps-c462] .search-icon[_ngcontent-dps-c462]>.fa-search[_ngcontent-dps-c462]{width:29px;height:25px;color:#000;text-align:center}.global-search-bar[_ngcontent-dps-c462]{position:relative}.xplore-logo-container[_ngcontent-dps-c444]{display:flex;flex-direction:column}.xplore-logo-container[_ngcontent-dps-c444] a[_ngcontent-dps-c444]{align-self:center}.xplore-logo-container[_ngcontent-dps-c444] img.xplore-logo[_ngcontent-dps-c444]{width:160px;height:40px}.ieee-logo-container[_ngcontent-dps-c446]{display:flex;flex-direction:column}.ieee-logo-container[_ngcontent-dps-c446] .ieee-logo[_ngcontent-dps-c446]{width:100px;align-self:flex-end}.footer-new[_ngcontent-dps-c454]{display:flex;background-color:#17445a;padding-top:3rem;color:#fff}@media only screen and (max-width:767px){.footer-new[_ngcontent-dps-c454]{padding-top:.1rem}}.footer-new[_ngcontent-dps-c454]>div[_ngcontent-dps-c454]{padding-bottom:2rem}.footer-new[_ngcontent-dps-c454] h3[_ngcontent-dps-c454]{font-weight:800;font-family:"IBM Plex Serif",Arial,sans-serif}.footer-new[_ngcontent-dps-c454] ul[_ngcontent-dps-c454]{list-style-type:none;padding-left:0}.footer-new[_ngcontent-dps-c454] li[_ngcontent-dps-c454]{padding:.35rem 0;text-transform:uppercase}.footer-new[_ngcontent-dps-c454] a[_ngcontent-dps-c454]{width:100%;text-decoration:none;color:#fff}.footer-new[_ngcontent-dps-c454] a[_ngcontent-dps-c454]:hover{color:#e4a42c}.footer-new[_ngcontent-dps-c454] .follow[_ngcontent-dps-c454] ul[_ngcontent-dps-c454]{display:flex}.footer-new[_ngcontent-dps-c454] .follow[_ngcontent-dps-c454] ul[_ngcontent-dps-c454] li[_ngcontent-dps-c454]{padding:0 .5rem}.footer-new[_ngcontent-dps-c454] .follow[_ngcontent-dps-c454] ul[_ngcontent-dps-c454] li[_ngcontent-dps-c454]:first-child{padding-left:0}.footer-new[_ngcontent-dps-c454] .footer-wrapper[_ngcontent-dps-c454]{flex-grow:1;max-width:1680px;margin:0 auto}.footer-new[_ngcontent-dps-c454] .flexible-row-col[_ngcontent-dps-c454]{display:flex;padding-bottom:3rem}@media only screen and (max-width:767px){.footer-new[_ngcontent-dps-c454] .flexible-row-col[_ngcontent-dps-c454]{padding-bottom:.1rem;flex-direction:column}}.footer-new[_ngcontent-dps-c454] .footer-col[_ngcontent-dps-c454]{flex-grow:1;padding-left:2rem}@media only screen and (max-width:767px){.footer-new[_ngcontent-dps-c454] .footer-col[_ngcontent-dps-c454]{padding-top:1rem}.footer-new[_ngcontent-dps-c454] .footer-col[_ngcontent-dps-c454]:first-child{padding-top:2rem}.footer-new[_ngcontent-dps-c454] .footer-col[_ngcontent-dps-c454]:last-child{padding-bottom:.25rem}}.footer-new[_ngcontent-dps-c454] .footer-bottom-section[_ngcontent-dps-c454] p[_ngcontent-dps-c454]{margin:0;padding-left:2rem;padding-right:2rem}.footer-new[_ngcontent-dps-c454] .footer-bottom-section[_ngcontent-dps-c454] p[_ngcontent-dps-c454]:last-child{padding-top:1rem}.footer-new[_ngcontent-dps-c454] .footer-bottom-section[_ngcontent-dps-c454] p[_ngcontent-dps-c454] span[_ngcontent-dps-c454]{padding-left:0}.footer-new[_ngcontent-dps-c454] .footer-bottom-section[_ngcontent-dps-c454] p[_ngcontent-dps-c454] .ethics-reporting-link[_ngcontent-dps-c454] i[_ngcontent-dps-c454]{padding-left:.25rem}.footer-new[_ngcontent-dps-c454] .nowrap[_ngcontent-dps-c454]{white-space:nowrap}.signout[_ngcontent-dps-c445]{font-size:.75rem;padding-left:1rem;padding-right:1.25rem;padding-top:.5rem}.signout[_ngcontent-dps-c445] a[_ngcontent-dps-c445]{display:block}.inst-detail[_ngcontent-dps-c445]{display:flex;background:#FFF;margin-top:-18px;border-radius:0 0 .4rem .4rem;height:71px}@media only screen and (max-width:767px){.inst-detail[_ngcontent-dps-c445]{height:auto;border-radius:0;justify-content:center;flex-wrap:wrap}}.inst-text-container[_ngcontent-dps-c445]{padding-left:.5rem;padding-right:.5rem;margin-top:.25rem;display:flex;margin-bottom:1rem;border-right:1px solid #dddddd}.inst-text-container.no-inst-logo[_ngcontent-dps-c445]{max-width:160px}.access-text[_ngcontent-dps-c445]{white-space:nowrap;font-size:12px}.inst-name[_ngcontent-dps-c445]{font-size:12px}.Typeahead-input[_ngcontent-dps-c69]{border-radius:0}.disqus-container[_ngcontent-dps-c196]{padding:0 1em}@media only screen and (max-width:767px){.document-sidebar[_ngcontent-dps-c196]{position:absolute;height:calc(100% - 129px);z-index:11000;right:0;padding:0;max-width:40vw;transform:translate(40vw);transition:transform .25s ease-in-out 0s}}@media only screen and (max-width:575px){.document-sidebar[_ngcontent-dps-c196]{max-width:80vw;transform:translate(80vw)}}@media only screen and (max-width:767px){.document-sidebar-content[_ngcontent-dps-c196]{max-height:calc(100% - 129px);width:100%;overflow:scroll;position:absolute}}.document-sidebar.top-spacing[_ngcontent-dps-c196]{margin-top:35px}@media only screen and (max-width:767px){.document-sidebar.top-spacing[_ngcontent-dps-c196]{margin-top:8.5rem}}@media only screen and (min-width:768px){.document-sidebar-rel-art[_ngcontent-dps-c196]{padding:0 15px 35px}}.header-rel-art-toggle-mobile[_ngcontent-dps-c196]{top:-2px}.document-title[_ngcontent-dps-c148]{letter-spacing:normal}.document-title[_ngcontent-dps-c148]{margin:0}.document-title-fix[_ngcontent-dps-c148]{flex-grow:1;width:100%}.pdf-btn-container[_ngcontent-dps-c148]{margin-left:28px}.document-header-inner-container[_ngcontent-dps-c148]{max-width:100%;width:100%}.document-header-breadcrumbs-container[_ngcontent-dps-c148]{padding:.4rem 1rem .8rem;margin:0;font-size:.8em}.document-header-breadcrumbs-container[_ngcontent-dps-c148] #help[_ngcontent-dps-c148]{font-size:14px}.document-header-metrics-banner[_ngcontent-dps-c148]{padding:.4rem 1rem .8rem;width:100%}.document-header-metrics-banner.ccby-document[_ngcontent-dps-c148]{padding-bottom:0}.document-header-title-container[_ngcontent-dps-c148]{padding:.4rem 1rem .8rem;display:flex}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-dps-c148]{flex-direction:column}}.document-header-title-container[_ngcontent-dps-c148] .right-container[_ngcontent-dps-c148]{margin-left:auto;display:flex;flex-direction:column}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-dps-c148] .right-container[_ngcontent-dps-c148]{margin-left:0;margin-right:auto}}.document-banner-access[_ngcontent-dps-c148]{width:100%;display:flex}.breadcrumbs-separator[_ngcontent-dps-c148]{padding:.4rem}.btn-container[_ngcontent-dps-c148]{display:flex}.btn-container[_ngcontent-dps-c148] .cite-this-related-btn-wrapper[_ngcontent-dps-c148] .cite-this-btn[_ngcontent-dps-c148]{padding:.5rem;border:2px solid #069;font-weight:700}.publisher-title-tooltip[_ngcontent-dps-c148]{margin-top:.3em;padding-right:30px}@media only screen and (max-width:767px){.document-header-title-container[_ngcontent-dps-c148]{position:relative}}.copyright-icon[_ngcontent-dps-c151]{font-size:1.25rem}.doc-share-tool[_ngcontent-dps-c144] i[_ngcontent-dps-c144]{font-size:1.2rem}.doc-share-tool[_ngcontent-dps-c144] i.fa-share-alt[_ngcontent-dps-c144]{color:#069}[_nghost-dps-c167]{width:100%}.ft-toc[_ngcontent-dps-c167]{height:52px;border-top:1px solid #e5e5e5;border-bottom:1px solid #e5e5e5}.ft-toc[_ngcontent-dps-c167]>div[_ngcontent-dps-c167]{margin-top:13px}.ft-toc[_ngcontent-dps-c167] a.toc-link[_ngcontent-dps-c167]{font-size:1.2em;font-weight:700}.ft-toc[_ngcontent-dps-c167] a.toc-link[_ngcontent-dps-c167]:hover{text-decoration:none}.ft-toc[_ngcontent-dps-c167] a.toc-link[_ngcontent-dps-c167] img[_ngcontent-dps-c167]{position:relative;top:-2px;margin-right:.3em}.full-text-toc-wrapper[_ngcontent-dps-c167] .previous-next-nav-ctrl[_ngcontent-dps-c167]{overflow:visible}.stats-document-container-fullTextSection[_ngcontent-dps-c167]{padding:0 15px}.stats-document-container-rh[_ngcontent-dps-c167]{padding-right:1em;padding-left:1em}.toc-container[_ngcontent-dps-c167]{margin-bottom:.7em;font-weight:700}.toc-container[_ngcontent-dps-c167] a[_ngcontent-dps-c167]:hover{text-decoration:none}.hide-full-text[_ngcontent-dps-c167]{max-height:0;overflow:hidden}.accordion-header[_ngcontent-dps-c172]{color:#069;display:flex;align-items:center}.accordion-header[_ngcontent-dps-c172] .accordion-chevron[_ngcontent-dps-c172] .fa[_ngcontent-dps-c172]{font-size:1.5rem}.accordion-header[_ngcontent-dps-c172]:hover{color:#0081c1}.document-all-references[_ngcontent-dps-c195]{position:fixed;top:0;right:0;width:30vw;min-width:450px;padding:1rem 1rem 3rem;box-shadow:3px 10px 10px #000;overflow:auto;height:100vh;box-sizing:border-box;z-index:99999;transform:translate(100%);background-color:#fff;transition:transform .25s ease-in-out 0s}.header[_ngcontent-dps-c195]{display:flex;align-items:center;padding:.5rem 0}.header[_ngcontent-dps-c195] h1[_ngcontent-dps-c195]{margin:0;font-weight:400;color:#333}.header[_ngcontent-dps-c195] a[_ngcontent-dps-c195]{margin-left:auto;font-size:1.25rem;color:#333}i.help-link[_ngcontent-dps-c54]{color:#069}i.help-link[_ngcontent-dps-c54]:hover{color:#0081c1}.help-link-icon[_ngcontent-dps-c54]{font-size:1.2rem}.breadcrumb-help-link-icon[_ngcontent-dps-c54]{font-size:1rem}a[_ngcontent-dps-c54]:hover{text-decoration:none}.pdf-btn-link[_ngcontent-dps-c115]{height:36px;background-color:#ff3500;padding:0 25px;display:flex;align-items:center;justify-content:center;border-radius:2px;color:#fff}.pdf-btn-link[_ngcontent-dps-c115] .icon[_ngcontent-dps-c115]{font-size:1.15rem;color:#fff;margin-right:.45rem}.pdf-btn-link[_ngcontent-dps-c115]>span[_ngcontent-dps-c115]{font-weight:700}.red-pdf[_ngcontent-dps-c115]{font-size:1.3rem;margin-right:.15rem;color:#fc0d1b;font-style:normal}.red-pdf[_ngcontent-dps-c115]:after{color:#fc0d1b}.document-authors-banner[_ngcontent-dps-c141] .authors-container[_ngcontent-dps-c141]{padding:0 0 0 1rem}.stats-document-authors-banner[_ngcontent-dps-c141]{padding:.25rem 1rem .25rem 0}.authors-minimized[_ngcontent-dps-c141]{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.toc-container[_ngcontent-dps-c149]{padding:.6em .5em;border-bottom:1px solid #dddddd}.toc-heading[_ngcontent-dps-c149]{font-size:1em;padding-bottom:.5em}.toc-list[_ngcontent-dps-c149]{list-style:none;padding:0}.toc-list-item[_ngcontent-dps-c149]{padding:.5em 0}.toc-list-link[_ngcontent-dps-c149]{display:flex;font-size:.9em}.toc-list-icon[_ngcontent-dps-c149]{margin-right:5px}.toc-show-more-btn[_ngcontent-dps-c149]{font-size:.85em;font-weight:700}.cover-image-document[_ngcontent-dps-c112] img[_ngcontent-dps-c112]{width:25rem;height:200px;cursor:pointer;border:1px solid #069}@media only screen and (max-width:767px){.cover-image-document[_ngcontent-dps-c112] img[_ngcontent-dps-c112]{width:100%;max-height:100%;height:100%}}.header-rel-art-pub[_ngcontent-dps-c194]{margin:.25rem 0}.header-rel-art-pub[_ngcontent-dps-c194]:last-child{margin:0 0 .5rem}.header-rel-art[_ngcontent-dps-c194]{font-size:.9375rem;color:#333;border:1px solid #e5e5e5;border-top:5px #0081C1 solid;background-color:#f8f8f8}@media only screen and (max-width:767px){.header-rel-art[_ngcontent-dps-c194]{border-bottom:none}}.header-rel-art-action[_ngcontent-dps-c194] a[_ngcontent-dps-c194],.header-rel-art-title[_ngcontent-dps-c194]{font-family:Helvetica-Nue-Bold,Arial,sans-serif}.header-rel-art-list[_ngcontent-dps-c194]{font-family:Helvetica Regular,Arial,sans-serif}.MathJax_Display{text-align:center;margin:1em 0em;position:relative;display:block!important;text-indent:0;max-width:none;max-height:none;min-width:0;min-height:0;width:100%}.MathJax{display:inline;font-style:normal;font-weight:normal;line-height:normal;font-size:100%;font-size-adjust:none;text-indent:0;text-align:left;text-transform:none;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;direction:ltr;max-width:none;max-height:none;min-width:0;min-height:0;border:0;padding:0;margin:0}.MathJax:focus,body :focus .MathJax{display:inline-table}.MathJax nobr{border:0;padding:0;margin:0;max-width:none;max-height:none;min-width:0;min-height:0;vertical-align:0;line-height:normal;text-decoration:none}.MathJax span{display:inline;position:static;border:0;padding:0;margin:0;vertical-align:0;line-height:normal;text-decoration:none;box-sizing:content-box}.MathJax nobr{white-space:nowrap!important}.MathJax *{transition:none;-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none}.MathJax_Processing{visibility:hidden;position:fixed;width:0;height:0;overflow:hidden}.MathJax_Processed{display:none!important}</style><meta name=cToken content=eyJhbGciOiJIUzUxMiIsInppcCI6IkRFRiJ9.eNqqVkosKFCyUoooyMkvSlXSUcosLgZyK2Dc1AqgrKGZmamZmZGJmQlQPrEEKmBqaW5mUgsAAAD__w.l9oKvz-NUvdsdNJ4KR84ZrmBfgRhM55w8V92eTHpZi8UCBS4iaUh7FTLKnkoofkO8ZgNIbwWG39JTvoCxs-2Zw class=sf-hidden><link type=image/x-icon rel="shortcut icon" href=data:,><style>.sf-hidden{display:none!important}</style><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:;"><style>img[src="data:,"],source[src="data:,"]{display:none!important}</style><body class="body-resp cmpl_embed_complete"><div style="visibility:hidden;overflow:hidden;position:absolute;top:0px;height:1px;width:auto;padding:0px;border:0px none;margin:0px;text-align:left;text-indent:0px;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal" class=sf-hidden></div><div style=display:none id=lightningjs-usabilla_live></div><div id=MathJax_Message>Typesetting math: 10%</div><g:compress>
 <style>--></style>
 <style media="screen, print">--></style>
 <style media="screen, print">--></style>
 <style>--></style>
 <style media="screen, print">--></style>
</g:compress>
 
 
 
 
 
 
 <p class=JumpLink id=PageTop><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/# title="Click here to Skip to main content" accesskey=s>Skip to Main Content</a></p>
 <div id=global-notification class="row stats-global-notification">
 <div class="hide u-hide-important col Notification Notification--global Notification--fixed">
 <a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/ class="Notification-close js-close" aria-label="close message button"><i class="fa fa-close"></i></a>
 <div class=Notification-header></div>
 <div class=Notification-text></div>
 </div>
 </div>
 <div id=LayoutWrapper>
 <div class=container-fluid>
 <div class=row>
 <div class=col>
 
 
 
 
 
<div class=Header id=xplore-header data-service=true data-inst=true data-web=false style=display:none></div>
 <div id=global-alert-message></div>
 

<div class=ng2-app>
 
 
 
 
 <div class=global-content-wrapper>
 <xpl-root _nghost-dps-c455 ng-version=13.3.11><xpl-meta-nav _ngcontent-dps-c455><div class=ng2-xplore-meta-nav><div class="metanav-container u-flex-display-flex u-flex-justify-center"><div class="stats-metanav xplore-meta-nav"><div class="meta-nav-ieee-links hide-mobile text-sm-md-lh"><ul class="meta-nav-menu u-flex-display-flex u-m-0"><li class="meta-nav-item stats-extLink stats-Unav_exit_aaa"><a href=http://www.ieee.org/ id=u-home class=ieeeorg>IEEE.org</a><li class="meta-nav-item stats-extLink ieee-xplore">IEEE <em>Xplore</em><li class="meta-nav-item stats-extLink"><a href=http://standards.ieee.org.thi.idm.oclc.org/ id=u-standards class=exitstandardsorg>IEEE SA</a><li class="meta-nav-item stats-extLink"><a href=http://spectrum.ieee.org.thi.idm.oclc.org/ id=u-spectrum class=exitspectrum>IEEE Spectrum</a><li class="meta-nav-item stats-extLink"><a href=http://www.ieee.org/sitemap.html id=u-more class=exitmoreieeesites>More Sites</a></ul></div><div class="meta-nav-user-links u-flex-display-flex text-sm-md-lh"><ul class="u-relative u-m-0 nav-right icons-panel"><li class=u-flex-display-flex><div class="col-4 hide-desktop"></div><div class=cart-container><ul class="u-flex-display-flex u-pl-0"><li id=global-header-cart-count class="meta-nav-item stats-mnEvLinks"><a title="View Cart" tabindex=0 class="cart stats-Unav_exit_Cart" style=white-space:nowrap href="https://www.ieee.org/cart/public/myCart/page.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore"><span id=cartCount>Cart&nbsp;</span></a><div id=mc_ieee-mini-cart-include_wrapper class="content-r cart-summary product-cart" style=display:none></div></ul><ul class="u-flex-display-flex u-pl-0"><li class="meta-nav-item stats-mnEvLinks hide-desktop"><a title="Create Account" class="create-account-new stats-Unav_CreateAcct hide-desktop" href="https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&amp;sourceCode=xplore&amp;car=IEEE-Xplore&amp;autoSignin=Y&amp;signinurl=https%3A%2F%2Fieeexplore-ieee-org.thi.idm.oclc.org%2FXplore%2Flogin.jsp%3Furl%3D%2FXplore%2Fhome.jsp%26reason%3Dauthenticate&amp;url=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214?arnumber=9088214"><i class="fas fa-user-plus"></i></a><li class="meta-nav-item stats-mnEvLinks u-flex-display-flex u-ml-auto personal-signin-container hide-desktop"><a title="Sign In" class="stats-Unav_P_SignIn hide-desktop"><i aria-hidden=true class="fas fa-sign-in-alt"></i></a></ul></div><div class=text-sm-md-lh><ul class="u-flex-display-flex u-pl-0"><li class="meta-nav-item stats-mnEvLinks"><a title="Create Account" class="create-account-new stats-Unav_CreateAcct hide-mobile" href="https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&amp;sourceCode=xplore&amp;car=IEEE-Xplore&amp;autoSignin=Y&amp;signinurl=https%3A%2F%2Fieeexplore-ieee-org.thi.idm.oclc.org%2FXplore%2Flogin.jsp%3Furl%3D%2FXplore%2Fhome.jsp%26reason%3Dauthenticate&amp;url=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214?arnumber=9088214">Create Account</a><li class="meta-nav-item stats-mnEvLinks u-flex-display-flex u-ml-auto personal-signin-container"><a title="Sign In" class="stats-Unav_P_SignIn hide-mobile u-pr-05">Personal Sign In</a></ul></div></ul></div></div></div></div></xpl-meta-nav><xpl-global-notification _ngcontent-dps-c455 _nghost-dps-c63></xpl-global-notification><xpl-header _ngcontent-dps-c455 _nghost-dps-c449><div _ngcontent-dps-c449 class=main-header><xpl-navbar _ngcontent-dps-c449 _nghost-dps-c459><div _ngcontent-dps-c459 class="navbar-container not-homepage inst-logged-in"><div _ngcontent-dps-c459 class=top-navbar><div _ngcontent-dps-c459 class="hamburger-menu hide-desktop"><a _ngcontent-dps-c459><i _ngcontent-dps-c459 aria-hidden=true class="fa fa-bars"></i></a></div><div _ngcontent-dps-c459 class=left-side-container><div _ngcontent-dps-c459 class=left-side-content><div _ngcontent-dps-c459 class=xplore-logo-wrapper><xpl-xplore-logo _ngcontent-dps-c459 _nghost-dps-c444><div _ngcontent-dps-c444 class=xplore-logo-container><a _ngcontent-dps-c444 accesskey=1 title="Delivering full text access to the world's highest quality technical literature in engineering and technology" alt="IEEE Advancing Technology for Humanity" href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplore/home.jsp><img _ngcontent-dps-c444 alt="IEEE Xplore logo - Link to home" class=xplore-logo src="data:image/svg+xml;base64,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"></a></div></xpl-xplore-logo></div><div _ngcontent-dps-c459 class="primary-menu hide-mobile text-base-md-lh"><ul _ngcontent-dps-c459><li _ngcontent-dps-c459><div _ngcontent-dps-c459><a _ngcontent-dps-c459 tabindex=0 class="menu-link stats-browse-book"> Browse <i _ngcontent-dps-c459 aria-hidden=true class="fas fa-chevron-down"></i></a></div><li _ngcontent-dps-c459><div _ngcontent-dps-c459><a _ngcontent-dps-c459 tabindex=0 class="menu-link stats-my-settings"> My Settings <i _ngcontent-dps-c459 aria-hidden=true class="fas fa-chevron-down"></i></a></div><li _ngcontent-dps-c459><div _ngcontent-dps-c459><a _ngcontent-dps-c459 tabindex=0 class="menu-link stats-get-help"> Help <i _ngcontent-dps-c459 aria-hidden=true class="fas fa-chevron-down"></i></a></div></ul></div></div></div><div _ngcontent-dps-c459 class="institution-container hide-mobile inst-logged-in"><div _ngcontent-dps-c459><xpl-institution-details _ngcontent-dps-c459 _nghost-dps-c445><div _ngcontent-dps-c445><div _ngcontent-dps-c445 class=inst-detail><div _ngcontent-dps-c445 class=u-flex-display-flex><div _ngcontent-dps-c445 class="inst-text-container no-inst-logo"><span _ngcontent-dps-c445 class=right-line><span _ngcontent-dps-c445 class=access-text>Access provided by:</span><h4 _ngcontent-dps-c445 class=inst-name>Technische Hochschule Ingolstadt</h4></span></div><div _ngcontent-dps-c445 class=signout><a _ngcontent-dps-c445 title="Sign Out" target=_self href="https://ieeexplore-ieee-org.thi.idm.oclc.org/servlet/Login?logout=/document/9088214/?arnumber=9088214">Sign Out</a></div></div></div></div></xpl-institution-details></div></div><div _ngcontent-dps-c459 class="right-side-container inst-logged-in"><div _ngcontent-dps-c459 class=row><xpl-ieee-logo _ngcontent-dps-c459 _nghost-dps-c446><div _ngcontent-dps-c446 class="ieee-logo-container hide-mobile"><img _ngcontent-dps-c446 alt="IEEE logo - Link to IEEE main site homepage" class=ieee-logo src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz48c3ZnIGlkPSJhIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNDAuMDcgNjkuOTkiPjxkZWZzPjxzdHlsZT4uYntmaWxsOm5vbmU7fS5jLC5ke2ZpbGw6I2ZmZjt9LmR7ZmlsbC1ydWxlOmV2ZW5vZGQ7fTwvc3R5bGU+PC9kZWZzPjxwb2x5Z29uIGNsYXNzPSJjIiBwb2ludHM9IjgwLjA2IDYyLjI4IDgwLjA2IDkuMjYgOTQuNzUgOS4yNiA5NC43NSA2Mi4yOCA4MC4wNiA2Mi4yOCA4MC4wNiA2Mi4yOCIvPjxwb2x5Z29uIGNsYXNzPSJjIiBwb2ludHM9IjEwMi40NiA2Mi4yOCAxMDIuNDYgOS4yNiAxNDIuODQgOS4yNiAxNDIuODQgMTkuNTQgMTE3LjE0IDE5LjU0IDExNy4xNCAzMC40OCAxNDAuNzkgMzAuNDggMTQwLjc5IDQwLjc2IDExNy4xNCA0MC43NiAxMTcuMTQgNTIgMTQyLjg0IDUyIDE0Mi44NCA2Mi4yOCAxMDIuNDYgNjIuMjggMTAyLjQ2IDYyLjI4Ii8+PHBvbHlnb24gY2xhc3M9ImMiIHBvaW50cz0iMTUxLjA3IDYyLjI4IDE1MS4wNyA5LjI2IDE5MS40NSA5LjI2IDE5MS40NSAxOS41NCAxNjUuNzYgMTkuNTQgMTY1Ljc2IDMwLjQ4IDE4OS4zOSAzMC40OCAxODkuMzkgNDAuNzYgMTY1Ljc2IDQwLjc2IDE2NS43NiA1MiAxOTEuNDUgNTIgMTkxLjQ1IDYyLjI4IDE1MS4wNyA2Mi4yOCAxNTEuMDcgNjIuMjgiLz48cG9seWdvbiBjbGFzcz0iYyIgcG9pbnRzPSIxOTkuNjggNjIuMjggMTk5LjY4IDkuMjYgMjQwLjA3IDkuMjYgMjQwLjA3IDE5LjU0IDIxNC4zNyAxOS41NCAyMTQuMzcgMzAuNDggMjM4LjAxIDMwLjQ4IDIzOC4wMSA0MC43NiAyMTQuMzcgNDAuNzYgMjE0LjM3IDUyIDI0MC4wNyA1MiAyNDAuMDcgNjIuMjggMTk5LjY4IDYyLjI4IDE5OS42OCA2Mi4yOCIvPjxyZWN0IGNsYXNzPSJiIiB5PSIwIiB3aWR0aD0iMjQwLjA3IiBoZWlnaHQ9IjY5Ljk4Ii8+PHBhdGggY2xhc3M9ImMiIGQ9Ik0zMi45NSw5Ljg2YzIuMzQtMS44Miw1LjI5LS4zMSw3LjI2LDEuMzcsMi4wNiwxLjU4LDQuMjEsMy4zNiw2LjEsNS4xOWwuMzQsLjJjNS4yMiw0Ljg1LDEwLjA3LDEwLjIzLDE0LjA2LDE1LjkzLC42NSwuOTksMS4yMiwyLjExLC44NiwzLjQzLTEuMzMsMy4zMi00LjAzLDUuOTctNi4zOSw4Ljg3LTUuMTEsNS41LTEwLjUsMTAuODItMTYuNTMsMTUuMDYtMS4yMiwuODYtMi44NywxLjY2LTQuMzMsLjk3LTQuNDQtMi4zMi04LjE4LTYuMTMtMTIuMDctOS42LTQuNTEtNC4xOC05LjAzLTkuMDUtMTIuNDUtMTQuMTMtLjUxLS43MS0uNjUtMS42LS42NC0yLjU0LC40NC0xLjc2LDEuNjQtMy4yLDIuNzgtNC42OCw0LjA0LTUuMTQsOC45OC0xMC4wMiwxMy45Ny0xNC40OCwuMTItLjExLC4zMy0uMzYsLjUxLS40NywyLjE0LTEuODUsNC4yNy0zLjUsNi41Mi01LjExaDBabTYuODYtNS4zNWwtMi42NS0zLjc2Yy0uMzMtLjItLjczLS41My0xLjA5LS42Mi0uNzgtLjM0LTEuNjMsLjA3LTIuMjYsLjU2bC00LjgsNi4yNUMyMS4zMiwxNi40NiwxMi4yMSwyNS4zNiwyLjI3LDMyLjE4Yy0uNzksLjYxLTEuOTUsMS4yMS0yLjIsMi4yMy0uMjYsLjkxLC4yMSwxLjY2LC43OCwyLjIzLDcuOTYsNS40OCwxNS41NywxMS45OSwyMi4zNCwxOS40MSwxLjIsMS4yNiwyLjE4LDIuNTIsMy4zNSwzLjcsMS45OCwyLjY0LDQuMzEsNS4yOSw2LjEzLDguMTIsLjU3LC42LC42NywxLjU4LDEuNTYsMS44NywuNywuMjQsMS41NiwuNDEsMi4yMywwbC42Ni0uNjdjOC4yNC0xMS42NywxOC42OC0yMi4xNCwzMC4zNS0zMC42LDEuMTctMS4wNCwzLjQtMS40MywzLjQzLTMuMzctLjA4LS44OS0uNi0xLjc3LTEuMzctMi4yNmwtLjE5LS4wM2MtNi00LjA4LTExLjYtOC43LTE2Ljg3LTEzLjk1bC01LjgzLTYuMDljLTIuMzYtMi42Mi00LjY1LTUuNTEtNi44My04LjI4aDBabS02LDYuNzljMi4zOS0xLjYzLDQuNTQsLjY1LDYuNCwxLjk1LDcuMTcsNS43NywxMy45NCwxMi4zOCwxOS4xLDE5Ljg0LC42NSwuOTYsLjk0LDIuNDYsLjM2LDMuNTUtMS4zNCwyLjIzLTMuMTQsNC4yNy00Ljg1LDYuMzN2LjExYy00LjI2LDQuNTQtOC43Nyw5LjItMTMuNjMsMTMuMDUtMi4zNywxLjQ1LTQuNTUsNC42LTcuNjEsMi42NS02Ljk2LTUuMDktMTMuNjMtMTEuNDItMTkuMjgtMTguMjEtLjk2LTEuNDktMi40NS0yLjcxLTMuMTItNC40MS0uOTMtMi4yOSwxLjEyLTMuOTQsMi4yOS01LjY2LDUuNzYtNy4xMSwxMi45Mi0xNC4wNywyMC4zNS0xOS4yMWgwWm0xLjUxLDQuNjhsLS42NSwyLjA4LTMuMjUsOS4zMmMuODEsLjA4LDEuODIsMCwyLjYyLC4wOHYuMDhsLS40OCwxMC41NCwuMDgsLjExYzEuMDQsLjEsMi4zNCwuMTUsMy40LS4wM3YtLjE3bC0uNDItMTAuMjMsLjA2LS4zNSwyLjg2LS4wNWMtMS40My0zLjc3LTIuODEtNy41OC00LjE1LTExLjRoLS4wOFptLTExLjA4LDE0LjZjLTEuODIsLjkyLTQuNTEsMi4zLTQuMjEsNC44LC4zOSwxLjM3LDEuODcsMi4yNSwzLjA0LDIuODEsNi40NywyLjg1LDE1LjA3LDIuOTUsMjEuOSwuODIsMS42OS0uNjUsMy45NS0xLjU5LDQuMjYtMy42Ni0uMDItMS43MS0xLjgzLTIuNzgtMy4xMi0zLjQ0di0uMDhjLjQ1LS4xOCwuOTctLjM0LDEuNDQtLjM5di0uMDVjLTIuMjktLjQxLTQuNS0xLjAxLTYuNzItMS42LC40MywuOTcsLjczLDIsMS4wOSwzLjAxLC42Ny0uMjEsMS4zNS0uMzgsMi4wNy0uNDcsMS4xNywuNDUsMi44NCwxLjExLDMuMDEsMi41NywuMTMsMS4zNy0xLjM3LDIuMDMtMi4zMSwyLjU5LTUuMDEsMS44Mi0xMC42NSwyLTE1Ljg4LC42Mi0xLjQ0LS40Ni0zLjUzLTEuMDQtMy43MS0yLjg4LDEuMDMtMi40MiwzLjY4LTIuOTIsNS44OS0zLjUxLTEuMTQtLjczLTIuMy0xLjM4LTMuNC0yLjItMS4xOSwuMDctMi4yOCwuNTktMy4zNSwxLjA2aDBabTkuMjUsMTAuNTFjLS4xOSwzLjc0LS4yNCw3LjIxLS41MiwxMC45NiwxLjQ4LC4xMywzLjE1LC4yNiw0LjczLC4wM2wtLjQ3LTEwLjQ2LS4wOC0uNWMtMS4yLC4wNS0yLjM0LC4xLTMuNjYtLjA0aDBaIi8+PHBhdGggY2xhc3M9ImQiIGQ9Ik00Ny4zMiw2Mi41MWMtMS4zMSwwLTIuNDcsLjkxLTIuNDcsMi40NnMxLjE2LDIuNDcsMi40NywyLjQ3LDIuNDctLjkxLDIuNDctMi40Ny0xLjE3LTIuNDYtMi40Ny0yLjQ2Wm0wLDQuMzZoMGMtLjk4LDAtMS43Ni0uNzYtMS43Ni0xLjlzLjc5LTEuODksMS43Ni0xLjg5LDEuNzcsLjc1LDEuNzcsMS44OS0uODEsMS45LTEuNzcsMS45Wm0xLjEtMi40MmMwLS42MS0uNC0uOC0xLjA5LS44aC0uOTl2Mi42NWguNTd2LTEuMTJoLjI3bC42MSwxLjEyaC42OGwtLjY4LTEuMTZjLjM2LS4wMywuNjQtLjIsLjY0LS42OVptLTEuMDEsLjI5aC0uNTF2LS42NGguNGMuMjIsMCwuNTEsLjAyLC41MSwuMywwLC4yOS0uMTUsLjM0LS40LC4zNFoiLz48L3N2Zz4="></div></xpl-ieee-logo></div></div></div><div _ngcontent-dps-c459 class="bottom-navbar hide-desktop"><div _ngcontent-dps-c459 class=inst-details-container><xpl-institution-details _ngcontent-dps-c459 _nghost-dps-c445><div _ngcontent-dps-c445><div _ngcontent-dps-c445 class=inst-detail><div _ngcontent-dps-c445 class=u-flex-display-flex><div _ngcontent-dps-c445 class="inst-text-container no-inst-logo"><span _ngcontent-dps-c445 class=right-line><span _ngcontent-dps-c445 class=access-text>Access provided by:</span><h4 _ngcontent-dps-c445 class=inst-name>Technische Hochschule Ingolstadt</h4></span></div><div _ngcontent-dps-c445 class=signout><a _ngcontent-dps-c445 title="Sign Out" target=_self href="https://ieeexplore-ieee-org.thi.idm.oclc.org/servlet/Login?logout=/Xplore/guesthome.jsp">Sign Out</a></div></div></div></div></xpl-institution-details></div></div></div></xpl-navbar><div _ngcontent-dps-c449><div _ngcontent-dps-c449 class="search-bar-container fill-background not-homepage"><xpl-search-bar-migr _ngcontent-dps-c449 _nghost-dps-c462><div _ngcontent-dps-c462 class=search-bar><form _ngcontent-dps-c462 novalidate class="search-bar-wrapper ng-untouched ng-pristine ng-valid"><div _ngcontent-dps-c462 class=drop-down><label _ngcontent-dps-c462><select _ngcontent-dps-c462 aria-label="content type dropdown"><option _ngcontent-dps-c462 selected>All<option _ngcontent-dps-c462>Books<option _ngcontent-dps-c462>Conferences<option _ngcontent-dps-c462>Courses<option _ngcontent-dps-c462>Journals &amp; Magazines<option _ngcontent-dps-c462>Standards<option _ngcontent-dps-c462>Authors<option _ngcontent-dps-c462>Citations</select></label></div><div _ngcontent-dps-c462 class="search-field all"><div _ngcontent-dps-c462 class=search-field-icon-container><div _ngcontent-dps-c462 class=global-search-bar><xpl-typeahead-migr _ngcontent-dps-c462 placeholder name=search-term ulclass="search-within-results ui-autocomplete ui-front ui-menu ui-widget ui-widget-content ui-corner-all" minchars=3 _nghost-dps-c69><div _ngcontent-dps-c69 class="Typeahead text-sm-md-lh"><input _ngcontent-dps-c69 autocomplete=off aria-label="Enter search text" class="Typeahead-input ng-untouched ng-pristine ng-valid" placeholder value type=text></div></xpl-typeahead-migr></div><div _ngcontent-dps-c462 class=search-icon><button _ngcontent-dps-c462 type=submit aria-label=Search class="fa fa-search"></button></div></div></div></form><div _ngcontent-dps-c462 class=below-search-bar><div _ngcontent-dps-c462 class="advanced-search-wrapper text-sm-md-lh"><div _ngcontent-dps-c462 class=advanced-search-div><a _ngcontent-dps-c462 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/search/advanced target=_self><span _ngcontent-dps-c462>ADVANCED SEARCH </span><i _ngcontent-dps-c462 aria-hidden=true class="fas fa-caret-right adv-search-arrow sf-hidden"></i></a></div></div></div></div></xpl-search-bar-migr></div></div></div></xpl-header><div _ngcontent-dps-c455 class=global-ng-wrapper><router-outlet _ngcontent-dps-c455></router-outlet><xpl-document-details _nghost-dps-c196><div _ngcontent-dps-c196 class="row document ng-document stats-document"><div _ngcontent-dps-c196 class="document-main global-content-width-w-rr"><section _ngcontent-dps-c196 class="document-main-leaderboard-ad col-12"><xpl-leaderboard-ad _ngcontent-dps-c196 class=hide-desktop _nghost-dps-c127><div _ngcontent-dps-c127 class="Ads-leaderboard ad-panel" style=display:none><div _ngcontent-dps-c127 class="ad-leaderboard-ad-container sf-hidden"><div _ngcontent-dps-c127 xplgoogleadmigr class="Ads-leaderBoardTablet sf-hidden"></div><div _ngcontent-dps-c127 xplgoogleadmigr class="Ads-leaderBoardMobile sf-hidden"></div></div></div></xpl-leaderboard-ad></section><section _ngcontent-dps-c196 class="document-main-header row"><div _ngcontent-dps-c196 class=col-12><xpl-document-header _ngcontent-dps-c196 _nghost-dps-c148><section _ngcontent-dps-c148 class="document-header row"><div _ngcontent-dps-c148 class="document-header-breadcrumbs-container col-12"><div _ngcontent-dps-c148 class="breadcrumbs col text-sm-md-lh"><span _ngcontent-dps-c148><a _ngcontent-dps-c148 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/browse/periodicals/title/>Journals &amp; Magazines</a><span _ngcontent-dps-c148 class=breadcrumbs-separator> &gt;</span></span><span _ngcontent-dps-c148><a _ngcontent-dps-c148 href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a><span _ngcontent-dps-c148 class=breadcrumbs-separator> &gt;</span></span><span _ngcontent-dps-c148><a _ngcontent-dps-c148 href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/tocresult.jsp?isnumber=8948470&amp;punumber=6287639">Volume: 8</a><span _ngcontent-dps-c148 class=breadcrumbs-separator></span></span><xpl-help-link _ngcontent-dps-c148 id=help tooltiptype=breadcrumb _nghost-dps-c54><a _ngcontent-dps-c54 target=_blank tooltipclass=helplink-tooltip triggers=hover class="icon-size-md u-flex-display-inline" href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/ieee-xplore-training/working-with-documents#interactive-html aria-label="Help using Journal &amp; Magazine documents"><i _ngcontent-dps-c54 class="fa fa-question-circle help-link breadcrumb-help-link-icon"></i></a></xpl-help-link></div></div><div _ngcontent-dps-c148 class="document-header-inner-container row"><div _ngcontent-dps-c148 class=col-12><div _ngcontent-dps-c148 class="row stats-document-header"><div _ngcontent-dps-c148 class="row document-title-fix"><div _ngcontent-dps-c148 class="document-header-title-container col"><div _ngcontent-dps-c148 class="left-container w-100"><h1 _ngcontent-dps-c148 class="document-title text-2xl-md-lh"><span _ngcontent-dps-c148>An Experiment-Based Review of Low-Light Image Enhancement Methods</span></h1><div _ngcontent-dps-c148 class="u-mb-1 u-mt-05 btn-container"><div _ngcontent-dps-c148 class=publisher-title-tooltip><xpl-publisher _ngcontent-dps-c148 tooltipplacement=right _nghost-dps-c107><span _ngcontent-dps-c107 class="text-base-md-lh publisher-info-container black-tooltip"><span _ngcontent-dps-c107 xplhighlight><span _ngcontent-dps-c107><span _ngcontent-dps-c107 class=title>Publisher: </span><span _ngcontent-dps-c107>IEEE</span></span></span></span></xpl-publisher></div><div _ngcontent-dps-c148 class=cite-this-related-btn-wrapper><xpl-cite-this-modal _ngcontent-dps-c148 _nghost-dps-c128><div _ngcontent-dps-c148><button _ngcontent-dps-c148 placement=bottom class="layout-btn-white cite-this-btn">Cite This</button></div></xpl-cite-this-modal></div><div _ngcontent-dps-c148 class="black-tooltip tool-tip-pdf-button"><div _ngcontent-dps-c148 placement=bottom class="pdf-btn-container hide-mobile"><xpl-view-pdf _ngcontent-dps-c148 placement=document-page-desktop _nghost-dps-c115><div _ngcontent-dps-c115><div _ngcontent-dps-c115><a _ngcontent-dps-c115 class="pdf-btn-link stats-document-lh-action-downloadPdf_2 pdf" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/stamp/stamp.jsp?tp=&amp;arnumber=9088214"><i _ngcontent-dps-c115 class="icon-size-md icon red-pdf fas fa-file-pdf"></i><span _ngcontent-dps-c115>PDF</span></a></div></div></xpl-view-pdf><xpl-login-modal-trigger _ngcontent-dps-c148 _nghost-dps-c139></xpl-login-modal-trigger></div></div></div></div><div _ngcontent-dps-c148 class=right-container></div></div></div><div _ngcontent-dps-c148 class=document-main-subheader><div _ngcontent-dps-c148 class=document-main-author-banner><div _ngcontent-dps-c148 class="document-authors-banner stats-document-authors-banner"><div _ngcontent-dps-c148 class="row authors-banner-row u-flex-align-items-center u-flex-wrap-nowrap"><xpl-author-banner _ngcontent-dps-c148 class=authors-banner-row-middle _nghost-dps-c141><div _ngcontent-dps-c141 class="document-authors-banner stats-document-authors-banner"><div _ngcontent-dps-c141 class="row authors-banner-row u-flex-wrap-nowrap"><div _ngcontent-dps-c141 class=authors-banner-row-middle><div _ngcontent-dps-c141 class="authors-container stats-document-authors-banner-authorsContainer"><div _ngcontent-dps-c141 class="authors-info-container overflow-ellipsis text-base-md-lh authors-minimized" id=indexTerms-container-1665659765513-0><span _ngcontent-dps-c141 class=authors-info><span _ngcontent-dps-c141 class=blue-tooltip><a _ngcontent-dps-c141 placement="bottom auto" triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37538907700><span _ngcontent-dps-c141>Wencheng Wang</span></a></span><span _ngcontent-dps-c141 class=u-px-02><a _ngcontent-dps-c141 target=_blank href=https://orcid.org/0000-0002-0888-9225><i _ngcontent-dps-c141 class="icon icon-orcid"></i></a></span><span _ngcontent-dps-c141>; </span></span><span _ngcontent-dps-c141 class=authors-info><span _ngcontent-dps-c141 class=blue-tooltip><a _ngcontent-dps-c141 placement="bottom auto" triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37085728648><span _ngcontent-dps-c141>Xiaojin Wu</span></a></span><span _ngcontent-dps-c141>; </span></span><span _ngcontent-dps-c141 class=authors-info><span _ngcontent-dps-c141 class=blue-tooltip><a _ngcontent-dps-c141 placement="bottom auto" triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37403856800><span _ngcontent-dps-c141>Xiaohui Yuan</span></a></span><span _ngcontent-dps-c141 class=u-px-02><a _ngcontent-dps-c141 target=_blank href=https://orcid.org/0000-0001-6897-4563><i _ngcontent-dps-c141 class="icon icon-orcid"></i></a></span><span _ngcontent-dps-c141>; </span></span><span _ngcontent-dps-c141 class=authors-info><span _ngcontent-dps-c141 class=blue-tooltip><a _ngcontent-dps-c141 placement="bottom auto" triggers=hover href=https://ieeexplore-ieee-org.thi.idm.oclc.org/author/37085727769><span _ngcontent-dps-c141>Zairui Gao</span></a></span><span _ngcontent-dps-c141 class=u-px-02><a _ngcontent-dps-c141 target=_blank href=https://orcid.org/0000-0001-8830-3687><i _ngcontent-dps-c141 class="icon icon-orcid"></i></a></span><span _ngcontent-dps-c141></span></span></div></div></div></div></div></xpl-author-banner><div _ngcontent-dps-c148 class="u-flex-display-flex u-flex-align-items-center nowrap text-base-md-lh"><div _ngcontent-dps-c148 class="authors-view-all-link-container hide-mobile"><a _ngcontent-dps-c148 class=text-base-md-lh>All Authors</a></div><div _ngcontent-dps-c148 class="authors-mobile-view-all-container blue-tooltip hide-desktop"><a _ngcontent-dps-c148 placement=bottom-right triggers=click:click class=authors-viewall-link><i _ngcontent-dps-c148 class=authors-viewall-icon></i></a></div></div></div></div></div><div _ngcontent-dps-c148 class="document-header-metrics-banner row ccby-document"><div _ngcontent-dps-c148 class="document-banner col stats-document-banner"><xpl-login-modal-trigger _ngcontent-dps-c148 _nghost-dps-c139></xpl-login-modal-trigger><button _ngcontent-dps-c148 class="sip-modal-button stats-document-banner-viewDocument"><div _ngcontent-dps-c148 class=main-txt> View Document </div></button><div _ngcontent-dps-c148 class="document-banner-metric-container row"><button _ngcontent-dps-c148 class="document-banner-metric text-base-md-lh col"><div _ngcontent-dps-c148 class=document-banner-metric-count>40</div><div _ngcontent-dps-c148>Paper</div><div _ngcontent-dps-c148>Citations</div></button><button _ngcontent-dps-c148 class="document-banner-metric text-base-md-lh col"><div _ngcontent-dps-c148 class=document-banner-metric-count>8741</div><div _ngcontent-dps-c148><div _ngcontent-dps-c148>Full</div><div _ngcontent-dps-c148>Text Views</div></div></button></div><div _ngcontent-dps-c148 class=document-banner-access><div _ngcontent-dps-c148 class="document-access-container hide-mobile"><div _ngcontent-dps-c148 class=document-access-icon><i _ngcontent-dps-c148 class="icon-size-md u-mr-05 fas fa-lock-open-alt"></i><span _ngcontent-dps-c148>Open Access</span></div></div><div _ngcontent-dps-c148 class="document-disqus-anchor-container hide-mobile-imp"><a _ngcontent-dps-c148 tabindex=0><i _ngcontent-dps-c148 class="icon-size-md color-xplore-blue fas fa-comment u-mr-05"></i><span _ngcontent-dps-c148>Comment(s)</span></a></div></div></div><div _ngcontent-dps-c148 class=document-mobile-access-container><div _ngcontent-dps-c148 class=document-access-icon><i _ngcontent-dps-c148 class="icon-size-md u-mr-05 fas fa-lock-open-alt"></i></div></div><div _ngcontent-dps-c148 class="document-disqus-anchor-container hide-desktop"><a _ngcontent-dps-c148 tabindex=0><i _ngcontent-dps-c148 class="icon-size-md color-xplore-blue fas fa-comment"></i></a></div><div _ngcontent-dps-c148 class="col-7-24 black-tooltip hide-mobile"><xpl-document-toolbar _ngcontent-dps-c148 _nghost-dps-c147><div _ngcontent-dps-c147 class="col-actions stats-document-container-lh u-printing-invisible-ie u-printing-invisible-ff"><div _ngcontent-dps-c147 class=action-item-container><ul _ngcontent-dps-c147 class="icon-size-md doc-actions doc-toolbar stats-document-lh-actions black-tooltip"><li _ngcontent-dps-c147 placement=bottom class=doc-actions-item><a _ngcontent-dps-c147 target=blank class="doc-actions-link stats_ReferencesView_Doc_Details_9088214" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/dwnldReferences?arnumber=9088214"><i _ngcontent-dps-c147 class="icon-size-md color-xplore-blue fas fa-registered"></i></a><li _ngcontent-dps-c147 placement=bottom class="doc-actions-item white-blue-border-tooltip"><xpl-document-social-media _ngcontent-dps-c147 _nghost-dps-c144><button _ngcontent-dps-c144 triggers=click class=doc-share-tool><i _ngcontent-dps-c144 aria-hidden=true class="fa fa-share-alt"></i></button></xpl-document-social-media><li _ngcontent-dps-c147 placement=bottom class="stats-permission doc-actions-item disabled-look enable-hover"><a _ngcontent-dps-c147 class="doc-actions-link stats_Doc_Details_Copyright_9088214"><i _ngcontent-dps-c147 class="color-xplore-blue icon-size-md far fa-copyright"></i></a><li _ngcontent-dps-c147 placement=bottom class="doc-actions-item white-blue-border-tooltip save-to disabled-look enable-hover"><a _ngcontent-dps-c147 placement=bottom-right triggers=click class=doc-save-tool><i _ngcontent-dps-c147 class="icon-size-md color-xplore-blue fas fa-folder-open"></i></a><li _ngcontent-dps-c147 placement=bottom class=doc-actions-item><xpl-manage-alerts _ngcontent-dps-c147 class="white-blue-border-tooltip alerts-popover" _nghost-dps-c146><a _ngcontent-dps-c146 triggers=click:click class="doc-actions-link stats-document-lh-action-alerts hide-mobile"><i _ngcontent-dps-c146 class="icon-size-md color-xplore-blue fas fa-bell"></i><span _ngcontent-dps-c146 class=doc-actions-text>Alerts</span></a><div _ngcontent-dps-c146 class="manage-alerts-popover-content hide-desktop"><h1 _ngcontent-dps-c146 class=header>Alerts</h1><div _ngcontent-dps-c146 class=manage-alerts-link><a _ngcontent-dps-c146 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/alerts/citation> Manage Content Alerts <i _ngcontent-dps-c146 class="icon icon-courses-chevron-blue"></i></a></div><div _ngcontent-dps-c146 class=manage-alerts-link><a _ngcontent-dps-c146> Add to Citation Alerts <i _ngcontent-dps-c146 class="icon icon-courses-chevron-blue"></i></a></div></div></xpl-manage-alerts></ul></div></div></xpl-document-toolbar></div></div><div _ngcontent-dps-c148 class="ccby-indicator u-pl-2"> Under a <a _ngcontent-dps-c148 target=_blank href=https://creativecommons.org/licenses/by/4.0/>Creative Commons License</a></div></div></div></div></div><hr _ngcontent-dps-c148></section></xpl-document-header></div></section><div _ngcontent-dps-c196 class="row document-main-body"><div _ngcontent-dps-c196 class="document-main-left-trail col-5-24"><div _ngcontent-dps-c196 class=col-24-24><div _ngcontent-dps-c196 class=row><nav _ngcontent-dps-c196 class="col-24-24 bg-ltgry tab-nav text-base-md-lh"><div _ngcontent-dps-c196 id=document-tabs class="doc-tabs-list stats-document-tabs"><div _ngcontent-dps-c196 routerlinkactive=active class="browse-pub-tab active"><a _ngcontent-dps-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214>Abstract</a></div><xpl-full-text-toc _ngcontent-dps-c196 class=hide-mobile _nghost-dps-c149><div _ngcontent-dps-c149 class=toc-container><div _ngcontent-dps-c149 class=toc-heading>Document Sections</div><ul _ngcontent-dps-c149 class=toc-list><li _ngcontent-dps-c149 class=toc-list-item><a _ngcontent-dps-c149 class=toc-list-link tabindex=0><div _ngcontent-dps-c149 class=toc-list-icon>I.</div><div _ngcontent-dps-c149 class=toc-list-icon></div><div _ngcontent-dps-c149>Introduction</div></a><li _ngcontent-dps-c149 class=toc-list-item><a _ngcontent-dps-c149 class=toc-list-link tabindex=0><div _ngcontent-dps-c149 class=toc-list-icon>II.</div><div _ngcontent-dps-c149 class=toc-list-icon></div><div _ngcontent-dps-c149>Classification of Low-Light Image Enhancement Algorithms</div></a><li _ngcontent-dps-c149 class=toc-list-item><a _ngcontent-dps-c149 class=toc-list-link tabindex=0><div _ngcontent-dps-c149 class=toc-list-icon>III.</div><div _ngcontent-dps-c149 class=toc-list-icon></div><div _ngcontent-dps-c149>Evaluation Methods</div></a><li _ngcontent-dps-c149 class=toc-list-item><a _ngcontent-dps-c149 class=toc-list-link tabindex=0><div _ngcontent-dps-c149 class=toc-list-icon>IV.</div><div _ngcontent-dps-c149 class=toc-list-icon></div><div _ngcontent-dps-c149>Analysis of Different Enhancement Methods</div></a><li _ngcontent-dps-c149 class=toc-list-item><a _ngcontent-dps-c149 class=toc-list-link tabindex=0><div _ngcontent-dps-c149 class=toc-list-icon>V.</div><div _ngcontent-dps-c149 class=toc-list-icon></div><div _ngcontent-dps-c149>Conclusion</div></a></ul><button _ngcontent-dps-c149 class=toc-show-more-btn><span _ngcontent-dps-c149>Show Full Outline</span><i _ngcontent-dps-c149 class="fa fa-caret-down"></i></button></div></xpl-full-text-toc><div _ngcontent-dps-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-dps-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/authors>Authors</a></div><div _ngcontent-dps-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-dps-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/figures>Figures</a></div><div _ngcontent-dps-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-dps-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/references>References</a></div><div _ngcontent-dps-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-dps-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/citations>Citations</a></div><div _ngcontent-dps-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-dps-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/keywords>Keywords</a></div><div _ngcontent-dps-c196 routerlinkactive=active class=browse-pub-tab><a _ngcontent-dps-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/metrics>Metrics</a></div><div _ngcontent-dps-c196 routerlinkactive=active class="browse-pub-tab similar"><a _ngcontent-dps-c196 class=document-tab-link href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/similar>More Like This</a></div></div></nav></div></div></div><div _ngcontent-dps-c196 class=document-main-content-container><xpl-left-side-bar _ngcontent-dps-c196 _nghost-dps-c151><div _ngcontent-dps-c151 xplscrollsnapmigr scrollreset=true offsetfrom=100 fromelementid=mobile-tab-pane tillelementid=full-text-footer offsetto=-800 cssclasstostick=document-mobile-leftrail-stick class="col-2 col-actions ng-col-actions hide-desktop stats-document-container-lh u-printing-invisible-ie u-printing-invisible-ff col-actions-mobile-closed ng-col-actions-mobile-closed"><div _ngcontent-dps-c151 id=left-rail-container><div _ngcontent-dps-c151 class=doc-actions-mobile-expand-button></div><ul _ngcontent-dps-c151 class="doc-actions stats-document-lh-actions"><li _ngcontent-dps-c151 class=doc-actions-item><xpl-view-pdf _ngcontent-dps-c151 placement=document-page-mobile _nghost-dps-c115><div _ngcontent-dps-c115><div _ngcontent-dps-c115><a _ngcontent-dps-c115 target=_blank class="doc-actions-link stats-document-lh-action-downloadPdf_2 pdf" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/stamp/stamp.jsp?tp=&amp;arnumber=9088214"><i _ngcontent-dps-c115 class="icon-size-md icon red-pdf fas fa-file-pdf"></i> Download PDF </a></div></div></xpl-view-pdf><xpl-login-modal-trigger _ngcontent-dps-c151 _nghost-dps-c139></xpl-login-modal-trigger><li _ngcontent-dps-c151 class=doc-actions-item><a _ngcontent-dps-c151 target=blank class="doc-actions-link stats_ReferencesView_Doc_Details_9088214" href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/dwnldReferences?arnumber=9088214"><i _ngcontent-dps-c151 class="icon-size-md color-xplore-blue fas fa-registered"></i> View References </a><li _ngcontent-dps-c151 class="doc-actions-item white-blue-border-tooltip"><a _ngcontent-dps-c151 class=doc-actions-link><xpl-document-social-media _ngcontent-dps-c151 tooltipplacement=right placement=document-page-mobile _nghost-dps-c144><button _ngcontent-dps-c144 triggers=click class=doc-share-tool><i _ngcontent-dps-c144 aria-hidden=true class="fa fa-share-alt"></i></button></xpl-document-social-media></a><li _ngcontent-dps-c151 class="stats-permission doc-actions-item disabled-look black-tooltip"><a _ngcontent-dps-c151 placement=right triggers=click class="doc-actions-link stats_Doc_Details_Copyright_9088214"><i _ngcontent-dps-c151 class="copyright-icon far fa-copyright"></i> Request Permissions </a><li _ngcontent-dps-c151 class="doc-actions-item disabled-look black-tooltip"><a _ngcontent-dps-c151 placement=right triggers=click autoclose=outside class="doc-actions-link stats-document-lh-action-downloadPdf_3"><i _ngcontent-dps-c151 class="icon-size-md color-xplore-blue fas fa-folder-open"></i> Save to </a><li _ngcontent-dps-c151 class=doc-actions-item><a _ngcontent-dps-c151 class="doc-actions-link stats-document-lh-action-alerts"><i _ngcontent-dps-c151 class="icon-size-md color-xplore-blue fas fa-bell"></i> Alerts </a></ul></div></div></xpl-left-side-bar><section _ngcontent-dps-c196 class="tab-pane col-24-24 u-printing-display-inline-ie u-printing-display-inline-ff"><div _ngcontent-dps-c196 id=mobile-tab-pane></div><div _ngcontent-dps-c196 class=document-main-left-trail-content><div _ngcontent-dps-c196><router-outlet _ngcontent-dps-c196></router-outlet><xpl-document-abstract _nghost-dps-c154><section _ngcontent-dps-c154 class="document-abstract document-tab"><div _ngcontent-dps-c154 class="col-12 hide-desktop mobile-graphical-abstract"><div _ngcontent-dps-c154 class=abstract-graphic><xpl-graphical-abstract-modal _ngcontent-dps-c154 styleclassname=cover-image-document _nghost-dps-c112><a _ngcontent-dps-c112 class="cover-image-document stats-graphical_abstract_thumbnail_9088214" aria-label="The main techniques of low-light image enhancement developed over the past decades are reviewed and these methods are divides into seven classes."><img _ngcontent-dps-c112 src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="660" height="295"><rect fill-opacity="0"/></svg>' style="background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-3)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important"></a></xpl-graphical-abstract-modal><div _ngcontent-dps-c154 class=abstract-graphic-caption><span _ngcontent-dps-c154 xplmathjax>The main techniques of low-light image enhancement developed over the past decades are reviewed and these methods are divides into seven classes.</span></div></div></div><div _ngcontent-dps-c154 class="abstract-mobile-div hide-desktop"><div _ngcontent-dps-c154 class=row><div _ngcontent-dps-c154 class=mobile-col-12><div _ngcontent-dps-c154 class=u-pb-1><strong _ngcontent-dps-c154> Abstract:</strong><span _ngcontent-dps-c154 xplmathjax>Images captured under poor illumination conditions often exhibit characteristics such as low brightness, low contrast, a narrow gray range, and color distortion, as well ...</span><span _ngcontent-dps-c154><a _ngcontent-dps-c154 class=mobile-toggle-btn>View more</a></span></div></div></div><div _ngcontent-dps-c154 class="metadata-toggle-btn mobile-content"><strong _ngcontent-dps-c154><i _ngcontent-dps-c154 class=icon-caret-abstract></i><span _ngcontent-dps-c154>Metadata</span></strong></div></div><div _ngcontent-dps-c154 class="abstract-desktop-div hide-mobile text-base-md-lh"><div _ngcontent-dps-c154 class="abstract-text row"><div _ngcontent-dps-c154 class=col-12><div _ngcontent-dps-c154 class=u-mb-1><strong _ngcontent-dps-c154> Abstract:</strong><div _ngcontent-dps-c154 xplmathjax>Images captured under poor illumination conditions often exhibit characteristics such as low brightness, low contrast, a narrow gray range, and color distortion, as well as considerable noise, which seriously affect the subjective visual effect on human eyes and greatly limit the performance of various machine vision systems. The role of low-light image enhancement is to improve the visual effect of such images for the benefit of subsequent processing. This paper reviews the main techniques of low-light image enhancement developed over the past decades. First, we present a new classification of these algorithms, dividing them into seven categories: gray transformation methods, histogram equalization methods, Retinex methods, frequency-domain methods, image fusion methods, defogging model methods and machine learning methods. Then, all the categories of methods, including subcategories, are introduced in accordance with their principles and characteristics. In addition, various quality evaluation methods for enhanced images are detailed, and comparisons of different algorithms are discussed. Finally, the current research progress is summarized, and future research directions are suggested.</div></div></div></div><div _ngcontent-dps-c154 data-tealium_data='{"docType": "Journal"}' class="u-pb-1 stats-document-abstract-publishedIn"><strong _ngcontent-dps-c154>Published in: </strong><a _ngcontent-dps-c154 class=stats-document-abstract-publishedIn href="https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/RecentIssue.jsp?punumber=6287639">IEEE Access</a><span _ngcontent-dps-c154> ( <span _ngcontent-dps-c154>Volume: 8</span>) </span></div><div _ngcontent-dps-c154 class="row u-pt-1"><div _ngcontent-dps-c154 class=col-6><div _ngcontent-dps-c154 class=u-pb-1><strong _ngcontent-dps-c154>Page(s): </strong> 87884 <span _ngcontent-dps-c154>- 87917</span></div><div _ngcontent-dps-c154 class="u-pb-1 doc-abstract-pubdate"><strong _ngcontent-dps-c154>Date of Publication:</strong> 06 May 2020 <xpl-help-link _ngcontent-dps-c154 arialabel="Get help with using Publication Dates" helplinktext="Help with using Publication Dates" helplink=http://ieeexplore.ieee.org.thi.idm.oclc.org/Xplorehelp/Help_Pubdates.html _nghost-dps-c54><a _ngcontent-dps-c54 target=_blank tooltipclass=helplink-tooltip triggers=hover class="icon-size-md u-flex-display-inline" href=http://ieeexplore.ieee.org.thi.idm.oclc.org/Xplorehelp/Help_Pubdates.html aria-label="Get help with using Publication Dates"><i _ngcontent-dps-c54 class="fa fa-question-circle help-link help-link-icon"></i></a></xpl-help-link></div><div _ngcontent-dps-c154 class=u-pb-1><div _ngcontent-dps-c154><div _ngcontent-dps-c154><strong _ngcontent-dps-c154>Electronic ISSN:</strong> 2169-3536 </div></div></div></div><div _ngcontent-dps-c154 class=col-6><div _ngcontent-dps-c154 class=u-pb-1><strong _ngcontent-dps-c154>INSPEC Accession Number: </strong> 19659086 </div><div _ngcontent-dps-c154 class="u-pb-1 stats-document-abstract-doi"><strong _ngcontent-dps-c154>DOI: </strong><a _ngcontent-dps-c154 append-to-href="?src=document" target=_blank href=https://doi-org.thi.idm.oclc.org/10.1109/ACCESS.2020.2992749>10.1109/ACCESS.2020.2992749</a></div><div _ngcontent-dps-c154 class="u-pb-1 doc-abstract-publisher"><xpl-publisher _ngcontent-dps-c154 _nghost-dps-c107><span _ngcontent-dps-c107 class="text-base-md-lh publisher-info-container black-tooltip"><span _ngcontent-dps-c107 xplhighlight><span _ngcontent-dps-c107><span _ngcontent-dps-c107 class=title>Publisher: </span><span _ngcontent-dps-c107>IEEE</span></span></span></span></xpl-publisher></div></div><div _ngcontent-dps-c154 class="col-12 u-pb-1 stats-document-abstract-fundedBy"><div _ngcontent-dps-c154 role=button><strong _ngcontent-dps-c154><i _ngcontent-dps-c154 class=icon-caret-abstract></i>Funding Agency: </strong></div></div></div><div _ngcontent-dps-c154 class=row><div _ngcontent-dps-c154 class=col-12><div _ngcontent-dps-c154 class=abstract-graphic><div _ngcontent-dps-c154 class="row u-flex-justify-center"><xpl-graphical-abstract-modal _ngcontent-dps-c154 styleclassname=cover-image-document _nghost-dps-c112><a _ngcontent-dps-c112 class="cover-image-document stats-graphical_abstract_thumbnail_9088214" aria-label="The main techniques of low-light image enhancement developed over the past decades are reviewed and these methods are divides into seven classes."><img _ngcontent-dps-c112 src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="660" height="295"><rect fill-opacity="0"/></svg>' style="background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-3)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important"></a></xpl-graphical-abstract-modal></div><div _ngcontent-dps-c154 class=abstract-graphic-caption><span _ngcontent-dps-c154 xplmathjax>The main techniques of low-light image enhancement developed over the past decades are reviewed and these methods are divides into seven classes.</span></div></div></div><div _ngcontent-dps-c154 class="show-full-abstract col-12"><a _ngcontent-dps-c154 class=document-abstract-toggle-btn> Hide Full Abstract <i _ngcontent-dps-c154 class="fa fa-angle-up"></i></a></div></div></div></section></xpl-document-abstract></div><xpl-leaderboard-middle-ad _ngcontent-dps-c196 class=hide-desktop _nghost-dps-c155><div _ngcontent-dps-c155 class="Ads-leaderboard ad-panel"><div _ngcontent-dps-c155 class="row u-flex-wrap-nowrap"><div _ngcontent-dps-c155 class=ads-close-container><i _ngcontent-dps-c155 aria-hidden=true class=ads-close-button></i></div></div><div _ngcontent-dps-c155 class=ad-leaderboard-ad-container><div _ngcontent-dps-c155 xplgoogleadmigr class=Ads-leaderBoardMiddleTablet><div id=div-gpt-ad-1606861708257-0 style="width:576px;height:71px;display:none;margin:0px auto;padding-bottom:0.5em"></div></div><div _ngcontent-dps-c155 xplgoogleadmigr class=Ads-leaderBoardMiddleMobile><div id=div-gpt-ad-1606861708357-0 style="width:320px;height:50px;margin:0px auto;padding-bottom:0.5em" data-google-query-id=CJjot5WK3foCFeKe_QcdKCQJEA><div id=google_ads_iframe_/3890430/IEEEXplore/DocDetailsMiddle_1__container__ style="border:0pt none"></div></div></div></div></div></xpl-leaderboard-middle-ad><xpl-document-full-text _ngcontent-dps-c196 _nghost-dps-c167><section _ngcontent-dps-c167><div _ngcontent-dps-c167 id=toc-wrapper class="row full-text-toc-wrapper"><div _ngcontent-dps-c167 xplscrollsnapmigr cssclasstostick=document-toc-stick fromelementid=toc-wrapper tillelementid=full-text-footer offsetfrom=150 offsetto=-800 scrollreset=true class="col-12 u-align-center ft-toc previous-next-nav-ctrl hide-desktop"><div _ngcontent-dps-c167 class="toc-container hide-desktop"><a _ngcontent-dps-c167 ngclass="{'disabled': !toc}" class="toc-link {'disabled': !toc}"><img _ngcontent-dps-c167 src="data:image/png;base64,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"> Contents </a></div></div></div><hr _ngcontent-dps-c167><div _ngcontent-dps-c167 class="row document-full-text-content"><div _ngcontent-dps-c167 id=full-text-section class="col col-text stats-document-container-fullTextSection u-printing-display-inline-ie u-printing-display-inline-ff" style=font-size:15px><span _ngcontent-dps-c167 id=full-text-header></span><div _ngcontent-dps-c167><div _ngcontent-dps-c167 xplmathjax xplfulltextdomhandler parentid=full-text-section xpllazyloadfigures class="document-text hide-full-text ng-non-bindable stats-document-dynamicFullTextOrSnippet-container">
<response><accesstype>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via <a class=vglnk href=https://creativecommons.org/licenses/by/4.0/ rel=nofollow><span>https</span><span>://</span><span>creativecommons</span><span>.</span><span>org</span><span>/</span><span>licenses</span><span>/</span><span>by</span><span>/</span><span>4</span><span>.</span><span>0</span><span>/</span></a> to obtain full-text articles and stipulations in the API documentation.</accesstype><div id=BodyWrapper class=ArticlePage xmlns:ieee=http://www.ieeexplore.ieee.org.thi.idm.oclc.org><div id=article>
<div class=section id=sec1><div class="header article-hdr"><div class=kicker>
 SECTION I.</div><h2>Introduction</h2></div><p>With the rapid development of computer vision technology, digital image processing systems have been widely used in many fields, such as industrial production <a ref-type=bibr anchor=ref1 id=context_ref_1_1>[1]</a>, video monitoring <a ref-type=bibr anchor=ref2 id=context_ref_2_1>[2]</a>, intelligent transportation <a ref-type=bibr anchor=ref3 id=context_ref_3_1>[3]</a>, and remote sensing monitoring, and thus play important roles in industrial production <a ref-type=bibr anchor=ref4 id=context_ref_4_1>[4]</a>, daily life <a ref-type=bibr anchor=ref5 id=context_ref_5_1>[5]</a>, military applications <a ref-type=bibr anchor=ref6 id=context_ref_6_1>[6]</a>, etc. However, some uncontrollable factors often exist during the process of image acquisition, resulting in various image defects. In particular, under poor illumination conditions, such as indoors, nighttime, or cloudy days, the light reflected from the object surface may be weak; consequently, the image quality of such a low-light image may be seriously degraded due to color distortions and noise <a ref-type=bibr anchor=ref7 id=context_ref_7_1>[7]</a>–<a ref-type=bibr anchor=ref10 id=context_ref_10_1>[10]</a>. After image conversion, storage, transmission and other operations, the quality of this kind of low-light image is seriously further reduced.<p>Low light, as the name implies, refers to the environmental conditions where illuminance does not meet the normal standard <a ref-type=bibr anchor=ref11 id=context_ref_11_1>[11]</a>. Any images captured in an environment with relatively weak light are often regarded as low-light images <a ref-type=bibr anchor=ref12 id=context_ref_12_1>[12]</a>, <a ref-type=bibr anchor=ref13 id=context_ref_13_1>[13]</a>. Nevertheless, it has thus far been impossible to identify specific theoretical values that define a low-light environment in practical applications, and consequently, no unified standard exists. Therefore, each image-sensor manufacturer has its own standards; for example, Hikvision usually classifies low-light environments into the following categories: dark level (0.01 Lux - 0.1 Lux), moonlight level (0.001 Lux - 0.01 Lux) and starlight level (less than 0.001 Lux). Images captured in these types of environments exhibit characteristics such as low brightness, low contrast, a narrow gray range and color distortion as well as considerable noise <a ref-type=bibr anchor=ref14 id=context_ref_14_1>[14]</a>, <a ref-type=bibr anchor=ref15 id=context_ref_15_1>[15]</a>. <a ref-type=fig anchor=fig1 class=fulltext-link>Fig. 1</a> shows three images with low brightness and their corresponding gray histograms, where the X-axis shows the grayscale values and the Y-axis represents the number of pixels. The pixel values of these images are mainly focused in the lower range due to the lack of illumination, and the gray difference of the corresponding pixels between the various channels of the color image is limited. There is only a small gap between the maximum and minimum gray levels of the image. The whole color layer exhibits deviations, and the edge information is weak; consequently, it is difficult to distinguish details of the image. These characteristics reduce the usability of such images, seriously degrade their subjective visual effect, and greatly limit the functionality of various visual systems <a ref-type=bibr anchor=ref16 id=context_ref_16_1>[16]</a>–<a ref-type=bibr anchor=ref18 id=context_ref_18_1>[18]</a>.
<div class="figure figure-full" id=fig1><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang1-2992749-large.gif data-fig-id=fig1><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang1-2992749-small.gif data-alt="FIGURE 1. - Examples of low-light images."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 1. </b><fig><p>Examples of low-light images.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>To weaken the impact of video/image acquisition from low-illumination environments, researchers have pursued various improvements from both the hardware and software perspectives. One approach is to improve the image acquisition system hardware <a ref-type=bibr anchor=ref19 id=context_ref_19_1>[19]</a>–<a ref-type=bibr anchor=ref21 id=context_ref_21_1>[21]</a>. Another is to process the images after they are generated. Because low-illumination cameras use high-performance charge-coupled device (CCD) or complementary metal–oxide–semiconductor (CMOS) technology, professional low-light circuits, and filters as the core components to improve the imaging quality for low-light-level imaging, their manufacturing process is highly rigorous, and the technology is complex <a ref-type=bibr anchor=ref22 id=context_ref_22_1>[22]</a>. Although some professional low-light cameras produced by companies such as Sony, Photonis, SiOnyx and Texas Instruments (TI) have appeared on the market, they are not widely used in daily life because of their high prices. As an alternative approach, the improvement of software algorithms offers great flexibility, and improving the quality of low-light videos and images by means of digital image processing has always been an important direction of research. Therefore, it is of great significance and practical value to study enhancement algorithms for low-light images to improve the performance of imaging devices.<p>The main purpose of low-light image enhancement is to improve the overall and local contrast of the image, improve its visual effect, and transform the image into a form more suitable for human observation or computer processing, while avoiding noise amplification and achieving good real-time performance. <a ref-type=bibr anchor=ref23 id=context_ref_23_1>[23]</a>–<a ref-type=bibr anchor=ref27 id=context_ref_27_1>[27]</a>. To this end, it is essential to enhance the validity and availability of data captured under low illumination to obtain clear images or videos <a ref-type=bibr anchor=ref28 id=context_ref_28_1>[28]</a>. Such enhancement can not only render images more consistent with the subjective visual perception of individuals and improve the reliability and robustness of outdoor visual systems but also allow such images to be more conveniently analyzed and processed by computer vision equipment, which is of great importance for promoting the development of image information mining <a ref-type=bibr anchor=ref29 id=context_ref_29_1>[29]</a>, <a ref-type=bibr anchor=ref30 id=context_ref_30_1>[30]</a>. Related research results can be widely applied in fields such as urban traffic monitoring, outdoor video acquisition, satellite remote sensing, and military aviation investigation and can be used as a reference for studies on topics such as underwater image analysis and haze image clarity <a ref-type=bibr anchor=ref31 id=context_ref_31_1>[31]</a>. Moreover, as an important branch of research in the field of image processing, low-light image enhancement has interdisciplinary and innovative appeal and broad application prospects and has become a focus of interdisciplinary research in recent years <a ref-type=bibr anchor=ref32 id=context_ref_32_1>[32]</a>. A large number of researchers at home and abroad have been paying increasing attention to this field for quite some time <a ref-type=bibr anchor=ref33 id=context_ref_33_1>[33]</a>–<a ref-type=bibr anchor=ref38 id=context_ref_38_1>[38]</a>.<p>In the real world, color images are most commonly used, so most of the algorithms are either designed for color image enhancement or derived from gray image enhancement methods. The major methods are listed below.
<ol><li><p>Enhancement based on the RGB (red, green, blue) color space. The specific steps are as follows. The three color components (R, G and B) are extracted from the original RGB color image. Then, these three components are each individually enhanced using a grayscale image enhancement method. Finally, the three components are merged, and the enhanced results are output. The specific principle is visually summarized in <a ref-type=fig anchor=fig2 class=fulltext-link>Fig. 2</a>. This method is simple but can result in serious color deviations in the enhanced images because it neglects the correlations between the components.</p><li><p>Enhancement based on the HSI (hue, saturation, intensity) color space (or the YCbCr, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\text{L}{}^\ast \text{a}{}^\ast \text{b}</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-1-Frame tabindex=0></span>
</tex-math></inline-formula>, YUV color space) <a ref-type=bibr anchor=ref39 id=context_ref_39_1>[39]</a>–<a ref-type=bibr anchor=ref42 id=context_ref_42_1>[42]</a>. The brightness component <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-2-Frame tabindex=0></span>
</tex-math></inline-formula> in the HSI color space is separate from and unrelated to the chrominance component <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>H</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-3-Frame tabindex=0></span>
</tex-math></inline-formula>, i.e., the color information of an image. When the chrominance does not change, the brightness and saturation will determine all of the image information. Hence, to enhance a color image, the <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-4-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>S</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-5-Frame tabindex=0></span>
</tex-math></inline-formula> components are usually enhanced separately while maintaining the same chromaticity <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>H</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-6-Frame tabindex=0></span>
</tex-math></inline-formula>. <a ref-type=fig anchor=fig3 class=fulltext-link>Fig. 3</a> shows the flow chart of this enhancement approach.</p></ol>
<div class="figure figure-full" id=fig2><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang2-2992749-large.gif data-fig-id=fig2><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang2-2992749-small.gif data-alt="FIGURE 2. - Image enhancement in the RGB color space."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 2. </b><fig><p>Image enhancement in the RGB color space.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig3><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang3-2992749-large.gif data-fig-id=fig3><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang3-2992749-small.gif data-alt="FIGURE 3. - Image enhancement in the HSI color space."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 3. </b><fig><p>Image enhancement in the HSI color space.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>In recent years, a common method used to process color images has been to leave one component unchanged while enhancing the other components based on a space transformation. Notably, the available transformations of the color space are diverse, and the form of an image is not limited to a certain space. Regardless of the color space used, the processing steps are similar to those of an enhancement method based on the HSI space <a ref-type=bibr anchor=ref43 id=context_ref_43_1>[43]</a>–<a ref-type=bibr anchor=ref45 id=context_ref_45_1>[45]</a>. Studies on low-light image enhancement technology are currently still being conducted by many researchers. Although promising findings have been obtained, this technology is still not mature. In particular, the available algorithms often have a better effect in a certain aspect than in others. Thus, the field still has significant research value and offers a large development space that is attractive to researchers.<p>The remainder of this paper is organized as follows. <a ref-type=sec anchor=sec2 class=fulltext-link>Section II</a> introduces a classification of low-light image enhancement algorithms according to their underlying principles, and the characteristics of the different categories of algorithms are analyzed in detail. In <a ref-type=sec anchor=sec3 class=fulltext-link>Section III</a>, related quality assessment criteria for enhanced images are described. Several experiments implemented to test the performance of the representative methods are described in <a ref-type=sec anchor=sec4 class=fulltext-link>Section IV</a>, and the conclusions are summarized and future research directions suggested in the last section.</p></div>
<div class=section id=sec2><div class="header article-hdr"><div class=kicker>
 SECTION II.</div><h2>Classification of Low-Light Image Enhancement Algorithms</h2></div><p>Scholars worldwide have proposed many image enhancement algorithms for images captured under low-illumination conditions to improve low-light videos and images from different perspectives <a ref-type=bibr anchor=ref1 id=context_ref_1_2>[1]</a>, <a ref-type=bibr anchor=ref3 id=context_ref_3_2>[3]</a>, <a ref-type=bibr anchor=ref7 id=context_ref_7_2>[7]</a>. In accordance with the algorithms used for brightness enhancement, this paper divides these processing methods into seven classes: gray transformation methods, histogram equalization (HE) methods, Retinex methods, frequency-domain methods, image fusion methods, defogging model methods and machine learning methods. These methods can be further divided into different subclasses in accordance with the differences in their principles. The overall classification is depicted in <a ref-type=fig anchor=fig4 class=fulltext-link>Fig. 4</a>.
<div class="figure figure-full" id=fig4><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang4-2992749-large.gif data-fig-id=fig4><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang4-2992749-small.gif data-alt="FIGURE 4. - Classification of low-light image enhancement algorithms."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 4. </b><fig><p>Classification of low-light image enhancement algorithms.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><div class=section_2 id=sec2a><h3>A. Gray Transformation Methods</h3><p>A gray transformation method is a spatial-domain image enhancement algorithm based on the principle of transforming the gray values of single pixels into other gray values by means of a mathematical function <a ref-type=bibr anchor=ref46 id=context_ref_46_2a>[46]</a>, which is usually called a mapping-based approach. Such a method enhances an image by modifying the distribution and dynamic range of the gray values of the pixels <a ref-type=bibr anchor=ref1 id=context_ref_1_2a>[1]</a>, <a ref-type=bibr anchor=ref7 id=context_ref_7_2a>[7]</a>. The main subclasses of this type method include linear and nonlinear transformations.<div class=section_2 id=sec2a1><h4>1) Linear Transformation</h4><p>A linear transformation of gray values, also known as a linear stretching, is a linear function of the gray values of the input image <a ref-type=bibr anchor=ref1 id=context_ref_1_2a1>[1]</a>, and the formula is as follows:<disp-formula id=deqn1 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} g(x,y) = C \cdot f(x,y) + R\tag{1}\end{equation*}</span><span class="MathJax_Display MathJax_Processed"><span class=MathJax id=MathJax-Element-7-Frame tabindex=0><nobr><span class=math id=MathJax-Span-29><span style=display:inline-block;position:relative;width:0em;height:0px;font-size:123%><span style=position:absolute><span class=mrow id=MathJax-Span-30><span class=mtable id=MathJax-Span-31 style=min-width:14.565em><span style=display:inline-block;position:relative;width:100%;height:0px;min-width:14.565em><span style=display:inline-block;position:absolute;width:10.409em;height:0px;clip:rect(-0.938em,1010.41em,0.438em,-1000em);top:0em;left:50%;margin-left:-5.205em><span style=position:absolute;clip:rect(3.123em,1010.41em,4.448em,-1000em);top:-4.011em;left:0em><span style=display:inline-block;position:relative;width:10.409em;height:0px><span style=position:absolute;clip:rect(3.098em,1010.41em,4.423em,-1000em);top:-3.986em;left:50%;margin-left:-5.205em><span class=mtd id=MathJax-Span-35><span class=mrow id=MathJax-Span-36><span class=mi id=MathJax-Span-37 style=font-family:MathJax_Math;font-style:italic>g<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span><span class=mo id=MathJax-Span-38 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-39 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-40 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-41 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-42 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-43 style=font-family:MathJax_Main;padding-left:0.278em>=</span><span class=mi id=MathJax-Span-44 style=font-family:MathJax_Math;font-style:italic;padding-left:0.278em>C<span style=display:inline-block;overflow:hidden;height:1px;width:0.045em></span></span><span class=mo id=MathJax-Span-45 style=font-family:MathJax_Main;padding-left:0.222em>⋅</span><span class=mi id=MathJax-Span-46 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span class=mo id=MathJax-Span-47 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-48 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-49 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-50 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-51 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-52 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=mi id=MathJax-Span-53 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>R</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;position:absolute;width:1.278em;height:0px;clip:rect(-0.888em,1001.18em,0.438em,-1000em);top:0em;right:0em;margin-right:0em><span style=position:absolute;clip:rect(3.098em,1001.18em,4.423em,-1000em);top:-3.986em;right:0em><span class=mtd id=mjx-eqn-1><span class=mrow id=MathJax-Span-33><span class=mtext id=MathJax-Span-34 style=font-family:MathJax_Main>(1)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span></span></span></span></nobr></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} g(x,y) = C \cdot f(x,y) + R\tag{1}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f(x,y)</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-8-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>g(x,y)</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-9-Frame tabindex=0></span>
</tex-math></inline-formula> represent the input and output images, respectively, and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>C</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-10-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-11-Frame tabindex=0></span>
</tex-math></inline-formula> are the coefficients of the linear transformation. An image can be enhanced to different degrees by adjusting the values of the coefficients in the above formula. A corresponding transformation curve is shown in <a ref-type=fig anchor=fig5 class=fulltext-link>Fig. 5(a)</a>. A common formula for a linear gray stretch is as follows:<disp-formula id=deqn2 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} g(x,y) = \frac {f(x,y)-f_{\min }}{f_{\max }-f_{\min }} (g_{\max }-g_{\min }) + g_{\min }\tag{2}\end{equation*}</span><span class="MathJax_Display MathJax_Processed"><span class=MathJax id=MathJax-Element-12-Frame tabindex=0><nobr><span class=math id=MathJax-Span-76><span style=display:inline-block;position:relative;width:0em;height:0px;font-size:123%><span style=position:absolute><span class=mrow id=MathJax-Span-77><span class=mtable id=MathJax-Span-78 style=min-width:22.979em><span style=display:inline-block;position:relative;width:100%;height:0px;min-width:22.979em><span style=display:inline-block;position:absolute;width:18.823em;height:0px;clip:rect(-1.596em,1018.82em,1.096em,-1000em);top:0em;left:50%;margin-left:-9.412em><span style=position:absolute;clip:rect(2.415em,1018.82em,5.107em,-1000em);top:-4.011em;left:0em><span style=display:inline-block;position:relative;width:18.823em;height:0px><span style=position:absolute;clip:rect(2.373em,1018.82em,5.064em,-1000em);top:-3.969em;left:50%;margin-left:-9.412em><span class=mtd id=MathJax-Span-82><span class=mrow id=MathJax-Span-83><span class=mi id=MathJax-Span-84 style=font-family:MathJax_Math;font-style:italic>g<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span><span class=mo id=MathJax-Span-85 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-86 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-87 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-88 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-89 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-90 style=font-family:MathJax_Main;padding-left:0.278em>=</span><span class=mfrac id=MathJax-Span-91 style=padding-left:0.278em><span style=display:inline-block;position:relative;width:5.927em;height:0px;margin-right:0.12em;margin-left:0.12em><span style=position:absolute;clip:rect(3.098em,1005.81em,4.423em,-1000em);top:-4.736em;left:50%;margin-left:-2.903em><span class=mrow id=MathJax-Span-92><span class=mi id=MathJax-Span-93 style=font-family:MathJax_Math;font-style:italic>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span class=mo id=MathJax-Span-94 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-95 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-96 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-97 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-98 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-99 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=msubsup id=MathJax-Span-100 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.744em;height:0px><span style=position:absolute;clip:rect(3.143em,1000.55em,4.378em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-101 style=font-family:MathJax_Math;font-style:italic>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.49em><span class=texatom id=MathJax-Span-102><span class=mrow id=MathJax-Span-103><span class=mo id=MathJax-Span-104 style=font-size:70.7%;font-family:MathJax_Main>min</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.143em,1004.85em,4.378em,-1000em);top:-3.325em;left:50%;margin-left:-2.423em><span class=mrow id=MathJax-Span-105><span class=msubsup id=MathJax-Span-106><span style=display:inline-block;position:relative;width:1.881em;height:0px><span style=position:absolute;clip:rect(3.143em,1000.55em,4.378em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-107 style=font-family:MathJax_Math;font-style:italic>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.49em><span class=texatom id=MathJax-Span-108><span class=mrow id=MathJax-Span-109><span class=mo id=MathJax-Span-110 style=font-size:70.7%;font-family:MathJax_Main>max</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-111 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=msubsup id=MathJax-Span-112 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.744em;height:0px><span style=position:absolute;clip:rect(3.143em,1000.55em,4.378em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-113 style=font-family:MathJax_Math;font-style:italic>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.49em><span class=texatom id=MathJax-Span-114><span class=mrow id=MathJax-Span-115><span class=mo id=MathJax-Span-116 style=font-size:70.7%;font-family:MathJax_Main>min</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(0.854em,1005.93em,1.247em,-1000em);top:-1.304em;left:0em><span style="display:inline-block;overflow:hidden;vertical-align:0em;border-top:1.3px solid;width:5.927em;height:0px"></span><span style=display:inline-block;width:0px;height:1.084em></span></span></span></span><span class=mo id=MathJax-Span-117 style=font-family:MathJax_Main>(</span><span class=msubsup id=MathJax-Span-118><span style=display:inline-block;position:relative;width:1.868em;height:0px><span style=position:absolute;clip:rect(3.406em,1000.48em,4.378em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-119 style=font-family:MathJax_Math;font-style:italic>g<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.477em><span class=texatom id=MathJax-Span-120><span class=mrow id=MathJax-Span-121><span class=mo id=MathJax-Span-122 style=font-size:70.7%;font-family:MathJax_Main>max</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-123 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=msubsup id=MathJax-Span-124 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.731em;height:0px><span style=position:absolute;clip:rect(3.406em,1000.48em,4.378em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-125 style=font-family:MathJax_Math;font-style:italic>g<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.477em><span class=texatom id=MathJax-Span-126><span class=mrow id=MathJax-Span-127><span class=mo id=MathJax-Span-128 style=font-size:70.7%;font-family:MathJax_Main>min</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mo id=MathJax-Span-129 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-130 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=msubsup id=MathJax-Span-131 style=padding-left:0.222em><span style=display:inline-block;position:relative;width:1.731em;height:0px><span style=position:absolute;clip:rect(3.406em,1000.48em,4.378em,-1000em);top:-4.011em;left:0em><span class=mi id=MathJax-Span-132 style=font-family:MathJax_Math;font-style:italic>g<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;top:-3.861em;left:0.477em><span class=texatom id=MathJax-Span-133><span class=mrow id=MathJax-Span-134><span class=mo id=MathJax-Span-135 style=font-size:70.7%;font-family:MathJax_Main>min</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;position:absolute;width:1.278em;height:0px;clip:rect(-0.87em,1001.18em,0.455em,-1000em);top:0em;right:0em;margin-right:0em><span style=position:absolute;clip:rect(3.098em,1001.18em,4.423em,-1000em);top:-3.969em;right:0em><span class=mtd id=mjx-eqn-2><span class=mrow id=MathJax-Span-80><span class=mtext id=MathJax-Span-81 style=font-family:MathJax_Main>(2)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span></span></span></span></nobr></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} g(x,y) = \frac {f(x,y)-f_{\min }}{f_{\max }-f_{\min }} (g_{\max }-g_{\min }) + g_{\min }\tag{2}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{\max }</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-13-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{\min }</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-14-Frame tabindex=0></span>
</tex-math></inline-formula> represent the maximum and minimum gray values of the input image, respectively, and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>g_{\max }</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-15-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>g_{\min }</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-16-Frame tabindex=0></span>
</tex-math></inline-formula> represent the maximum and minimum gray values of the output image, respectively <a ref-type=bibr anchor=ref7 id=context_ref_7_2a1>[7]</a>. Thus, the dynamic range of the image is transformed from <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>[f_{\min },f_{\max }]</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-17-Frame tabindex=0></span>
</tex-math></inline-formula> to <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>[g_{\min },g_{\max }]</span><span class="MathJax MathJax_Processed sf-hidden" id=MathJax-Element-18-Frame tabindex=0></span>
</tex-math></inline-formula> to enhance the brightness and contrast. Sometimes, the gray values in only a specific area of the image need to be stretched or compressed by applying a piecewise linear transformation to adjust the contrast; the formula for such a transformation is as follows.<disp-formula id=deqn3 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} g(x,y) = \begin{cases} \dfrac {c}{a} \,f(x,y) &amp; 0 \leq f(x,y) \leq a\\[6pt] \dfrac {d-c}{b-a} (f(x,y)-a)+c &amp; a \leq f(x,y) \leq b\\[6pt] \dfrac {h-d}{e-b} (f(x,y)-b)+d &amp; b \leq f(x,y) \leq e\\ \end{cases}\tag{3}\end{align*}</span><span class="MathJax_Display MathJax_Processed"><span class=MathJax id=MathJax-Element-19-Frame tabindex=0><nobr><span class=math id=MathJax-Span-194><span style=display:inline-block;position:relative;width:0em;height:0px;font-size:123%><span style=position:absolute><span class=mrow id=MathJax-Span-195><span class=mtable id=MathJax-Span-196 style=min-width:26.718em><span style=display:inline-block;position:relative;width:100%;height:0px;min-width:26.718em><span style=display:inline-block;position:absolute;width:22.562em;height:0px;clip:rect(-4.258em,1022.56em,3.758em,-1000em);top:0em;left:50%;margin-left:-11.281em><span style=position:absolute;clip:rect(4.414em,1022.56em,12.43em,-1000em);top:-8.672em;left:0em><span style=display:inline-block;position:relative;width:22.562em;height:0px><span style=position:absolute;clip:rect(4.414em,1022.56em,12.43em,-1000em);top:-8.672em;right:0em><span class=mtd id=MathJax-Span-200><span class=mrow id=MathJax-Span-201><span class=mi id=MathJax-Span-202 style=font-family:MathJax_Math;font-style:italic>g<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span><span class=mo id=MathJax-Span-203 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-204 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-205 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-206 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-207 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-208 style=font-family:MathJax_Main;padding-left:0.278em>=</span><span class=mrow id=MathJax-Span-209 style=padding-left:0.278em><span class=mo id=MathJax-Span-210 style=vertical-align:4.096em><span style=display:inline-block;position:relative;width:0.889em;height:0px><span style=position:absolute;font-family:MathJax_Size4;top:-3.112em;left:0em>⎧<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;font-family:MathJax_Size4;top:2.782em;left:0em>⎩<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;font-family:MathJax_Size4;top:0.085em;left:0em>⎨<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:-2.851em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:-2.59em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:-2.33em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:-2.069em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:-1.808em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:-1.547em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:-1.287em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:-1.026em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:0.996em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:1.256em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:1.517em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:1.778em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:2.039em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:2.299em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:2.56em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span><span style=font-family:MathJax_Size4;position:absolute;top:2.821em;left:0em>⎪<span style=display:inline-block;width:0px;height:4.011em></span></span></span></span><span class=mtable id=MathJax-Span-211 style=padding-right:0.167em;padding-left:0.167em><span style=display:inline-block;position:relative;width:17.234em;height:0px><span style=position:absolute;clip:rect(4.414em,1009.7em,12.43em,-1000em);top:-8.672em;left:0em><span style=display:inline-block;position:relative;width:9.696em;height:0px><span style=position:absolute;clip:rect(2.73em,1003.8em,4.869em,-1000em);top:-6.988em;left:0em><span class=mtd id=MathJax-Span-212><span class=mrow id=MathJax-Span-213><span class=mstyle id=MathJax-Span-214><span class=mrow id=MathJax-Span-215><span class=mfrac id=MathJax-Span-216><span style=display:inline-block;position:relative;width:0.649em;height:0px;margin-right:0.12em;margin-left:0.12em><span style=position:absolute;clip:rect(3.406em,1000.43em,4.184em,-1000em);top:-4.687em;left:50%;margin-left:-0.216em><span class=mi id=MathJax-Span-217 style=font-family:MathJax_Math;font-style:italic>c</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.407em,1000.51em,4.183em,-1000em);top:-3.325em;left:50%;margin-left:-0.265em><span class=mi id=MathJax-Span-218 style=font-family:MathJax_Math;font-style:italic>a</span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(0.854em,1000.65em,1.247em,-1000em);top:-1.304em;left:0em><span style="display:inline-block;overflow:hidden;vertical-align:0em;border-top:1.3px solid;width:0.649em;height:0px"></span><span style=display:inline-block;width:0px;height:1.084em></span></span></span></span></span></span><span class=mspace id=MathJax-Span-219 style=height:0em;vertical-align:0em;width:0.167em;display:inline-block;overflow:hidden></span><span class=mi id=MathJax-Span-220 style=font-family:MathJax_Math;font-style:italic>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span class=mo id=MathJax-Span-221 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-222 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-223 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-224 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-225 style=font-family:MathJax_Main>)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(2.478em,1009.56em,4.941em,-1000em);top:-4.122em;left:0em><span class=mtd id=MathJax-Span-238><span class=mrow id=MathJax-Span-239><span class=mstyle id=MathJax-Span-240><span class=mrow id=MathJax-Span-241><span class=mfrac id=MathJax-Span-242><span style=display:inline-block;position:relative;width:2.3em;height:0px;margin-right:0.12em;margin-left:0.12em><span style=position:absolute;clip:rect(3.154em,1002.17em,4.255em,-1000em);top:-4.687em;left:50%;margin-left:-1.089em><span class=mrow id=MathJax-Span-243><span class=mi id=MathJax-Span-244 style=font-family:MathJax_Math;font-style:italic>d<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span><span class=mo id=MathJax-Span-245 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=mi id=MathJax-Span-246 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>c</span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.154em,1002.16em,4.255em,-1000em);top:-3.325em;left:50%;margin-left:-1.09em><span class=mrow id=MathJax-Span-247><span class=mi id=MathJax-Span-248 style=font-family:MathJax_Math;font-style:italic>b</span><span class=mo id=MathJax-Span-249 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=mi id=MathJax-Span-250 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>a</span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(0.854em,1002.3em,1.247em,-1000em);top:-1.304em;left:0em><span style="display:inline-block;overflow:hidden;vertical-align:0em;border-top:1.3px solid;width:2.3em;height:0px"></span><span style=display:inline-block;width:0px;height:1.084em></span></span></span></span></span></span><span class=mo id=MathJax-Span-251 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-252 style=font-family:MathJax_Math;font-style:italic>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span class=mo id=MathJax-Span-253 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-254 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-255 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-256 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-257 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-258 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=mi id=MathJax-Span-259 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>a</span><span class=mo id=MathJax-Span-260 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-261 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=mi id=MathJax-Span-262 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>c</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(2.478em,1009.7em,4.941em,-1000em);top:-1.183em;left:0em><span class=mtd id=MathJax-Span-275><span class=mrow id=MathJax-Span-276><span class=mstyle id=MathJax-Span-277><span class=mrow id=MathJax-Span-278><span class=mfrac id=MathJax-Span-279><span style=display:inline-block;position:relative;width:2.441em;height:0px;margin-right:0.12em;margin-left:0.12em><span style=position:absolute;clip:rect(3.154em,1002.32em,4.255em,-1000em);top:-4.687em;left:50%;margin-left:-1.161em><span class=mrow id=MathJax-Span-280><span class=mi id=MathJax-Span-281 style=font-family:MathJax_Math;font-style:italic>h</span><span class=mo id=MathJax-Span-282 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=mi id=MathJax-Span-283 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>d<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.154em,1002.11em,4.255em,-1000em);top:-3.325em;left:50%;margin-left:-1.058em><span class=mrow id=MathJax-Span-284><span class=mi id=MathJax-Span-285 style=font-family:MathJax_Math;font-style:italic>e</span><span class=mo id=MathJax-Span-286 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=mi id=MathJax-Span-287 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>b</span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(0.854em,1002.44em,1.247em,-1000em);top:-1.304em;left:0em><span style="display:inline-block;overflow:hidden;vertical-align:0em;border-top:1.3px solid;width:2.441em;height:0px"></span><span style=display:inline-block;width:0px;height:1.084em></span></span></span></span></span></span><span class=mo id=MathJax-Span-288 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-289 style=font-family:MathJax_Math;font-style:italic>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span class=mo id=MathJax-Span-290 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-291 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-292 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-293 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-294 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-295 style=font-family:MathJax_Main;padding-left:0.222em>−</span><span class=mi id=MathJax-Span-296 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>b</span><span class=mo id=MathJax-Span-297 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-298 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=mi id=MathJax-Span-299 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>d<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:8.672em></span></span><span style=position:absolute;clip:rect(3.915em,1006.52em,11.045em,-1000em);top:-7.805em;left:10.696em><span style=display:inline-block;position:relative;width:6.538em;height:0px><span style=position:absolute;clip:rect(3.098em,1006.52em,4.423em,-1000em);top:-6.988em;left:0em><span class=mtd id=MathJax-Span-226><span class=mrow id=MathJax-Span-227><span class=mn id=MathJax-Span-228 style=font-family:MathJax_Main>0</span><span class=mo id=MathJax-Span-229 style=font-family:MathJax_Main;padding-left:0.278em>≤</span><span class=mi id=MathJax-Span-230 style=font-family:MathJax_Math;font-style:italic;padding-left:0.278em>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span class=mo id=MathJax-Span-231 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-232 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-233 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-234 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-235 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-236 style=font-family:MathJax_Main;padding-left:0.278em>≤</span><span class=mi id=MathJax-Span-237 style=font-family:MathJax_Math;font-style:italic;padding-left:0.278em>a</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.098em,1006.46em,4.423em,-1000em);top:-4.122em;left:0em><span class=mtd id=MathJax-Span-263><span class=mrow id=MathJax-Span-264><span class=mi id=MathJax-Span-265 style=font-family:MathJax_Math;font-style:italic>a</span><span class=mo id=MathJax-Span-266 style=font-family:MathJax_Main;padding-left:0.278em>≤</span><span class=mi id=MathJax-Span-267 style=font-family:MathJax_Math;font-style:italic;padding-left:0.278em>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span class=mo id=MathJax-Span-268 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-269 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-270 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-271 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-272 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-273 style=font-family:MathJax_Main;padding-left:0.278em>≤</span><span class=mi id=MathJax-Span-274 style=font-family:MathJax_Math;font-style:italic;padding-left:0.278em>b</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span><span style=position:absolute;clip:rect(3.098em,1006.37em,4.423em,-1000em);top:-1.183em;left:0em><span class=mtd id=MathJax-Span-300><span class=mrow id=MathJax-Span-301><span class=mi id=MathJax-Span-302 style=font-family:MathJax_Math;font-style:italic>b</span><span class=mo id=MathJax-Span-303 style=font-family:MathJax_Main;padding-left:0.278em>≤</span><span class=mi id=MathJax-Span-304 style=font-family:MathJax_Math;font-style:italic;padding-left:0.278em>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span class=mo id=MathJax-Span-305 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-306 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-307 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-308 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-309 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-310 style=font-family:MathJax_Main;padding-left:0.278em>≤</span><span class=mi id=MathJax-Span-311 style=font-family:MathJax_Math;font-style:italic;padding-left:0.278em>e</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:7.805em></span></span></span></span><span class=mo id=MathJax-Span-312></span></span></span></span><span style=display:inline-block;width:0px;height:8.672em></span></span></span><span style=display:inline-block;width:0px;height:8.672em></span></span></span><span style=display:inline-block;position:absolute;width:1.278em;height:0px;clip:rect(-0.913em,1001.18em,0.413em,-1000em);top:0em;right:0em;margin-right:0em><span style=position:absolute;clip:rect(3.098em,1001.18em,4.423em,-1000em);top:-4.011em;right:0em><span class=mtd id=mjx-eqn-3><span class=mrow id=MathJax-Span-198><span class=mtext id=MathJax-Span-199 style=font-family:MathJax_Main>(3)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span></span></span></span></nobr></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} g(x,y) = \begin{cases} \dfrac {c}{a} \,f(x,y) &amp; 0 \leq f(x,y) \leq a\\[6pt] \dfrac {d-c}{b-a} (f(x,y)-a)+c &amp; a \leq f(x,y) \leq b\\[6pt] \dfrac {h-d}{e-b} (f(x,y)-b)+d &amp; b \leq f(x,y) \leq e\\ \end{cases}\tag{3}\end{align*}
</span></span></disp-formula>
<div class="figure figure-full" id=fig5><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang5ab-2992749-large.gif data-fig-id=fig5><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang5ab-2992749-small.gif data-alt="FIGURE 5. - Linear transformation curves."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 5. </b><fig><p>Linear transformation curves.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>The various functions in the piecewise formula are represented by colored polylines in the coordinate system corresponding to the transformation. The positions of the discontinuity points must be determined individually for each specific image. An example of such a piecewise linear transformation curve is shown in <a ref-type=fig anchor=fig5 class=fulltext-link>Fig. 5(b)</a>.<p>In the piecewise linear transformation method, parameter optimization can be performed only based on experience or with considerable human participation; thus, it lacks an adaptive mechanism. Additionally, it is difficult to achieve the optimal enhancement effect <a ref-type=bibr anchor=ref46 id=context_ref_46_2a1>[46]</a>, <a ref-type=bibr anchor=ref47 id=context_ref_47_2a1>[47]</a>, <a ref-type=bibr anchor=ref47 id=context_ref_47_2a1>[47]</a>–<a ref-type=bibr anchor=ref49 id=context_ref_49_2a1>[49]</a>. To overcome these problems, a hybrid genetic algorithm combined with differential evolution has been applied for image enhancement processing <a ref-type=bibr anchor=ref50 id=context_ref_50_2a1>[50]</a>. The optimal transformation curve was obtained through the adaptive mutation and quick search capabilities of this algorithm. In summary, the principle of linear image enhancement is simple and fast to execute, but the effect is not satisfactory, with some image details typically being lost due to uneven image enhancement.</p></div><div class=section_2 id=sec2a2><h4>2) Nonlinear Transformation</h4><p>The basic idea of a nonlinear gray transformation is to use a nonlinear function to transform the gray values of an image <a ref-type=bibr anchor=ref51 id=context_ref_51_2a2>[51]</a>. Frequently used nonlinear transformation functions include logarithmic functions, gamma functions and various other improved functions <a ref-type=bibr anchor=ref52 id=context_ref_52_2a2>[52]</a>, <a ref-type=bibr anchor=ref53 id=context_ref_53_2a2>[53]</a>. A logarithmic transformation function implies that there is a logarithmic relationship between the value of each pixel in the output image and the value of the corresponding pixel in the input image. This type of transformation is suitable for an excessively dark image because it can stretch the lower gray values of the image while compressing the dynamic range of the pixels with higher gray values <a ref-type=bibr anchor=ref54 id=context_ref_54_2a2>[54]</a>. The typical formula is as follows:<disp-formula id=deqn4 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} g(x,y) = \log (1+c \times f(x,y))\tag{4}\end{equation*}</span><span class="MathJax_Display MathJax_Processed"><span class=MathJax id=MathJax-Element-20-Frame tabindex=0><nobr><span class=math id=MathJax-Span-313><span style=display:inline-block;position:relative;width:0em;height:0px;font-size:123%><span style=position:absolute><span class=mrow id=MathJax-Span-314><span class=mtable id=MathJax-Span-315 style=min-width:16.535em><span style=display:inline-block;position:relative;width:100%;height:0px;min-width:16.535em><span style=display:inline-block;position:absolute;width:12.379em;height:0px;clip:rect(-0.938em,1012.28em,0.438em,-1000em);top:0em;left:50%;margin-left:-6.19em><span style=position:absolute;clip:rect(3.123em,1012.28em,4.448em,-1000em);top:-4.011em;left:0em><span style=display:inline-block;position:relative;width:12.379em;height:0px><span style=position:absolute;clip:rect(3.098em,1012.28em,4.423em,-1000em);top:-3.986em;left:50%;margin-left:-6.19em><span class=mtd id=MathJax-Span-319><span class=mrow id=MathJax-Span-320><span class=mi id=MathJax-Span-321 style=font-family:MathJax_Math;font-style:italic>g<span style=display:inline-block;overflow:hidden;height:1px;width:0.003em></span></span><span class=mo id=MathJax-Span-322 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-323 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-324 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-325 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-326 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-327 style=font-family:MathJax_Main;padding-left:0.278em>=</span><span class=mi id=MathJax-Span-328 style=font-family:MathJax_Main;padding-left:0.278em>log</span><span class=mo id=MathJax-Span-329></span><span class=mo id=MathJax-Span-330 style=font-family:MathJax_Main>(</span><span class=mn id=MathJax-Span-331 style=font-family:MathJax_Main>1</span><span class=mo id=MathJax-Span-332 style=font-family:MathJax_Main;padding-left:0.222em>+</span><span class=mi id=MathJax-Span-333 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>c</span><span class=mo id=MathJax-Span-334 style=font-family:MathJax_Main;padding-left:0.222em>×</span><span class=mi id=MathJax-Span-335 style=font-family:MathJax_Math;font-style:italic;padding-left:0.222em>f<span style=display:inline-block;overflow:hidden;height:1px;width:0.06em></span></span><span class=mo id=MathJax-Span-336 style=font-family:MathJax_Main>(</span><span class=mi id=MathJax-Span-337 style=font-family:MathJax_Math;font-style:italic>x</span><span class=mo id=MathJax-Span-338 style=font-family:MathJax_Main>,</span><span class=mi id=MathJax-Span-339 style=font-family:MathJax_Math;font-style:italic;padding-left:0.167em>y<span style=display:inline-block;overflow:hidden;height:1px;width:0.006em></span></span><span class=mo id=MathJax-Span-340 style=font-family:MathJax_Main>)</span><span class=mo id=MathJax-Span-341 style=font-family:MathJax_Main>)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span><span style=display:inline-block;position:absolute;width:1.278em;height:0px;clip:rect(-0.888em,1001.18em,0.438em,-1000em);top:0em;right:0em;margin-right:0em><span style=position:absolute;clip:rect(3.098em,1001.18em,4.423em,-1000em);top:-3.986em;right:0em><span class=mtd id=mjx-eqn-4><span class=mrow id=MathJax-Span-317><span class=mtext id=MathJax-Span-318 style=font-family:MathJax_Main>(4)</span></span></span><span style=display:inline-block;width:0px;height:4.011em></span></span></span></span></span></span></span></span></span></nobr></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} g(x,y) = \log (1+c \times f(x,y))\tag{4}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>c</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-21-Frame tabindex=0></span>
</tex-math></inline-formula> is a control parameter.<p>The shapes of several logarithmic transformation functions are shown in <a ref-type=fig anchor=fig6 class=fulltext-link>Fig. 6(a)</a>. A logarithmic transformation function stretches the gray values of pixels in low-gray-value areas and compresses the values of pixels in high-gray-value areas.
<div class="figure figure-full" id=fig6><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang6ab-2992749-large.gif data-fig-id=fig6><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang6ab-2992749-small.gif data-alt="FIGURE 6. - Nonlinear transformation curves."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 6. </b><fig><p>Nonlinear transformation curves.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>The gamma function is a nonlinear transformation with broad application whose formula is as follows:<disp-formula id=deqn5 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} g(x,y) = f(x,y)^{\gamma }\tag{5}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-22-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} g(x,y) = f(x,y)^{\gamma }\tag{5}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\gamma </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-23-Frame tabindex=0></span>
</tex-math></inline-formula> denotes the gamma correction parameter, which is usually a constant. Several gamma transformation curves are shown in <a ref-type=fig anchor=fig6 class=fulltext-link>Fig. 6(b)</a>.<p>As shown in <a ref-type=fig anchor=fig6 class=fulltext-link>Fig. 6(b)</a>, several different transformation curves can be obtained by varying the parameter <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\gamma </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-24-Frame tabindex=0></span>
</tex-math></inline-formula>. When <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\gamma &gt; 1</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-25-Frame tabindex=0></span>
</tex-math></inline-formula>, the transformation will stretch the dynamic range of the low-gray-value areas of the image and compress the range of the high-gray-value areas. In contrast, when <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\gamma &lt; 1</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-26-Frame tabindex=0></span>
</tex-math></inline-formula>, the transformation will compress low gray values and stretch high gray values. When <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\gamma = 1</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-27-Frame tabindex=0></span>
</tex-math></inline-formula>, the output remains the same <a ref-type=bibr anchor=ref55 id=context_ref_55_2a2>[55]</a>. Therefore, different gray regions of an image can be selectively stretched or compressed by adjusting this parameter to obtain a better enhancement effect. Drago <i>et al.</i> suggested that the dynamic range of an image can be effectively compressed by mapping the gray values of the image using an adaptive logarithmic function <a ref-type=bibr anchor=ref56 id=context_ref_56_2a2>[56]</a>. Tao <i>et al.</i> used a gray value corresponding to the cumulative histogram of an image with a value of 0.1 to self-adaptively obtain a nonlinear mapping function that can enhance the brightness of dark regions while inhibiting the enhancement of bright regions <a ref-type=bibr anchor=ref57 id=context_ref_57_2a2>[57]</a>. Likewise, a low-light color image enhancement algorithm based on a logarithmic processing model was proposed by Tian <i>et al.</i> <a ref-type=bibr anchor=ref58 id=context_ref_58_2a2>[58]</a>. First, this algorithm applies a nonlinear enhancement process to the brightness components of an image; then, the membership function is amended by introducing an enhancement operator. Finally, the enhanced image is obtained by means of the inverse transform of the membership function. Huang <i>et al.</i> <a ref-type=bibr anchor=ref59 id=context_ref_59_2a2>[59]</a> proposed an adaptive gamma correction algorithm in which the gamma correction parameter is adaptively obtained in accordance with the cumulative probability distribution histogram. However, because this gamma correction method relies on a single parameter, it is prone to cause overenhancement of bright areas. To overcome this shortcoming, a double gamma function was constructed by Zhi <i>et al.</i> and adjusted based on the distribution characteristics of the illumination map <a ref-type=bibr anchor=ref60 id=context_ref_60_2a2>[60]</a>, thus improving the gray values in low-brightness areas while suppressing the gray values in local high-brightness regions. Moreover, an arctangent hyperbola has been used to map the hue component of an image to an appropriate range by Yu <i>et al.</i> <a ref-type=bibr anchor=ref61 id=context_ref_61_2a2>[61]</a>, and later, low-light image enhancement based on the optimal hyperbolic tangent profile was proposed <a ref-type=bibr anchor=ref62 id=context_ref_62_2a2>[62]</a>. Nonlinear transformation requires more complex calculations and consequently a longer time than linear transformation <a ref-type=bibr anchor=ref63 id=context_ref_63_2a2>[63]</a>, <a ref-type=bibr anchor=ref64 id=context_ref_64_2a2>[64]</a>.<p>In summary, gray transformation can highlight gray areas of interest and has the advantages of simple implementation and fast speed. However, such methods do not consider the overall gray distribution of an image; consequently, their enhancement ability is limited, and their adaptability is poor.</p></div></div><div class=section_2 id=sec2b><h3>B. Histogram Equalization (HE) Methods</h3><p>If the pixel values of an image are evenly distributed across all possible gray levels, then the image shows high contrast and a large dynamic range. On the basis of this characteristic, the HE algorithm uses the cumulative distribution function (CDF) to adjust the output gray levels to have a probability density function that corresponds to a uniform distribution; in this way, hidden details in dark areas can be made to reappear, and the visual effect of the input image can be effectively enhanced <a ref-type=bibr anchor=ref65 id=context_ref_65_2b>[65]</a>, <a ref-type=bibr anchor=ref66 id=context_ref_66_2b>[66]</a>.<div class=section_2 id=sec2b1><h4>1) Principle of HE</h4><p>In the HE method, the CDF is used as the transformation curve for the image gray values <a ref-type=bibr anchor=ref67 id=context_ref_67_2b1>[67]</a>–<a ref-type=bibr anchor=ref71 id=context_ref_71_2b1>[71]</a>. Let <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-28-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>L</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-29-Frame tabindex=0></span>
</tex-math></inline-formula> denote an image and its gray levels, respectively. <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-30-Frame tabindex=0></span>
</tex-math></inline-formula> represents the gray value at the position with coordinates <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-31-Frame tabindex=0></span>
</tex-math></inline-formula>, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>N</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-32-Frame tabindex=0></span>
</tex-math></inline-formula> represents the total number of pixels in the image, and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>n_{k}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-33-Frame tabindex=0></span>
</tex-math></inline-formula> represents the number of pixels of gray level <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>k</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-34-Frame tabindex=0></span>
</tex-math></inline-formula>. Then, the gray-level probability density function of image <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-35-Frame tabindex=0></span>
</tex-math></inline-formula> is defined as <disp-formula id=deqn6 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} p(k) = \frac {n_{k}}{N},\quad (k=0,1,2,\ldots,L-1)\tag{6}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-36-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} p(k) = \frac {n_{k}}{N},\quad (k=0,1,2,\ldots,L-1)\tag{6}\end{equation*}
</span></span></disp-formula><p>The CDF of the gray levels of image <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-37-Frame tabindex=0></span>
</tex-math></inline-formula> is <disp-formula id=deqn7 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} c(k) = \sum \limits _{r=0}^{k} p(r),\quad k=0,1,2,\ldots,L-1\tag{7}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-38-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} c(k) = \sum \limits _{r=0}^{k} p(r),\quad k=0,1,2,\ldots,L-1\tag{7}\end{equation*}
</span></span></disp-formula><p>The standard HE algorithm maps the original image to an enhanced image with an approximately uniform gray-level distribution based on the CDF. The mapping relationship is as follows:<disp-formula id=deqn8 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} f(k) = (L-1) \times c(k)\tag{8}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-39-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} f(k) = (L-1) \times c(k)\tag{8}\end{equation*}
</span></span></disp-formula><p>An example of HE is shown in <a ref-type=fig anchor=fig7 class=fulltext-link>Fig. 7</a>, in which (a) presents the input low-light image, (b) displays the histogram of the input low-light image, (c) presents the enhanced image after HE, and (d) displays the histogram of the enhanced image. The principle of the standard HE algorithm is simple and can be executed in real time. However, the brightness of the enhanced image will be uneven, and some details may be lost due to gray-level merging.
<div class="figure figure-full" id=fig7><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang7abcd-2992749-large.gif data-fig-id=fig7><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang7abcd-2992749-small.gif data-alt="FIGURE 7. - Example of HE on a grayscale image."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 7. </b><fig><p>Example of HE on a grayscale image.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p></p></div><div class=section_2 id=sec2b2><h4>2) Basic Models of HE Methods</h4><p>Depending on the regions considered in the calculation, HE methods can be divided into global histogram equalization (GHE) and local histogram equalization (LHE) <a ref-type=bibr anchor=ref72 id=context_ref_72_2b2>[72]</a>. The general concept of a GHE algorithm is illustrated by the model shown in <a ref-type=fig anchor=fig8 class=fulltext-link>Fig. 8</a>, where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>X</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-40-Frame tabindex=0></span>
</tex-math></inline-formula> represents the original image, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>Y</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-41-Frame tabindex=0></span>
</tex-math></inline-formula> represents the enhanced image generated by the HE algorithm, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>Y=f(X)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-42-Frame tabindex=0></span>
</tex-math></inline-formula> represents the traditional HE process or an improved version, and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>X_{1}, X_{2}, X_{3},\cdots,X_{n}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-43-Frame tabindex=0></span>
</tex-math></inline-formula> represent <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>n</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-44-Frame tabindex=0></span>
</tex-math></inline-formula> subimages composed of pixels in the original image that satisfy certain conditions according to a given property, which is defined as <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>Q(x)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-45-Frame tabindex=0></span>
</tex-math></inline-formula>. The parameter <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>x</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-46-Frame tabindex=0></span>
</tex-math></inline-formula> represents the magnitude of the image gray value, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>Y_{1}, Y_{2}, Y_{3},\cdots,Y_{n}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-47-Frame tabindex=0></span>
</tex-math></inline-formula> denote the equalized images corresponding to the <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>n</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-48-Frame tabindex=0></span>
</tex-math></inline-formula> subimages, and the image <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>Y</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-49-Frame tabindex=0></span>
</tex-math></inline-formula> after equalization is obtained by merging the subimages in accordance with the pixel positions.
<div class="figure figure-full" id=fig8><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang8-2992749-large.gif data-fig-id=fig8><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang8-2992749-small.gif data-alt="FIGURE 8. - Basic model of GHE algorithms."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 8. </b><fig><p>Basic model of GHE algorithms.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>The GHE model has several advantages, such as relatively few calculations and high efficiency, and it is especially suitable for the enhancement of overall darker or brighter images <a ref-type=bibr anchor=ref58 id=context_ref_58_2b2>[58]</a>. However, it is difficult for a global algorithm, which conducts statistical operations based on the gray values of the whole image, to obtain the optimal recovered values for each local region. Such an algorithm is unable to adapt to the local brightness characteristics of the input image, and consequently, the sense of depth in the image will be decreased after processing.<p>To solve this problem, many scholars have proposed that an LHE algorithm should be used instead, and such algorithms have hence been put into wide practice. The basic idea of LHE is to apply the HE operation separately to various local areas of an image. The original image is spatially divided into multiple subblocks, and equalization is conducted separately on each subblock to adaptively enhance the local information of the image to achieve the desired enhancement effect. LHE methods can be further divided into three approaches <a ref-type=bibr anchor=ref72 id=context_ref_72_2b2>[72]</a>, namely, LHE with nonoverlapping subblocks, LHE with overlapping subblocks and LHE with partially overlapping subblocks, as shown in <a ref-type=fig anchor=fig9 class=fulltext-link>Fig. 9</a>. The implementation process for these algorithms is as follows.
<ol><li><p>For an input image of a given size, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>M\times N</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-50-Frame tabindex=0></span>
</tex-math></inline-formula>, a subblock with dimensions of <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>m\times n</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-51-Frame tabindex=0></span>
</tex-math></inline-formula> is defined in the upper left corner of the image, and additional subblocks are defined by moving along the horizontal and vertical directions with step sizes of <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>h</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-52-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>w</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-53-Frame tabindex=0></span>
</tex-math></inline-formula>, respectively.</p><li><p>HE processing is applied to each subblock in the same manner as for a GHE algorithm.<p>Then, the results are added to the output image, and the cumulative number of subblock processing rounds for each pixel is recorded.</p><li><p>The next subblocks are defined by moving horizontally with the horizontal step size <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>h</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-54-Frame tabindex=0></span>
</tex-math></inline-formula> and vertically with the vertical step size <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>w</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-55-Frame tabindex=0></span>
</tex-math></inline-formula>. For each subblock that does not exceed the image boundary, step (ii) is repeated; if no such unprocessed subblocks remain, the method proceeds to the next step.</p><li><p>The output image is obtained by dividing the gray value of each pixel in the output image by the corresponding cumulative number of subblock processing rounds. The disadvantages of this algorithm are the local block effects and the large number of calculations.</p></ol>
<div class="figure figure-full" id=fig9><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang9abc-2992749-large.gif data-fig-id=fig9><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang9abc-2992749-small.gif data-alt="FIGURE 9. - Basic model of LHE algorithms."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 9. </b><fig><p>Basic model of LHE algorithms.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p></p></div><div class=section_2 id=sec2b3><h4>3) HE Algorithms</h4><p>Many algorithms have been developed based on the classic HE approach. For example, Kim first proposed brightness-preserving bi-histogram equalization (BBHE) to maintain the image brightness <a ref-type=bibr anchor=ref73 id=context_ref_73_2b3>[73]</a>, in which the input image is divided into two subimages <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I_{L}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-56-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I_{U}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-57-Frame tabindex=0></span>
</tex-math></inline-formula> (satisfying the conditions <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I=I_{L} \cup I_{U}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-58-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I=I_{L} \cap I_{U}=\Phi </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-59-Frame tabindex=0></span>
</tex-math></inline-formula>) using the mean brightness of the original image as a threshold. HE is then applied to each subimage to address the issue of uneven brightness in local areas of the enhanced image. Subsequently, Wang <i>et al.</i> proposed the equal-area dualistic subimage histogram equalization (DSIHE) algorithm <a ref-type=bibr anchor=ref74 id=context_ref_74_2b3>[74]</a>. This algorithm uses the median gray value of the original image as a threshold to divide the image into two parts of the same size to maximize its entropy value, thus overcoming the loss of image information caused by the standard HE algorithm. Later, Chen proposed the minimum mean brightness error bi-histogram equalization (MMBEBHE) model <a ref-type=bibr anchor=ref75 id=context_ref_75_2b3>[75]</a>, which minimizes the mean brightness error between the output image and the original image. Furthermore, Shen <i>et al.</i> proposed the iterative brightness bi-histogram equalization (IBBHE) algorithm <a ref-type=bibr anchor=ref76 id=context_ref_76_2b3>[76]</a>, in which the segmentation threshold is selected through an iterative method to drive the mean to converge to the optimum while avoiding the confusion between target and background that can occur in traditional HE. Similarly, a BBHE algorithm that preserves color information was proposed by Tian <i>et al.</i> <a ref-type=bibr anchor=ref77 id=context_ref_77_2b3>[77]</a>. This algorithm not only retains the color information of the input image but also enhances the image details. Other optimization methods, including local approaches, have also been continuously emerging. For example, a standard adaptive histogram equalization (AHE) algorithm was proposed in <a ref-type=bibr anchor=ref78 id=context_ref_78_2b3>[78]</a>, while in <a ref-type=bibr anchor=ref79 id=context_ref_79_2b3>[79]</a>, a block iterative histogram method was used to enhance the image contrast, and a moving template was used for partially overlapped subblock histogram equalization (POSHE) processing for each part of the image. Liu <i>et al.</i> <a ref-type=bibr anchor=ref80 id=context_ref_80_2b3>[80]</a> proposed an LHE method that uses a histogram-number-based gray-level protection mechanism on the basis of nonoverlapping subblocks. The spatial positions of the pixels in each block are taken into account when setting the weights, thus effectively eliminating the block effect. Huang and Yeh <a ref-type=bibr anchor=ref81 id=context_ref_81_2b3>[81]</a> proposed a novel LHE algorithm that can improve the contrast of an image while maintaining its brightness. Reza <a ref-type=bibr anchor=ref82 id=context_ref_82_2b3>[82]</a> proposed the contrast-limited adaptive histogram equalization (CLAHE) algorithm. This algorithm effectively mitigates the block effect that arises in the enhancement process and limits local contrast enhancement by setting a threshold, thus avoiding excessive enhancement of the image contrast. The CLAHE method can be combined with the Wiener filter (WF) or a finite impulse response filter (FIRF) for image contrast enhancement, as discussed in <a ref-type=bibr anchor=ref83 id=context_ref_83_2b3>[83]</a> and <a ref-type=bibr anchor=ref84 id=context_ref_84_2b3>[84]</a>, respectively. Based on BBHE and recursive mean-separate histogram equalization (RMSHE) <a ref-type=bibr anchor=ref85 id=context_ref_85_2b3>[85]</a>, an LHE algorithm that maintains image brightness was proposed in <a ref-type=bibr anchor=ref86 id=context_ref_86_2b3>[86]</a>. In this algorithm, RMSHE is applied to each subblock, thus effectively maintaining the mean brightness of the subblocks. Based on DSIHE and RMSHE, a POSHE method with equal-area recursive decomposition was proposed in <a ref-type=bibr anchor=ref87 id=context_ref_87_2b3>[87]</a>. In this algorithm, multiple equal-area recursive decompositions are implemented on the subblocks in combination with DSIHE and RMSHE to more effectively maintain the image brightness compared with DSIHE. Simultaneously, a bilinear difference measure is used to eliminate the differences in contrast between each subblock and the two externally adjacent subblocks, thus improving the visual effect of the image. In addition, an LHE algorithm based on entropy maximization and brightness maintenance was proposed in <a ref-type=bibr anchor=ref88 id=context_ref_88_2b3>[88]</a> that maximizes the entropy of the subblocks while leaving their mean brightness unchanged before and after equalization, thus effectively enhancing the image details. In <a ref-type=bibr anchor=ref89 id=context_ref_89_2b3>[89]</a>, the contextual and variational contrast (CVC) enhancement algorithm was proposed; in this algorithm, the two-dimensional histogram and context information model of the input image are used to implement nonlinear data mapping to enhance a weakly lighted image. In <a ref-type=bibr anchor=ref59 id=context_ref_59_2b3>[59]</a>, an HE method with gamma correction was proposed and achieved a balance between the quality of the output image and the computing time. However, the HE methods discussed above typically fail to effectively eliminate the potentially severe interference of noise in weakly illuminated images; in fact, they may even amplify such noise. Therefore, researchers have proposed interpolation-based HE algorithms, in which linear interpolation methods are used to determine the transformation function for the current pixels, thus overcoming the ”block effect” caused by nonoverlapping subblocks in HE and achieving a better enhancement effect. In recent years, the newly proposed algorithms have all been combined with image analysis. The background brightness-preserving histogram equalization (BBPHE) algorithm <a ref-type=bibr anchor=ref90 id=context_ref_90_2b3>[90]</a> divides the input image into the background region and the target region, while the dominant orientation-based texture histogram equalization (DOTHE) algorithm <a ref-type=bibr anchor=ref91 id=context_ref_91_2b3>[91]</a> divides the image into textured and smooth regions. Other typical algorithms include gain-controllable clipped histogram equalization (GCCHE) <a ref-type=bibr anchor=ref72 id=context_ref_72_2b3>[72]</a>, recursive subimage histogram equalization (RSIHE) <a ref-type=bibr anchor=ref92 id=context_ref_92_2b3>[92]</a>, entropy-based dynamic subhistogram equalization (EDSHE) <a ref-type=bibr anchor=ref93 id=context_ref_93_2b3>[93]</a>, dynamic histogram equalization (DHE) <a ref-type=bibr anchor=ref94 id=context_ref_94_2b3>[94]</a>, brightness-preserving dynamic histogram equalization (BPDHE) <a ref-type=bibr anchor=ref95 id=context_ref_95_2b3>[95]</a>, bi-histogram equalization with a plateau limit (BHEPL) <a ref-type=bibr anchor=ref96 id=context_ref_96_2b3>[96]</a>, median-mean-based subimage-clipped histogram equalization (MMSICHE) <a ref-type=bibr anchor=ref97 id=context_ref_97_2b3>[97]</a>, exposure-based subimage histogram equalization (ESIHE) <a ref-type=bibr anchor=ref98 id=context_ref_98_2b3>[98]</a>, adaptively modified histogram equalization (AMHE) <a ref-type=bibr anchor=ref99 id=context_ref_99_2b3>[99]</a>, weighted histogram equalization (WHE) <a ref-type=bibr anchor=ref100 id=context_ref_100_2b3>[100]</a>, a histogram modification framework (HMF) <a ref-type=bibr anchor=ref101 id=context_ref_101_2b3>[101]</a>, gap adjustment for histogram equalization (CegaHE) <a ref-type=bibr anchor=ref102 id=context_ref_102_2b3>[102]</a>, and unsharp masking with histogram equalization (UMHE) <a ref-type=bibr anchor=ref103 id=context_ref_103_2b3>[103]</a>.<p>To illustrate the performance of HE methods on color images, the AMHE <a ref-type=bibr anchor=ref99 id=context_ref_99_2b3>[99]</a>, BBHE <a ref-type=bibr anchor=ref73 id=context_ref_73_2b3>[73]</a>, CLAHE <a ref-type=bibr anchor=ref82 id=context_ref_82_2b3>[82]</a>, DSIHE <a ref-type=bibr anchor=ref74 id=context_ref_74_2b3>[74]</a>, HE <a ref-type=bibr anchor=ref66 id=context_ref_66_2b3>[66]</a>, RMSHE <a ref-type=bibr anchor=ref85 id=context_ref_85_2b3>[85]</a>, RSIHE <a ref-type=bibr anchor=ref92 id=context_ref_92_2b3>[92]</a>, and WHE <a ref-type=bibr anchor=ref100 id=context_ref_100_2b3>[100]</a> algorithms are tested here in both the RGB and HSI color spaces. The test image and its results are shown in <a ref-type=fig anchor=fig10 class=fulltext-link>Figs. 10</a>–<a ref-type=fig anchor=fig11 class=fulltext-link></a><a ref-type=fig anchor=fig12 class=fulltext-link>12</a>.
<ol><li><p>Equalization and merging of the three R, G and B subimages<p>The contrast of gray images can be effectively enhanced; however, for a color image with three components (R, G and B), serious color distortion of the image may be observed if the final image is obtained by simply equalizing and merging the R, G, and B subimages after equalization. The main reason is that the traditional HE algorithm excessively enhances the image brightness. If the mean brightness of one of the three R, G and B subimages of a color image is too dark or too bright, then the mean brightness of this subimage after equalization will be near the median gray value of this component. As a result, the color corresponding to this subimage will be either strengthened or weakened after enhancement, resulting in obvious color distortion and inconsistency in the final color image. Therefore, the primary goal of HE for a color image using this method is to maintain the mean brightness of the image while enhancing the image contrast.</p><li><p>In the HSI model, only the brightness component is equalized. In this method, the input color image is first converted from the RGB color space to the HSI color space, and then HE enhancement is applied to the brightness component I. Finally, the color image is converted back to the RGB space. In this way, the number of equalizations is reduced from 3 to 1. However, some calculations are still needed for the transformation between the color spaces, and there is still a risk of excessive image enhancement.</p></ol>
<div class="figure figure-full" id=fig10><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang10-2992749-large.gif data-fig-id=fig10><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang10-2992749-small.gif data-alt="FIGURE 10. - Image decomposition (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 10. </b><fig><p>Image decomposition (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig11><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang11abcdeffh-2992749-large.gif data-fig-id=fig11><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang11abcdeffh-2992749-small.gif data-alt="FIGURE 11. - Image enhancement using RGB model (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 11. </b><fig><p>Image enhancement using RGB model (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig12><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang12abcdefgh-2992749-large.gif data-fig-id=fig12><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang12abcdefgh-2992749-small.gif data-alt="FIGURE 12. - Image enhancement using HSI model."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 12. </b><fig><p>Image enhancement using HSI model.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>In summary, HE algorithms can effectively enhance low-light images and are often used in combination with other methods. The visual effect of such an image can be improved based on the contrast and detail enhancement provided by an HE algorithm. However, these methods can also easily cause a loss of color fidelity and the generation of noise, resulting in image distortion.</p></div></div><div class=section_2 id=sec2c><h3>C. Retinex Methods</h3><p>The Retinex theory, namely, the theory of the retinal cortex, established by Land and McCann, is based on the perception of color by the human eye and the modeling of color invariance <a ref-type=bibr anchor=ref104 id=context_ref_104_2c>[104]</a>. The essence of this theory is to determine the reflective nature of an object by removing the effects of the illuminating light from the image. According to Retinex theory, the human visual system processes information in a specific way during the transmission of visual information, thus removing a series of uncertain factors such as the intensity of the light source and unevenness of light. Consequently, only information that reflects essential characteristics of the object, such as the reflection coefficient, is retained <a ref-type=bibr anchor=ref105 id=context_ref_105_2c>[105]</a>–<a ref-type=bibr anchor=ref109 id=context_ref_109_2c>[109]</a>. Based on the illumination-reflection model (as shown in <a ref-type=fig anchor=fig13 class=fulltext-link>Fig. 13</a>), an image can be expressed as the product of a reflection component and an illumination component <a ref-type=bibr anchor=ref110 id=context_ref_110_2c>[110]</a>:<disp-formula id=deqn9 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} I(x,y) = R(x,y)L(x,y)\tag{9}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-60-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} I(x,y) = R(x,y)L(x,y)\tag{9}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R(x,y) </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-61-Frame tabindex=0></span>
</tex-math></inline-formula> is the reflection component, which represents the reflective characteristics of the object surface; <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>L(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-62-Frame tabindex=0></span>
</tex-math></inline-formula> is the illumination component, which depends on the environmental light characteristics; and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-63-Frame tabindex=0></span>
</tex-math></inline-formula> is the received image. <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>L(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-64-Frame tabindex=0></span>
</tex-math></inline-formula> determines the dynamic range of the image, whereas <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-65-Frame tabindex=0></span>
</tex-math></inline-formula> determines the inherent nature of the image. According to Retinex theory, if <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>L(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-66-Frame tabindex=0></span>
</tex-math></inline-formula> can be estimated from <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-67-Frame tabindex=0></span>
</tex-math></inline-formula>, then the reflection component can be separated from the total amount of light, and the influence of the illumination component on the image can be reduced to enhance the image <a ref-type=bibr anchor=ref111 id=context_ref_111_2c>[111]</a>. The Retinex algorithm features a sharpening capability, color constancy, large dynamic range compression and high color fidelity. The general process of the Retinex algorithm is shown in <a ref-type=fig anchor=fig14 class=fulltext-link>Fig. 14</a>, where <i>Log</i> denotes the logarithmic operation and <i>Exp</i> denotes the exponential operation.
<div class="figure figure-full" id=fig13><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang13-2992749-large.gif data-fig-id=fig13><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang13-2992749-small.gif data-alt="FIGURE 13. - Light reflection model."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 13. </b><fig><p>Light reflection model.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig14><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang14-2992749-large.gif data-fig-id=fig14><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang14-2992749-small.gif data-alt="FIGURE 14. - General process of the Retinex algorithm."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 14. </b><fig><p>General process of the Retinex algorithm.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>Many researchers have proposed effective image enhancement algorithms based on the Retinex theory. First, Land proposed that the illumination component could be estimated by using a random path algorithm to reduce the effect of uneven illumination. However, this random path algorithm is complex and has a common effect. Later, a two-dimensional path selection method, namely, the central Retinex algorithm, was proposed. Its core idea is as follows: an appropriate surround function is selected to determine the weighting of the pixel values in the neighborhood of the current pixel, which are then used to replace the current pixel value. Subsequently, Jobson <i>et al.</i> proposed the single-scale Retinex (SSR) algorithm <a ref-type=bibr anchor=ref112 id=context_ref_112_2c>[112]</a>, <a ref-type=bibr anchor=ref113 id=context_ref_113_2c>[113]</a>, followed by the multiscale Retinex (MSR) algorithm and the multiscale Retinex algorithm with color restoration (MSRCR) <a ref-type=bibr anchor=ref114 id=context_ref_114_2c>[114]</a>, <a ref-type=bibr anchor=ref115 id=context_ref_115_2c>[115]</a>.<div class=section_2 id=sec2c1><h4>1) Single-Scale Retinex (SSR)</h4><p>Essentially, the SSR algorithm obtains a reflection image by estimating the ambient brightness. The formula is as follows:<disp-formula id=deqn10 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} \log R_{i}(x,j) = \log I_{i} (x,y) - \log [G(x,y)^{\ast }I_{i}(x,y)]\tag{10}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-68-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} \log R_{i}(x,j) = \log I_{i} (x,y) - \log [G(x,y)^{\ast }I_{i}(x,y)]\tag{10}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-69-Frame tabindex=0></span>
</tex-math></inline-formula> represents the input image, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-70-Frame tabindex=0></span>
</tex-math></inline-formula> represents the reflection image, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>i</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-71-Frame tabindex=0></span>
</tex-math></inline-formula> represents the various color channels, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-72-Frame tabindex=0></span>
</tex-math></inline-formula> represents the position of a pixel in the image, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>G(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-73-Frame tabindex=0></span>
</tex-math></inline-formula> represents the Gaussian surround function, and * represents the convolution operator.<p>The formula for the Gaussian surround function is <disp-formula id=deqn11 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} G(x,y) = Ke^{\left({-\frac {x^{2}+y^{2}}{\sigma ^{2}}}\right)}\tag{11}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-74-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} G(x,y) = Ke^{\left({-\frac {x^{2}+y^{2}}{\sigma ^{2}}}\right)}\tag{11}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\sigma </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-75-Frame tabindex=0></span>
</tex-math></inline-formula> is a scale parameter. The smaller the value of this parameter is, the larger the dynamic range compression of the image is, and the clearer the local values are. <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>K</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-76-Frame tabindex=0></span>
</tex-math></inline-formula> is a normalization factor to ensure that the Gaussian function satisfies <disp-formula id=deqn12 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} \int \!\!\!\int G(x,y) dxdy =1\tag{12}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-77-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} \int \!\!\!\int G(x,y) dxdy =1\tag{12}\end{equation*}
</span></span></disp-formula><p>To better mimic the characteristics of the human visual system, automatic gain compensation is often required; that is, the output image is mapped to [0, 255] using a linear gray stretching algorithm. The mathematical formula is as follows:<disp-formula id=deqn13 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} R_{i}^{\prime }(x,y) = 255 \times \frac {R_{i}(x,y)-R_{\min }}{R_{\max }-R_{\min }}\tag{13}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-78-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} R_{i}^{\prime }(x,y) = 255 \times \frac {R_{i}(x,y)-R_{\min }}{R_{\max }-R_{\min }}\tag{13}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R_{i}^{\prime }(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-79-Frame tabindex=0></span>
</tex-math></inline-formula> is the output after gray stretching of the <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>i^{th}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-80-Frame tabindex=0></span>
</tex-math></inline-formula> color channel, and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R_{\max }</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-81-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R_{\min }</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-82-Frame tabindex=0></span>
</tex-math></inline-formula> are the maximum and minimum gray levels of the original image, respectively. <a ref-type=fig anchor=fig15 class=fulltext-link>Fig. 15</a> shows the enhancement results obtained using the SSR method when <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\sigma </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-83-Frame tabindex=0></span>
</tex-math></inline-formula> is 15, 80 and 250, respectively.
<div class="figure figure-full" id=fig15><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang15abcd-2992749-large.gif data-fig-id=fig15><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang15abcd-2992749-small.gif data-alt="FIGURE 15. - Enhancement with the SSR algorithm (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 15. </b><fig><p>Enhancement with the SSR algorithm (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>However, the SSR algorithm has some limitations. It is difficult to maintain a balance between detailed information enhancement and color fidelity in images processed with this algorithm due to the use of a single scale parameter.</p></div><div class=section_2 id=sec2c2><h4>2) Multiscale Retinex (MSR)</h4><p>To maintain a balance between dynamic range compression and color constancy, Jobson, Rahman <i>et al.</i> extended the single-scale algorithm to a multiscale algorithm, namely, the MSR algorithm <a ref-type=bibr anchor=ref114 id=context_ref_114_2c2>[114]</a>, which is expressed as follows:<disp-formula id=deqn14-deqn15 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} MSR=&amp;\log R_{i} (x,y) \\=&amp;\sum \limits _{N}^{k=1} \omega _{k} \{\log I_{i} (x,y)-\log [G_{k} (x,y)^{\ast }I_{i}(x,y)]\}\qquad \tag{14}\\ \sum \limits _{k=1}^{N} \omega _{k}=&amp;1\tag{15}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-84-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} MSR=&amp;\log R_{i} (x,y) \\=&amp;\sum \limits _{N}^{k=1} \omega _{k} \{\log I_{i} (x,y)-\log [G_{k} (x,y)^{\ast }I_{i}(x,y)]\}\qquad \tag{14}\\ \sum \limits _{k=1}^{N} \omega _{k}=&amp;1\tag{15}\end{align*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>i</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-85-Frame tabindex=0></span>
</tex-math></inline-formula> represents the three color channels; <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>k</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-86-Frame tabindex=0></span>
</tex-math></inline-formula> represents the Gaussian surround scales; <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>N</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-87-Frame tabindex=0></span>
</tex-math></inline-formula> is the number of scales, generally 3; and the <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\omega </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-88-Frame tabindex=0></span>
</tex-math></inline-formula> parameters are the scale weights. Compared with the SSR algorithm, the MSR algorithm can take advantage of the benefits of multiple scales. The MSR algorithm not only enhances image details and contrast but also produces enhanced images that exhibit better color consistency and an improved visual effect.</p></div><div class=section_2 id=sec2c3><h4>3) Multiscale Retinex With Color Restoration (MSRCR)</h4><p>During the process of image enhancement, the SSR or MSR algorithm is applied separately to the three color channels, R, G and B. Therefore, compared with the original image, the relative proportions of the three color channels may change after enhancement, thus resulting in color distortion. To overcome this problem, MSRCR has been proposed. This algorithm includes a color recovery factor <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>C</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-89-Frame tabindex=0></span>
</tex-math></inline-formula> for each channel, which is calculated based on the proportional relationship among the three color channels in the input image and is then used to correct the color of the output image to eliminate color distortion.<p>The formula for the color recovery factor is as follows:<disp-formula id=deqn16 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} C_{i}(x,y)=f\left({\frac {I_{i}(x,y)}{\sum \limits _{i}^{3} I_{i}(x,y)}}\right)\tag{16}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-90-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} C_{i}(x,y)=f\left({\frac {I_{i}(x,y)}{\sum \limits _{i}^{3} I_{i}(x,y)}}\right)\tag{16}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-91-Frame tabindex=0></span>
</tex-math></inline-formula> denotes the mapping function, and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>C(x,y)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-92-Frame tabindex=0></span>
</tex-math></inline-formula> is the color recovery factor. Jobson <i>et al.</i> found that the best color recovery effect is achieved when the mapping function is a logarithmic function, namely, <disp-formula id=deqn17 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} C_{i} (x,y) = \beta \times \log \left({\alpha \times \frac {I_{i}(x,y)}{\sum \limits _{i}^{3} I_{i}(x,y)}}\right)\tag{17}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-93-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} C_{i} (x,y) = \beta \times \log \left({\alpha \times \frac {I_{i}(x,y)}{\sum \limits _{i}^{3} I_{i}(x,y)}}\right)\tag{17}\end{equation*}
</span></span></disp-formula><p>The mathematical expression for the MSRCR algorithm can be obtained by combining formulas <a ref-type=disp-formula anchor=deqn18 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/#deqn18 class=fulltext-link>(18)</a> and <a ref-type=disp-formula anchor=deqn14-deqn15 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/#deqn14-deqn15 class=fulltext-link>(15)</a>:<disp-formula id=deqn18 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} MSRCR=&amp;\log R_{i} (x,y) \\=&amp;\sum \limits _{N}^{k=1} C_{i}\omega _{k} \{\log I_{i} (x,y)-\log [G_{k} (x,y)^{\ast }I_{i}(x,y)]\}\quad \\ {}\tag{18}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-94-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} MSRCR=&amp;\log R_{i} (x,y) \\=&amp;\sum \limits _{N}^{k=1} C_{i}\omega _{k} \{\log I_{i} (x,y)-\log [G_{k} (x,y)^{\ast }I_{i}(x,y)]\}\quad \\ {}\tag{18}\end{align*}
</span></span></disp-formula><p>The algorithm takes advantage of the convolution operation with Gaussian functions. Dynamic range compression and color constancy are achieved for features at large, medium and small scales, thus yielding a relatively ideal visual effect. Experimental results obtained with the SSR, MSR and MSRCR algorithms are shown in <a ref-type=fig anchor=fig16 class=fulltext-link>Fig. 16</a>.
<div class="figure figure-full" id=fig16><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang16abcd-2992749-large.gif data-fig-id=fig16><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang16abcd-2992749-small.gif data-alt="FIGURE 16. - Enhancement with different Retinex algorithms."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 16. </b><fig><p>Enhancement with different Retinex algorithms.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p></p></div><div class=section_2 id=sec2c4><h4>4) Other Retinex Algorithms</h4><p>Retinex theory conforms to the characteristics of human visual perception; consequently, it has been widely applied and developed. Many enhancement algorithms have been proposed based on Retinex theory <a ref-type=bibr anchor=ref116 id=context_ref_116_2c4>[116]</a>–<a ref-type=bibr anchor=ref119 id=context_ref_119_2c4>[119]</a>. Kimmel <i>et al.</i> used the transcendental hypothesis to propose a Retinex algorithm based on a variational framework <a ref-type=bibr anchor=ref120 id=context_ref_120_2c4>[120]</a>, in which the problem of illumination estimation is transformed into an optimal quadratic programming problem. Despite its high complexity, this algorithm achieves good results. Elad <i>et al.</i> proposed a noniterative Retinex algorithm that can process the edges in an image and suppress noise in dark areas <a ref-type=bibr anchor=ref121 id=context_ref_121_2c4>[121]</a>. Meylan <i>et al.</i> proposed a new model for presenting images with a high dynamic range and adapting images both globally and locally to the human visual system. In this algorithm, an adaptive filter is applied to reduce the chromatic aberrations caused by halo effects and brightness modification <a ref-type=bibr anchor=ref122 id=context_ref_122_2c4>[122]</a>. Xu <i>et al.</i> proposed a rapid Retinex image enhancement method that eliminates the halo phenomenon encountered with the traditional Retinex algorithm in areas of high light and dark contrast <a ref-type=bibr anchor=ref123 id=context_ref_123_2c4>[123]</a>.<p>Likewise, Marcelo <i>et al.</i> proposed a kernel-based Retinex (KBR) method, which relies on calculating the expected value of a suitable random-variable-weighted kernel function to reduce color error and improve details in a shadow image <a ref-type=bibr anchor=ref124 id=context_ref_124_2c4>[124]</a>. In 2011, Ng <i>et al.</i> proposed a total variation model based on the Retinex algorithm; in this model, the illumination component is spatially smooth, and the reflection component is piecewise continuous. Moreover, a fast calculation method was used to solve the minimization problem posed by the variation model. Finally, the validity of the proposed model was verified through experiments <a ref-type=bibr anchor=ref125 id=context_ref_125_2c4>[125]</a>. In addition, Fu <i>et al.</i> proposed a weighted variation model for the simultaneous reflectivity and illumination estimation (SRIE) of observed images; this model can precisely retain the estimated reflectivity while inhibiting noise to some extent <a ref-type=bibr anchor=ref126 id=context_ref_126_2c4>[126]</a>. Petro <i>et al.</i> proposed a multiscale Retinex algorithm with chromaticity preservation (MSRCP) <a ref-type=bibr anchor=ref127 id=context_ref_127_2c4>[127]</a>. First, the image brightness data are processed using the MSR algorithm; then, the results are mapped to each channel in accordance with the original proportional relationship among the R, G and B channels. Thus, the image is enhanced while retaining the original color distribution, and the grayish color typically observed in images enhanced using the MSRCR algorithm is effectively improved <a ref-type=bibr anchor=ref128 id=context_ref_128_2c4>[128]</a>. Later, Matin <i>et al.</i> optimized the MSRCP method using particle swarm optimization (PSO) to avoid manual adjustment of the parameters <a ref-type=bibr anchor=ref129 id=context_ref_129_2c4>[129]</a>. Chen and Beghdadi <a ref-type=bibr anchor=ref130 id=context_ref_130_2c4>[130]</a> proposed an image enhancement algorithm based on Retinex and a histogram stretch method to maintain the natural color of images. Shen and Hwang <a ref-type=bibr anchor=ref131 id=context_ref_131_2c4>[131]</a> proposed an image enhancement algorithm based on Retinex with a robust envelope. To avoid color distortion, Jang <i>et al.</i> <a ref-type=bibr anchor=ref132 id=context_ref_132_2c4>[132]</a> proposed an image enhancement algorithm based on the use of MSR to estimate the main color of an image. Inspired by Retinex theory, Wang <i>et al.</i> <a ref-type=bibr anchor=ref133 id=context_ref_133_2c4>[133]</a>, Xiao <i>et al.</i> <a ref-type=bibr anchor=ref134 id=context_ref_134_2c4>[134]</a> used a bionic method to enhance images. Chang Hsing Lee <i>et al.</i> proposed an adaptive MSR algorithm <a ref-type=bibr anchor=ref135 id=context_ref_135_2c4>[135]</a> based on brightness classification. For pixels in dark areas and bright areas, higher weights were given to larger-scale SSR components to enhance the overall visual effect of the image. Wang <i>et al.</i> <a ref-type=bibr anchor=ref136 id=context_ref_136_2c4>[136]</a> proposed a bright-pass filter in combination with neighborhood brightness information to maintain image naturalness, not only improving the image contrast but also better maintaining natural brightness without requiring naturalness preserved enhancement (NPE). Based on Retinex theory, some scholars have separated the reflection and illumination components and then enhanced the latter using a local nonlinear transformation model to render a brighter and more natural image <a ref-type=bibr anchor=ref137 id=context_ref_137_2c4>[137]</a>. As an alternative, an enhancement adjustment factor has been introduced <a ref-type=bibr anchor=ref138 id=context_ref_138_2c4>[138]</a> to adjust the enhancement degrees of different brightness values to avoid noise amplification and color distortion. Fu <i>et al.</i> <a ref-type=bibr anchor=ref139 id=context_ref_139_2c4>[139]</a> proposed a Retinex algorithm based on a variation framework that effectively enhances the contour details of an image while suppressing abnormal enhancement. In 2014, Zhao proposed a Retinex algorithm based on a Markov random field model. This algorithm estimates the illumination component of an image by means of guided filtering and solves for the reflection component of the object of interest based on the Markov random field model; additionally, this algorithm solves problems such as detail loss, color distortion and halo effects encountered when the MSRCR algorithm is used to process nighttime color images <a ref-type=bibr anchor=ref140 id=context_ref_140_2c4>[140]</a>. Later, Zhao <i>et al.</i> <a ref-type=bibr anchor=ref141 id=context_ref_141_2c4>[141]</a> proposed a Retinex algorithm based on weighted least squares. Jae <i>et al.</i> <a ref-type=bibr anchor=ref142 id=context_ref_142_2c4>[142]</a> proposed an MSR algorithm based on subband decomposition with a fusion strategy. Moreover, Liu <i>et al.</i> <a ref-type=bibr anchor=ref143 id=context_ref_143_2c4>[143]</a> combined the Retinex algorithm with bilateral filtering, thereby effectively improving the color distortion and detail loss in the final image but also increasing the complexity of the algorithm <a ref-type=bibr anchor=ref144 id=context_ref_144_2c4>[144]</a>, <a ref-type=bibr anchor=ref145 id=context_ref_145_2c4>[145]</a>. Yin <i>et al.</i> <a ref-type=bibr anchor=ref146 id=context_ref_146_2c4>[146]</a>, Mulyantini and Choi <a ref-type=bibr anchor=ref147 id=context_ref_147_2c4>[147]</a>, Zhang <i>et al.</i> <a ref-type=bibr anchor=ref148 id=context_ref_148_2c4>[148]</a>, Ji <i>et al.</i> <a ref-type=bibr anchor=ref149 id=context_ref_149_2c4>[149]</a>, and Zhang <i>et al.</i> <a ref-type=bibr anchor=ref150 id=context_ref_150_2c4>[150]</a> proposed Retinex-based algorithms combined with guided filters <a ref-type=bibr anchor=ref151 id=context_ref_151_2c4>[151]</a>. Particularly during the early stage of research on Retinex algorithms, scholars obtained many fruitful findings.<p>In short, Retinex algorithms have clear benefits and can be easily implemented. These methods can not only increase the contrast and brightness of an image but also has obvious advantages in terms of color image enhancement. However, these algorithms use the Gaussian convolution template for illumination estimation and do not have the ability to preserve edges; consequently, they may lead to halo phenomena in some regions with sharp boundaries or cause the whole image to be too bright.</p></div></div><div class=section_2 id=sec2d><h3>D. Frequency-Domain Methods</h3><p>With the development of multiscale image analysis technology, research on image enhancement algorithms has been extended from the spatial domain to the frequency domain <a ref-type=bibr anchor=ref152 id=context_ref_152_2d>[152]</a>. Image enhancement methods based on the frequency domain transform an image into the frequency domain for filtering by means of Fourier analysis, and the final image is then inversely transformed back into the spatial domain. Typical frequency-domain methods include homomorphic filtering (HF) and wavelet transform (WT) methods.<div class=section_2 id=sec2d1><h4>1) Homomorphic Filtering (HF)</h4><p>HF-based enhancement methods use the characteristics of the illumination-reflection model to transform the illumination and reflection components in the form of a sum in the logarithmic domain rather than a product. A high-pass filter is used to enhance the high-frequency reflection component and suppress the low-frequency illumination component in the Fourier transform domain <a ref-type=bibr anchor=ref7 id=context_ref_7_2d1>[7]</a>.<p>The specific steps of the HF process are listed as follows.
<ol><li><p>In the illumination-reflection model, the illumination component is multiplied by the reflection component, which cannot be transformed into the frequency domain. Therefore, to allow these components to be processed separately, the logarithmic transformation should first be implemented to transform these multiplicative components into additive components. Taking the logarithm of both sides of <a ref-type=disp-formula anchor=deqn10 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/#deqn10 class=fulltext-link>equation (10)</a> yields the following:<disp-formula id=deqn19 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} \text {ln}~I(x,y) = \text {ln}~L(x,y) + \text {ln}~R(x,y)\tag{19}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-95-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} \text {ln}~I(x,y) = \text {ln}~L(x,y) + \text {ln}~R(x,y)\tag{19}\end{equation*}
</span></span></disp-formula></p><li><p>The image is transformed from the spatial domain into the frequency domain by means of the Fourier transform, i.e., the Fourier transform is applied to both sides of the above equation:<disp-formula id=deqn20 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} F[\text {ln}~I(x,y)] = F[\text {ln}~L(x,y) + \text {ln}~R(x,y)]\tag{20}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-96-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} F[\text {ln}~I(x,y)] = F[\text {ln}~L(x,y) + \text {ln}~R(x,y)]\tag{20}\end{equation*}
</span></span></disp-formula><p>This equation can be written more concisely as <disp-formula id=deqn21 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} I(u,v) = L(u,v) + R(u,v)\tag{21}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-97-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} I(u,v) = L(u,v) + R(u,v)\tag{21}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-98-Frame tabindex=0></span>
</tex-math></inline-formula>, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>L(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-99-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-100-Frame tabindex=0></span>
</tex-math></inline-formula> are the Fourier transforms of <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-101-Frame tabindex=0></span>
</tex-math></inline-formula>, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>L(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-102-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-103-Frame tabindex=0></span>
</tex-math></inline-formula>, respectively. The spectral function <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>L(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-104-Frame tabindex=0></span>
</tex-math></inline-formula> is mainly concentrated in the low-frequency range, while the spectral function <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-105-Frame tabindex=0></span>
</tex-math></inline-formula> is mainly concentrated in the high-frequency range.</p><li><p>For contrast enhancement, an appropriate high-pass filter is selected, and the <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>R(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-106-Frame tabindex=0></span>
</tex-math></inline-formula> component in the frequency domain is enhanced by the transfer function <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>H(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-107-Frame tabindex=0></span>
</tex-math></inline-formula>. The resulting expression is as follows:<disp-formula id=deqn22 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} S(u,v) = H(u,v) I(u,v) = H(u,v)L(u,v)+H(u,v)R(u,v) \\ {}\tag{22}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-108-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} S(u,v) = H(u,v) I(u,v) = H(u,v)L(u,v)+H(u,v)R(u,v) \\ {}\tag{22}\end{align*}
</span></span></disp-formula></p><li><p>The inverse Fourier transform is used to transform the image from the frequency domain back into the spatial domain. Let <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>s(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-109-Frame tabindex=0></span>
</tex-math></inline-formula> denote the inverse Fourier transform corresponding to <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>S(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-110-Frame tabindex=0></span>
</tex-math></inline-formula>; then, the inverse Fourier transform of <a ref-type=disp-formula anchor=deqn10 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/#deqn10 class=fulltext-link>equation (10)</a> is <disp-formula id=deqn23 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} s(u,v)=&amp;F^{-1} (H(u,v)L(u,v))+F^{-1}(H(u,v)R(u,v)) \\=&amp;h_{L}(x,y) + h_{R} (u,v)\tag{23}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-111-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} s(u,v)=&amp;F^{-1} (H(u,v)L(u,v))+F^{-1}(H(u,v)R(u,v)) \\=&amp;h_{L}(x,y) + h_{R} (u,v)\tag{23}\end{align*}
</span></span></disp-formula><p>Therefore, the enhanced image corresponds to the superposition of the illumination component and reflection component.</p><li><p>The inverse logarithmic transform <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>G(x,y)=\text {exp}[s(x,y)]</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-112-Frame tabindex=0></span>
</tex-math></inline-formula> is applied to <a ref-type=disp-formula anchor=deqn24 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/#deqn24 class=fulltext-link>equation (24)</a> to obtain the final corrected image. Thus, by taking the exponent of both sides of <a ref-type=disp-formula anchor=deqn24 href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/#deqn24 class=fulltext-link>equation (24)</a>, the image after frequency-domain correction is obtained as follows:<disp-formula id=deqn24 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} G(x,y) = \text {exp} |h_{L}(x,y)|\cdot \text {exp} |h_{R}(x,y)|\tag{24}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-113-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} G(x,y) = \text {exp} |h_{L}(x,y)|\cdot \text {exp} |h_{R}(x,y)|\tag{24}\end{equation*}
</span></span></disp-formula></p></ol><p><p>Therefore, the core of the HF technique is to design an appropriate filter <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>H(u,v)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-114-Frame tabindex=0></span>
</tex-math></inline-formula> based on the image properties characterized by the illumination component and the reflection component in combination with a frequency filter and a gray transformation to compress the dynamic range and enhance the contrast. A homomorphic filter has the following general form:<disp-formula id=deqn25 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} H(u,v) = (\gamma _{H}-\gamma _{L})H_{hp} (u,v) + \gamma _{L}\tag{25}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-115-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} H(u,v) = (\gamma _{H}-\gamma _{L})H_{hp} (u,v) + \gamma _{L}\tag{25}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\gamma _{L}&lt; 1</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-116-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\gamma _{H}&lt; 1</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-117-Frame tabindex=0></span>
</tex-math></inline-formula>; the purpose of these parameters is to control the scope of the filter amplitude. <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>H_{hp}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-118-Frame tabindex=0></span>
</tex-math></inline-formula> is usually a high-pass filter, such as a Gaussian high-pass filter, a Butterworth high-pass filter, or a Laplacian filter. If a Gaussian filter is used as <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>H_{hp}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-119-Frame tabindex=0></span>
</tex-math></inline-formula>, then <disp-formula id=deqn26 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} H_{hp} = 1 -\text {exp}[-c\times (D^{2}(u,v)/D_{0}^{2})]\tag{26}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-120-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} H_{hp} = 1 -\text {exp}[-c\times (D^{2}(u,v)/D_{0}^{2})]\tag{26}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>c</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-121-Frame tabindex=0></span>
</tex-math></inline-formula> is a constant that controls the form of the filter.<p>The larger the value of the transition gradient from low frequency to high frequency is, the steeper the slope, as shown in <a ref-type=fig anchor=fig17 class=fulltext-link>Fig. 17</a>.
<div class="figure figure-full" id=fig17><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang17-2992749-large.gif data-fig-id=fig17><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang17-2992749-small.gif data-alt="FIGURE 17. - Amplitude-frequency curve of a homomorphic filter."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 17. </b><fig><p>Amplitude-frequency curve of a homomorphic filter.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>The specific algorithm flow of the HF method is shown in <a ref-type=fig anchor=fig18 class=fulltext-link>Fig. 18</a>. In this figure, <i>Log</i> is the logarithmic transform, FFT is the fast Fourier transform, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>H</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-122-Frame tabindex=0></span>
</tex-math></inline-formula>(<i>u,v</i>) is the frequency filtering function, IFFT is the inverse FFT, and <i>Exp</i> is the exponential operation.
<div class="figure figure-full" id=fig18><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang18-2992749-large.gif data-fig-id=fig18><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang18-2992749-small.gif data-alt="FIGURE 18. - Flowchart of the HF process."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 18. </b><fig><p>Flowchart of the HF process.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>The traditional HF algorithm requires two Fourier transforms and thus is not suitable for real-time processing. To address this issue, some scholars have proposed an HF algorithm based on a spatial filter <a ref-type=bibr anchor=ref153 id=context_ref_153_2d1>[153]</a>, <a ref-type=bibr anchor=ref154 id=context_ref_154_2d1>[154]</a>. The main idea is similar to that of the traditional HF algorithm. First, the original image is transformed into the logarithmic domain, and then, the output of a low-pass filter is used to estimate the illumination component. Finally, the reflection component is added to obtain the enhanced image. Because the traditional HF algorithm does not account for the local features of the image space, Zhang and Xie <a ref-type=bibr anchor=ref155 id=context_ref_155_2d1>[155]</a> proposed an HF algorithm based on the block discrete cosine transform (DCT) and removed the block effect after HF by considering the average boundaries with adjacent subimages as well as the characteristics of the DCT. Images processed with this algorithm show a good effect in terms of local contrast. A two-channel HF color image enhancement method based on the HSV color space was proposed in <a ref-type=bibr anchor=ref41 id=context_ref_41_2d1>[41]</a> by Han Lina <i>et al.</i> First, the input color image is transformed from RGB space into HSV space, thus obtaining separate chroma-, saturation- and brightness-channel images. Then, the saturation (S)-channel image is enhanced via Butterworth HF, and the brightness (V)-channel image is enhanced via Gaussian HF. Finally, the image is transformed back into RGB space to obtain the enhanced image. <a ref-type=fig anchor=fig19 class=fulltext-link>Fig. 19</a> shows the effects of enhancement processing with a Gaussian high-pass filter and presents the histograms corresponding to the images. The image brightness is improved after the HF process, but the image details are fuzzy.
<div class="figure figure-full" id=fig19><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang19abcd-2992749-large.gif data-fig-id=fig19><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang19abcd-2992749-small.gif data-alt="FIGURE 19. - HF effect (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 19. </b><fig><p>HF effect (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>Each coin has two sides: HF has the advantage of better maintaining the original image content, but its disadvantage is that it requires two Fourier transforms, namely, one exponential operation and one logarithmic operation, and therefore involves more calculation <a ref-type=bibr anchor=ref156 id=context_ref_156_2d1>[156]</a>–<a ref-type=bibr anchor=ref158 id=context_ref_158_2d1>[158]</a>. If the cut-off frequency of the high-pass filter is too high, then the dynamic range will be compressed and details will be lost. If the cut-off frequency is too low, the dynamic range compression will be minimal, and the algorithm will lack self-adaptability. This method is based on the premise of uniform illumination; consequently, the enhancement effect is poor for nighttime images with both bright and dark areas.<p>HF algorithms can remove uneven regions generated by light while maintaining the contour information of an image. However, such an algorithm requires two Fourier transformations, i.e., one exponential operation and one logarithmic operation, for each pixel in an image; therefore, its computational burden is large.</p></div><div class=section_2 id=sec2d2><h4>2) Wavelet Transform (WT)</h4><p>Similar to the Fourier transform, the WT is a mathematical transform that uses a group of functions called a wavelet function basis to represent or approximate a signal <a ref-type=bibr anchor=ref159 id=context_ref_159_2d2>[159]</a>. The WT can be used not only to characterize the local features of signals in the time and frequency domains but also to conduct a multiscale analysis of functions or signals through operations such as scaling and translation. Thus, great progress in image contrast enhancement has been achieved using WT methods. In a WT-based image enhancement algorithm, the input image is first decomposed into low-frequency and high-frequency image components; then, image components at different frequencies are separately enhanced to highlight the details of the image. The main idea of the wavelet analysis method is to apply wavelet decomposition to the original image to obtain the wavelet coefficients for different subbands, adjust these wavelet coefficients, and then apply the inverse transformation to the new coefficients to obtain the processed image. Such an image enhancement algorithm can enhance an image at multiple scales based on the WT. It is believed that low-illumination conditions have a greater influence on high-frequency image components, which are generally concentrated at the edges of an image and in contour regions <a ref-type=bibr anchor=ref160 id=context_ref_160_2d2>[160]</a>. Therefore, a WT-based algorithm will enhance the high-frequency components of the input image and suppress its low-frequency components. In particular, the dual-tree complex WT can usually achieve satisfactory results <a ref-type=bibr anchor=ref161 id=context_ref_161_2d2>[161]</a>–<a ref-type=bibr anchor=ref165 id=context_ref_165_2d2>[165]</a>.<p>The basic process of WT-based image enhancement is as follows. Processing of the displacement <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\tau </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-123-Frame tabindex=0></span>
</tex-math></inline-formula> is carried out for the function <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\psi (t)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-124-Frame tabindex=0></span>
</tex-math></inline-formula> describing the basic wavelet (parent wavelet); then, a wavelet sequence can be obtained by taking the inner product between the processed <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\psi (t)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-125-Frame tabindex=0></span>
</tex-math></inline-formula> and the signal <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>x(t)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-126-Frame tabindex=0></span>
</tex-math></inline-formula> to be analyzed at various scales <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>a</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-127-Frame tabindex=0></span>
</tex-math></inline-formula>.<disp-formula id=deqn27 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} WT_{x} (a,\tau) = \frac {1}{\sqrt {a}} \int _{-\infty }^{+\infty } x(t) \psi ^{\ast } \left ({\frac {t-\tau }{a}}\right)dt\quad (a&gt;0)\tag{27}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-128-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} WT_{x} (a,\tau) = \frac {1}{\sqrt {a}} \int _{-\infty }^{+\infty } x(t) \psi ^{\ast } \left ({\frac {t-\tau }{a}}\right)dt\quad (a&gt;0)\tag{27}\end{equation*}
</span></span></disp-formula><p>The equivalent expression in the frequency domain is <disp-formula id=deqn28 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} WT_{x} (a,\tau) = \frac {\sqrt {a}}{2\pi } \int _{-\infty }^{+\infty } X(\omega) \psi ^{\ast } (a\omega)e^{j\omega \tau }~d\omega\tag{28}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-129-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} WT_{x} (a,\tau) = \frac {\sqrt {a}}{2\pi } \int _{-\infty }^{+\infty } X(\omega) \psi ^{\ast } (a\omega)e^{j\omega \tau }~d\omega\tag{28}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>X(\omega)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-130-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\psi (\omega)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-131-Frame tabindex=0></span>
</tex-math></inline-formula> represent the Fourier transforms of <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>x(t)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-132-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\psi (t)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-133-Frame tabindex=0></span>
</tex-math></inline-formula>, respectively.<p>In standard WT-based image enhancement, the input image is usually first decomposed into one low-pass subimage and three directional high-pass subimages, namely, a horizontal detail image, a vertical detail image, and a diagonal detail image. The low-pass subimage represents the low-frequency information in the image, which corresponds to smooth regions. The high-pass subimages represent the high-frequency information in the image, which correspond to detailed image information. Based on the characteristics of these subimages, the most effective method is selected to enhance the coefficients of the different frequency components. Finally, the enhanced image in the spatial domain is obtained through inverse transformation.<p>The steps of WT-based image enhancement are as follows <a ref-type=bibr anchor=ref163 id=context_ref_163_2d2>[163]</a>, <a ref-type=bibr anchor=ref164 id=context_ref_164_2d2>[164]</a>.
<ol><li><p>The original image is input.</p><li><p>The low-frequency and high-frequency components of the original image are obtained via wavelet decomposition.</p><li><p>The wavelet coefficients are nonlinearly enhanced with a functional relationship that satisfies <disp-formula id=deqn29 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} W_{o} = \begin{cases} W_{i} + G\times (T-1) &amp; W_{i}&gt;T\\ G \times W_{i}&amp; |W_{i}| \leq T\\ W_{i}-G\times (T-1) &amp; W_{i}&lt; -T \end{cases}\tag{29}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-134-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} W_{o} = \begin{cases} W_{i} + G\times (T-1) &amp; W_{i}&gt;T\\ G \times W_{i}&amp; |W_{i}| \leq T\\ W_{i}-G\times (T-1) &amp; W_{i}&lt; -T \end{cases}\tag{29}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>G</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-135-Frame tabindex=0></span>
</tex-math></inline-formula> is the gain for the wavelet coefficient, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>T</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-136-Frame tabindex=0></span>
</tex-math></inline-formula> is the threshold for the wavelet coefficient, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>W_{i}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-137-Frame tabindex=0></span>
</tex-math></inline-formula> is the wavelet coefficient after image decomposition, and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>W_{o}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-138-Frame tabindex=0></span>
</tex-math></inline-formula> is the wavelet coefficient after enhancement.</p><li><p>The enhanced wavelet coefficients are inversely transformed to obtain the reconstructed enhanced image.</p></ol><p><p>The basic flow of the WT-based image enhancement process is illustrated in <a ref-type=fig anchor=fig20 class=fulltext-link>Fig. 20</a>.
<div class="figure figure-full" id=fig20><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang20-2992749-large.gif data-fig-id=fig20><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang20-2992749-small.gif data-alt="FIGURE 20. - Flowchart of WT-based image enhancement."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 20. </b><fig><p>Flowchart of WT-based image enhancement.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>The results of WT-based image enhancement are shown in <a ref-type=fig anchor=fig21 class=fulltext-link>Fig. 21</a>, where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>n</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-139-Frame tabindex=0></span>
</tex-math></inline-formula> is the wavelet scale.
<div class="figure figure-full" id=fig21><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang21-2992749-large.gif data-fig-id=fig21><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang21-2992749-small.gif data-alt="FIGURE 21. - WT-based low-light image enhancement (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 21. </b><fig><p>WT-based low-light image enhancement (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>In low-light images, it is difficult to distinguish image noise from image details. A high-frequency analysis is conducted on the WT-decomposed image, and then, to process the decomposed wavelet coefficients, various thresholds and enhancement factors are applied to effectively remove noise while enhancing detail components. Generally, the enhancement effect is better than that of traditional image enhancement algorithms <a ref-type=bibr anchor=ref166 id=context_ref_166_2d2>[166]</a>. Zong <i>et al.</i> proposed a contrast enhancement method based on multiscale wavelet analysis <a ref-type=bibr anchor=ref167 id=context_ref_167_2d2>[167]</a>. In this method, a multiscale nonlinear high-pass function is used to process the wavelet coefficients, thus enabling the enhancement of ultrasonic images. Loza <i>et al.</i> proposed an adaptive contrast enhancement method based on the statistics of local wavelet coefficients <a ref-type=bibr anchor=ref168 id=context_ref_168_2d2>[168]</a>. A model for the local wavelet coefficients was established on the basis of the binary Cauchy distribution, thus yielding a nonlinear enhancement function for wavelet coefficient compression. A WT-based image enhancement algorithm based on a knee function and gamma correction (KGWT) has been proposed in which an improved knee function and a gamma transform function are used to enhance the low-frequency coefficients <a ref-type=bibr anchor=ref169 id=context_ref_169_2d2>[169]</a>. Then, after enhancement, the low-frequency coefficients are combined with the high-frequency coefficients, and finally, the inverse WT is applied to obtain the enhanced image. The KGWT algorithm improves the overall brightness and contrast of images. A WT-based image enhancement algorithm based on contrast entropy was proposed in <a ref-type=bibr anchor=ref170 id=context_ref_170_2d2>[170]</a>. After wavelet decomposition, the low-frequency components of the image are enhanced via HE, and the high-frequency components are enhanced by maximizing the contrast entropy. Likewise, in <a ref-type=bibr anchor=ref171 id=context_ref_171_2d2>[171]</a>, the singular value matrices of low-frequency images were obtained with an enhanced wavelet decomposition approach, which also achieved an improved image enhancement effect. A fast and adaptive enhancement algorithm for low-light images based on the WT was proposed in <a ref-type=bibr anchor=ref172 id=context_ref_172_2d2>[172]</a>. In this algorithm, the RGB input image is transformed into HSV space, and the discrete wavelet transform (DWT) is applied to the brightness (<inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>V</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-140-Frame tabindex=0></span>
</tex-math></inline-formula>) image to separate high-frequency and low-frequency subbands. The illumination components in the low-frequency WT subbands of the image are rapidly estimated and removed using bilateral filtering, while a fuzzy transformation is used to realize the enhancement and denoising of edge and texture information. WT-based image enhancement theory is often combined with other approaches, such as fuzzy theory, image fusion, and HE. As discussed in <a ref-type=bibr anchor=ref173 id=context_ref_173_2d2>[173]</a>, <a ref-type=bibr anchor=ref174 id=context_ref_174_2d2>[174]</a>, after wavelet decomposition is performed on the original image, HE can be performed on each subband image individually. Finally, the inverse WT can be used to reconstruct the enhanced, noise-reduced image <a ref-type=bibr anchor=ref175 id=context_ref_175_2d2>[175]</a>. The WT approach has also been combined with Retinex theory to enhance low-light images, thus achieving a better enhancement effect <a ref-type=bibr anchor=ref176 id=context_ref_176_2d2>[176]</a>, <a ref-type=bibr anchor=ref177 id=context_ref_177_2d2>[177]</a>. Russo <a ref-type=bibr anchor=ref178 id=context_ref_178_2d2>[178]</a> proposed a method of improving image quality by means of multiscale equalization in the wavelet domain. Chen <a ref-type=bibr anchor=ref179 id=context_ref_179_2d2>[179]</a> proposed an image enhancement method that combines wavelet and fractional differential models. The WT can reflect both the time-domain and frequency-domain features of an image. Specifically, this model not only extracts edge information from an image but also extracts its overall structure, which is consistent with the needs of low-light image enhancement. However, because the wavelet basis needs to be defined in advance, the application of this algorithm is limited.<p>The curvelet transform is a multiscale analysis method developed based on the WT that can overcome the limitations of the WT by enhancing the curved edges in an image <a ref-type=bibr anchor=ref180 id=context_ref_180_2d2>[180]</a>. Starck <i>et al.</i> <a ref-type=bibr anchor=ref181 id=context_ref_181_2d2>[181]</a> proposed a multiscale analysis method based on the curvelet transform and compared it with the WT algorithm to demonstrate its superiority for color image enhancement. The curvelet-transform-based enhancement algorithm achieves a better effect for noisy images; however, it is not as effective as the WT method for noiseless or nearly noiseless images. In <a ref-type=bibr anchor=ref182 id=context_ref_182_2d2>[182]</a>, the WT was combined with the curvelet transform to achieve image enhancement with edge preservation. The specific steps are as follows. First, the curvelet transform is applied to remove noise without the loss of edge details; then, the image is enhanced using the WT. In <a ref-type=bibr anchor=ref183 id=context_ref_183_2d2>[183]</a>, an improved enhancement algorithm for noisy low-light color images based on the second-generation curvelet transform was proposed. A compromise factor for the YUV (luminance and chromaticity) color space and an improved gain function were used to suppress, elevate or maintain the curvelet coefficients. This approach effectively suppresses noise while optimally recovering both the edges and smooth parts in an image acquired under low illumination. Thus, the image enhancement quality is effectively improved. The main advantage of image enhancement methods based on the WT is that they allow the time- and frequency-domain features of images to be analyzed on multiple scales <a ref-type=bibr anchor=ref184 id=context_ref_184_2d2>[184]</a>. Another advantage of wavelet analysis lies in the refined local analysis capabilities, as such methods have better local characteristics in both the spatial and frequency domains and thus are beneficial for analyzing and highlighting the details of an image. Wavelet analysis is mainly used for infrared images <a ref-type=bibr anchor=ref185 id=context_ref_185_2d2>[185]</a>, <a ref-type=bibr anchor=ref186 id=context_ref_186_2d2>[186]</a> and medical image enhancement <a ref-type=bibr anchor=ref187 id=context_ref_187_2d2>[187]</a>. The disadvantage is that overbright illumination cannot be avoided.<p>In summary, frequency-domain-based algorithms can effectively highlight the details of an image through enhancement of the wavelet coefficients, but they can also easily magnify the noise in the image. Like other frequency-domain transformation methods, these image enhancement methods require large amounts of calculation, and the selection of the transformation parameters often requires manual intervention.</p></div></div><div class=section_2 id=sec2e><h3>E. Methods Based on Image Fusion</h3><p>Another direction of research on low-light image enhancement involves methods based on image fusion techniques <a ref-type=bibr anchor=ref188 id=context_ref_188_2e>[188]</a>. In these methods, many images of the same scene are obtained with different sensors, or additional images are obtained with the same sensor using various imaging methods or at different times. Finally, as much useful information as possible is extracted from every image to synthesize a high-quality image, thus improving the utilization rate of the image information. The synthesized image can reflect multilevel information from the original images to comprehensively describe the scene, thus allowing the available image information to better meet the requirements of both human observers and computer vision systems.<div class=section_2 id=sec2e1><h4>1) Multispectral Image Fusion</h4><p>Multispectral image fusion is an improved method of obtaining the details of a low-light imaged scene by fusing a visible image with an infrared image. Near-infrared (NIR) light has a longer wavelength and stronger penetration ability than does visible light, allowing redundant information to be removed from a filtered infrared image. Additionally, a low-light visible image can provide rich background information; consequently, better images can be obtained through image fusion. For example, in a method developed by the US Naval Research Laboratory (NRL), images obtained with an infrared thermal imager are integrated with the R-, G- and B-channel images obtained with a low-light night vision device to obtain night vision color images <a ref-type=bibr anchor=ref189 id=context_ref_189_2e1>[189]</a>–<a ref-type=bibr anchor=ref191 id=context_ref_191_2e1>[191]</a>. Toet <i>et al.</i> proposed a pseudocolor fusion algorithm for infrared and visible images <a ref-type=bibr anchor=ref192 id=context_ref_192_2e1>[192]</a>; this algorithm enhances the clarity of the image details while retaining the unique information captured by various sensors. Additionally, this algorithm involves only addition and subtraction operations and thus can be implemented using simple hardware in real time <a ref-type=bibr anchor=ref193 id=context_ref_193_2e1>[193]</a>. Zhu <i>et al.</i> proposed a fusion framework for night vision applications called night vision context enhancement (FNCE) <a ref-type=bibr anchor=ref194 id=context_ref_194_2e1>[194]</a>, in which the fused result is obtained by combining decomposed images using three different rules.<p>Furthermore, many scholars have studied the use of night vision technology for single- and double-channel low-light color fusion based on bispectral and trispectral features. Vision technology has been developed based on low-light and infrared thermal image fusion, low-light and longwave infrared image fusion, ultraviolet and low-light image fusion, and even trispectral color fusion based on low-light, medium-wave and longwave infrared images <a ref-type=bibr anchor=ref195 id=context_ref_195_2e1>[195]</a>–<a ref-type=bibr anchor=ref197 id=context_ref_197_2e1>[197]</a>. However, the visible and infrared images need to be acquired simultaneously, which constrains such algorithms in terms of the hardware conditions necessary to support them. Moreover, the intelligence and adaptability of these algorithms are poor, and their parameters need to be artificially set. Therefore, these algorithms have still not been widely adopted.</p></div><div class=section_2 id=sec2e2><h4>2) Image Fusion Based on Background Highlighting</h4><p>Generally, image fusion methods based on background highlighting rely on the integration of low-light images with daytime images to enhance the image details, thus improving the visual effect of the low-light images <a ref-type=bibr anchor=ref198 id=context_ref_198_2e2>[198]</a>. The general process is described as follows. First, an image is obtained in the daytime under reasonably sufficient lighting conditions for use as the source of the background for the fused image. Then, another image is obtained in the same position under low illumination, and the background of this image is removed. The remainder of the latter image is taken as the foreground of the fused image. Finally, the background and foreground are integrated into a single image using a suitable algorithm. For example, Raskar <i>et al.</i> estimated the intensity of the mixed gradient field of multiple low-light images and daytime images of the same scene, thus improving the visual effect of the low-light images <a ref-type=bibr anchor=ref199 id=context_ref_199_2e2>[199]</a>. Rao <i>et al.</i> proposed a low-light enhancement method based on video frame fusion <a ref-type=bibr anchor=ref200 id=context_ref_200_2e2>[200]</a>. The foreground area of each low-light video frame was fused with the background area from a daytime video frame of the same scene to improve the brightness of the low-light video and compensate for detail loss. In <a ref-type=bibr anchor=ref201 id=context_ref_201_2e2>[201]</a>, daytime images from the same site at various times were fused, and the final fused image was obtained using a moving object extraction technique and weighting processing based on brightness estimation theory. This process is shown in <a ref-type=fig anchor=fig22 class=fulltext-link>Fig. 22</a>.
<div class="figure figure-full" id=fig22><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang22-2992749-large.gif data-fig-id=fig22><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang22-2992749-small.gif data-alt="FIGURE 22. - Fusion based on background highlighting."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 22. </b><fig><p>Fusion based on background highlighting.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>Multi-image fusion methods such as those presented in <a ref-type=bibr anchor=ref202 id=context_ref_202_2e2>[202]</a>, <a ref-type=bibr anchor=ref203 id=context_ref_203_2e2>[203]</a> achieve a better enhancement effect but require high-quality daytime video information from the same scene. For example, such methods are not suitable for use in underground mines, because no high-quality daytime video information is available for such areas. Therefore, the applications of these algorithms are limited. Moreover, the large number of iterations required complicates the calculation.</p></div><div class=section_2 id=sec2e3><h4>3) Fusion Based on Multiple Exposures</h4><p>Image fusion is the process of combining multiple images of the same scene into a single high-quality image that contains more information than any single input image. Petschnigg <i>et al.</i> proposed a method of obtaining various images with both flash and nonflash technologies and then realizing low-light image enhancement through image fusion <a ref-type=bibr anchor=ref204 id=context_ref_204_2e3>[204]</a>. In this method, a flash image is captured to record detailed information of the scene, and a nonflash image is captured to record the brightness information of the background. Then, the image detail information is integrated with the background brightness information. The resulting image contains not only the details from the flash image but also the brightness information from the nonflash image. Similarly, high-dynamic-range (HDR) <a ref-type=bibr anchor=ref205 id=context_ref_205_2e3>[205]</a>–<a ref-type=bibr anchor=ref207 id=context_ref_207_2e3>[207]</a> imaging using multiexposure fusion (MEF) techniques has become very popular in recent years. MEF methods use multiple images of the same scene with different exposures. The final HDR image is obtained by synthesizing the best details from the images corresponding to each exposure time. A gradient-domain HDR compression algorithm was proposed in <a ref-type=bibr anchor=ref208 id=context_ref_208_2e3>[208]</a>. In this algorithm, different gradients are proportionally compressed in the gradient domain of the images, and Poisson’s equation is solved in the modified gradient domain to obtain output images with a low dynamic range. This algorithm can also reveal detailed information in areas of various brightness in HDR night images. Li <i>et al.</i> proposed an image enhancement algorithm based on multiple image fusion <a ref-type=bibr anchor=ref209 id=context_ref_209_2e3>[209]</a>. In this algorithm, multiple images of the same scene are first acquired with different exposure times, and then, various details are extracted from each image. Finally, these details are integrated to generate an enhanced HDR image <a ref-type=bibr anchor=ref210 id=context_ref_210_2e3>[210]</a>. Merianos and Mitianoudis combined two image fusion methods, one for the fusion of the luminance channel and one for the fusion of the color channels. The fusion output thus derived outperforms both individual methods <a ref-type=bibr anchor=ref211 id=context_ref_211_2e3>[211]</a>. In Ref. <a ref-type=bibr anchor=ref212 id=context_ref_212_2e3>[212]</a>, the author proposed a new fusion approach in the spatial domain using a propagated image filter. In the proposed approach, a weight map is calculated for every input image using the propagated image filter and gradient-domain postprocessing.<p>The fusion of multiple images acquired from the same scene can be applied to effectively enhance low-light images. Because good-quality image information from the same scene is needed, methods of this kind have stringent requirements in terms of image acquisition; in particular, the camera equipment needs to be stable. Since a long shooting time is required, this method cannot be applied for real-time imaging or video enhancement. Moreover, the enhancement effect for images of globally low brightness is poor.</p></div><div class=section_2 id=sec2e4><h4>4) Fusion Based on a Single Image</h4><p>Many scholars have studied the synthesis of the entire dynamic range of a scene <a ref-type=bibr anchor=ref213 id=context_ref_213_2e4>[213]</a>, <a ref-type=bibr anchor=ref214 id=context_ref_214_2e4>[214]</a>, including details extracted in a variety of ways from a single image, to break the dependence on image sequences, as shown in <a ref-type=fig anchor=fig23 class=fulltext-link>Fig. 23</a>. Le and Li <a ref-type=bibr anchor=ref215 id=context_ref_215_2e4>[215]</a> improved the contrast of an image by fusing the original image with the image obtained after a logarithmic transformation. Yamakawa and Sugita presented an image fusion technique that used a source image and the Retinex-processed image to achieve high visibility in both bright and dark areas <a ref-type=bibr anchor=ref216 id=context_ref_216_2e4>[216]</a>. In Ref. <a ref-type=bibr anchor=ref217 id=context_ref_217_2e4>[217]</a>, Wang <i>et al.</i> adaptively generated two new images based on nonlinear functional transformations in accordance with the illumination-reflection model and multiscale theory and used a principal component analysis (PCA)-based fusion method to enhance a low-light image. In <a ref-type=bibr anchor=ref218 id=context_ref_218_2e4>[218]</a>, an adaptive histogram separation method was used to construct underexposed and overexposed images from an original image sequence; these images were then separately processed, and finally, HDR images were generated via multiexposure image fusion. In addition, Fu <i>et al.</i> <a ref-type=bibr anchor=ref219 id=context_ref_219_2e4>[219]</a> proposed an image enhancement algorithm based on the fusion of the results of multiple enhancement techniques. This algorithm integrates multiple image enhancement techniques by means of a linear weighted fusion strategy to improve the enhancement effect. However, this strategy is too complex to satisfy real-time requirements. The algorithm proposed in <a ref-type=bibr anchor=ref220 id=context_ref_220_2e4>[220]</a> integrates the color contrast, saturation and exposure brightness of an original or preprocessed image by incorporating MSRCR into a pyramid algorithm using the gold tower technique and specifying different weight parameters depending on the image information to achieve the effective color enhancement of a traditional low-light image. A camera response model (CRM) is often adopted for generating multiple images <a ref-type=bibr anchor=ref221 id=context_ref_221_2e4>[221]</a>. In <a ref-type=bibr anchor=ref222 id=context_ref_222_2e4>[222]</a>, the authors proposed a single-image-based method of generating HDR images based on camera response function (CRF) reconstruction. Ying <i>et al.</i> <a ref-type=bibr anchor=ref223 id=context_ref_223_2e4>[223]</a> proposed a novel bioinspired enhancement model in which the source image is generated on the basis of a simulated CRM, and the weight matrix for image fusion is designed using illumination estimation techniques <a ref-type=bibr anchor=ref224 id=context_ref_224_2e4>[224]</a>, <a ref-type=bibr anchor=ref225 id=context_ref_225_2e4>[225]</a>. Unlike the model presented in <a ref-type=bibr anchor=ref223 id=context_ref_223_2e4>[223]</a>, the model presented in the later cited papers avoids any heuristic judgment of whether an image pixel is underexposed and thus is more flexible in generating more intermediate enhancement results. In Ref. <a ref-type=bibr anchor=ref226 id=context_ref_226_2e4>[226]</a>, a framework based on a CRM and a weighted least squares strategy was proposed in which every pixel is adjusted in accordance with a calculated exposure map and Retinex theory; this framework can preserve details while improving contrast, color correction, and noise suppression. In addition, Zhou <i>et al.</i> <a ref-type=bibr anchor=ref227 id=context_ref_227_2e4>[227]</a> generated multiple enhanced images based on a lightness-aware CRM and then performed mid-level fusion of these images based on a patch-based image decomposition model. This model, however, has a limited ability to improve images in which one area is already overenhanced. In this case, the overenhanced area is even more strongly enhanced, resulting in the loss of important details.
<div class="figure figure-full" id=fig23><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang23-2992749-large.gif data-fig-id=fig23><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang23-2992749-small.gif data-alt="FIGURE 23. - Fusion based on a single image."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 23. </b><fig><p>Fusion based on a single image.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>In short, the main idea of methods based on image fusion is that useful information on the same target collected from multiple sources can be further utilized, without requiring a physical model, to obtain a final high-quality image through image processing and computer technology.<p>These fusion-based methods are simple and can achieve good results. However, they require two or more different images of the same scene; therefore, it is difficult to realize image enhancement within a short time, as is needed for real-time monitoring situations, and these methods are difficult to apply and popularize in practice.</p></div></div><div class=section_2 id=sec2f><h3>F. Methods Based on Defogging Models</h3><p>As one branch of the field of image enhancement, image defogging techniques have seen great progress and produced good results in recent years. In 2009, He Kaiming proposed the dark channel prior theory for images, which has been widely applied <a ref-type=bibr anchor=ref228 id=context_ref_228_2f>[228]</a>. In 2011, a low-light enhancement algorithm <a ref-type=bibr anchor=ref229 id=context_ref_229_2f>[229]</a>, also called a bright channel prior method <a ref-type=bibr anchor=ref230 id=context_ref_230_2f>[230]</a>, <a ref-type=bibr anchor=ref231 id=context_ref_231_2f>[231]</a>, was proposed by Dong <i>et al.</i> based on defogging theory; this method relies on the statistical analysis of a dark primary color version of a low-light inverted image and a dark primary color version of a foggy image. The main idea of the algorithm is that when an RGB image captured in a dark environment is inverted, the visual effect is similar to that of a daytime image acquired in a foggy environment (as shown in <a ref-type=fig anchor=fig24 class=fulltext-link>Fig. 24</a>). Hence, a defogging algorithm based on a dark channel prior can be used to process the inverted low-light image; then, the image can be inverted again to obtain an enhanced low-light image.
<div class="figure figure-full" id=fig24><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang24-2992749-large.gif data-fig-id=fig24><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang24-2992749-small.gif data-alt="FIGURE 24. - Comparison of histograms of foggy, low-light and inverted images."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 24. </b><fig><p>Comparison of histograms of foggy, low-light and inverted images.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>This enhancement method greatly improves the image brightness and enhances the visual details of the image by analyzing the features of the low-light image and modeling them with a foggy image degradation model. The basic process is shown in <a ref-type=fig anchor=fig25 class=fulltext-link>Fig. 25</a>. Low-light image enhancement methods based on dark primary color defogging techniques have certain issues. For example, low-light images inevitably contain noise; however, the foggy image degradation model used in such an algorithm does not consider the effect of noise. Therefore, the image noise will typically be amplified, which will visually impact the results of image enhancement <a ref-type=bibr anchor=ref232 id=context_ref_232_2f>[232]</a>–<a ref-type=bibr anchor=ref234 id=context_ref_234_2f>[234]</a>. Considering the need for noise processing, Liu Yang <i>et al.</i> optimized the processing speed of such an algorithm without accurately extracting the transmittance; therefore, the final enhanced images exhibited a blocky effect. Zhang <i>et al.</i> also proposed an optimized algorithm, in which the parameters for transmittance estimation are selected directly based on experience; consequently, the robustness of this algorithm is poor <a ref-type=bibr anchor=ref235 id=context_ref_235_2f>[235]</a>. Jiang <i>et al.</i> used filters to remove details and introduced a pyramid operation to calculate a smooth transmission coefficient, which not only improved the processing speed but also yielded better naturalness <a ref-type=bibr anchor=ref236 id=context_ref_236_2f>[236]</a>. Simultaneously, the noise was suppressed. Later, Song <i>et al.</i> <a ref-type=bibr anchor=ref237 id=context_ref_237_2f>[237]</a> improved upon this model to overcome an issue related to block artifacts. Then, Pang introduced a gamma transformation to improve the image contrast <a ref-type=bibr anchor=ref238 id=context_ref_238_2f>[238]</a>. By combining the defogging approach with bilateral filtering, Zhang <i>et al.</i> proposed a low-light image enhancement method that can operate in real time. After the parameters are initially estimated using a dark channel prior, they are optimized using a bilateral filter; thus, the effect of noise is reduced <a ref-type=bibr anchor=ref239 id=context_ref_239_2f>[239]</a>. Tao <i>et al.</i> combined a bright channel prior with a convolutional neural network (CNN) <a ref-type=bibr anchor=ref240 id=context_ref_240_2f>[240]</a>, and Park <i>et al.</i> combined a bright channel prior <a ref-type=bibr anchor=ref241 id=context_ref_241_2f>[241]</a> with a Retinex enhancement algorithm. Both achieved improved results <a ref-type=bibr anchor=ref242 id=context_ref_242_2f>[242]</a>. A fast enhancement algorithm for low-light video has been proposed by combining Retinex theory with dark channel prior theory <a ref-type=bibr anchor=ref243 id=context_ref_243_2f>[243]</a>, and this algorithm can be further combined with scene detection, edge compensation and interframe compensation techniques for video enhancement. In <a ref-type=bibr anchor=ref244 id=context_ref_244_2f>[244]</a>, a method was proposed to solve the transmittance problem based on a foggy degradation model and a CNN, in which the transmission map and atmospheric light map are amended by means of guided filtering to obtain an enhanced low-light image. Recently, an enhancement method with strong illumination mitigation and bright halo suppression has been presented, which combines a dehazing algorithm with a dark channel prior and a denoising method to achieve a better visual effect <a ref-type=bibr anchor=ref245 id=context_ref_245_2f>[245]</a>.
<div class="figure figure-full" id=fig25><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang25-2992749-large.gif data-fig-id=fig25><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang25-2992749-small.gif data-alt="FIGURE 25. - Framework of low-light image enhancement based on a dark channel prior."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 25. </b><fig><p>Framework of low-light image enhancement based on a dark channel prior.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>Algorithms based on defogging models offer good performance with low computational complexity. However, their physical interpretation is somewhat lacking, and they are still susceptible to overenhancement in some detailed areas. Inverted low-light images have their own unique characteristics, and the direct application of defogging algorithms to such images is still not an ideal approach for image enhancement.</p></div><div class=section_2 id=sec2g><h3>G. Methods Based on Machine Learning</h3><p>Most existing low-light image enhancement techniques are model-based techniques rather than data-driven techniques. Only in recent years have methods based on machine learning for image enhancement begun to emerge in significant numbers <a ref-type=bibr anchor=ref246 id=context_ref_246_2g>[246]</a>–<a ref-type=bibr anchor=ref248 id=context_ref_248_2g>[248]</a>. For example, in <a ref-type=bibr anchor=ref249 id=context_ref_249_2g>[249]</a>, the reflection component of an object was represented using a sparse representation method, and the image details contained in the reflection component were learned using a dictionary learning method, thus achieving an improved enhancement effect. However, noise can easily be introduced during the machine learning process. An image enhancement method based on a color estimation model (CEM) was proposed by Fu <i>et al.</i> <a ref-type=bibr anchor=ref250 id=context_ref_250_2g>[250]</a>, in which the dynamic range of color images in the RGB color space was controlled by adjusting the CEM parameters to effectively inhibit oversaturation of the enhanced images. Fotiadou <i>et al.</i> proposed a low-light image enhancement algorithm based on a sparse image representation <a ref-type=bibr anchor=ref251 id=context_ref_251_2g>[251]</a> in which both a low-light condition dictionary and a daylight condition dictionary were established. The sparse constraint was used as prior knowledge to update the dictionaries, and low-light image blocks were used to approximately estimate the corresponding daylight images. An image enhancement algorithm based on fuzzy rule reasoning <a ref-type=bibr anchor=ref252 id=context_ref_252_2g>[252]</a> was proposed in which three traditional enhancement methods were combined by applying fuzzy theory and machine learning to establish a set of fuzzy rules, and the best enhancement algorithm was adaptively selected for different images to achieve image enhancement. This method can also be used to objectively and accurately evaluate the image enhancement effect.<p>Since 2016, several deep-learning-based methods for image enhancement have also emerged. For example, Yan <i>et al.</i> proposed the first deep-learning-based method for photo adjustment <a ref-type=bibr anchor=ref253 id=context_ref_253_2g>[253]</a>. Lore <i>et al.</i> adopted a stacked sparse denoising autoencoder in a framework for training an LLNet for low-light image enhancement <a ref-type=bibr anchor=ref254 id=context_ref_254_2g>[254]</a>. In this framework, a sparsity regularized reconstruction loss was taken as the loss function, and deep learning based on the self-encoder approach was used to learn the features of image signals acquired under various low-illumination conditions to realize adaptive brightness adjustment and denoising. Park <i>et al.</i> proposed a dual autoencoder network model based on Retinex theory <a ref-type=bibr anchor=ref255 id=context_ref_255_2g>[255]</a>; in this model, a stacked autoencoder was combined with a convolutional autoencoder to realize low-light enhancement and noise reduction. The stacked autoencoder, with a small number of hidden units, was used to estimate the smooth illumination component in the space, and the convolutional autoencoder was used to process two-dimensional image information to reduce the amplification of noise during the process of brightness enhancement.<p>CNNs have been used as the basis of deep learning frameworks in many research works <a ref-type=bibr anchor=ref256 id=context_ref_256_2g>[256]</a>–<a ref-type=bibr anchor=ref259 id=context_ref_259_2g>[259]</a>. Tao <i>et al.</i> proposed a low-light CNN (LLCNN) in which a multistage characteristic map was used to generate an enhanced image by learning from low-light images with different nuclei <a ref-type=bibr anchor=ref260 id=context_ref_260_2g>[260]</a>. In <a ref-type=bibr anchor=ref261 id=context_ref_261_2g>[261]</a>, a global illumination-aware and detail-preserving network (GLADNet) was designed. In this network, the input image is first scaled to a certain size and then passed to encoder and decoder networks to generate global prior knowledge of the illumination. Based on this prior information and the original images, a convolutional network is then used to reconstruct the image details. Ignatov <i>et al.</i> took a different approach of learning a mapping between images acquired by a mobile phone camera and a digital single-lens reflex (DSLR) camera. They built a dataset consisting of images of the same scene taken by the different cameras <a ref-type=bibr anchor=ref262 id=context_ref_262_2g>[262]</a> and presented an end-to-end deep learning approach for translating ordinary photos into DSLR-quality images. Lv <i>et al.</i> proposed a new network (MBLLEN) consisting of a feature extraction module (FEM), an enhancement module (EM) and a fusion module (FM) <a ref-type=bibr anchor=ref263 id=context_ref_263_2g>[263]</a>, which produces output images via feature fusion. Gabriel <i>et al.</i> designed a deep convolutional neural network (DCNN) <a ref-type=bibr anchor=ref264 id=context_ref_264_2g>[264]</a> based on a large dataset of HDR images, and Liu <i>et al.</i> trained the DCNN using only synthetic data to recover the details lost due to quantization <a ref-type=bibr anchor=ref265 id=context_ref_265_2g>[265]</a>. Gharbi <i>et al.</i> constructed a learning framework based on a deep bilateral network, thus achieving real-time processing for image enhancement <a ref-type=bibr anchor=ref266 id=context_ref_266_2g>[266]</a>. Later, Chen <i>et al.</i> <a ref-type=bibr anchor=ref267 id=context_ref_267_2g>[267]</a> introduced a dataset of raw short-exposure low-light images (the See-in-the-Dark (SID) database) and developed a pipeline for processing these images based on a fully convolutional network (FCN). Through end-to-end training, a good improvement over the traditional method of low-light image processing was achieved.<p>Based on Retinex theory, Shen <i>et al.</i> <a ref-type=bibr anchor=ref268 id=context_ref_268_2g>[268]</a> analyzed the performance of the MSR algorithm from the perspective of a CNN framework and proposed a method of enhancing low-light images by using an MSR network (MSR-net) based on a CNN architecture, while Guo <i>et al.</i> <a ref-type=bibr anchor=ref269 id=context_ref_269_2g>[269]</a> proposed our pipeline neural network consisting of a denoising net and low light image enhancement net (LLIE-net). Wei <i>et al.</i> assumed that observed images could be decomposed into their reflectance and illumination components, collected a LOw-Light (LOL) dataset containing low-/normal-light image pairs, and proposed a deep network called Retinex-Net <a ref-type=bibr anchor=ref270 id=context_ref_270_2g>[270]</a>. Li <i>et al.</i> designed a network called LightenNet based on a CNN architecture <a ref-type=bibr anchor=ref271 id=context_ref_271_2g>[271]</a>; this network takes a weakly illuminated image as input and outputs a corresponding illumination map, which is subsequently used to generate an enhanced image based on the Retinex model. Zhang <i>et al.</i> built a simple yet effective network called the Kindling the Darkness (KinD) network <a ref-type=bibr anchor=ref272 id=context_ref_272_2g>[272]</a>, which is composed of a layer decomposition net, a reflectance restoration net, and an illumination adjustment net, and trained it on pairs of images captured under different exposure conditions.<p>Inspired by the multiple image fusion method, Cai <i>et al.</i> <a ref-type=bibr anchor=ref273 id=context_ref_273_2g>[273]</a> proposed a framework based on a CNN trained to enhance single images. In this work, thirteen different MEF and HDR compression methods were used to generate an enhanced image for each series of images from a large-scale multiexposure image dataset. Finally, low-light images were enhanced by the CNN after end-to-end training on the low-contrast and high-contrast image dataset. Yang <i>et al.</i> used two CNNs to build a tool for RGB image enhancement <a ref-type=bibr anchor=ref274 id=context_ref_274_2g>[274]</a> in which intermediate HDR images are first generated from the input RGB images to ultimately produce high-quality LDR images. Nevertheless, generating HDR images from single images is a challenging problem. To handle both local and global features, Kinoshita and Kiya proposed an architecture consisting of a local encoder, a global encoder, and a decoder trained on tone-mapped images obtained from existing HDR images <a ref-type=bibr anchor=ref275 id=context_ref_275_2g>[275]</a>. Experimental results showed its excellent performance compared with conventional image enhancement methods, including CNN-based methods.<p>In contrast to supervised learning methods, generative adversarial network (GAN)-based methods can be used for image enhancement without training on pairs of images <a ref-type=bibr anchor=ref276 id=context_ref_276_2g>[276]</a>, <a ref-type=bibr anchor=ref277 id=context_ref_277_2g>[277]</a>. For example, Meng <i>et al.</i> proposed a GAN-based framework for nighttime image enhancement, which takes advantage of GANs’ powerful ability to generate images from real data distributions, and the results demonstrate its effectiveness. To our knowledge, this was the first time that GANs were applied for the purpose of nighttime image enhancement <a ref-type=bibr anchor=ref278 id=context_ref_278_2g>[278]</a>. In Ref. <a ref-type=bibr anchor=ref279 id=context_ref_279_2g>[279]</a>, the authors fully utilized the advantages of both GANs and CNNs, using a transitive CNN to map the enhanced images back to the space of the source images to relax the need for paired ground-truth photos. Kim <i>et al.</i> <a ref-type=bibr anchor=ref280 id=context_ref_280_2g>[280]</a> applied local illumination to make the training images and used an advanced generative adversarial network to build Low-Lightgan. The key advantages of such networks are that they can be trained easily and can achieve better experimental results than traditional enhancers <a ref-type=bibr anchor=ref279 id=context_ref_279_2g>[279]</a>.<p>Undoubtedly, deep-learning-based methods can achieve excellent performance in low-light image enhancement, and they also represent a major trend of current development in image processing research. However, such methods must be supported by large datasets, and an increase in the complexity of a model will lead to a sharp increase in the time complexity of the corresponding algorithm. With the steady growth of research on low-light image enhancement, not only are some low-light data available from widely used public benchmark datasets such as PASCAL VOC <a ref-type=bibr anchor=ref281 id=context_ref_281_2g>[281]</a>, ImageNet <a ref-type=bibr anchor=ref282 id=context_ref_282_2g>[282]</a>, and Microsoft COCO <a ref-type=bibr anchor=ref283 id=context_ref_283_2g>[283]</a>, but researchers are also building public datasets specifically designed for low-light image processing, such as SID <a ref-type=bibr anchor=ref267 id=context_ref_267_2g>[267]</a> and EDD (Exclusively Dark Dataset) <a ref-type=bibr anchor=ref284 id=context_ref_284_2g>[284]</a>.</p></div></div>
<div class=section id=sec3><div class="header article-hdr"><div class=kicker>
 SECTION III.</div><h2>Evaluation Methods</h2></div><p>Image quality assessment (IQA) focuses mainly on two aspects, namely, the fidelity of the image and the readability of the image, which can be regarded as subjective and objective evaluation standards, respectively. A subjective evaluation method measures image quality on the basis of the subjective perception of the human visual system, i.e., whether the image conveys a certain experience. However, it is still difficult to accurately simulate the human visual system. Therefore, current subjective evaluation systems based on the human visual system can evaluate image quality only qualitatively rather than quantitatively <a ref-type=bibr anchor=ref285 id=context_ref_285_3>[285]</a>.<div class=section_2 id=sec3a><h3>A. Subjective Evaluation</h3><p>In a subjective evaluation method, human observers are asked to evaluate the quality of processed images in accordance with their visual effects based on a predetermined evaluation scale. Such an evaluation depends on subjective assessment of the image processing results to determine the advantages and disadvantages of a particular algorithm. The score is typically divided into 5 grades (1-5 points), and the number of raters should typically be no fewer than 20 <a ref-type=bibr anchor=ref286 id=context_ref_286_3a>[286]</a>. Some of the raters should have experience in image processing, while some should not. The raters will evaluate the visual effects of the images in accordance with their personal experience or agreed-upon evaluation criteria. To ensure fairness and equity, the final scores will be weighted to obtain the final subjective quality evaluation result for each image. The typical evaluation standards are summarized in <a ref-type=table anchor=table1 class=fulltext-link>Table 1</a>.<div class="figure figure-full table" id=table1><div class=figcaption><b class=title>TABLE 1 </b>
Criteria for Subjective Assessment</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t1-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t1-2992749-small.gif data-alt="Table 1- 
Criteria for Subjective Assessment"><div class=zoom title="View Larger Image"></div></a></div></div><p><p>This method is simple and can reflect the visual quality of images. Such a subjective evaluation can accurately represent the visual perception of the majority of observers. However, such an evaluation lacks stability and can be easily affected by the experimental conditions as well as the knowledge background, emotional state, motivation and degree of fatigue of the observer. In studies related to image enhancement, it is necessary to provide key details of different magnified parts of images for comparison to assess, e.g., lack of uniformity. However, this process is time consuming and arduous in practice and thus often cannot be applied in engineering applications.</p></div><div class=section_2 id=sec3b><h3>B. Objective Evaluation</h3><p>An objective evaluation is an evaluation using specific data and based on certain objective criteria. To the best of our knowledge, there are no IQA methods that have been specifically designed for the evaluation of low-light image enhancement methods. Hence, different researchers utilize different strategies to evaluate their results. At present, the objective evaluation methods for image enhancement can be divided into full-reference methods and no-reference methods depending on whether they require reference images (ground-truth images or synthetic images). Objective evaluation methods have various advantages, such as simple calculations, fast execution, ease of quantitative calculation based on a constructed model, and high stability; therefore, data from objective evaluations are generally adopted as image quality scores <a ref-type=bibr anchor=ref287 id=context_ref_287_3b>[287]</a>.<div class=section_2 id=sec3b1><h4>1) No-Reference IQA (NIQA) Metrics</h4><p>Since no objective reference image is available in the case of a low-light input image, most methods that are suitable for low-light image enhancement assessment are based on NIQA metrics. The most common NIQA metrics include the mean value (MV), standard difference (STD), average gradient (AG), and information entropy (IE). In addition, there are several general methods available for image quality evaluation, including the Blind/Referenceless Image Spatial QUality Evaluator (BRISQUE) <a ref-type=bibr anchor=ref288 id=context_ref_288_3b1>[288]</a>, the Naturalness Image Quality Evaluator (NIQE) <a ref-type=bibr anchor=ref289 id=context_ref_289_3b1>[289]</a>, the BLind Image Integrity Notator using DCT Statistics (BLIINDS-II) <a ref-type=bibr anchor=ref290 id=context_ref_290_3b1>[290]</a>, the blind tone-mapped image quality index (BTMQI) <a ref-type=bibr anchor=ref291 id=context_ref_291_3b1>[291]</a>, gradient ratioing at visible edges (GRVE) <a ref-type=bibr anchor=ref292 id=context_ref_292_3b1>[292]</a>, the autoregressive-based image sharpness metric (ARISM) <a ref-type=bibr anchor=ref293 id=context_ref_293_3b1>[293]</a>, the no-reference image quality metric for contrast distortion (NIQMC) <a ref-type=bibr anchor=ref294 id=context_ref_294_3b1>[294]</a>, the Global Contrast Factor (GCF) <a ref-type=bibr anchor=ref295 id=context_ref_295_3b1>[295]</a>, the average information content (AIC) <a ref-type=bibr anchor=ref296 id=context_ref_296_3b1>[296]</a>, the effective measure of enhancement (EME) <a ref-type=bibr anchor=ref297 id=context_ref_297_3b1>[297]</a>, PixDist <a ref-type=bibr anchor=ref298 id=context_ref_298_3b1>[298]</a>, and the no-reference free-energy-based robust metric (NFERM) <a ref-type=bibr anchor=ref299 id=context_ref_299_3b1>[299]</a>. The commonly used NIQA metrics and related references are shown in <a ref-type=table anchor=table2 class=fulltext-link>Table 2</a>. Descriptions of several of these metrics follow.
<ol><li><p>Mean value (MV). The MV mainly refers to the mean of the gray values of an image, and it mainly reflects the color or degree of brightness of the image. The smaller the image mean is, the darker the image. Conversely, the larger the mean is, the brighter the image, and the lighter the colors. The formula is as follows:<disp-formula id=deqn30 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} \mu = \frac {1}{M \times N} \sum \limits ^{M}_{i=1} \sum \limits ^{N}_{j=1} f(i,j)\tag{30}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-141-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} \mu = \frac {1}{M \times N} \sum \limits ^{M}_{i=1} \sum \limits ^{N}_{j=1} f(i,j)\tag{30}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>M</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-142-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>N</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-143-Frame tabindex=0></span>
</tex-math></inline-formula> are the width and height, respectively, of the image and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-144-Frame tabindex=0></span>
</tex-math></inline-formula> is the gray value at pixel point <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-145-Frame tabindex=0></span>
</tex-math></inline-formula>.</p><li><p>Standard difference (STD). The variance of the gray values reflects the degree of dispersion of the image relative to the mean and thus is a measure of the contrast within a certain range. The larger the variance is, the more information is contained in the image, and the better the visual effect. When the variance is smaller, the information contained in the image is less, and the image is more monochromatic and uniform. The formula is <disp-formula id=deqn31 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} STD = \sqrt {\frac {\sum \limits ^{M}_{i=1} \sum \limits ^{N}_{j=1} f(i,j)(f(i,j)-\mu)^{2}}{M \times N}}\tag{31}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-146-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} STD = \sqrt {\frac {\sum \limits ^{M}_{i=1} \sum \limits ^{N}_{j=1} f(i,j)(f(i,j)-\mu)^{2}}{M \times N}}\tag{31}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>M</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-147-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>N</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-148-Frame tabindex=0></span>
</tex-math></inline-formula> are the image width and height, respectively; <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-149-Frame tabindex=0></span>
</tex-math></inline-formula> is the gray value at pixel point <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-150-Frame tabindex=0></span>
</tex-math></inline-formula>; and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\mu </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-151-Frame tabindex=0></span>
</tex-math></inline-formula> is the MV of the image</p><li><p>Average gradient (AG). The AG represents the clarity of an image, reflecting the image’s ability to express contrasting details. This metric measures the rate of change in the image values based on changes in the contrast of minute details or the relative clarity of the image. In an image, faster gray changes in a certain direction result in larger image gradients; therefore, this metric can be used to determine whether or not an image is clear. The AG can be expressed as <disp-formula id=deqn32 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} AG = \frac {1}{M \times N} \sum \limits ^{M}_{i=1} \sum \limits ^{N}_{j=1} \sqrt {\frac {(\partial f/\partial x)^{2}+(\partial f/\partial y)^{2}}{2}}\tag{32}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-152-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} AG = \frac {1}{M \times N} \sum \limits ^{M}_{i=1} \sum \limits ^{N}_{j=1} \sqrt {\frac {(\partial f/\partial x)^{2}+(\partial f/\partial y)^{2}}{2}}\tag{32}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>M</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-153-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>N</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-154-Frame tabindex=0></span>
</tex-math></inline-formula> are the image width and height, respectively, and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\partial f/\partial x</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-155-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\partial f/\partial x</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-156-Frame tabindex=0></span>
</tex-math></inline-formula> are the horizontal and vertical gradients, respectively.</p><li><p>Information entropy (IE). Entropy can be used as a measure of an amount of information and is widely used to evaluate image quality <a ref-type=bibr anchor=ref300 id=context_ref_300_3b1>[300]</a>, <a ref-type=bibr anchor=ref48 id=context_ref_48_3b1>[48]</a>. A static image is regarded as an information source with random output; the set <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>A</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-157-Frame tabindex=0></span>
</tex-math></inline-formula> of source symbols is defined as the set of all possible symbols {<inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>a_{i}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-158-Frame tabindex=0></span>
</tex-math></inline-formula>}, and the probability of source symbol <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>a_{i}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-159-Frame tabindex=0></span>
</tex-math></inline-formula> is <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>P(a_{i})</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-160-Frame tabindex=0></span>
</tex-math></inline-formula>. Thus, the average information quantity of an image can be expressed as <disp-formula id=deqn33 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} H = -\sum \limits _{i=1}^{L} P(a_{i}) \log _{2} P(a_{i})\tag{33}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-161-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} H = -\sum \limits _{i=1}^{L} P(a_{i}) \log _{2} P(a_{i})\tag{33}\end{equation*}
</span></span></disp-formula></p></ol><div class="figure figure-full table" id=table2><div class=figcaption><b class=title>TABLE 2 </b>
NIQA Metrics and Related References</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t2-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t2-2992749-small.gif data-alt="Table 2- 
NIQA Metrics and Related References"><div class=zoom title="View Larger Image"></div></a></div></div><p><p>According to entropy theory, the larger the IE value is, the larger the amount of information contained in the image, and the richer the image detail.</p></div><div class=section_2 id=sec3b2><h4>2) Full-Reference IQA (FIQA) Metrics</h4><p>The most common FIQA metrics include the mean square error (MSE), the peak signal-to-noise ratio (PSNR), the structural similarity index metric (SSIM) <a ref-type=bibr anchor=ref301 id=context_ref_301_3b2>[301]</a>, and the lightness order error (LOE) <a ref-type=bibr anchor=ref136 id=context_ref_136_3b2>[136]</a>. Other available FIQA metrics include the patch-based contrast quality index (PCQI) <a ref-type=bibr anchor=ref302 id=context_ref_302_3b2>[302]</a>, the colorfulness-based PCQI (CPCQI) <a ref-type=bibr anchor=ref303 id=context_ref_303_3b2>[303]</a>, the Gradient Magnitude Similarity Deviation (GMSD) <a ref-type=bibr anchor=ref304 id=context_ref_304_3b2>[304]</a>, the visual information fidelity (VIF) <a ref-type=bibr anchor=ref305 id=context_ref_305_3b2>[305]</a>, the visual saliency index (VSI) <a ref-type=bibr anchor=ref306 id=context_ref_306_3b2>[306]</a>, the tone-mapped image quality index (TMQI) <a ref-type=bibr anchor=ref307 id=context_ref_307_3b2>[307]</a>, the Statistical Naturalness Measure (SNM) <a ref-type=bibr anchor=ref307 id=context_ref_307_3b2>[307]</a>, and the Feature SIMilarity Index (FSIM) <a ref-type=bibr anchor=ref308 id=context_ref_308_3b2>[308]</a>. The commonly used NIQA metrics and related references are shown in <a ref-type=table anchor=table3 class=fulltext-link>Table 3</a>. Descriptions of several of these metrics follow.
<ol><li><p>Mean square error (MSE). This metric represents the direct deviation between the enhanced image and the original image; it has the same meaning as the absolute mean brightness error (AMBE) <a ref-type=bibr anchor=ref75 id=context_ref_75_3b2>[75]</a>.<disp-formula id=deqn34 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} MSE = \frac {1}{M \times N}\sum \limits ^{M}_{i=1} \sum \limits ^{N}_{j=1} [f(i,j)-f_{e}(i,j)]\tag{34}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-162-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} MSE = \frac {1}{M \times N}\sum \limits ^{M}_{i=1} \sum \limits ^{N}_{j=1} [f(i,j)-f_{e}(i,j)]\tag{34}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>M</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-163-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>N</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-164-Frame tabindex=0></span>
</tex-math></inline-formula> are the width and height, respectively, of the image; <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-165-Frame tabindex=0></span>
</tex-math></inline-formula> represents the input image; and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{e}(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-166-Frame tabindex=0></span>
</tex-math></inline-formula> represents the enhanced image. In an image quality evaluation, a smaller MSE value indicates higher similarity between the enhanced and original images.</p><li><p>Peak signal-to-noise ratio (PSNR). The PSNR of an image is the most extensively and commonly used objective evaluation method for measuring the image denoising effect. The larger the PSNR value is, the smaller the difference between the images before and after processing. An excessively high PSNR indicates that the effect of the denoising algorithm is not obvious. A smaller PSNR indicates a greater difference between the images before and after processing. An excessively low PSNR may suggest that the image is distorted. The specific expression is as follows:<disp-formula id=deqn35 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} PSNR = 10~\text {lg} \frac {f^{2}_{\max }}{MSE}\tag{35}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-167-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} PSNR = 10~\text {lg} \frac {f^{2}_{\max }}{MSE}\tag{35}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{\max }</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-168-Frame tabindex=0></span>
</tex-math></inline-formula> is the maximum gray value, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{\max }=255</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-169-Frame tabindex=0></span>
</tex-math></inline-formula>.</p><li><p>Structural similarity index metric (SSIM). The above methods do not consider the characteristics of the human visual system when assessing image quality; they compute only a simple random error between the input image and the processed image and analyze the difference between the input and output images from a mathematical perspective. Therefore, the above metrics cannot fully and accurately reflect the image quality. Researchers have found that natural images exhibit certain special structural features, such as strong correlations between pixels, and these correlations capture a large amount of important structural information for an image. Therefore, Wang <i>et al.</i> proposed a method based on structural similarity for evaluating image quality <a ref-type=bibr anchor=ref301 id=context_ref_301_3b2>[301]</a>. The SSIM evaluates the quality of a processed image relative to the reference image based on comparisons of luminance (<inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>l</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-170-Frame tabindex=0></span>
</tex-math></inline-formula>(<inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>{f}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-171-Frame tabindex=0></span>
</tex-math></inline-formula>, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{e})</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-172-Frame tabindex=0></span>
</tex-math></inline-formula>), contrast (<inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>c</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-173-Frame tabindex=0></span>
</tex-math></inline-formula>(<inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>{f}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-174-Frame tabindex=0></span>
</tex-math></inline-formula>, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{e})</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-175-Frame tabindex=0></span>
</tex-math></inline-formula>) and structure (<inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>s</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-176-Frame tabindex=0></span>
</tex-math></inline-formula>(<inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>{f}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-177-Frame tabindex=0></span>
</tex-math></inline-formula>, <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{e})</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-178-Frame tabindex=0></span>
</tex-math></inline-formula>) between the two images. These three values are combined to obtain the overall similarity measure. The formula is as follows:<disp-formula id=deqn36 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} SSIM = F [1(f,f_{e}),c(f,f_{e}),s(f,f_{e})]\tag{36}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-179-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} SSIM = F [1(f,f_{e}),c(f,f_{e}),s(f,f_{e})]\tag{36}\end{equation*}
</span></span></disp-formula><p>The flow of the SSIM algorithm is shown in <a ref-type=fig anchor=fig26 class=fulltext-link>Fig. 26</a>.<p>The degree of similarity between the two images is reflected by the value of the SSIM; the minimum value is 0, and the maximum value is 1. A value closer to 1 indicates that the two images are more similar. Taking the human visual system as the starting point, this method can effectively simulate human visual perception to extract information about the structure of an image. The evaluation result is very close to the subjective perception of the human eye; therefore, this metric is widely used in image quality evaluations.</p><li><p>Lightness order error (LOE). Considering that the relative order of lightness of different image areas reflects the direction of the light source and the variation in illumination, Ref. <a ref-type=bibr anchor=ref136 id=context_ref_136_3b2>[136]</a> proposed the LOE metric to measure the discrepancy in lightness order between an original image <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-180-Frame tabindex=0></span>
</tex-math></inline-formula> and its enhanced version <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>I_{\mathrm {e}}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-181-Frame tabindex=0></span>
</tex-math></inline-formula>. The LOE is defined as <disp-formula id=deqn37 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{equation*} LOE = \frac {1}{M \times N} \sum \limits _{i=1}^{M}\sum \limits _{j=1}^{N} RD(i,j)\tag{37}\end{equation*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-182-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{equation*} LOE = \frac {1}{M \times N} \sum \limits _{i=1}^{M}\sum \limits _{j=1}^{N} RD(i,j)\tag{37}\end{equation*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>RD(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-183-Frame tabindex=0></span>
</tex-math></inline-formula> is the difference in the relative lightness order between the original image <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-184-Frame tabindex=0></span>
</tex-math></inline-formula> and its enhanced version <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{e}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-185-Frame tabindex=0></span>
</tex-math></inline-formula> at pixel <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-186-Frame tabindex=0></span>
</tex-math></inline-formula>. This difference is defined as follows:<disp-formula id=deqn38 class=display-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\begin{align*} RD(i,j) = \sum \limits _{x}^{M}\sum \limits _{y}^{N} U (L(i,j),L(x,y)) \oplus U (L_{e}(i,j),L_{e}(x,y)) \\ {}\tag{38}\end{align*}</span><span class="MathJax_Display MathJax_Processing"><span class=MathJax id=MathJax-Element-187-Frame tabindex=0></span></span>
</tex-math><span class=formula><span class=link>View Source</span><img class=document-ft-image aria-describedby=qtip-0 style="display:inline;background-blend-mode:normal!important;background-clip:content-box!important;background-position:50% 50%!important;background-color:rgba(0,0,0,0)!important;background-image:var(--sf-img-5)!important;background-size:100% 100%!important;background-origin:content-box!important;background-repeat:no-repeat!important" title="Right-click on figure or equation for MathML and additional features." data-hasqtip=0 alt="Right-click on figure for MathML and additional features." src='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="24" height="20"><rect fill-opacity="0"/></svg>' width=24 height=20 border=0><span class="tex tex2jax_ignore" style=display:none>\begin{align*} RD(i,j) = \sum \limits _{x}^{M}\sum \limits _{y}^{N} U (L(i,j),L(x,y)) \oplus U (L_{e}(i,j),L_{e}(x,y)) \\ {}\tag{38}\end{align*}
</span></span></disp-formula> where <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>M</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-188-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>N</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-189-Frame tabindex=0></span>
</tex-math></inline-formula> are the image width and height, respectively; <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>\oplus </span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-190-Frame tabindex=0></span>
</tex-math></inline-formula> is the exclusive-or operator; and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>L(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-191-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>L_{e}(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-192-Frame tabindex=0></span>
</tex-math></inline-formula> are the maximum values among the three color channels at location <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>(i,j)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-193-Frame tabindex=0></span>
</tex-math></inline-formula> for <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-194-Frame tabindex=0></span>
</tex-math></inline-formula> and <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>f_{e}</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-195-Frame tabindex=0></span>
</tex-math></inline-formula>, respectively. The function <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>U(p,q)</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-196-Frame tabindex=0></span>
</tex-math></inline-formula> returns a value of 1 if <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>p&gt;=q</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-197-Frame tabindex=0></span>
</tex-math></inline-formula>; otherwise, it returns 0. The smaller the LOE value is, the better the lightness order is preserved.</p></ol><div class="figure figure-full table" id=table3><div class=figcaption><b class=title>TABLE 3 </b>
FIQA Metrics and Related References</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t3-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t3-2992749-small.gif data-alt="Table 3- 
FIQA Metrics and Related References"><div class=zoom title="View Larger Image"></div></a></div></div>
<div class="figure figure-full" id=fig26><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang26-2992749-large.gif data-fig-id=fig26><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang26-2992749-small.gif data-alt="FIGURE 26. - Flowchart of the SSIM algorithm."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 26. </b><fig><p>Flowchart of the SSIM algorithm.</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>The above measures have the following advantages: they are simple to calculate, have clear physical meanings, and enable mathematically convenient optimizations.</p></div></div></div>
<div class=section id=sec4><div class="header article-hdr"><div class=kicker>
 SECTION IV.</div><h2>Analysis of Different Enhancement Methods</h2></div><p>To compare the enhancement effects of various algorithms as well as the consistency of subjective and objective evaluation, experiments using many methods are presented in this paper for illustration. A test platform was built based on a desktop computer to verify the algorithms. This system includes an Intel(R) Core(TM) i7-6700 CPU @3.4 GHz with 16 GB RAM and the Windows 10 operating system.<div class=section_2 id=sec4a><h3>A. Subjective Evaluation</h3><p>The test images shown in <a ref-type=fig anchor=fig27 class=fulltext-link>Fig. 27</a> represent three different illumination conditions, namely, uniform low light, uneven illumination and nighttime; the source images are named ‘Flowers.bmp’, ‘Building.bmp’ and ‘Lawn.bmp’, respectively. In addition, we adopt two pairs of images for reference-based comparisons, where each pair consists of a low-light image, as shown in the top row of <a ref-type=fig anchor=fig28 class=fulltext-link>Fig. 28</a>, and a corresponding well-exposed image, as shown in the bottom row. In <a ref-type=fig anchor=fig28 class=fulltext-link>Fig. 28</a>, the image on the left is named ‘Desk.bmp’, and the image on the right is ‘Road.bmp’.
<div class="figure figure-full" id=fig27><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang27abc-2992749-large.gif data-fig-id=fig27><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang27abc-2992749-small.gif data-alt="FIGURE 27. - Low-light images under three illumination conditions (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 27. </b><fig><p>Low-light images under three illumination conditions (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig28><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang28ab-2992749-large.gif data-fig-id=fig28><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang28ab-2992749-small.gif data-alt="FIGURE 28. - Two pairs of images with different exposures (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 28. </b><fig><p>Two pairs of images with different exposures (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p><p>The experimental results are shown in <a ref-type=fig anchor=fig29 class=fulltext-link>Figs. 29</a>–<a ref-type=fig anchor=fig30 class=fulltext-link></a><a ref-type=fig anchor=fig31 class=fulltext-link></a><a ref-type=fig anchor=fig32 class=fulltext-link></a><a ref-type=fig anchor=fig33 class=fulltext-link>33</a>. In these figures, panel (a) contains the original image, and panels (b)-(r) display the results of many enhancement methods: Gamma correction <a ref-type=bibr anchor=ref3 id=context_ref_3_4a>[3]</a>, AHE <a ref-type=bibr anchor=ref78 id=context_ref_78_4a>[78]</a>, WT <a ref-type=bibr anchor=ref166 id=context_ref_166_4a>[166]</a>, BIMEF <a ref-type=bibr anchor=ref223 id=context_ref_223_4a>[223]</a>, CegaHE <a ref-type=bibr anchor=ref102 id=context_ref_102_4a>[102]</a>, CRM <a ref-type=bibr anchor=ref225 id=context_ref_225_4a>[225]</a>, CVC <a ref-type=bibr anchor=ref89 id=context_ref_89_4a>[89]</a>, Dong <i>et al.</i> <a ref-type=bibr anchor=ref229 id=context_ref_229_4a>[229]</a>, MBLLEN <a ref-type=bibr anchor=ref263 id=context_ref_263_4a>[263]</a>, HMF <a ref-type=bibr anchor=ref101 id=context_ref_101_4a>[101]</a>, LIME <a ref-type=bibr anchor=ref13 id=context_ref_13_4a>[13]</a>, MF <a ref-type=bibr anchor=ref219 id=context_ref_219_4a>[219]</a>, HE <a ref-type=bibr anchor=ref66 id=context_ref_66_4a>[66]</a>, MSRCP <a ref-type=bibr anchor=ref127 id=context_ref_127_4a>[127]</a>, MSRCR <a ref-type=bibr anchor=ref115 id=context_ref_115_4a>[115]</a>, NPE <a ref-type=bibr anchor=ref136 id=context_ref_136_4a>[136]</a>, and SRIE <a ref-type=bibr anchor=ref126 id=context_ref_126_4a>[126]</a>. As shown in panels (b)-(r), all of these image enhancement methods improve the visual effect of the original image to some degree. The details become clearer with the Gamma, WT, AHE, HMF, CVC, MSRCP and SRIE methods, but the overall level of brightness is dark. Especially when the Gamma and CVC methods are used, the enhancement effects for the three types of images are similar, while the WT method makes the output image blurred. The AHE method achieves a better effect when processing uniformly illuminated low-light images, but a wheel halo effect appears in unevenly illuminated low-light images. Although the CegaHE, HE, and MSRCR methods can brighten the entire image, the hue changes dramatically for an image with uneven illumination, resulting in the loss of the real color of the original scene. Although the image brightness after processing with the SRIE method is not high, this method achieves a consistent image processing effect for all three types of images, and the tone recovery effect is superior. By comparison, the Dong, MBLLEN, MF, NPE, LIME and CRM methods demonstrate outstanding performance in color and detail enhancement, and their visual effects are obviously superior to those of the other abovementioned image enhancement methods. However, when the Dong and NPE methods are used to process unevenly illuminated low-light images such as the ‘Building’ and ‘Road’ images, overenhancement appears at the boundaries. The MF method, MBLLEN method and CRM method better maintain the color of the original images compared with the above methods, but their overall effect is no better than that of the LIME method. The LIME method considers both brightness and hue information and maintains excellent realistic effects. Hence, the LIME method achieves higher color fidelity from the perspective of human visual perception.
<div class="figure figure-full" id=fig29><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang29abcdefghijklmnopqr-2992749-large.gif data-fig-id=fig29><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang29abcdefghijklmnopqr-2992749-small.gif data-alt="FIGURE 29. - Experimental results on ‘Flowers.bmp’ (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 29. </b><fig><p>Experimental results on ‘Flowers.bmp’ (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig30><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang30abcdefghijklmnopqr-2992749-large.gif data-fig-id=fig30><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang30abcdefghijklmnopqr-2992749-small.gif data-alt="FIGURE 30. - Experimental results on ‘Building.bmp’ (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 30. </b><fig><p>Experimental results on ‘Building.bmp’ (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig31><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang31abcdefghijklmnopqr-2992749-large.gif data-fig-id=fig31><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang31abcdefghijklmnopqr-2992749-small.gif data-alt="FIGURE 31. - Experimental results on ‘Lawn.bmp’ (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 31. </b><fig><p>Experimental results on ‘Lawn.bmp’ (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig32><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang32abcdefghijklmnopqr-2992749-large.gif data-fig-id=fig32><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang32abcdefghijklmnopqr-2992749-small.gif data-alt="FIGURE 32. - Experimental results on ‘Desk.bmp’ (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 32. </b><fig><p>Experimental results on ‘Desk.bmp’ (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div>
<div class="figure figure-full" id=fig33><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang33abcdefghijklmnopqr-2992749-large.gif data-fig-id=fig33><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang33abcdefghijklmnopqr-2992749-small.gif data-alt="FIGURE 33. - Experimental results on ‘Road.bmp’ (figure best viewed in color)."><div class=zoom title="View Larger Image"></div></a></div><div class=figcaption><b class=title>FIGURE 33. </b><fig><p>Experimental results on ‘Road.bmp’ (figure best viewed in color).</p></fig></div><p class=links><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9088214/all-figures class=all>Show All</a></p></div><p></p></div><div class=section_2 id=sec4b><h3>B. Objective Evaluation</h3><p>Based on the above images, experiments for objective quality evaluation were performed using various IQA methods, including both NIQA and FIQA metrics.<div class=section_2 id=sec4b1><h4>1) NIQA-Based Evaluation</h4><p>Eight metrics, namely, STD, IE, AG, BLIINDS-II <a ref-type=bibr anchor=ref290 id=context_ref_290_4b1>[290]</a>, NIQE <a ref-type=bibr anchor=ref289 id=context_ref_289_4b1>[289]</a>, BRISQUE <a ref-type=bibr anchor=ref288 id=context_ref_288_4b1>[288]</a>, the contrast enhancement-based contrast-changed image quality measure (CEIQ) <a ref-type=bibr anchor=ref311 id=context_ref_311_4b1>[311]</a>, and the spatial–spectral entropy-based quality measure (SSEQ) <a ref-type=bibr anchor=ref312 id=context_ref_312_4b1>[312]</a>, were employed for NIQA-based evaluation. The experimental results obtained on ‘Flowers.bmp’, ‘Building.bmp’ and ‘Lawn.bmp’ are shown in <a ref-type=table anchor=table4 class=fulltext-link>Tables 4</a>–<a ref-type=table anchor=table5 class=fulltext-link></a><a ref-type=table anchor=table6 class=fulltext-link>6</a>, and the best score in terms of each metric is highlighted in bold. These data show that the different evaluation metrics assign different scores to the same image enhancement algorithm and that the interpretations of the evaluation results are completely opposite in some cases. The reason is that the traditional IQA metrics used in this evaluation consider different aspects of the image obtained after enhancement. For the three images, no method gets the best score on all metrics. The HE method achieves the highest scores in terms of the IE and CEIQ metrics on the three images. The LIME method achieves the best scores in terms of the AG metric for ‘Building.bmp’ and ‘Lawn.bmp’. The CVC method achieves the best score on STD metric for “Flower.bmp” and ‘Lawn.bmp’. Overall, the HE and CVC methods are the top two scoring methods based on these metrics. However, to some extent, the distortion of chrominance information causes the results of the objective evaluation to be opposite to those of the subjective evaluation.<div class="figure figure-full table" id=table4><div class=figcaption><b class=title>TABLE 4 </b>
Objective Evaluation of Various Methods on ‘Flowers.bmp’ Using NIQA Metrics</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t4-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t4-2992749-small.gif data-alt="Table 4- 
Objective Evaluation of Various Methods on ‘Flowers.bmp’ Using NIQA Metrics"><div class=zoom title="View Larger Image"></div></a></div></div><div class="figure figure-full table" id=table5><div class=figcaption><b class=title>TABLE 5 </b>
Objective Evaluation of Various Methods on ‘Building.bmp’ Using NIQA Metrics</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t5-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t5-2992749-small.gif data-alt="Table 5- 
Objective Evaluation of Various Methods on ‘Building.bmp’ Using NIQA Metrics"><div class=zoom title="View Larger Image"></div></a></div></div><div class="figure figure-full table" id=table6><div class=figcaption><b class=title>TABLE 6 </b>
Objective Evaluation of Various Methods on ‘Lawn.bmp’ Using NIQA Metrics</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t6-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t6-2992749-small.gif data-alt="Table 6- 
Objective Evaluation of Various Methods on ‘Lawn.bmp’ Using NIQA Metrics"><div class=zoom title="View Larger Image"></div></a></div></div><p></p></div><div class=section_2 id=sec4b2><h4>2) FIQA-Based Evaluation</h4><p>For the FIQA-based evaluation, eleven metrics were selected, namely, MSE, PSNR, SSIM <a ref-type=bibr anchor=ref301 id=context_ref_301_4b2>[301]</a>, LOE <a ref-type=bibr anchor=ref136 id=context_ref_136_4b2>[136]</a>, PCQI <a ref-type=bibr anchor=ref302 id=context_ref_302_4b2>[302]</a>, GMSD <a ref-type=bibr anchor=ref304 id=context_ref_304_4b2>[304]</a>, VIF <a ref-type=bibr anchor=ref305 id=context_ref_305_4b2>[305]</a>, VSI <a ref-type=bibr anchor=ref306 id=context_ref_306_4b2>[306]</a>, FSIM <a ref-type=bibr anchor=ref308 id=context_ref_308_4b2>[308]</a>, RVSIM <a ref-type=bibr anchor=ref313 id=context_ref_313_4b2>[313]</a>, and IFC <a ref-type=bibr anchor=ref314 id=context_ref_314_4b2>[314]</a>. The reference images for ‘Desk.bmp’ and ‘Road.bmp’ are shown in panel (a) of <a ref-type=fig anchor=fig33 class=fulltext-link>Fig. 33</a> and Fig. 34, and the experimental data are listed in <a ref-type=table anchor=table7 class=fulltext-link>Tables 7</a> – <a ref-type=table anchor=table8 class=fulltext-link>8</a>, where the best score in terms of each metric is highlighted in bold. From these data, it can be seen that the best scores in terms of the different metrics are relatively concentrated among certain image enhancement algorithms. For example, the CegaHE method earns the best scores according to seven of the above eleven metrics on ‘Desk.bmp’. For the image ‘Road.bmp’, BIMEF, CRM and MF achieve four, four and two of the best scores, respectively. In addition, these three methods have the same scores in terms of the GMSD and VSI metrics for the evaluation on ‘Road.bmp’. To some extent, FIQA-based evaluations provide a more accurate description of the images and are more consistent with subjective evaluation results than NIQA-based evaluations are for cases in which reference images are available.<div class="figure figure-full table" id=table7><div class=figcaption><b class=title>TABLE 7 </b>
Objective Evaluation of Various Methods on ‘Desk.bmp’ Using FIQA Metrics</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t7-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t7-2992749-small.gif data-alt="Table 7- 
Objective Evaluation of Various Methods on ‘Desk.bmp’ Using FIQA Metrics"><div class=zoom title="View Larger Image"></div></a></div></div><div class="figure figure-full table" id=table8><div class=figcaption><b class=title>TABLE 8 </b>
Objective Evaluation of Various Methods on ‘Road.bmp’ Using FIQA Metrics</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t8-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t8-2992749-small.gif data-alt="Table 8- 
Objective Evaluation of Various Methods on ‘Road.bmp’ Using FIQA Metrics"><div class=zoom title="View Larger Image"></div></a></div></div><p></p></div></div><div class=section_2 id=sec4c><h3>C. Time Complexity</h3><p>To test the processing speeds of the various methods, experiments were performed using images of various sizes, and all algorithms were run using MATLAB except the MELLEN method <a ref-type=bibr anchor=ref263 id=context_ref_263_4c>[263]</a>. <a ref-type=table anchor=table9 class=fulltext-link>Table 9</a> shows that the Retinex-based methods (MSR, MSRCR, and MSRCP) have high computational complexities because of their multiscale Gaussian filtering operations. The NPE, SRIE and MELLEN methods have the lowest computational efficiency for processing a single image because of the use of iterative computations to find the optimal solution. When processing an image with pixel dimensions of <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>3200\times 2400</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-198-Frame tabindex=0></span>
</tex-math></inline-formula>, their processing times are 206 seconds, 649 seconds and 890 seconds, respectively. In contrast, gamma correction and the various HE-based methods (AHE, HMF, and CegaHE) are faster, and their run times are only slightly affected by an increase in the image size. In particular, when the gamma correction method is run on an image of <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>3200\times 2400</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-199-Frame tabindex=0></span>
</tex-math></inline-formula> pixels, it needs only 70 milliseconds to run, which is 1/12700th of the run time of the SRIE method. Therefore, the gamma correction method has an absolute advantage in terms of run time. For images with pixel dimensions of <inline-formula><tex-math notation=LaTeX><span class=MathJax_Preview>1600\times 1200</span><span class="MathJax MathJax_Processing sf-hidden" id=MathJax-Element-200-Frame tabindex=0></span>
</tex-math></inline-formula>, the gamma method methods and the HE-based methods can be used under real-time conditions.<div class="figure figure-full table" id=table9><div class=figcaption><b class=title>TABLE 9 </b>
Comparison of Time Complexity (Unit: Seconds)</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t9-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t9-2992749-small.gif data-alt="Table 9- 
Comparison of Time Complexity (Unit: Seconds)"><div class=zoom title="View Larger Image"></div></a></div></div><p><p>The IQA metrics considered above are not completely consistent with subjective human perception and thus are not suitable for the direct evaluation of enhanced low-light images. They need to be combined with subjective evaluations based on human vision. Therefore, there is a great need to design and develop an objective quality assessment method for low-light image enhancement that shows good agreement with the mechanism of human vision.</p></div></div>
<div class=section id=sec5><div class="header article-hdr"><div class=kicker>
 SECTION V.</div><h2>Conclusion</h2></div><p>This paper summarizes seven widely used classes of low-light image enhancement algorithms and their improved versions and describes the underlying principles of the different methods. Then, it introduces the current quality evaluation system for low-light images and identifies the problems with this existing system. Finally, many representative image enhancement methods are evaluated using both subjective and objective evaluation methods. The characteristics and performance of the existing methods are analyzed and summarized, and the shortcomings of the present work in this field are further revealed. The essential purpose of low-light image enhancement is to improve the image contrast both globally and locally in a certain range of the gray space in accordance with the distribution of the gray values of the original image pixels. Simultaneously, it should be ensured that the enhanced image shows good image quality with regard to the characteristics of human visual perception, noise suppression, image entropy maximization, brightness maintenance, etc. The merits and shortcomings of the various methods are summarized in <a ref-type=table anchor=table10 class=fulltext-link>Table 10</a>.<div class="figure figure-full table" id=table10><div class=figcaption><b class=title>TABLE 10 </b>
Merits and Shortcomings of Different Methods</div><div class=img-wrap><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t10-2992749-large.gif><img class="document-ft-image load-shimmer" src=data:, alt data-lazy=/mediastore_new/IEEE/content/media/6287639/8948470/9088214/wang.t10-2992749-small.gif data-alt="Table 10- 
Merits and Shortcomings of Different Methods"><div class=zoom title="View Larger Image"></div></a></div></div><p><p>Based on the limitations of the current methods, care must be taken in image enhancement to ensure an appropriate balance among several factors, such as the image color, visual effect and information entropy, while attempting to improve the visibility of the image contrast. However, the existing algorithms all have certain disadvantages, such as loss of detail, color distortion, or high computational complexity; thus, current low-light image enhancement techniques cannot guarantee the performance of a vision system in a low-light environment. In future research on low-illumination image enhancement, researchers should focus on the following tasks:
<ol><li><p>Improve the robustness and adaptive capabilities of low-light image enhancement algorithms. The robustness and adaptive capabilities of the existing methods are insufficient to meet the requirements of practical applications. The ideal method should be able to adaptively adjust to different application conditions and different types of low-light images.</p><li><p>Reduce the computational complexity of the available algorithms. To satisfy the needs of practical applications, real-time methods are often in demand; however, most of the existing methods currently require a long processing time. In addition, the results of the existing methods are still susceptible to certain problems, such as color deviations and detail ambiguity. The high-performance processors in graphics processing units (GPUs) allow such algorithms to be parallelized, which can significantly improve their processing speed and may ultimately enable real-time image enhancement.</p><li><p>Establish a standard quality evaluation system. At present, there are too few specialized low-light image datasets, and the quality evaluation system is not mature. This limits the further development of this research field and the selection of suitable enhancement and restoration methods for practical applications.</p><li><p>Develop a video-based enhancement algorithm. Currently, most of the research in this field has focused on single images, and research on video enhancement has not received sufficient attention; by contrast, video processing plays a greater role in practical applications. There is an urgent need to solve the problems related to the efficiency of low-illumination video processing, interframe consistency and so on.</p></ol><p><p>In summary, thus far, no image enhancement algorithm exists that is optimal in terms of all of the above issues simultaneously. Therefore, it is necessary to select the most suitable image enhancement algorithm based on application-specific requirements. It is hoped that image enhancement technology can be advanced to a higher level through in-depth studies of these enhancement algorithms, thus allowing this technology to play an important role in multiple disciplines.</p></div>
<h3>ACKNOWLEDGMENT</h3><p>The authors thank AJE for linguistic assistance during the preparation of this manuscript.</p>
<div class=section id=app1><h2></h2><h1> Appendix</h1><p>
</p>AbbreviationExpansion<table><tbody><tr><td>Abbreviation<td><p>Phrase</p><tr><td>HE<td><p>Histogram equalization</p><tr><td>CDF<td><p>Cumulative distribution function</p><tr><td>GHE<td><p>Global histogram equalization</p><tr><td>LHE<td><p>Local histogram equalization</p><tr><td>BBHE<td><p>Brightness-preserving bi-histogram equalization</p><tr><td>DSIHE<td><p>Dualistic subimage histogram equalization</p><tr><td>MMBEBHE<td><p>Minimum mean brightness error bi-histogram equalization</p><tr><td>IBBHE<td><p>Iterative of brightness bi-histogram equalization</p><tr><td>AHE<td><p>Adaptive histogram equalization</p><tr><td>POSHE<td><p>Partially overlapped subblock histogram equalization</p><tr><td>CLAHE<td><p>Contrast-limited adaptive histogram equalization</p><tr><td>FIRF<td><p>Finite impulse response filter</p><tr><td>RMSHE<td><p>Recursive mean-separate histogram equalization</p><tr><td>CVC<td><p>Contextual and variational contrast</p><tr><td>BBPHE<td><p>Background brightness-preserving histogram equalization</p><tr><td>GCCHE<td><p>Gain-controllable clipped histogram equalization</p><tr><td>RSIHE<td><p>Recursive subimage histogram equalization</p><tr><td>DHE<td><p>Dynamic histogram equalization</p><tr><td>BPDHE<td><p>Brightness-preserving dynamic histogram equalization</p><tr><td>EDSHE<td><p>Entropy- based dynamic subhistogram equalization</p><tr><td>BHEPL<td><p>Bi-histogram equalization with a plateau limit</p><tr><td>MMSICHE<td><p>Median-mean based subimage-clipped histogram equalization</p><tr><td>ESIHE<td><p>Exposure-based subimage histogram equalization</p><tr><td>AMHE<td><p>Adaptively modified histogram equalization</p><tr><td>WHE<td><p>Weighted histogram equalization</p><tr><td>HMF<td><p>Histogram modification framework</p><tr><td>CegaHE<td><p>Gap Adjustment for Histogram Equalization</p><tr><td>SSR<td><p>Single-scale Retinex</p><tr><td>MSR<td><p>Multiscale Retinex</p><tr><td>MSRCR<td><p>Multiscale Retinex with color restoration</p><tr><td>KBR<td><p>Kernel-based Retinex</p><tr><td>SRIE<td><p>Simultaneous reflectivity and illumination estimation</p><tr><td>MSRCP<td><p>Multiscale Retinex with chromaticity preservation</p><tr><td>NPE<td><p>Naturalness preserved enhancement</p><tr><td>WT<td><p>Wavelet transform</p><tr><td>DCT<td><p>Discrete cosine transform</p><tr><td>KGWT<td><p>Knee function and Gamma correction</p><tr><td>HDR<td><p>High dynamic range</p><tr><td>CNN<td><p>Convolutional neural network</p><tr><td>DCP<td><p>Dark channel priori</p><tr><td>CEM<td><p>Color estimation model</p><tr><td>LLCNN<td><p>Low-light CNN</p><tr><td>MEF<td><p>Multiple exposure image fusion</p><tr><td>IQA<td><p>Image quality assessment</p><tr><td>HVS<td><p>Human visual system</p><tr><td>AG<td><p>Average gradient</p><tr><td>MSE<td><p>Mean square error</p><tr><td>PSNR<td><p>Peak signal-to-noise</p><tr><td>SSIM<td><p>Structural similarity index</p><tr><td>BIQI<td><p>Blind image quality index</p><tr><td>BRISQUE<td><p>Blind/referenceless image spatial quality evaluation</p><tr><td>NIQE<td><p>Naturalness image quality evaluator</p></table><p></p></div>
</div></div></response>
</div><xpl-reference-pop-up _ngcontent-dps-c167 parentid=full-text-section _nghost-dps-c161></xpl-reference-pop-up><span _ngcontent-dps-c167 id=full-text-footer></span></div></div><div _ngcontent-dps-c167 class="col-3-24 u-pr-1 u-pl-1 col-buttons stats-document-container-rh u-printing-invisible-ie u-printing-invisible-ff"><xpl-document-buttons _ngcontent-dps-c167 _nghost-dps-c166><div _ngcontent-dps-c166 xplscrollsnapmigr cssclasstostick=document-side-menu-stick fromelementid=toc-wrapper tillelementid=full-text-footer offsetfrom=150 offsetto=-800 scrollreset=true id=scroll-snap-buttons-container class="document-doc-buttons stats-document-container-buttons"><ul _ngcontent-dps-c166 class=tools><li _ngcontent-dps-c166 id=search-popover xplpopoveranimateonscroll animateelementidposition=1 animatecontainerelementid=search-popover class="blue-tooltip increased-width special-left-tooltip"><a _ngcontent-dps-c166 triggers=click placement=left><i _ngcontent-dps-c166 class="fas fa-search icon-size-md color-gray-dark"></i></a><li _ngcontent-dps-c166 id=resizer-popover xplpopoveranimateonscroll animateelementidposition=1 animatecontainerelementid=resizer-popover class="blue-tooltip special-left-tooltip"><a _ngcontent-dps-c166 min-size=10 max-size=20 placement=left triggers=click><i _ngcontent-dps-c166 class="far fa-text-size icon-size-md color-gray-dark"></i></a></ul><xpl-back-to-top-button _ngcontent-dps-c166 section=full-text-header _nghost-dps-c67><ul _ngcontent-dps-c67 class="back-to-top sf-hidden"></ul></xpl-back-to-top-button></div></xpl-document-buttons></div></div></section></xpl-document-full-text><xpl-accordian-section _ngcontent-dps-c196 _nghost-dps-c192><div _ngcontent-dps-c192 role=tablist class="document-accordion-section-container hide-mobile"><xpl-document-accordion _ngcontent-dps-c192 class=accordion-panel-container _nghost-dps-c172><div _ngcontent-dps-c172><div _ngcontent-dps-c172 role=tab class="accordion-header accordion-button" id=authors-header aria-expanded=false aria-disabled=false><a _ngcontent-dps-c192 id=authors class="accordion-link text-base-md-lh">Authors</a><div _ngcontent-dps-c172 class=accordion-chevron><i _ngcontent-dps-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-dps-c172><div _ngcontent-dps-c172 role=tab class="accordion-header accordion-button" id=figures-header aria-expanded=false aria-disabled=false><a _ngcontent-dps-c192 id=figures class="accordion-link text-base-md-lh">Figures</a><div _ngcontent-dps-c172 class=accordion-chevron><i _ngcontent-dps-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-dps-c172><div _ngcontent-dps-c172 role=tab class="accordion-header accordion-button" id=references-header aria-expanded=false aria-disabled=false><a _ngcontent-dps-c192 id=references class="accordion-link text-base-md-lh">References</a><div _ngcontent-dps-c172 class=accordion-chevron><i _ngcontent-dps-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-dps-c172><div _ngcontent-dps-c172 role=tab class="accordion-header accordion-button" id=citations-header aria-expanded=false aria-disabled=false><a _ngcontent-dps-c192 id=citations class="accordion-link text-base-md-lh">Citations</a><div _ngcontent-dps-c172 class=accordion-chevron><i _ngcontent-dps-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-dps-c172><div _ngcontent-dps-c172 role=tab class="accordion-header accordion-button" id=keywords-header aria-expanded=false aria-disabled=false><a _ngcontent-dps-c192 id=keywords class="accordion-link text-base-md-lh">Keywords</a><div _ngcontent-dps-c172 class=accordion-chevron><i _ngcontent-dps-c172 class="fa fa-angle-down"></i></div></div></div><div _ngcontent-dps-c172><div _ngcontent-dps-c172 role=tab class="accordion-header accordion-button" id=metrics-header aria-expanded=false aria-disabled=false><a _ngcontent-dps-c192 id=metrics class="accordion-link text-base-md-lh">Metrics</a><div _ngcontent-dps-c172 class=accordion-chevron><i _ngcontent-dps-c172 class="fa fa-angle-down"></i></div></div></div></xpl-document-accordion></div></xpl-accordian-section></div></section></div><div _ngcontent-dps-c196 class="document-disqus-container col-24-24"><div _ngcontent-dps-c196 class=row><div _ngcontent-dps-c196 class="col-12 disqus-container"><xpl-disqus-migr _ngcontent-dps-c196 page=document _nghost-dps-c193><div id=disqus_recommendations style=margin-bottom:12px></div><div _ngcontent-dps-c193 id=disqus_thread></div><a _ngcontent-dps-c193 href=https://disqus.com/ class=dsq-brlink></a></xpl-disqus-migr></div></div></div></div></div><div _ngcontent-dps-c196 class="document-sidebar global-right-rail top-spacing"><div _ngcontent-dps-c196 class=header-rel-art-toggle-mobile><i _ngcontent-dps-c196 class=header-rel-art-toggle-icon></i></div><div _ngcontent-dps-c196 class=document-sidebar-content><div _ngcontent-dps-c196 class=hide-mobile><xpl-leaderboard-ad _ngcontent-dps-c196 _nghost-dps-c127><div _ngcontent-dps-c127 class="Ads-leaderboard ad-panel"><div _ngcontent-dps-c127 class="row u-flex-wrap-nowrap"></div><div _ngcontent-dps-c127 class=ad-leaderboard-ad-container><div _ngcontent-dps-c127 xplgoogleadmigr><div id=div-gpt-ad-1606861783116-0 style="width:300px;height:250px;display:none;margin:0px auto"></div></div></div></div></xpl-leaderboard-ad></div><div _ngcontent-dps-c196 class=document-sidebar-rel-art><xpl-related-article-list _ngcontent-dps-c196 _nghost-dps-c194><div _ngcontent-dps-c194 class=stats-document-header-relatedArticles><div _ngcontent-dps-c194 class=header-rel-art><div _ngcontent-dps-c194 class="header-rel-art-title text-base-md-lh"> More Like This </div><div _ngcontent-dps-c194 class=header-rel-art-list><div _ngcontent-dps-c194 class=header-rel-art-item><div _ngcontent-dps-c194 class="row text-base-md-lh"><a _ngcontent-dps-c194 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/9684913/><span _ngcontent-dps-c194>Attention-Guided Global-Local Adversarial Learning for Detail-Preserving Multi-Exposure Image Fusion</span></a></div><p _ngcontent-dps-c194 class="header-rel-art-pub text-sm-md-lh">IEEE Transactions on Circuits and Systems for Video Technology<p _ngcontent-dps-c194 class="header-rel-art-pub text-sm-md-lh">Published: 2022</p></div><div _ngcontent-dps-c194 class=header-rel-art-item><div _ngcontent-dps-c194 class="row text-base-md-lh"><a _ngcontent-dps-c194 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/document/7216870/><span _ngcontent-dps-c194>A back lighting color image enhancement method using color saturation and image fusion</span></a></div><p _ngcontent-dps-c194 class="header-rel-art-pub text-sm-md-lh">2015 IEEE International Conference on Consumer Electronics - Taiwan<p _ngcontent-dps-c194 class="header-rel-art-pub text-sm-md-lh">Published: 2015</p></div></div><div _ngcontent-dps-c194 class="header-rel-art-action text-base-md-lh"><a _ngcontent-dps-c194>Show More</a></div></div></div></xpl-related-article-list></div><div _ngcontent-dps-c196 class=hide-mobile><xpl-leaderboard-middle-ad _ngcontent-dps-c196 _nghost-dps-c155><div _ngcontent-dps-c155 class="Ads-leaderboard ad-panel"><div _ngcontent-dps-c155 class="row u-flex-wrap-nowrap"></div><div _ngcontent-dps-c155 class=ad-leaderboard-ad-container><div _ngcontent-dps-c155 xplgoogleadmigr><div id=div-gpt-ad-1606861708157-0 style="width:300px;height:600px;display:none;margin:0px auto"></div></div></div></div></xpl-leaderboard-middle-ad></div></div></div><xpl-reference-panel _ngcontent-dps-c196 _nghost-dps-c195><section _ngcontent-dps-c195 id=references-anchor class="document-all-references hide-mobile panel-closed"><div _ngcontent-dps-c195 class=header><h1 _ngcontent-dps-c195>References</h1><a _ngcontent-dps-c195><i _ngcontent-dps-c195 class="fas fa-times"></i></a></div><div _ngcontent-dps-c195 id=references-section-container class=document-ft-section-container><div _ngcontent-dps-c195><b _ngcontent-dps-c195>References is not available for this document.</b></div></div></section></xpl-reference-panel></div></xpl-document-details></div><xpl-footer _ngcontent-dps-c455 _nghost-dps-c454><footer _ngcontent-dps-c454 id=xplore-footer class="stats-footer footer-new"><div _ngcontent-dps-c454 class=footer-wrapper><div _ngcontent-dps-c454 class=flexible-row-col><div _ngcontent-dps-c454 class=footer-col><h3 _ngcontent-dps-c454 class=text-base-md-lh>IEEE Personal Account</h3><ul _ngcontent-dps-c454 class=text-sm-md-lh><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_blank href="https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Change username/password</a></ul></div><div _ngcontent-dps-c454 class=footer-col><h3 _ngcontent-dps-c454 class=text-base-md-lh>Purchase Details</h3><ul _ngcontent-dps-c454 class=text-sm-md-lh><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_blank href="https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_blank href=https://ieeexplore-ieee-org.thi.idm.oclc.org/articleSale/purchaseHistory.jsp>View Purchased Documents</a></ul></div><div _ngcontent-dps-c454 class=footer-col><h3 _ngcontent-dps-c454 class=text-base-md-lh>Profile Information</h3><ul _ngcontent-dps-c454 class=text-sm-md-lh><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_blank href="https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_blank href="https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_blank href="https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Technical interests</a></ul></div><div _ngcontent-dps-c454 class="footer-col need-help"><h3 _ngcontent-dps-c454 class=text-base-md-lh>Need Help?</h3><ul _ngcontent-dps-c454 class=text-sm-md-lh><li _ngcontent-dps-c454><a _ngcontent-dps-c454 href=tel:+1-800-678-4333> US &amp; Canada: +1 800 678 4333 </a><li _ngcontent-dps-c454><a _ngcontent-dps-c454 href=tel:+1-732-981-0060> Worldwide: +1 732 981 0060 </a><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact> Contact &amp; Support </a></ul></div><div _ngcontent-dps-c454 class="footer-col follow"><h3 _ngcontent-dps-c454 class=text-base-md-lh>Follow</h3><ul _ngcontent-dps-c454 class=icon-size-md><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_blank href=https://www.facebook.com/IEEEXploreDigitalLibrary/ aria-label="Follow us on Facebook" title="Follow IEEE Xplore on Facebook, opens in a new tab"><i _ngcontent-dps-c454 aria-hidden=true class="fab fa-facebook-f"></i></a><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_blank href=https://www.linkedin.com/showcase/ieee-xplore aria-label="Follow us on LinkedIn" title="Follow IEEE Xplore on LinkedIn, opens in a new tab"><i _ngcontent-dps-c454 aria-hidden=true class="fab fa-linkedin-in"></i></a><li _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_blank href="https://twitter.com/IEEEXplore?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor" aria-label="Follow us on Twitter" title="Follow IEEE Xplore on Twitter, opens in a new tab"><i _ngcontent-dps-c454 aria-hidden=true class="fab fa-twitter"></i></a></ul></div></div><div _ngcontent-dps-c454 class=footer-bottom-section><p _ngcontent-dps-c454 class=text-sm-md-lh><span _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/about-ieee-xplore.html>About IEEE <em _ngcontent-dps-c454>Xplore</em></a></span> | <span _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact>Contact Us</a></span> | <span _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_start.html>Help</a></span> | <span _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/accessibility-statement.html>Accessibility</a></span> | <span _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_Terms_of_Use.html>Terms of Use</a></span> | <span _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_self href=http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html>Nondiscrimination Policy</a></span> | <span _ngcontent-dps-c454 class=ethics-reporting-link><a _ngcontent-dps-c454 target=_blank href=http://www.ieee-ethics-reporting.org/>IEEE Ethics Reporting<i _ngcontent-dps-c454 class="fa fa-external-link-alt"></i></a></span> | <span _ngcontent-dps-c454><a _ngcontent-dps-c454 target=_self href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/overview-of-ieee-xplore/ieee-xplore-sitemap>Sitemap</a></span> | <span _ngcontent-dps-c454 class=nowrap><a _ngcontent-dps-c454 target=_self href=http://www.ieee.org/about/help/security_privacy.html>Privacy &amp; Opting Out of Cookies</a></span><p _ngcontent-dps-c454> A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. <p _ngcontent-dps-c454> © Copyright 2022 IEEE - All rights reserved. </p></div></div></footer></xpl-footer></xpl-root>
 </div>
 
 
 
</div>
 
<section id=xploreFooter class=hide-desktop>
 
 <div class="Footer stats-footer hide-mobile">
 <div class="pure-g Footer-sections">
 <div class=pure-u-1-4>
 <h3 class=Footer-header>IEEE Account</h3>
 <ul class=Footer-list>
 <li><a href="https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Change Username/Password</a></li>
 <li><a href="https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Update Address</a></li>
 </ul>
 </div>
 <div class=pure-u-1-4>
 <h3 class=Footer-header>Purchase Details</h3>
 <ul class=Footer-list>
 <li><a href="https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a></li>
 <li><a href="https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Order History</a></li>
 <li><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/articleSale/purchaseHistory.jsp>View Purchased Documents</a></li>
 </ul>
 </div>
 <div class=pure-u-1-4>
 <h3 class=Footer-header>Profile Information</h3>
 <ul class=Footer-list>
 <li><a href="https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a></li>
 <li><a href="https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a></li>
 <li><a href="https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=http://ieeexplore.ieee.org.thi.idm.oclc.org&amp;refSiteName=IEEE%20Xplore">Technical Interests</a></li>
 </ul>
 </div>
 <div class=pure-u-1-4>
 <h3 class=Footer-header>Need Help?</h3>
 <ul class=Footer-list>
 <li><strong>US &amp; Canada:</strong> +1 800 678 4333</li>
 <li><strong>Worldwide: </strong> +1 732 981 0060<br>
 </li>
 <li><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact>Contact &amp; Support</a></li>
 </ul>
 </div>
 </div>
 <div class=row>
 <div class="col-12 Footer-bottom">
 <div class="Footer-bottom-inner-div row">
 <div class=col>
 <ul class="Menu Menu--horizontal Menu--dividers u-mb-1">
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/about-ieee-xplore.html>About IEEE <em>Xplore</em></a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/contact>Contact Us</a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_start.html target=blank>Help</a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/accessibility-statement.html target=blank>Accessibility</a></li> 
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/Xplorehelp/Help_Terms_of_Use.html target=_blank>Terms of Use</a></li>
 <li class=Menu-item><a href=http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html>Nondiscrimination Policy</a></li>
 <li class=Menu-item><a href=https://ieeexplore-ieee-org.thi.idm.oclc.org/xpl/sitemap.jsp>Sitemap</a></li>
 <li class=Menu-item><a href=http://www.ieee.org/about/help/security_privacy.html target=blank>Privacy &amp; Opting Out of Cookies</a></li>
 </ul>
 <p class=Footer-bottom-terms>
 A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.<br>© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
 </p>
 </div>
 <div><i class=logo-ieee-white></i></div>
 </div>
 
 </div>
 </div>
 </div>
 
 
</section>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<style>--></style>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div style=width:1263px id=popup_overlay></div>
<g:compress>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
</g:compress>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 </div>
 </div>
 </div>
 
 
 
 </div>
 
<div id=cboxOverlay style=display:none></div><div id=colorbox role=dialog tabindex=-1 style=display:none></div><div style="position:absolute;width:0px;height:0px;overflow:hidden;padding:0px;border:0px none;margin:0px"><div id=MathJax_Font_Test style="position:absolute;visibility:hidden;top:0px;left:0px;width:auto;min-width:0px;max-width:none;padding:0px;border:0px none;margin:0px;white-space:nowrap;text-align:left;text-indent:0px;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal;font-size:40px;font-weight:normal;font-style:normal;font-size-adjust:none;font-family:MathJax_Size4,sans-serif" class=sf-hidden></div></div>