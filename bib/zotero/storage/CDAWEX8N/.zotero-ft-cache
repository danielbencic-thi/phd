Maths for Intelligent Systems
Marc Toussaint April, 2022

This script is primarily based on a lecture I gave 2015-2019 in Stuttgart. The current form also integrates notes and exercises from the Optimization lecture, and a little material from my Robotics and ML lectures. The ﬁrst full version was from 2019, since then I occasionally update it.

Contents

1 Speaking Maths

4

1.1 Describing systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.2 Should maths be tought with many application examples? Or abstractly? . . . 5

1.3 Notation: Some seeming trivialities . . . . . . . . . . . . . . . . . . . . . . . . 5

2 Functions & Derivatives

7

2.1 Basics on uni-variate functions . . . . . . . . . . . . . . . . . . . . . . . . . . 7

2.1.1 Continuous, diﬀerentiable, & smooth functions; 2.1.2 Polynomials, piecewise, basis functions, splines

2.2 Partial vs. total derivative, and chain rules . . . . . . . . . . . . . . . . . . . . 9

2.2.1 Partial Derivative; 2.2.2 Total derivative, computation graphs, forward and backward chain rules

2.3 Gradient, Jacobian, Hessian, Taylor Expansion . . . . . . . . . . . . . . . . . . 11

2.3.1 Gradient & Jacobian; 2.3.2 Hessian; 2.3.3 Taylor expansion

2.4 Derivatives with matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

2.4.1 Derivative Rules; 2.4.2 Example: GP regression; 2.4.3 Example: Logistic regression

2.5 Check your gradients numerically! . . . . . . . . . . . . . . . . . . . . . . . . 18

2.6 Examples and Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

2

Maths for Intelligent Systems, Marc Toussaint

2.6.1 More derivatives; 2.6.2 Multivariate Calculus; 2.6.3 Finite Diﬀerence Gradient Checking; 2.6.4 Backprop in a Neural Net; 2.6.5 Backprop in a Neural Net; 2.6.6 Logistic Regression Gradient & Hessian

3 Linear Algebra

21

3.1 Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

3.1.1 Why should we care for vector spaces in intelligent systems research?; 3.1.2 What is a vector?; 3.1.3 What is a vector space?

3.2 Vectors, dual vectors, coordinates, matrices, tensors . . . . . . . . . . . . . . . 22

3.2.1 A taxonomy of linear functions; 3.2.2 Bases and coordinates; 3.2.3 The dual vector space – and its coordinates; 3.2.4 Coordinates for every linear thing: tensors; 3.2.5 Finally: Matrices; 3.2.6 Coordinate transformations

3.3 Scalar product and orthonormal basis . . . . . . . . . . . . . . . . . . . . . . . 28

3.3.1 Properties of orthonormal bases

3.4 The Structure of Transforms & Singular Value Decomposition . . . . . . . . . 30

3.4.1 The Singular Value Decomposition Theorem

3.5 Point of departure from the coordinate-free notation . . . . . . . . . . . . . . 33

3.6 Filling SVD with life . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.6.1 Understand vv as a projection; 3.6.2 SVD for symmetric matrices; 3.6.3 SVD for general matrices

3.7 Eigendecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

3.7.1 Power Method; 3.7.2 Power Method including the smallest eigenvalue ; 3.7.3 Why should I care about Eigenvalues and Eigenvectors?

3.8 Beyond this script: Numerics to compute these things . . . . . . . . . . . . . . 38

3.9 Derivatives as 1-forms, steepest descent, and the covariant gradient . . . . . . 38

3.9.1 The coordinate-free view: A derivative takes a change-of-input vector as input, and returns a change of output; 3.9.2 Contra- and co-variance; 3.9.3 Steepest descent and the covariant gradient vector

3.10 Examples and Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

3.10.1 Basis; 3.10.2 From the Robotics Course; 3.10.3 Bases for Polynomials; 3.10.4 Projections; 3.10.5 SVD; 3.10.6 Bonus: Scalar product and Orthogonality; 3.10.7 Eigenvectors; 3.10.8 Covariance and PCA; 3.10.9 Bonus: RKHS

4 Optimization

47

4.1 Downhill algorithms for unconstrained optimization . . . . . . . . . . . . . . . 47

4.1.1 Why you shouldn’t trust the magnitude of the gradient; 4.1.2 Ensuring monotone and suﬃcient decrease: Backtracking line search, Wolfe conditions,

Maths for Intelligent Systems, Marc Toussaint

3

& convergence; 4.1.3 The Newton direction; 4.1.4 Gauss-Newton: a super important special case; 4.1.5 Quasi-Newton & BFGS: approximating the hessian from gradient observations 4.1.6 Conjugate Gradient; 4.1.7 Rprop*
4.2 The general optimization problem – a mathematical program . . . . . . . . . . 57
4.3 The KKT conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.4 Unconstrained problems to tackle a constrained problem . . . . . . . . . . . . 59
4.4.1 Augmented Lagrangian*
4.5 The Lagrangian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
4.5.1 How the Lagrangian relates to the KKT conditions 4.5.2 Solving mathematical programs analytically, on paper.; 4.5.3 Solving the dual problem, instead of the primal.; 4.5.4 Finding the “saddle point” directly with a primaldual Newton method.; 4.5.5 Log Barriers and the Lagrangian
4.6 Convex Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
4.6.1 Convex sets, functions, problems; 4.6.2 Linear and quadratic programs ; 4.6.3 The Simplex Algorithm; 4.6.4 Sequential Quadratic Programming
4.7 Blackbox & Global Optimization: It’s all about learning . . . . . . . . . . . . . 70
4.7.1 A sequential decision problem formulation; 4.7.2 Acquisition Functions for Bayesian Global Optimization*; 4.7.3 Classical model-based blackbox optimization (non-global)*; 4.7.4 Evolutionary Algorithms*
4.8 Examples and Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
4.8.1 Convergence proof; 4.8.2 Backtracking Line Search; 4.8.3 Gauss-Newton ; 4.8.4 Robust unconstrained optimization; 4.8.5 Lagrangian Method of Multipliers; 4.8.6 Equality Constraint Penalties and Augmented Lagrangian; 4.8.7 Lagrangian and dual function; 4.8.8 Optimize a constrained problem; 4.8.9 Network ﬂow problem; 4.8.10 Minimum fuel optimal control; 4.8.11 Reformulating an 1norm; 4.8.12 Restarts of Local Optima; 4.8.13 GP-UCB Bayesian Optimization

5 Probabilities & Information

81

5.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

5.1.1 Axioms, deﬁnitions, Bayes rule; 5.1.2 Standard discrete distributions ; 5.1.3 Conjugate distributions; 5.1.4 Distributions over continuous domain; 5.1.5 Gaussian; 5.1.6 “Particle distribution”

5.2 Between probabilities and optimization: neg-log-probabilities, exp-neg-energies, exponential family, Gibbs and Boltzmann . . . . . . . . . . . . . . . . . . . . . 87

5.3 Information, Entropie & Kullback-Leibler . . . . . . . . . . . . . . . . . . . . . 90

5.4 The Laplace approximation: A 2nd-order Taylor of log p . . . . . . . . . . . . . 91

5.5 Variational Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92

5.6 The Fisher information metric: 2nd-order Taylor of the KLD . . . . . . . . . . 92

4

Maths for Intelligent Systems, Marc Toussaint

5.7 Examples and Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
5.7.1 Maximum Entropy and Maximum Likelihood; 5.7.2 Maximum likelihood and KL-divergence; 5.7.3 Laplace Approximation; 5.7.4 Learning = Compression; 5.7.5 A gzip experiment; 5.7.6 Maximum Entropy and ML

A Gaussian identities

96

B 3D geometry basics (for robotics)

100

B.1 Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101

B.2 Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

B.2.1 Static transformations; B.2.2 Dynamic transformations; B.2.3 A note on aﬃne coordinate frames

B.3 Kinematic chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107

B.3.1 Rigid and actuated transforms; B.3.2 Jacobian & Hessian

C Further

109

Index

111

1 Speaking Maths
1.1 Describing systems
Systems can be described in many ways. Biologists describe their systems often using text, and lots and lots of data. Architects describe buildings using drawings. Physisics describe nature using diﬀerential equations, or optimality principles, or diﬀerential geometry and group theory. The whole point of science is to ﬁnd descriptions of systems—in the natural science descriptions that allow prediction, in the engineering sciences descriptions that enable the design of good systems, problem-solving systems.
And how should we describe intelligent systems? Robots, perception systems, machine learning systems? I think there are two main categories: the imperative way in terms of literal algorithms (code), or the declarative way in terms of formulating the problem. I prefer the latter.
The point of this lecture is to teach you to speak maths, to use maths to describe systems or problems. I feel that most maths courses rather teach to consume maths, or solve mathematical problems, or prove things. Clearly, this is also important. But for the purpose of intelligent systems research, it is essential to be skilled in expressing problems mathematically, before even thinking about solving them and deriving algorithms.

Maths for Intelligent Systems, Marc Toussaint

5

If you happen to attend a Machine Learning or Robotics course you’ll see that every problem is addressed the same way: You have an “intuitively formulated” problem; the ﬁrst step is to ﬁnd a mathematical formulation; the second step to solve it. The second step is often technical. The ﬁrst step is really the interesting and creative part. This is where you have to nail down the problem, i.e., nail down what it means to be successful or performant – and thereby describe “intelligence”, or at least a tiny aspect of it.
The “Maths for Intelligent Systems” course will recap essentials of multi-variate functions, linear algebra, optimization, and probabilities. These ﬁelds are essential to formulate problems in intelligent systems research and hopefully will equip you with the basics of speaking maths.

1.2 Should maths be tought with many application examples? Or abstractly?
Maybe this is the wrong question and implies a view on maths I don’t agree with. I think (but this is arguable) maths is nothing but abstractions of real-world things. At least I aim to teach maths as abstractions of real-world things. It is misleading to think that there is “pure maths” and then “applications”. Instead mathematical concepts, such as a vector, are abstractions of real-world things, such as faces, scenes, images, documents; and theorems, methods and algorithms that apply on vectors of course also apply to all the real-world things—subject to the limitations of this abstraction. So, the goal is not to teach you a lookup table of which method can be used in which application, but rather to teach which concepts maths oﬀers to abstract real-world things—so that you ﬁnd such abstractions yourself once you’ll have to solve a real-world problem.
But yes, I believe that maths – in our context – should ideally be taught with many exercises relating to AI problems. Perhaps the ideal would be:
• Teach Maths using AI exercises (where AI problems are formulated and treated analytically).
• Teach AI using coding exercises.
• Teach coding using maths-implementation exercises.
But I haven’t yet adopted this myself in my teaching.

1.3 Notation: Some seeming trivialities
Equations and mathematical expressions have a syntax. This is hardly ever made explicit1 and might seem trivial. But it is surprising how buggy mathematical statements can be in scientiﬁc papers (and oral exams). I don’t want to write much text about this, just some bullet points:
1Except perhaps by Go¨del’s incompleteness theorems and areas like automated theorem proving.

6

Maths for Intelligent Systems, Marc Toussaint

• Always declare mathematical objects.
• Be aware of variable and index scoping. For instance, if you have an equation, and one side includes a variable i, the other side doesn’t, this often is a notational bug. (Unless this equation actually makes a statement about independence on i.)
• Type checking. Within an equation, be sure to know exactly of what type each term is: vector? matrix? scalar? tensor? Is the type and dimension of both sides of the equation consistent?

• Decorations are ok, but really not necessary. It is much more important to declare all things. E.g., there are all kinds of decorations used for vectors, v, v, →−v , |v and matrices. But these are not necessary. Properly declaring all symbols is much more important.
• When declaring sets of indexed elements, I use the notation {xi}ni=1. Similarly for tuples: (xi)ni=1, (x1, .., xn), x1:n.
• When deﬁning sets, we write something like {f (x) : x ∈ R}, or {n ∈ N : ∃ {vi}ni=1 linearly independent, vi ∈ V }
• I usually use bracket [a = b] ∈ {0, 1} for the boolean indicator function of some expression. An alternative notation is I(a = b), or the Kronecker symbol δab.
• A tuple (a, b) ∈ A × B is an element of the product space.

• direct sum A ⊕ B is same as A × B in ﬁnite-dimensional spaces

• If f : X → Y , then minx f (x) ∈ Y is minimal function value (output); whereas argminx f (x) ∈ X is the input (“argument”) that minimizes the function. E.g., minx f (x) = f (argminx f (x)).
• One should distinguish between the inﬁmum infx f (x) and supremum supx f (x) from the min/max: the inf/sup refer to limits, while the min/max to values actually aquired by the function. I must admit I am sloppy in this regard and usually only write min/max.

• Never use multiple letters for one thing. E.g. length = 3 means l times e times n times g times t times h equals 3.

• There is a diﬀerence between → and →:

f : R → R, x → cos(x)

(1)

• The dot is used to help deﬁning functions with only some arguments ﬁxed:

f : A × B → C , f (a, ·) : B → C, b → f (a, b)

(2)

Another example is the typical declaration of an inner product: ·, · : V × V → R.

Maths for Intelligent Systems, Marc Toussaint

7

• The p-norm or Lp-norm is deﬁned as ||x||p = [ i xpi ]1/p. By default p = 2, that is ||x|| = ||x||2 = |x|, which is the L2-norm or Euclidean length of a vector. Also the L1-norm ||x||1 = i |xi| (aka Manhattan distance) plays an important role.

 a1

0



•

I

use

diag(a1, .., an)

=

  



...



  

for

a diagonal matrix.

And overload this







0

an 

so that diag(A) = (A11, .., Ann) is the diagonal vector of the matrix A ∈ Rn×n.

• A typical convention is

0n = (0, .., 0) ∈ Rn , 1n = (1, .., 1) ∈ Rn , In = diag(1n)

(3)

Also, ei = (0, .., 0, 1, 0, .., 0) ∈ Rn often denotes the ith column of the identity matrix, which of course are the coordinates of a basis vector ei ∈ V in a basis (ei)ni=1.
• The element-wise product of two matricies A and B is also called Hadamard product and notated A ◦ B (which has nothing to do with the concatenation of two operations). If there is need, perhaps also use this to notate the element-wise product of two vectors.

2 Functions & Derivatives
2.1 Basics on uni-variate functions
We super quickly recap the basics of functions R → R, which the reader might already know.

2.1.1 Continuous, diﬀerentiable, & smooth functions

• A function f : R → R is continuous at x if the limit limh→0 f (x + h) = f (x) exists and equals f (x) (from both sides).

•

A

function

f

:R→R

is

diferentiable

at

x

if f

(x) = limh→0

f (x+h)−f (x) h

exists.

Note, diﬀerentiable ⇒ continuous.

• A function is continuous/diﬀerentiable if it is continuous/diﬀerentiable at any x ∈ R.
• A function f is an element of Ck if it is k-fold continuously diﬀerentiable, i.e., if its k-th derivative f (k) is continuous. For example, C0 is the space of continuous functions, and C2 the space of twice continuously diﬀerentiable functions.

• A function f ∈ C∞ is called smooth (inﬁnitely often diﬀerentiable).

8

Maths for Intelligent Systems, Marc Toussaint

2.1.2 Polynomials, piece-wise, basis functions, splines

Let’s recap some basic functions that often appear in AI research.

• A polynomial of degree p is of the form f (x) =

p i=0

aixi,

which

is

a

weighed

sum of monimials 1, x, x2, ... Note that for multi-variate functions, the number

of monimials grows combinatorially with the degree and dimension. E.g., the

monimials of degree 2 in 3D space are x21, x1x2, x1x3, x22, x2x3, x23. In general,

we have

d p

monimials of degree p in d-dimensional space.

• Polynomials are smooth.

• I assume it is clear what piece-wise means (we have diﬀerent polynomials in disjoint intervals covering the input domain R).

• A set of basis functions {e1, ..., en} deﬁnes the space f = { i aiei : ai ∈ R} of functions that are linear combinations of the basis functions. More details on bases are discussed below for vector spaces in general. Here we note typical basis functions:
– Monimials, which form the basis of polynomials.
– Trigonometric functions sin(2πax), cos(2πax), with integers a, which form the Fourier basis of functions.
– Radial basis functions ϕ(|x − ci|), where we have n centers ci ∈ R, i = 1, .., n, and ϕ is typically some bell-shaped or local support function around zero. A very common one is the squared exponential ϕ(d) = exp{−ld2} (which you might call a non-normalized Gaussian with variance 1/l).

• The above functions are all examples of parameteric functions, which means that they can be speciﬁed by a ﬁnite number of parameters ai. E.g., when we have a ﬁnite set of basis functions, the functions can all be described by the ﬁnite set of weights in the linear combination.
However, in general a function f : R → R is an “inﬁnite-dimensional object”, i.e., it has inﬁnitely many degrees-of-freedom f (x), i.e., values at inﬁnitely many points x. In fact, sometimes it is useful to think of f as a “vector” of elements fx with continuous index x. Therefore, the space of all possible functions, and also the space of all continuous function C0 and smooth functions C∞, is inﬁnite-dimensional. General functions cannot be speciﬁed by a ﬁnite number of parameters, and they are called non-parameteric.
The core example are functions used for regression or classiﬁcation in Machine Learning, which are a linear combination of an inﬁnite set of basis functions. E.g., an inﬁnite set of radial basis functions ϕ(|x − c|) for all centers c ∈ R. This inﬁnite set of basis functions spans a function space called Hilbert space (in the ML context, “Reproducing Kernel Hilbert Space (RKHS)”), which is an inﬁnitedimensional vector space. Elements in that space are called non-parameteric.

Maths for Intelligent Systems, Marc Toussaint

9

• As a ﬁnal note, splines are parameteric functions that are often used in robotics and engineering in general. Splines usually are piece-wise polynomials that are continuously joined. Namely, a spline of degree p is in Cp−1, i.e., p − 1-fold continuously diﬀerentiable. A spline is not fully smooth, as the p-th derivative is discontinuous. E.g., a cubic spline has a piece-wise constant (“bang-bang”) jerk (3rd derivative). Cubic splines and B-splines (which are also piece-wise polynomials) are commonly used in robotics to describe motions. In computational design and graphics, B-splines are equally common to describe surfaces (in the form of Non-uniform rational basis spline, NURBS). Appendix ?? is a brief reference for splines.

2.2 Partial vs. total derivative, and chain rules

2.2.1 Partial Derivative

A multi-variate function f : Rn → R can be thought of as a function of n arguments, f (x1, .., xn).
Deﬁnition 2.1. The partial derivative of a function of multiple arguments f (x1, .., xn) is the standard derivative w.r.t. only one of its arguments,

∂ ∂xi f (x1, .., xn)

=

lim
h→0

f (x1, .., xi + h, .., xn) − f (x) h

.

(4)

2.2.2 Total derivative, computation graphs, forward and backward chain rules

Let me start with an example: We have three real-valued quantities x, g and f which depend on each other. Speciﬁcally,

f (x, g) = 3x + 2g and g(x) = 2x .

(5)

Question: What is the “derivative of f w.r.t. x”?

The correct answer is: Which one do you mean? The partial or total?

The partial derivative deﬁned above really thinks of f (x, g) as a function of two arguments, and does not at all care about whether there might be dependencies of these arguments. It only looks at f (x, g) alone and takes the partial derivative (=derivative w.r.t. one function argument):

∂

f (x, g) = 3

(6)

∂x

However, if you suddenly talk about h(x) = f (x, g(x)) as a function of the argument x only, that’s a totally diﬀerent story, and

∂

∂

h(x) = 3x + 2(2x) = 7

(7)

∂x

∂x

10

Maths for Intelligent Systems, Marc Toussaint

Bottom line, the deﬁnition of the partial derivative really depends on what you explicitly deﬁned as the arguments of the function.
To allow for a general treatment of diﬀerentiation with dependences we need to deﬁne a very useful concept:

Deﬁnition 2.2. A function network or computation graph is a directed acyclic graph (DAG) of n quantities xi where each quantity is a deterministic function of a set of parents π(i) ⊂ {1, .., n}, that is

xi = fi(xπ(i))

(8)

where xπ(i) = (xj)j∈π(i) is the tuple of parent values. This could also be called a deterministic Bayes net.

In a function network all values can be computed deterministically if the input values (which do have no parents) are given. Concerning diﬀerentiation, we may now ask: Assume we have a variation dx of some input value, how do all other values vary? The chain rules give the answer. It turns our there are two chain rules in function networks:

Identities 2.1 (Chain rule (Forward-Version)).

df

∂f dg

dx

=

(with ≡ 1, in case x ∈ π(f ))

(9)

dx

∂g dx

dx

g∈π(f )

Read this as follows: “The change of f with x is the sum of changes that come from its direct dependence on g ∈ π(f ), each multiplied the change of g with x.”

This

rule

deﬁnes

the

total

derivative

of

df dx

w.r.t.

x.

Note how diﬀerent these two

notions of derivatives are by deﬁnition: a partial derivative only looks at a function itself

and takes a limit of diﬀerences w.r.t. one argument—no notion of further dependencies.

The total derivative asks how, in a function network, one value changes with a change

of another.

The second version of the chain rule is:

Identities 2.2 (Chain rule (Backward-Version)).

df

df ∂g

=

dx

dg ∂x

g:x∈π(g)

df

(with ≡ 1, in case x ∈ π(f ))

(10)

df

Read this as follows: “The change of f with x is the sum of changes that arise from all changes of g which directly depend on x.”

Figure 1 illustrates the fwd and bwd versions of the chain rule. The bwd version allows

you

to

propagate

back,

given

gradients

df dg

from

top

to

g,

one

step

further

down,

from

top

to

x.

The

fwd

version

allows

you

to

propagate

forward,

given

gradients

dg dx

from

g

to

Maths for Intelligent Systems, Marc Toussaint

11

f

f

g g

x

x

Figure 1: General Chain Rule. Left: Forward-Version, Right: Backward-Version.

They

gray

arc

denotes

the

direct

dependence

∂f ∂x

,

which

appears

in

the

summations

via dx/dx ≡ 1, df /df ≡ 1.

bottom, one step further up, from f to bottom. Both versions are recursive equations.

If you would recursively plug in the deﬁnition for a given functional network, both of

them

would

yield

the

same

expression

of

df dx

in

terms

of

partial

derivatives

only.

Let’s compare to the chain rule as it is commonly found in other texts (written more precisely):

∂f (g(x)) ∂f (g)

∂g(x)

=

(11)

∂x

∂g g=g(x) ∂x

Note

that

we

here

very

explicitly

notated

that

∂f (g) ∂g

considers

f

to

be

a

function

of

the argument g, which is evaluted at g = g(x). Written like this, the rule is ﬁne. But

the above discussion and explicitly distinguishing between partial and total derivative is,

when things get complicated, less prone to confusion.

2.3 Gradient, Jacobian, Hessian, Taylor Expansion
2.3.1 Gradient & Jacobian
Let’s take the next step and consider functions f : Rn → Rd that map from n numbers to a d-dimensional output. In this case, we can take the partial derivative of each output w.r.t. each input argument, leading to a matrix of partial derivatives:
Deﬁnition 2.3. Given f : Rn → Rd, we deﬁne the derivative (also called Jacobian

12

Maths for Intelligent Systems, Marc Toussaint

matrix) as

 

∂ ∂x1

f1

(x)

∂ ∂x2

f1

(x)

...

∂

∂ xn

f1

(x)

 

∂ ∂x

f (x)

=

        

∂ ∂x1

f2(x) ...

∂ ∂x2

f2

(x)

...

∂

∂ xn

f2 ...

(x)

        





 

∂ ∂x1

fd

(x)

∂ ∂x2

fd

(x)

...

∂

∂ xn

fd

(x)

 

(12)

When the function only as one output dimension, f : Rn → R1, the partial derivative can be written as a vector. Unlike many other texts, I advocate for consistency with the Jacobian matrix (and contra-variance, see below) and deﬁne this to be a row vector:

Deﬁnition 2.4. We deﬁne the derivative of f : Rn → R as the row vector of partial derivatives

∂

∂

∂

f (x) = ( f, .., f ) .

(13)

∂x

∂x1

∂xn

Further, we deﬁne the gradient as the corresponding column “vector”

∂

∇f (x) = f (x) .

(14)

∂x

The “purpose” of a derivative is to output a change of function value when being multiplied to a change of input δ. That is, in ﬁrst order approximation, we have

f (x + δ) − f (x) =˙ ∂f (x) δ ,

(15)

where =˙ denotes “in ﬁrst order approximation”. This equation holds, no matter if the output space is Rd or R, or the input space and variation is δ ∈ Rn or δ ∈ R. In the gradient notation we have

f (x + δ) − f (x) =˙ ∇f (x) δ .

(16)

Jumping ahead to our later discussion of linear algebra: The above two equations are written in coordinates. But note that the equations are truly independent of the choice of vector space basis and independent of an optional metric or scalar product in V . The transpose should not be understood as a scalar product between two vectors, but rather as undoing the transpose in the deﬁnition of ∇f . All this is consistent to understanding the derivatives as coordinates of a 1-form, as we will introduce it later.
Given a certain direction d (with |d| = 1) we deﬁne the directional derivative as ∇f (x) d, and it holds

f (x + d) − f (x)

∇f (x) d = lim

.

(17)

→0

Maths for Intelligent Systems, Marc Toussaint

13

2.3.2 Hessian
Deﬁnition 2.5. We deﬁne the Hessian of a scalar function f : Rn → R as the symmetric matrix

∇2f (x)

=

∂ ∇f (x)
∂x

=

 ∂2

f 
  

∂ x1 ∂ x1 ∂2

f 


∂ x2 ∂ x1

    

...

f ∂2
∂ x1 ∂ x2
f ∂2
∂ x2 ∂ x2

... ...

∂2



f ∂ x1 ∂ xn
∂2

   

f ∂ x2 ∂ xn

 

...

    





f 


∂2

∂ xn ∂ x1

f ∂2
∂ xn ∂ x2

...

f ∂2

 

∂ xn ∂ xn

(18)

The Hessian can be thought of as the Jacobian of ∇f . Using the Hessian, we can express the 2nd order approximation of f as:

f (x + δ)

=¨

1 f (x) + ∂f (x) δ + δ

∇2f (x) δ .

(19)

2

For a uni-variate function f : R → R, the Hessian is just a single number, namely the second derivative f (x). In this section, let’s call this the “curvature” of the function (not to be confused with the Rimannian curvature of a manifold). In the uni-variate case, we have the obvious cases:

• If f (x) > 0, the function is locally “curved upwared” and convex (see also the formal Deﬁnition ??).
• If f (x) < 0, the function is locally “curved downward” and concave.

In the multi-variate case f : Rn → R, the Hessian matrix H is symmetric and we can
decompose it as H = i λihihi with eigenvalues λi and eigenvectors hi (which we will learn about in detail later). Importantly, all hi will be orthogonal to each other, forming a nice orthonormal basis.

This insight gives us a very strong intuition on how the Hessian H describes the local
curvature of the function f : λi gives the directional curvature, i.e., the curvature in the direction of eigenvector hi. If λi > 0, f is curved upward along hi; if λi < 0, f is curved downward along hi. Therefore, the eigenvalues λi tell us whether the function is locally curved upward, downward, or ﬂat in each of the orthogonal directions hi.

This

becomes

particular

intuitive

if

∂ ∂x

f

=

0

is

zero,

i.e.,

the

derivative

(slope)

of

the

function is zero in all directions. When the curvatures λi are positive in all directions,

the function is locally convex (upward parabolic) and x is a local minimum; if the

curvatures λi are all negative, the function is concave (downward parabolic) and x is a local maximum; if some curvatures are positive and some are negative along diﬀerent

directions hi, then the function curves down in some directions, and up in others, and

x is a saddle point.

Again jumping ahead, in the coordinate-free notation, the second derivative would be

14

Maths for Intelligent Systems, Marc Toussaint

deﬁned as the 2-form

d2f x : V × V → G,

(20)

(v, w) → lim df x+hw(v) − f x(v)

(21)

h→0

h

f (x + hw + lv) − f (x + hw) − f (x + lv) + f (x)

= lim

.

(22)

h,l→0

hl

The Hessian matrix are the coordinates of this 2-form (which would actually be a row vector of row vectors).

2.3.3 Taylor expansion

In 1D, we have

1 f (x + v) ≈ f (x) + f (x)v + f

(x)v2 + · · · +

1 f (k)(x)vk

(23)

2

k!

For f : Rn → R, we have

f (x +

v)

≈

f (x)

+ ∇f (x)

v

+

1 v

∇2f (x)v

+···

(24)

2

which is equivalent to

∂

1

∂2

f (x + v) ≈ f (x) + j ∂xj f (x)vj + 2 jk ∂xj∂xk f (x)vjvk + · · ·

(25)

2.4 Derivatives with matrices

The next section will introduce linear algebra from scratch – here we ﬁrst want to learn
how to practically deal with derivatives in matrix expressions. We think of matrices and vectors simply as arrays of numbers ∈ Rn×m and Rn. As a warmup, try to solve the following exercises:

(i) Let X, A be arbitrary matrices, A invertible. Solve for X:

XA + A = I

(26)

(ii) Let X, A, B be arbitrary matrices, (C − 2A ) invertible. Solve for X:

X C = [2A(X + B)]

(27)

(iii) Let x ∈ Rn, y ∈ Rd, A ∈ Rd×n. A obviously not invertible, but let A A be invertible. Solve for x:

(Ax − y) A = 0n

(28)

Maths for Intelligent Systems, Marc Toussaint

15

(iv) As above, additionally B ∈ Rn×n, B positive-deﬁnite. Solve for x:

(Ax − y) A + x B = 0n

(29)

(v) A core problem in Machine Learning: For β ∈ Rd, y ∈ Rn, X ∈ Rn×d, compute

argmin ||y − Xβ||2 + λ||β||2 .

(30)

β

(vi) A core problem in Robotics: For q, q0 ∈ Rn, φ : Rn → Rd, y∗ ∈ Rd non-linear but smooth, compute

argmin ||φ(q) − y∗||2C + ||q − q0||2W .

(31)

q

Use a local linearization of φ to solve this.

For problem (v), we want to ﬁnd a minimum for a matrix expression. We ﬁnd this by setting the derivative equal to zero. Here is the solution, and below details will become clear:

0 = ∂ ||y − Xβ||2 + λ||β||2

(32)

∂β

= 2(y − Xβ) (−X) + 2λβ

(33)

0 = −X (y − Xβ) + 2λβ

(34)

0 = −X y + (X X + 2λI)β

(35)

β = −(X X + 2λI)-1X y

(36)

Line 2 uses a standard rule for the derivative (see below) and gives a row vector equation. Line 3 transposes this to become a column vector equation.

2.4.1 Derivative Rules

As 2nd order terms are very common in AI methods, this is a very useful identity to learn:

Identities 2.3.

∂

∂

∂

f (x) Ag(x) = f (x) A g(x) + g(x) A f (x)

(37)

∂x

∂x

∂x

Note that using the ’gradient column’ convention this reads

∂

∂

∇x

f (x)

Ag(x)

=

[ g(x)] ∂x

A

f (x)

+ [ f (x)] ∂x

Ag(x)

(38)

16

Maths for Intelligent Systems, Marc Toussaint

which I ﬁnd impossible to remember, and mixes gradients-in-columns (∇) with gradientsin-rows (the Jacobian) notation.
Special cases and variants of this identity are:

∂

[whatever]x = [whatever] , if whatever is indep. of x

(39)

∂x

∂

a x=a

(40)

∂x

∂

Ax = A

(41)

∂x

∂

(Ax − b) (Cx − d) = (Ax − b) C + (Cx − d) A

(42)

∂x

∂ x Ax = x A + x A

(43)

∂x

∂ ||x|| =

∂

(x

1
x) 2

=

1 (x

x)−

1 2

2x

=

1 x

(44)

∂x

∂x

2

||x||

∂2

∂x2 (Ax + a) C(Bx + b) = A CB + B C A

(45)

Further useful identities are:

Identities 2.4 (Derivative Rules).

∂ |A| = |A| tr(A-1

∂ A)

(46)

∂θ

∂θ

∂ A-1 = −A-1 ( ∂ A) A-1

(47)

∂θ

∂θ

∂

∂

tr(A) = ∂θ

∂θ Aii

(48)

i

We can also directly take a derivative of a scalar value w.r.t. a matrix:

∂

a Xb = ab

(49)

∂X

∂ (a X CXb) = C Xab + CXba

(50)

∂X

∂

tr(X) = I

(51)

∂X

But if this leads to confusion I would recommend to never take a derivative w.r.t. a

matrix.

Instead, perhaps take the derivative w.r.t. a matrix element:

∂ ∂ Xij

a

Xb = aibj.

For completeness, here are the most important matrix identities (the appendix lists more):

Maths for Intelligent Systems, Marc Toussaint

17

Identities 2.5 (Matrix Identities).

(A-1 + B-1)-1 = A (A + B)-1 B = B (A + B)-1 A

(52)

(A-1 − B-1)-1 = A (B − A)-1 B

(53)

(A + U BV )-1 = A-1 − A-1U (B-1 + V A-1U )-1V A-1

(54)

(A-1 + B-1)-1 = A − A(B + A)-1A

(55)

(A + J BJ )-1J B = A-1J (B-1 + J A-1J )-1

(56)

(A + J BJ )-1A = I − (A + J BJ )-1J BJ

(57)

(54)=Woodbury; (56,57) holds for pos def A and B. See also the matrix cookbook.

2.4.2 Example: GP regression

An example from GP regression: The log-likelihood gradient w.r.t. a kernel hyperparameter:

log P (y|X, b)

=

1 −y

K-1y −

1

log |K| −

n

log 2π

(58)

2

2

2

where Kij = e−b(xi−xj)2 + σ2δij

(59)

∂ y K-1y = y (−K-1( ∂ K)K-1) = y (−K-1AK-1) ,

∂b

∂b

with Aij = −(xi − xj )2 e−b(xi−xj)2

(60)

∂

1

log |K| =

∂ |K| =

1

|K| tr(K-1 ∂ K) = tr(K-1A)

(61)

∂b

|K| ∂b

|K |

∂b

2.4.3 Example: Logistic regression

An example from logistic regression: We have the loss gradient and want the Hessian:

∇βL = X (p − x) + 2λIβ

(62)

ez where pi = σ(xi β) , σ(z) = 1 + ez , σ (z) = σ(z) (1 − σ(z)) (63)

∇β2L

=

∂ ∂β ∇βL

=

X

∂ p + 2λ
∂β

(64)

∂

∂β pi = pi(1 − pi) xi

(65)

∂ ∂β

p

=

diag([pi(1

−

pi)]ni=1)

X

=

diag(p

◦

(1

−

p))

X

(66)

∇β2L = X diag(p ◦ (1 − p)) X + 2λI

(67)

(Where ◦ is the element-wise product.)

18

Maths for Intelligent Systems, Marc Toussaint

2.5 Check your gradients numerically!
This is your typical work procedure when implementing a Machine Learning or AI’ish or Optimization kind of methods:
• You ﬁrst mathematically (on paper/LaTeX) formalize the problem domain, including the objective function.
• You derive analytically (on paper) the gradients/Hessian of your objective function.
• You implement the objective function and these analytic gradient equations in Matlab/Python/C++, using linear algebra packages.
• You test the implemented gradient equations by comparing them to a ﬁnite diﬀerence estimate of the gradients!
• Only if that works, you put everything together, interfacing the objective & gradient equations with some optimization algorithm

Algorithm 1 Finite Diﬀerence Jacobian Check

Input: x ∈ Rn, function f : Rn → Rn, function df : Rn → Rd×n 1: initialize Jˆ ∈ Rd×m, and = 10−6

2: for i = 1 : n do

3: Jˆ·i = [f (x + ei) − f (x − ei)]/2

// assigns the ith column of Jˆ

4: end for

5: if ||Jˆ − df (x)||∞ < 10−4 return true; else false

Here ei is the ith standard basis vector in Rn.

2.6 Examples and Exercises
2.6.1 More derivatives
a) In 3D, note that a × b = skew(a)b = −skew(b)a, where skew(v) is the skew matrix of v. What is the gradient of (a × b)2 w.r.t. a and b?
2.6.2 Multivariate Calculus Given tensors y ∈ Ra×...×z and x ∈ Rα×...×ω where y is a function of x, the Jacobian tensor J = ∂xy is in Ra×...×z×α×...×ω and has coeﬃcients
∂ Ji,j,k,...,l,m,n... = ∂xl,m,n,... yi,j,k,...

Maths for Intelligent Systems, Marc Toussaint

19

(All “output” indices come before all “input” indices.) Compute the following Jacobian tensors

(i)

∂ ∂x

x,

where

x

is

a

vector

(ii)

∂ ∂x

x

Ax,

where

A

is

a

matrix

(iii)

∂ ∂A

y

Ax,

where

x

and

y

are

vectors

(note, we take the derivative w.r.t. A)

(iv)

∂ ∂A

Ax

(v)

∂ ∂x

f

(x)

Ag(x),

where

f

and

g

are

vector-valued

functions

2.6.3 Finite Diﬀerence Gradient Checking

The following exercises will require you to code basic functions and derivatives. You can code in your prefereed language (Matlab, NumPy, Julia, whatever).

(i) Implement the following pseudo code for empirical gradient checking in the programming language of your choice:

Input: x ∈ Rn, function f : Rn → Rd, function df : Rn → Rd×n 1: initialize Jˆ ∈ Rd×n, and = 10−6

2: for i = 1 : n do

3: Jˆ·i = [f (x + ei) − f (x − ei)]/2

// assigns the ith column of Jˆ

4: end for

5: if ||Jˆ − df (x)||∞ < 10−4 return true; else false

Here ei is the ith standard basis vector in Rn.
(ii) Test this for
• f : x → Ax, df : x → A, where you sample x ∼randn(n,1) (N(0, 1) in Matlab) and A ∼randn(m,n)
• f : x → x x, df : x → 2x , where x ∼randn(n,1)

2.6.4 Backprop in a Neural Net
Consider the function f : Rh0 → Rh3 , f (x0) = W2σ(W1σ(W0x0))
where Wl ∈ Rhl+1×hl and σ : R → R is a diﬀerentiable activation function which is applied element-wise. We also describe the function as the computation graph: x0 → z1 = W0x0 → x1 = σ(z1) → z2 = W1x1 → x2 = σ(z2) → f = W2x2

20

Maths for Intelligent Systems, Marc Toussaint

Derive

pseudo

code

to

eﬃciently

compute

df dx0

.

(Ideally

also

for

deeper

networks.)

2.6.5 Backprop in a Neural Net

We consider again the function

f : Rh0 → Rh3 , f (x0) = W2σ(W1σ(W0x0)) ,

where Wl ∈ Rhl+1×hl and σ(z) = 1/(e−z + 1) is the sigmoid function which is applied element-wise. We established last time that

df

∂f =

∂x2 ∂z2 ∂x1

∂z1

dx0 ∂x2 ∂z2 ∂x1 ∂z1 ∂x0

with:

∂xl ∂zl

=

diag(xl

◦ (1 − xl))

,

∂zl+1 ∂xl

=

Wl

,

∂f ∂x2 = W2

Note: In the following we still let f be a h3-dimensional vector. For those that are confused with the resulting tensors, simplify to f being a single scalar output.

(i) Derive also the necessary equations to get the derivative w.r.t. the weight matrices Wl, that is the Jacobian tensor df
dWl

(ii)

Write

code

to

implement

f (x)

and

df dx0

and

df dWl

.

To test this, choose layer sizes (h0, h1, h2, h3) = (2, 10, 10, 2), i.e., 2 input and 2 output dimensions, and hidden layers of dimension 10.

For testing, choose random inputs sampled from x ∼randn(2,1)

And

choose

random

weight

matrices

Wl

∼

√ 1 rand(h[l+1],h[l]).
hl+1

Check the implemented Jacobian by comparing to the ﬁnite diﬀerence approximation.

Debugging Tip: If your ﬁrst try does not work right away, the typical approach to debug is to “comment out” parts of your function f and df . For instance, start with testing f (x) = W0x0; then test f (x) = σ(W0x0); then f (x) = W1σ(W0x0); then I’m sure all bugs are found.

(iii) Bonus: Try to train the network to become the identity mapping. In the sim-

plest case, use “stochastic gradient descent”, meaning that you sample an input,

compute

the

gradients

wl

=

d(f

(x)−x)2 dWl

,

and

make

tiny

updates

Wl

←

Wl

− αwl.

Maths for Intelligent Systems, Marc Toussaint

21

2.6.6 Logistic Regression Gradient & Hessian

Consider the function

n
L : Rd → R : L(β) = − yi log σ(xi β) + (1 − yi) log[1 − σ(xi β)] + λβ β ,
i=1

where xi ∈ Rd is the ith row of a matrix X ∈ Rn×d, and y ∈ {0, 1}n is a vector of 0s and 1s only. Here, σ(z) = 1/(e−z + 1) is the sigmoid function, with σ (z) = σ(z)(1 − σ(z)).

Derive

the

gradient

∂ ∂β

L(β

),

as

well

as

the

Hessian

∇2L(β)

=

∂2 ∂β2 L(β)

.

3 Linear Algebra
3.1 Vector Spaces
3.1.1 Why should we care for vector spaces in intelligent systems research?
We want to describe intelligent systems. For this we describe systems, or aspects of systems, as elements of a space:
– The input space X, output space Y in ML – The space of functions (or classiﬁers) f : X → Y in ML – The space of world states S and actions A in Reinforcement Learning – The space of policies π : S → A in RL – The space of feedback controllers π : x → u in robot control – The conﬁguration space Q of a robot – The space of paths x : [0, 1] → Q in robotics – The space of image segmentations s : I → {0, 1} in computer vision
Actually, some of these spaces are not vector spaces at all. E.g. the conﬁguration space of a robot might have ‘holes’, be a manifold with complex topology, or not even that (switch dimensionality at some places). But to do computations in these spaces one always either introduces (local) parameterizations that make them a vector space,2 or one focusses on local tangent spaces (local linearizations) of these spaces, which are vector spaces.
Perhaps the most important computation we want to do in these spaces is taking derivatives—to set them equal to zero, or do gradient descent, or Newton steps for
2E.g. by deﬁnition an n-dimensional manifold X is locally isomorphic to Rn.

22

Maths for Intelligent Systems, Marc Toussaint

optimization. But taking derivatives essentially requires the input space to (locally) be a vector space.3 So, we also need vector spaces because we need derivatives, and Linear
Algebra to deal with the resulting equations.

3.1.2 What is a vector?
A vector is nothing but an element of a vector space. It is in general not a column, array, or tuple of numbers. (But tuples of numbers are a special case of a vector space.)

3.1.3 What is a vector space?
Deﬁnition 3.1 (vector space). A vector spacea V is a space (=set) on which two operations, addition and multiplication, are deﬁned as follows
• addition + : V × V → V is an abelian group, i.e., – a, b ∈ V ⇒ a + b ∈ V (closed under +) – a + (b + c) = (a + b) + c (association) – a + b = b + a (commutation) – ∃ unique 0 ∈ V s.t. ∀v ∈ V : 0 + v = v (identity) – ∀v ∈ V : ∃ unique − v s.t. v + (−v) = 0 (inverse)
• multiplication · : R × V → V fulﬁls, for α, β ∈ R, – α(βv) = (αβ)v (association) – 1v = v (identity) – α(v + w) = αv + αw (distribution)
aWe only consider vector spaces over R.
Roughly, this deﬁnition says that a vector space is “closed under linear transformations”, meaning that we can add and scale vectors and they remain vectors.

3.2 Vectors, dual vectors, coordinates, matrices, tensors
In this section we explain what might be obvious: that once we have a basis, we can write vectors as (column) coordinate vectors, 1-forms as (row) coordinate vectors, and linear transformations as matrices. Only the last subsection becomes more practical, refers to concrete exercises, and explains how in practise not to get confused about basis
3Also when the space is actually a manifold; the diﬀerential is deﬁned as a 1-form on the local tangent.

Maths for Intelligent Systems, Marc Toussaint

23

transforms and coordinate representations in diﬀerent bases. So a practically oriented reader might want to skip to the last subsection.

3.2.1 A taxonomy of linear functions
For simplicity we consider only functions involving a single vector space V . But all that is said transfers to the case when multiple vector spaces V, W, ... were involved.
Deﬁnition 3.2. f : V → X linear ⇔ f (αv + βw) = αf (v) + βf (w), where X is any other vector space (e.g. X = R, or X = V × V ).

Deﬁnition 3.3. f : V × V × · · · × V → X multi-linear ⇔ f is linear in each input.
Many names are used for special linear functions—let’s make some explicit:
– f : V → R, called linear functional4, or 1-form, or dual vector. – f : V → V , called linear function, or linear transform, or vector-valued 1-form – f : V × V → R, called bilinear functional, or 2-form – f : V × V × V → R, called 3-form (or unspeciﬁcally ’multi-linear functional’) – f : V × V → V , called vector-valued 2-form (or unspeciﬁcally ’multi-linear map’) – f : V × V × V → V × V , called bivector-valued 3-form – f : V k → V m, called m-vector-valued k-form
This gives us a simple taxonomy of linear functions based on how many vectors a function eats, and how many it outputs. To give examples, consider some space X of systems (examples above), which might itself not be a vector space. But locally, around a speciﬁc x ∈ X, its tangent V is a vector space. Then
– f : X → R could be a cost function over the system space. – The diﬀerential df |x : V → R is a 1-form, telling us how f changes when ‘making a tangent
step’ v ∈ V . – The 2nd derivative d2f |x : V × V → R is a 2-form, telling us how df |x(v) changes when
‘making a tangent step’ w ∈ V . – The inner product ·, · : V × V → R is a 2-form.
Another example:
– f : Ri → Ro is a neural network that maps i input signals to o output signals. – Its derivative df |x : Ri → Ro is a vector-valued 1-form, telling us how each output changes
with a step v ∈ Ri in the input. – Its 2nd derivative d2f |x : Ri × Ri → Ro is a vector-valued 2-form.
4The word ’functional’ instead of ’function’ is especially used when V is a space of functions.

24

Maths for Intelligent Systems, Marc Toussaint

This is simply to show that vector-valued functions, 1-forms, and 2-forms are common. Instead of being a neural network, f could also be a mapping from one parameterization of a system to another, or the mapping from the joint angles of a robot to its hand position.

3.2.2 Bases and coordinates
We need to deﬁne some notions. I’m not commenting on these deﬁnitions—train yourself in reading maths...
Deﬁnition 3.4. span({vi}ki=1) = { i αivi : αi ∈ R}

Deﬁnition 3.5. {vi}ni=1 linearly independent ⇔

i αivi = 0 ⇒ ∀iαi = 0

Deﬁnition 3.6. dim(V ) = maxn{n ∈ N : ∃ {vi}ni=1 lin.indep., vi ∈ V }
Deﬁnition 3.7. B = (ei)ni=1 is a basis of V ⇔ span(B) = V and B lin.indep.
Deﬁnition 3.8. The tuple (v1, v2, .., vn) ∈ Rn is called coordinates of v ∈ V in the basis (ei)ni=1 iﬀ v = i viei
Note that Rn is also a vector space, and therefore coordinates v1:n ∈ Rn are also vectors, but in Rn, not V . So coordinates are vectors, but vectors in general not coordinates. Given a basis (ei)ni=1, we can describe every vector v as a linear combination v = i viei of basic elements—the basis vectors ei. This general idea, that “linear things” can be described as linear combinations of “basic elements” carries over also to functions. In fact, to all the types of functions we described above: 1-forms, 2-forms, bi-vector-valued k-forms, whatever. And if we describe all these als linear combinations of basic elements we automatically also introduce coordinates for these things. To get there, we ﬁrst have to introduce a second type of “basic elements”: 1-forms.
3.2.3 The dual vector space – and its coordinates
Deﬁnition 3.9. Given V , its dual space is V ∗ = {f : V → R linear} (the space of 1-forms). Every v∗ ∈ V ∗ is called 1-form or dual vector (sometimes also covector ).
First, it is easy to see that V ∗ is also a vector space: We can add two linear functionals, f = f1 + f2, and scale them, and it remains a linear functional.

Maths for Intelligent Systems, Marc Toussaint

25

Second, given a basis (ei)ni=1 of V , we deﬁne a corresponding dual basis (ei)ni=1 of V ∗ simply by

∀i,j : ei(ej ) = δij

(68)

where δij = [i = j] is the Kronecker delta. Note that

∀v ∈ V : ei(v) = vi

(69)

That is, ei is the 1-form that simply maps a vector to its ith coordinate. It can be shown that (ei)ni=1 is in fact a basis of V ∗. (Omitted.) That tells us a lot!
dim(V ∗) = dim(V ). That is, the space of 1-forms has the same dimension as V . At this place, geometric intuition should kick in: indeed, every linear function over V could be envisioned as a “plane” over V . Such a plane can be illustrated by its iso-lines and these can be uniquely determined by their orientation and distance (same dimensionality as V itself). Also, (assuming we’d know already what a transpose or scalar product is) every 1-form must be of the form f (v) = c v for some c ∈ V —so every f is uniquely described by a c ∈ V . Showing that the vector space V and its dual V ∗ are really twins.
The dual basis (ei)ni=1 introduces coordinates in the dual space: Every 1-form f can be described as a linear combination of basis 1-forms,

f = fiei

(70)

i

where the tuple (f1, f2, .., fn) are the coordinates of f . And

span({ei}ni=1) = V ∗ .

(71)

3.2.4 Coordinates for every linear thing: tensors

We now have the basic elements: the basis vectors (ei)ni=1 of V , and basis 1-forms (ei)ni=1 of V ∗. From these, we can describe, for instance, any bivector-valued 3-form as a linear combination as follows:

f : V ×V ×V →V ×V

(72)

f=

fijklm ei ⊗ ej ⊗ ek ⊗ el ⊗ em

ijklm

(73)

The ⊗ is called outer product (or tensor product), and v ⊗ w ∈ V × W if V and W
are ﬁnite vector spaces. For our purposes, we may think of v ⊗ w = (v, w) simply as
the tuple of both. Therefore ei ⊗ ej ⊗ ek ⊗ el ⊗ em is a 5-tuple and we have in total n5 such basis objects—and fijklm denotes the corresponding n5 coordinates. The ﬁrst two indices are contra-variant, the last three covariant—these notions are explained in
detail later.

26

Maths for Intelligent Systems, Marc Toussaint

3.2.5 Finally: Matrices As a special case of the above, every f : V → U can be described as a linear combination

f = fij ei ⊗ ej ,

(74)

ij

where (ej)nj=1 is a basis of V ∗ and (ei) a basis of U .

Let’s see how this ﬁts with some easier view, without all this fuss about 1-forms. We already understood that the operator ej(v) = vj simply picks the jth coordinate of a vector. Therefore

f (v) =

fij ei ⊗ ej (v) = fij eivj .

(75)

ij

ij

In case it helps, we can ‘derive’ this more slowly as

f (v) = f ( vkek) = vkf (ek) = vk

fij ei ⊗ ej ek

(76)

k

k

k

ij

= fij vk ei ej (ek) = fij vk ei δjk =

ijk

ijk

i

fij vj ei .
j

(77)

As a result, this tells us that the vector u = f (v) ∈ V has the coordinates ui = j fij vj. And the vector f (ej) ∈ V has the coordinates fij, that is, f (ej) = i fijei.
So there are n2 coordinates fij for a linear function f . The ﬁrst index is contra-variant, the second covariant (explained later). As it so happens, the whole world has agreed on a convention on how to write such coordinate numbers on sheets of 2-dimensional paper: as a matrix!

 f11 f12 · · · f1n 





f 

 

21

f22

···

f

2n

 

    

...

...

    





fn1 fn2 · · · fnn 

(78)

The ﬁrst (contra-variant) index spans columns; the second (covariant) spans rows. We call this and the respective deﬁnition of a matrix multiplication as the matrix convention.

Note that the identity map I : V → V can be written as

I = ei ⊗ ei , Iij = δij .

(79)

i

Equally, the (contra-variant) coordinates of a vector are written as columns

 v1 



v  

 

2

 

 
.   .   .  



vn 

(80)

Maths for Intelligent Systems, Marc Toussaint

27

and the (covariant) coordinates of a 1-form h : V → R as a row

(h1 h2 · · · hn )

(81)

u = f (v) is itself a vector, and its coordinates written as a column are

 u1   f11 f12 · · · f1n   v1 

 

 




u f 

2

 



. =  

.  

. .  

. .  



 

21











f22

···

f v   

2n

 

 

2

 

   
. .    . .    . .   

 

 

un  fn1 fn2 · · · fnn  vn 

(82)

where this matrix multiplication is deﬁned by ui = j fijvj, consistent to the above.

columns
vector
output space co-variant
contra-variant coordinates

rows 1-form co-vector derivative input space contra-variant co-variant coordinates

3.2.6 Coordinate transformations

The above was rather abstract. The exercises demonstrate representing vectors and transformations with coordinates and matrices in diﬀerent input and output bases. We just summarize here:

• We have two bases A = (a1, .., an) and B = (b1, .., bn), and the transformation T that maps each ai to bi, i.e., B = T A.

• Given a vector x we denote its coordinates in A by [x]A or brieﬂy as xA. And we denotes its coordinates in B as [x]B or xB. E.g., xAi is the ith coordinate in basis A.

• [bi]A are the coordinates of the new basis vectors in the old basis. The coordinate transformation matrix B is given with elements Bij = [bj]Ai . Note that

[x]A = B[x]B ,

(83)

i.e., while the basis transform T carries old basis ai vectors to new basis vectors bi, the matrix B carries coordinates [x]B in the new basis to coordinates [x]A in the
old basis! This is the origin of understanding that coordinates are contra-variant.

• Given a linear transform f in the vector space, we can represent it as a matrix in four ways, using basis A or B in the input and output spaces, respectively. If

28

Maths for Intelligent Systems, Marc Toussaint

[f ]AA = F is the matrix in old coordinates (using A for input and output), then [f ]BB = B-1F B is its matrix in new coordinates, [f ]AB = F B is its matrix using B for the input and A for the output space, and [f ]BA = B-1F is the matrix using A for input and B for output space.
• T itself is also a linear transform. [T ]AA = B is its matrix in old coordinates. And the same [T ]BB = B is also its matrix in new coordinates! [T ]BA = I is its matrix when using A for input and B for output space. And [T ]AB = B2 is its matrix using B for input and A for output space.

3.3 Scalar product and orthonormal basis

Please note that so far we have not in any way referred to a scalar product or a transpose. All the concepts above, dual vector space, bases, coordinates, matrix-vector multiplication, are fully independent of the notion of a scalar product or transpose. Columns and rows naturally appear as coordinates of vectors and 1-forms. But now we need to introduce scalar products.

Deﬁnition 3.10. A scalar product (also called inner product) of V is a symmetric positive deﬁnite 2-form

·, · : V × V → R .

(84)

with v, w = w, v and v, v > 0 for all v = 0 ∈ V .

Deﬁnition 3.11. Given a scalar product, we deﬁne for every v ∈ V its dual v∗ ∈ V ∗ as

v∗ = v, · = vi ei, · = vie∗i .

(85)

i

i

Note that ei and e∗i are in general diﬀerent 1-forms! The canonical dual basis (ei)ni=1 is independent of an introduction of a scalar product, they were the basis to introduce coordinates for linear functions, including matrices. And while such coordinates do depend on a choice of basis (ei)ni=1, they do not depend on a choice of scalar product.
The 1-forms (e∗i )ni=1 also form a basis for V ∗, but a diﬀerent one to the canonical basis, and one that depends on the notion of a scalar product. You can see this: the coordinates vi of v∗ in the basis (e∗i )ni=1 are identical to the coordinates vi of v in the basis (ei)ni=1, but diﬀerent to the coordinates (v∗)i of v∗ in the basis (ei)ni=1.
Deﬁnition 3.12. Given a scalar product, a set of vectors {vi}ni=1 is called orthonormal iﬀ vi, vj = δij .

Maths for Intelligent Systems, Marc Toussaint

29

Deﬁnition 3.13. Given a scalar product and basis (ei)ni=1, we deﬁne the metric tensor gij = ei, ej , which are the coordinates of the 2-form ·, · , that is

·, · = gij ei ⊗ ej .

(86)

ij

This also implies that

v, w = viwj ei, ej = viwjgij = v Gw .

(87)

ij

ij

Although related, do not confuse gij with the usual deﬁnition of a metric d(·, ·) in a metric space.

3.3.1 Properties of orthonormal bases

If we have an orthonormal basis (ei)ni=1, many thing simplify a lot. Throughout this subsection, we assume {ei} orthonormal.

• The metric tensor gij = ei, ej = δij is the identity matrix.5 Such a metric is also called Euclidean. The norm || i|| = 1. The canonical dual basis (ei)ni=1 and the one deﬁned via the scalar product (e∗i )ni=1 become identical, ei = e∗i = ei, · . Consequently, v and v∗ have the same coordinates vi = (v∗)i w.r.t. (ei)ni=1 and (ei)ni=1, respectively.
• The coordinates of vectors can now easily been computed:

v = viei ⇒ ei, v = ei, vj ej = ei, ej vj = vi

(88)

i

j

j

• The coordinates of a linear transform can equally easily been computed: Given
a linear transform f : V → U an arbitrary (e.g. non-orthonormal) input basis (vi)ni=1 of V , but an orthonormal basis (ui)ni=1, then

f = fij ui ⊗ vj ⇒
ij

ui, f vj = uj , fkluk ⊗ vl(vj ) = fkl uj , uk vl(vj )

kl

kl

= fklδjkδlj = fij
kl

(89) (90)

• The projection onto a basis vector is given by ei ei, · .
5Being picky, a metric is not a matrix but a twice covariant tensor (a row of rows). That’s why it is correctly called metric tensor.

30

Maths for Intelligent Systems, Marc Toussaint

• The projection onto the span of several basis vectors (e1, .., ek) is given by

• The identity mapping I : V → V is given by I =

dim(V i=1

)

ei

ei, ·

.

• The scalar product with an orthonormal basis is

k i=1

ei

ei, ·

.

v, w = viwj δij = viwi

ij

i

(91)

which, using matrix convention, can also be written as

 w1 



v, w

= (v1

v2

..

vn)




w 

2

 



. = v  



.  

.  

w=



i

viwi ,

wn 

(92)

where for the ﬁrst time we introduced the transpose which, in the matrix convention, swaps columns to rows and rows to columns.

As a general note, a row vector “eats a vector and outputs a scalar”. That is v : V → R should be thought of as a 1-form! Due to the matrix conventions, it generally is the case that “rows eat columns”, that is, every row index should always be thought of as relating to a 1-form (dual vector), and every column index as relating to a vector. That is totally consistent to our deﬁnition of coordinates.
For an orthonormal basis we also have

v∗(w) = v, w = v w .

(93)

That is, v is the coordinate representation of the 1-form v∗. (Which also says, that the coordinates of the 1-form v∗ in the special basis (e∗i )ni=1 ⊂ V ∗ coincide with the coordinates of the vector v.)

3.4 The Structure of Transforms & Singular Value Decomposition
We focus here on linear transforms (or “linear maps”) f : V → U from one vector space to another (or the same). It turns out that such transforms have a very speciﬁc and intuitive structure, which is captured by the singular value decompositon.

3.4.1 The Singular Value Decomposition Theorem We state the following theorem:

Maths for Intelligent Systems, Marc Toussaint

31

V
x

U
f

span(V )

σ

span(U ) f (x)

input (row) space: span(V ) input null space: V/span(V )

output (column) space: span(U ) output null space: U/span(U )

Figure 2: A linear transformation f =

k i=1

σi

ui

vi

can

be

described

as:

take

the

input

x, project it onto the ﬁrst input fundamental vector v1 to yield a scalar, stretch/squeeze

it by σ1, and “unproject” this into the ﬁrst output fundamental vector u1; repeat this

for all i = 1, .., k, and add up the results.

Theorem 3.1 (Singular Value Decomposition). Given two vector spaces V and U
with scalar products, dim(V) = n and dim(U) = m, for every linear transform f : V → U there exist a k ≤ n, m and orthonormal vectors {vi}ki=1 ⊂ V, orthonormal vectors {ui}ki=1 ⊂ U, and positive scalars σi > 0, i = 1, .., k, such that

k
f = σiuivi∗
i=1

(94)

As above, vi∗ = vi, · is the basis 1-form that picks the ith coordinate of a vector in the basis (vi)ki=1 ⊂ V.a
aNote that {vi}ki=1 may not be a full basis of V if k < n. But because {vi} is orthonormal, vi, · uniquely picks the ith coordinate no matter how {vi}ki=1 is completed with further n − k vectors to become a full basis.

We ﬁrst restate this theorem equivalently in coordinates.

Theorem 3.2 (Singular Value Decomposition). For every matrix A ∈ Rm×n there exists a k ≤ n, m and orthonormal vectors {vi}ki=1 ⊂ Rn, orthonormal vectors {ui} ⊂ Rm, and positive scalars σi > 0, i = 1, .., k, such that

k
A = σiuivi = U SV
i=1

(95)

where V = (v1, .., vk) ∈ Rn×k , U = (u1, .., uk) ∈ Rm×k contain the orthonormal bases vectors as columns and S = diag(σ1, .., σk).

Let me rephrase this in a sentence: Every matrix A can be expressed as a linear combination of only k rank-1 matrices. Rank-1 matrices are the most minimalistic kinds of

32

Maths for Intelligent Systems, Marc Toussaint

matrices and they are always of the form uv for some u and v. The rank-1 matrix uv takes an input x, projects it on v (measures its alignment with v), and “unprojects” into u (multiplies v x to the output vector u).

Just to explicitly show the transition from coordinate-free to the coordinate-based theorem, consider arbitrary orthonormal bases {ei}ni=1 ⊂ V and {eˆi}m i=1 ⊂ U. For x ∈ V we have

k

k

f (x) = σiui vi, x = σi( ulieˆl) vjiej , xkek

i=1

i=1

l

j

k

k

= σi( ulieˆl) vjixkδjk =

i=1

l

jk

l

k

uliσi vjixj eˆl

i=1

j

(96) (97)

= U SV x eˆl

(98)

l

l

where vji are the coordinates of vi, uli the coordinates of ui, U = (u1, .., uk) is the matrix containing {ui} as columns, V = (v1, .., vk) the matrix containing {vi} as columns, and S = diag(σ1, .., σk) the diagonal matrix with elements σi.
We add some deﬁnitions based on this:

Deﬁnition 3.14. The rank rank(f ) = rank(A) of a transform f or its matrix A is the unique minimal k.

Deﬁnition 3.15. The determinant of a transform f or its matrix A is

det(f ) = det(A) = det(S) =

±

n i=1

σi

for rank(f ) = n = m ,

(99)

0

otherwise

where ± depends on whether the transform is a reﬂection or not.

The last deﬁnition is a bit ﬂaky, as the ± is not properly deﬁned. If, alternatively, in the

above theorems we would require V and U to be rotations, that is, elements of SO(n)

(of the special orthogonal group); then negative σ’s would indicate such a reﬂection and

det(A) =

n i=1

σi.

But above we required σ’s to be strictly positive and V

and U

only

orthogonal. Fundamental space vectors vi and ui could ﬂip sign. The ± above indicates

how many ﬂip sign.

Deﬁnition 3.16. a) The row space (also called right or input fundamental space) of a transform f is span{vi}ri=an1k(f). The input null space (or right null space) V⊥ is the subspace orthogonal to the row space, such that v ∈ V⊥ ⇒ f (v) = 0.
b) The column space or (also called left or output fundamental space) of a transform f is span{ui}ri=an1k(f). The output null space (or left null space) U⊥ the

Maths for Intelligent Systems, Marc Toussaint

33

subspace orthogonal to the column space, such that u ∈ U⊥ ⇒ f (·), u = 0.

3.5 Point of departure from the coordinate-free notation
The coordinate-free introduction of vectors and transforms helps a lot to understand what these fundamentally are. Namely, that coordinate vectors and matrices are ’just’ coordinates and rely on a choice of basis; what a metric gij really is; that only for a Euclidean metric the inner product satisﬁes v, w = v w. Further, the coordinatefree view is essential to understand that vector coordinates behave diﬀerently to 1-form coordinates (e.g., “gradients”!) under a transformation of the basis. We discuss contraversus covariance of gradients at the end of this chapter.
However, we now understood that columns correspond to vectors, rows to 1-forms, and in the Euclidean case the 1-form v, · directly corresponds to v , in the non-Euclidean to v G. In applications we typically represent things from start in orthonormal bases (including perhaps non-Euclidean metrics), there is not much gain sticking to the coordinate-free notation in most cases. Only when the matrix notation gets confusing (and this happens, e.g. when trying to compute something like the “Jacobian of a Jacobian”, or applying the chain and product rule for a matrix expression ∂xf (x) A(x)b(x)) it is always a save harbour to remind what we actually talk about.
Therefore, in the rest of the notes we rely on the normal coordinate-based view. Only in some explanations we remind at the coordinate-free view when helpful.

3.6 Filling SVD with life

In the following we list some statements—all of them relate to the SVD theorem and

together they’re meant to give a more intuitive understanding of the equation A =

k i=1

σiui

vi

=

USV

.

3.6.1 Understand vv as a projection

• The projection of a vector x ∈ V onto a vector v ∈ V, is given by

1

vv

x = v2 v v, x or v2 x .

(100)

Here,

the

1 v2

is

normalizing

in

case

v

does

not

have

length

|v| = 1.

• The projection-on-v-matrix vv is symmetric, semi-pos-def, and has rank(vv ) = 1.

• The projection of a vector x ∈ V onto a subvector space span{vi}ki=1 for orthonor-

34

Maths for Intelligent Systems, Marc Toussaint

mal {vi}ki=1 is given by

x = vivi x = V V T x
i

(101)

where V = (v1, .., vk) ∈ Rn×k. The projection matrix V V for orthonormal V is symmetric, semi-pos-def, and has rank(V V ) = k.

• The expression i vivi is quite related to an SVD. Conversely it shows that the SVD represent every matrix as a liner combination of kind-of-projects, but these
kind-of-projects uv ﬁrst project onto v, but then unproject along u.

3.6.2 SVD for symmetric matrices

• Thm 2 ⇒ Every symmetric matrix A is of the form

A = λivivi = V ΛV
i

(102)

for orthonormal V = (v1, .., vk). Here λi = ±σi and Λ = diag(λ) is the diagonal matrix of λ’s. This describes nothing but a stretching/squeezing along orthogonal projections.

• The λi and vi are also the eigenvalues and eigenvectors of A, that is, for all i = 1, .., k:

Avi = λivi .

(103)

If A has full rank, then the SVD A = V SV = V SV -1 is therefore also the eigendecomposition of A.

• The pseudo-inverse of a symmetric matrix is

A† = λ-i1vivi = V S-1V
i

(104)

which simply does the reverse stretching/squeezing along the same orthogonal projections. Note that

AA† = A†A = V V

(105)

is the projection on {vi}ki=1. For full rank(A) = n we have V V = I and A† = A-1. For rank(A) < n, we have that A†y minimizes minx ||Ax − y||2, but there are inﬁnitly many x’s that minimize this, spanned by the null space of A. A†y is the minimizer closest to zero (with smallest norm).

Maths for Intelligent Systems, Marc Toussaint

35

•

Consider mean,

a data set

m i=1

xi

=

0.

D = {xi}m i=1, xi ∈ Rn. For simplicity The covariance matrix is deﬁned as

assume

it

has

zero

1

1

C= n

xixi = n X X

i

(106)

where (consistent to ML lecture convention) the data matrix X containes xi in the ith row. Each xixi is a projection. C is symmetric and semi-pos-dev. Using SVD we can write

C = λivivi

(107)

i

√ and λi is the d√ata variance along the eigenvector vi; λi the standard deviation along vi; and λivi the principle axis vectors that make the ellipsoid we typically

illustrate convariances with.

3.6.3 SVD for general matrices

• For full rank(A) = n, the determinant of a matrix is det(A) = ± deﬁne the volume spanned by any {bi}ni=1 as
vol({bi}ni=1) = det(B) , B = (b1, .., bn) ∈ Rn×n .

i σi. We may (108)

It follows that

vol({Abi}ni=1) = det(A) det(B)

(109)

that is, the volume is being multiplied with det(A), which is consistent with our intuition of transforms as stretchings/squeezings along orthonormal projections.

• The pseudo-inverse of a general matrix is

A† = σi-1viui = V S-1U .
i

(110)

If k = n (full input rank), rank(A A) = n and

(A A)-1A = (V SU U SV )-1V SU = V − S−2V -1V SU = V S-1U = A† (111)
and A† is also called left pseudoinverse because A†A = In. If k = m (full output rank), rank(AA ) = m and

A (AA )-1 = V SU (U SV V SU )-1 = V SU U − S−2U -1 = V S-1U = A† (112)
and A† is also called right pseudoinverse because AA† = Im.

36

Maths for Intelligent Systems, Marc Toussaint

• Assume m = n (same input/output dimension, or V = U), but k < n. Then there exist orthogonal V, U ∈ Rn×n such that

A = UDV ,

D

=

diag(σ1,

..,

σk ,

0,

..,

0)

=

S

0

0 

.

0

(113)

Here, V and U contain a full orthonormal basis instead of only k orthonormal
vectors. But the diagonal matrix D projects all but k of those to zero. Every square matrix A ∈ Rn×n can be written like this.

Deﬁnition 3.17 (Rotation). Given a scalar-product ·, · on V, a linear transform f : V → V is called rotation iﬀ it preserves the scalar product, that is,

∀v, w ∈ V : f (v), f (w) = v, w .

(114)

• Every rotation matrix is orthogonal, i.e., composed of columns of orthonormal vectors.
• Every rotation has rank n and σ1,..,n = 1. (No stretching/squeezing.)
• Every square matrix can be written as rotationU · scalingD · rotationV -1

3.7 Eigendecomposition

Deﬁnition 3.18. The eigendecomposition or diagonalization of a square matrix A ∈ Rn×n is (if it exists!)

A = QΛQ-1

(115)

where Λ = diag(λ1, .., λn) is a diagonal matrix of eigenvalues. Each column qi of Q is an eigenvector.

• First note that, unlike SVD, this is not a Theorem but just a deﬁnition: If such a decomposition exists, it is called eigendecomposition. But it exists for almost any square matrix.
• The set of eigenvalues is the set of roots of the characteristic polynomial pA(λ) = det(λI − A). Why? Because then A − λI has ’volume’ zero (or rank < n), showing that there exists a vector that is mapped to zero, that is 0 = (A − λI)x = Ax − λx.
• Is every square matrix diagonalizable? No, but only an n2−1-dimensional subset of matrices are not diagonalizable; most matrices are. A matrix is not diagonalizable if an eigenvalue λ has multiplicity k (more precisely, λ is a root of pA(λ) with

Maths for Intelligent Systems, Marc Toussaint

37

multiplicity k), but n − rank(A − λI) (the dimensionality of the span of the eigenvectors of λ!) is less than k. Therefore, the eigenvectors of λ are not linearly independent; they do not span the necessary k dimensions. So, only very “special” matrices are not diagonalizable. Random matrices are (with prob 1).
• Symmetric matrices? → SVD
• Rotations? Not real. But complex! Think of oscillating projection onto eigenvector. If φ is the rotation angle, e±iφ are eigenvalues.

3.7.1 Power Method
To ﬁnd the largest eigenvector of A, initialize x randomly and iterate 1
x ← Ax , x ← x ||x||

(116)

– If this converges, x must be an eigenvector and λ = x Ax the eigenvalue.
– If A is diagonalizable, and x is initially a non-zero linear combination of all eigen vectors, then it is obvious that x will converge to the “largest” (in absolute terms |λi|) eigenvector (=eigenvector with largest eigenvalue). Actually, if the largest (by norm) eigenvector is negative, then it doesn’t really converge but ﬂip sign at every iteration.

3.7.2 Power Method including the smallest eigenvalue

A trick, hard to ﬁnd in the literature, to also compute the smallest eigenvalue and -vector is the following. We assume all eigenvalues to be positive. Initialize x and y randomly, iterate

x ← Ax , λ ← ||x|| , x ← x/λ , y ← (λI − A)y , y ← y/||y||

(117)

Then y will converge to the smallest eigenvector, and λ − ||y|| will be its eigenvalue. Note that (in the limit) A − λI only has negative eigenvalues, therefore ||y|| should be positive. Finding smallest eigenvalues is a common problem in model ﬁtting.

3.7.3 Why should I care about Eigenvalues and Eigenvectors?
– Least squares problems (ﬁnding smallest eigenvalue/-vector); (e.g. Camera Calibration, Bundle Adjustment, Plane Fitting)
– PCA – stationary distributiuons of Markov Processes – Page Rank – Spectral Clustering – Spectral Learning (e.g., as approach to training HMMs)

38

Maths for Intelligent Systems, Marc Toussaint

3.8 Beyond this script: Numerics to compute these things
We will not go into details of numerics. Nathan’s script gives a really nice explaination of the QR-method. I just mention two things:
(i) The most important forms of matrices for numerics are diagonal matrices, orthogonal matrices, and upper triangular matrices. One reason is that all three types can very easily be inverted. A lot of numerics is about ﬁnding decompositions of general matrices into products of these special-form matrices, e.g.:
– QR-decomposition: A = QR with Q orthogonal and R upper triangular. – LU-decomposition: A = LU with U and L upper triangular. – Cholesky decomposition: (symmetric) A = C C with C upper triangular – Eigen- & singular value decompositions
Often, these decompositions are intermediate steps to compute eigenvalue or singular value decompositions.
(ii) Use linear algebra packages. At the origin of all is LAPACK; browse through http: //www.netlib.org/lapack/lug/ to get an impression of what really has been one of the most important algorithms in all technical areas of the last half century. Modern wrappers are: Matlab (Octave), which originated as just a console interface to LAPACK; the C++-library Eigen; or the Python NumPy.

3.9 Derivatives as 1-forms, steepest descent, and the covariant gradient

3.9.1 The coordinate-free view: A derivative takes a change-of-input vector as input, and returns a change of output

We previously deﬁned the Jacobian of a function f : Rn → Rd from vector to vector space. We repeat deﬁning a derivative more generally in coordinate-free form:

Deﬁnition 3.19. Given a function f : V → G we deﬁne the diﬀerential

f (x + hv) − f (x)

df

x:

V

→ G,

v → lim
h→0

h

(118)

This deﬁnition holds whenever G is a continuous space that allows the deﬁnition of this limit and the limit exists (f is diﬀerentiable). The notation df |x reads “the diﬀerential at location x”, i.e., evaluating this derivative at location x.
Note that df |x is a mapping from a “tangent vector” v (a change-of-input vector) to an output-change. Further, by this deﬁnition df |x is linear. df |x(v) is the directional derivative we mentioned before. Therefore df |x is a G-valued 1-form. As discussed earlier, we can introduce coordinates for 1-forms; these coordinates are what typically

Maths for Intelligent Systems, Marc Toussaint

39

is called the “gradient” or “Jacobian”. But here we explicitly see that we speak of coordinates of a 1-form.

3.9.2 Contra- and co-variance

In Section 3.2.6 we summarized the eﬀects of a coorindate transformation. We recap the same here again also for derivatives and scalar products.
We have a vector space V , and a function f : V → R. We’ll be interested in the change of function value df |x(δ) for change of input δ ∈ V , as well as the value of the scalar product δ, δ . All these quantities are deﬁned without any reference to coordinates; we’ll check now how their coordinate representations change with a change of basis.
As in Section 3.2.6 we have two bases A = (a1, .., an) and B = (b1, .., bn), and the transformation T that maps each ai to bi, i.e., B = T A. Given a vector δ we denote its coordinates in A by [δ]A, and its coordinates in B by [δ]B. Let T AA = B be the matrix representation of T in the old A coordinates (B contains the new basis vectors b as columns).

• We previously learned that

[δ]A = B[δ]B

(119)

that is, the matrix B carries new coordinates to old ones. These coordinates are said to be contra-variant: they transform ‘against’ the transformation of the basis vectors.

• We require that

df |x(δ) = [∂xf ]A[δ]A = [∂xf ]B[δ]B

(120)

must be invariant, i.e., the change of function value for δ should not depend on whether we compute it using A or B coordinates. It follows

[∂xf ]A[δ]A = [∂xf ]AB[δ]B = [∂xf ]B[δ]B [∂xf ]AB = [∂xf ]B

(121) (122)

that is, the matrix B carries old 1-form-coordinates to new 1-form-coordinates. Therefore, such 1-form-coordinates are called co-variant: they transform ‘with’ the transformation of basis vectors.

• What we just wrote for the derivative df |x(δ) we could equally write and argue for any 1-form v∗ ∈ V ∗; we always require that the value v∗(δ) is invariant.

• We also require that the scalar product δ, δ is invariant. Let δ, δ = [δ]A [G]A[δ]A = [δ]B [G]B[δ]B

(123)

40

Maths for Intelligent Systems, Marc Toussaint

where [G]A and [G]B are the 2-form-coordinates (metric tensor) in the old and new basis. It follows

[δ]A [G]A[δ]A = [δ]B B [G]AB[δ]B

(124)

[G]B = B [G]AB

(125)

that is, the matrix carries the old 2-form-coordinates to new ones. These coordinates are called twice co-variant.

Consider the following example: We have the function f : R2 → R, f (x) = x1 +x2. The

function’s partial derivative is of course

∂f ∂x

= (1

1).

Now let’s transform the coordinates

of the space: we introduce new coordinates (z1, z2) = (2x1, x2) or z = B-1x with B =

1 2
0

0


.

1

The

same

function,

written

in

the

new

coordinates,

is

f (z) =

1 2

z1

+

z2.

The

partial

derivatives

of

that

same

function,

written

in

these

new

coordinates,

is

∂f ∂z

=

(

1 2

1).

Generally, consider we have two kinds of mathematical objects and when we multiply them together this gives us a scalar. The scalar shouldn’t depend on any choice of coordinate system and is therefore invariant against coordinate transforms. Then, if one of the objects transforms in a covariant (“transforming with the transformation”) manner, the other object must transform in a contra-variant (“transforming contrary to the transformation”) manner to ensure that the resulting scalar is invariant. This is a general principle: whenever two things multiply together to give an invariant thing, one should transform co- the other contra-variant.

Let’s also check Wikipedia:

– “For a vector to be basis-independent, the components [=coordinates] of the vector must contra-vary with a change of basis to compensate. That is, the matrix that transforms the vector of components must be the inverse of the matrix that transforms the basis vectors. The components of vectors (as opposed to those of dual vectors) are said to be contravariant.
– For a dual vector (also called a covector) to be basis-independent, the components of the dual vector must co-vary with a change of basis to remain representing the same covector. That is, the components must be transformed by the same matrix as the change of basis matrix. The components of dual vectors (as opposed to those of vectors) are said to be covariant.”

Ordinary gradient descent of the form x ← x + α∇f adds objects of diﬀerent types: contra-variant coordinates x with co-variant partial derivatives ∇f . Clearly, adding two such diﬀerent types leads to an object who’s transformation under coordinate transforms is strange—and indeed the ordinary gradient descent is not invariant under transformations.

3.9.3 Steepest descent and the covariant gradient vector
Let’s deﬁne the steepest descent direction to be the one where, when you make a step of length 1, you get the largest decrease of f in its linear (=1st order Taylor) approximation.

Maths for Intelligent Systems, Marc Toussaint

41

Deﬁnition 3.20. Given f : V → R and a norm ||x||2 = x, x (or scalar product) deﬁned on V , we deﬁne the steepest descent vector δ∗ ∈ V as the vector:

δ∗

=

argmin df
δ

x(δ)

s.t.

||δ||2 = 1

(126)

Note that for this deﬁnition we need to assume we have a scalar product, otherwise the length=1 constraint is not deﬁned. Also recall that df |x(δ) = ∂xf (x)δ = ∇f (x) δ are equivalent notations.

Clearly, if we have coordinates in which the norm is Euclidean then

||δ||2 = δ δ ⇒ δ∗ ∝ −∇f (x)

(127)

However, if we have coordinates in which the metric is non-Euclidean, we have:

Theorem 3.3 (Steepest Descent Direction (Covariant gradient)). For a general scalar product v, w = v Gw (with metric tensor G), the steepest descent direction is

δ∗ ∝ −G-1∇f (x)

(128)

Proof: Let G = B B (Cholesky decomposition) and z = Bδ
δ∗ = argmin ∇f δ s.t. δ Gδ = 1
δ
= B-1 argmin ∇f B-1z s.t. z z = 1
z
∝ B-1[−B- ∇f ] = −G-1∇f

(129) (130) (131)

For a coordinate transformation B, recall that new metric becomes G˜ = B GB, and the new gradient ∇f = B ∇f . Therefore, the new steepest descent is

δ∗ = −[G˜]-1∇f = −B-1G-1B- B ∇f = −B-1G-1∇f

(132)

and therefore transformes like normal contra-variant coordinates of a vector.

There is an important special case of this, when f is a function over the space of probability distributions and G is the Fisher metric, which we’ll discuss later.

3.10 Examples and Exercises

3.10.1 Basis

Given a linear transform f : R2 → R2,

f (x)

=

Ax

=

7


−10


x

.

5 −8 

42

Maths for Intelligent Systems, Marc Toussaint

Consider the basis B =

  
1 , 2    

,

which we also simply refer to by the matrix B

=

1 1    

1

1

2


.

1

Given

a

vector

x

in

the

vector

space

R2,

we

denote

its

coordinates

in

basis

B

with xB.

(i) Show that x = BxB.
(ii) What is the matrix F B of f in the basis B, i.e., such that [f (x)]B = F BxB? Prove the general equation F B = B-1AB.
(iii) Provide F B numerically

Note

that

for

a

matrix

M

=

a


c

b


,

d

M -1 =

1 d 
ad−bc −c

−b  a

3.10.2 From the Robotics Course
You have a book lying on the table. The edges of the book deﬁne the basis B, the edges of the table deﬁne basis A. Initially A and B are identical (also their origins align). Now we rotate the book by 45◦ counter-clock-wise about its origin.
(i) Given a dot p marked on the book at position pB = (1, 1) in the book coordinate frame, what are the coordinates pA of that dot with respect to the table frame?
(ii) Given a point x with coordinates xA = (0, 1) in table frame, what are its coordinates xB in the book frame?
(iii) What is the coordinate transformation matrix from book frame to table frame, and from table frame to book frame?

3.10.3 Bases for Polynomials

Consider the set V of all polynomials

n i=0

αixi

of

degree

n,

where

x

∈

R

and

αi

∈

R

for each i = 0, . . . , n.

(i) Is this set of functions a vector space? Why? (ii) Consider two diﬀerent bases of this vector space:
A = {1, x, x2, . . . , xn} and
B = {1, 1 + x, 1 + x + x2, . . . , 1 + x + . . . + xn}.

Maths for Intelligent Systems, Marc Toussaint

43

Let f (x) = 1 + x + x2 + x3 be one element in V . (This function f is a vector in the vector space V , so from here on we refer to it as a vector rather than a function.) What are the coordinates [f ]A of this vector in basis A? What are the coordinates [f ]B of this vector in basis B?
(iii) What matrix IBA allows you to convert between coordinates [f ]A and [f ]B, i.e. [f ]B = IBA[f ]A? Which matrix IAB does the same in the opposite direction, i.e. [f ]A = IAB[f ]B? What is the relationship between IAB and IBA?
(iv) What does the diﬀerence between coordinates [f ]A − [f ]B represent?
(v) Consider the linear transform t : V → V that maps basis elements of A directly to basis elements of B:
1→1 x→1+x x2 → 1 + x + x2 ... xn → 1 + x + x2 + · · · + xn
• What is the matrix T A for the linear transform t in the basis A, i.e., such that [t(f )]A = T A[f ]A? (Basis A is used for both, input and output spaces.)
• What is the matrix T B for the linear transform t in the basis B, i.e., such that [t(f )]B = T B[f ]B? (Basis B is used for both, input and output spaces.)
• What is the matrix T BA if we use A as input space basis, and B as output space basis, i.e., such that [t(f )]B = T BA[f ]A?
• What is the matrix T AB if we use B as input space basis, and A as output space basis, i.e., such that [t(f )]A = T AB[f ]B?
• Show that T B = IBAT AIAB (cp. Exercise 1(b)). Also note that T AB = T AIAB and T BA = IBAT A.

3.10.4 Projections

(i) In Rn, a plane (through the origin) is typically described by the linear equation

c x=0,

(133)

where c ∈ Rn parameterizes the plane. Provide the matrix that describes the orthogonal projection onto this plane. (Tip: Think of the projection as I minus a rank-1 matrix.)

44

Maths for Intelligent Systems, Marc Toussaint

(ii) In Rn, let’s have k linearly independent {vi}ki=1, which form the matrix V = (v1, .., vk) ∈ Rn×k. Let’s formulate a projection using an optimality principle, namely,

α∗(x) = argmin ||x − V α||2 .
α∈Rk

(134)

Derive the equation for the optimal α∗(x) from the optimality principle.

(For information only: Note that V α =

k i=1

αivi

is

just

the

linear

combination

of vi’s with coeﬃcients α. The projection of a vector x is then x|| = V α∗(x).)

3.10.5 SVD

Consider the matrices

1 0 0

1 1 1 

A

=

0 0

−2 0

0 0

,

B

=

1 2

0 1

−1

0

 

000

01 2

(135)

(i) Describe their 4 fundamental spaces (dimensionality, possible basis vectors).
(ii) Find the SVD of A (using pen and paper only!)
(iii) Given an arbitrary input vector x ∈ R3, provide the linear transformation matrices PA and PB that project x into the input null space of matrix A and B, respectively.
(iv) Compute the pseudo inverse A†.
(v) Determine all solutions to the linear equations Ax = y and Bx = y with y = (2, 3, 0, 0). What is the more general expression for an arbitrary y?

3.10.6 Bonus: Scalar product and Orthogonality

(i) Show that f (x, y) = 2x1y1 − x1y2 − x2y1 + 5x2y2 is an scalar product on R2.

(ii) In the space of functions with the scalar product

f, g

=

2π 0

f (x)g(x)dx,

what

is the scalar product of sin(x) with sin(2x)? (Graphical argument is ok.)

(iii) What property does a matrix M have to satisfy in order to be a valid metric tensor, i.e. such that x M y is a valid scalar product?

Maths for Intelligent Systems, Marc Toussaint

45

3.10.7 Eigenvectors

(i) A symmetric matrix A ∈ Rn×n is called positive semideﬁnite (PSD) if x Ax ≥ 0, ∀x ∈ Rn. (PSD is usually only used with symmetric matrices.) Show that all
eigenvalues of a PSD matrix are non-negative.

(ii) Show that if v is an eigenvector of A with eigenvalue λ, then v is also an eigenvector of Ak for any positive integer k. What is the corresponding eigenvalue?

(iii) Let v be an eigenvector of A with eigenvalue λ and w an eigenvector of A with a diﬀerent eigenvalue µ = λ. Show that v and w are orthogonal.
(iv) Suppose A ∈ Rn×n has eigenvalues λ1, . . . , λn ∈ R. What are the eigenvalues of A + αI for α ∈ R and I an identity matrix?
(v) Assume A ∈ Rn×n is diagonalizable, i.e., it has n linearly independent eigenvectors, each with a diﬀerent eigenvalue. Initialize x ∈ Rn as a random normalized vector and iterate the two steps
1 x ← Ax , x ← x
||x||

Prove that (under certain conditions) these iterations converge to the eigenvector x with a largest (in absolute terms |λi|) eigenvalue of A. How fast does this converge? In what sense does it converge if the largest eigenvalue is negative? What if eigenvalues are not diﬀerent? Other convergence conditions?

(vi) Let A be a positive deﬁnite matrix with λmax its largest eigenvalue (in absolute terms |λi|). What do we get when we apply power iteration method to the matrix B = A − λmaxI? How can we get the smallest eigenvalue of A?
(vii) Consider the following variant of the previous power iteration:

1

1

z ← Ax , λ ← x z , y ← (λI − A)y , x ← z , y ← y .

||z||

||y||

If A is a positive deﬁnite matrix, show that the algorithm can give an estimate of the smallest eigenvalue of A.

3.10.8 Covariance and PCA

Suppose we’re given a collection of zero-centered data points D = {xi}Ni=1, with each xi ∈ Rn. The covariance matrix is deﬁned as

1N

1

C= n

xixi

=

X n

X

i=1

46

Maths for Intelligent Systems, Marc Toussaint

where (consistent to ML lecture convention) the data matrix X contains each xi as a row, i.e., X = (x1, .., xN ).
If we project D onto some unit vector v ∈ Rn, then the variance of the projected data points is v Cv. Show that the direction that maximizes this variance is the largest eigenvector of C. (Hint: Expand v in terms of the eigenvector basis of C and exploit the constraint v v = 1.)

3.10.9 Bonus: RKHS
In machine learning we often work in spaces of functions called Reproducing Kernel Hilbert Spaces. These spaces are constructed from a certain type of function called the kernel. The kernel k : Rd × Rd → R takes two d-dimensional inputs k(x, x ), and from the kernel we construct a basis for the space of function, namely B = {k(x, ·)}x∈Rd . Note that this is a set of inﬁnite element: each x ∈ Rd adds a basis function k(x, ·) to the basis B. The scalar product between two basis functions kx = k(x, ·) and kx = k(x , ·) is deﬁned to be the kernel evaluation itself: kx, kx = k(x, x ). The kernel function is therefore required to be a positive deﬁnite function so that it deﬁnes a viable scalar product.
(i) Show that for any function f ∈ span B it holds
f, kx = f (x)
(ii) Assume we only have a ﬁnite set of points {D = {xi}ni=1}, which deﬁnes a ﬁnite basis {kxi }ni=1 ⊂ B. This ﬁnite function basis spans a subspace FD = span{kxi : xi ∈ D} of the space of all functions. For a general function f , we decompose it f = fs + f⊥ with fs ∈ FD and ∀g ∈ FD : f⊥, g = 0, i.e., f⊥ is orthogonal to FD. Show that for every xi ∈ D:
f (xi) = fs(xi)
(Note: This shows that the function values of any function f at the data points D only depend on the part fs which is inside the spann of {kxi : xi ∈ D}. This implies the so-called representer theorem, which is fundamental in kernel machines: A loss can only depend on function values f (xi) at data points, and therefore on fs. The part f⊥ can only increase the complexity (norm) of a function. Therefore, the simplest function to optimize any loss will have f⊥ = 0 and be within span{kxi : xi ∈ D}.)
(iii) Within span{kxi : xi ∈ D}, what is the coordinate representation of the scalar product?

Maths for Intelligent Systems, Marc Toussaint

47

4 Optimization

4.1 Downhill algorithms for unconstrained optimization
We discuss here algorithms that have one goal: walk downhill as quickly as possible. These target at eﬃciently ﬁnding local optima—in constrast to global optimization methods, which try to ﬁnd the global optimum.
For such downhill walkers, there are two essential things to discuss: the stepsize and the step direction. When discussing the stepsize we’ll hit on topics like backtracking line search, the Wolfe conditions and its implications in a basic convergence proof. The discussion of the step direction will very much circle around Newton’s method and thereby also cover topics like quasi-Newton methods (BFGS), Gauss-Newton, covariant and conjugate gradients.

4.1.1 Why you shouldn’t trust the magnitude of the gradient
Consider the following 1D function and naive gradient descent x ← x − α∇f for some ﬁxed and small α.

large gradient large step?

small gradient small step?

• In plateaus we’d make small steps, at steep slopes (here close to the optimum) we make huge steps, very likely overstepping the optimum. In fact, for some α the algorithm might indeﬁnitely loop a non-sensical sequence of very slowly walking left on the plateau, then accellerating, eventually overstepping the optimum, then being thrown back far to the right again because of the huge negative gradient on the left.
• Generally, never trust an algorithm that depends on the scaling—or choice of units—of a function! An optimization algorithm should be invariant on whether you measure costs in cents or Euros! Naive gradient descent x ← x − α∇f is not!

48

Maths for Intelligent Systems, Marc Toussaint

As a conclusion, the gradient ∇f gives a reasonable descent direction, but its magnitude is really arbitrary and no good indication of a good stepsize. Therefore, it often makes sense to just compute the step direction

1

δ=−

∇f (x)

|∇f (x)|

(136)

and iterate x ← x + αδ for some appropriate stepsize.

4.1.2 Ensuring monotone and suﬃcient decrease: Backtracking line search, Wolfe conditions, & convergence

The ﬁrst idea is simple: If a step would increase the objective value, reduce the stepsize.

We typically use multiplicative stepsize adaptations: Reduce α ←

− α

α

with

− α

≈

0.5;

and increase α ←

+α α with

+ α

≈ 1.2.

A

simple

monotone

gradient

descent

algorithm

reads as follows (the blue part is explained later). Here, the step vector δ is always

Algorithm 2 Plain gradient descent with backtracking line search

Input: initial x ∈ Rn, functions f (x) and ∇f (x), tolerance θ, parameters (defaults:

+ α

=

1.2,

− α

=

0.5,

δmax

=

∞,

ls = 0.01)

1: initialize stepsize α = 1

2: repeat

3:

δ

←

−

∇f |∇f

(x) (x)|

// (alternative: δ = −∇f (x))

4: while f (x + αδ) > f (x)+ ls∇f (x) (αδ) do

5:

α ← −α α

6: end while

// backtracking line search // decrease stepsize

7: x ← x + αδ

8:

α ← min{

+ α

α,

δmax

}

9: until |αδ| < θ

// increase stepsize // perhaps for 10 iterations in sequence

normalized and α is adapted on the ﬂy; decreasing when f (x + αδ) is not suﬃciently smaller than f (x).

This suﬃciently smaller is described by the blue part and is called the (1st) Wolfe condition

f (x + αδ) > f (x) + ls∇f (x) (αδ) .

(137)

Figure 3 illustrates this. Note that ∇f (x) (αδ) is a negative value and describes how much the objective would decrease if f was linear. But f is of course not linear; we cannot expect that a step would really decrease f that much. Instead we require that it decreases by a fraction of this expectation. ls describes this fraction and is typically chosen very moderate, e.g. ls ∈ [0.01, 0.1]. So, the Wolfe conditions requires that f descreases by the ls-fraction of what it would decrease if f was linear. Note that for α → 0 the Wolfe condition will always be fulﬁlled for smooth functions, because f “becomes locally linear”.

Maths for Intelligent Systems, Marc Toussaint

49

Figure 3: The 1st Wolfe condition: f (x) + ∇f (x) (αδ) is the tangent, which describes the expected decrease of f (x + αδ) if f was linear. We cannot expect this to be the case; so f (x + αδ) > f (x) + ls∇f (x) (αδ) weakens this condition.

You’ll proove the following theorem in the exercises. It is fundamental to convex optimization and proves that Alg 2 is eﬃcient for convex objective functions:

Theorem 4.1 (Exponential convergence on convex functions). Let f : Rν → R be an objective function where the eigenvalues λ of the Hessian ∇2f (x) are for any x.

Further, assume that Hessian eigenvalues λ are lower bounded by m > 0 and upper

bounded by M > m, with m, M ∈ R, at any location x ∈ Rn. (f is convex.) Then

Algorithm

2

converges

exponentially

with

convergence

rate

(1

−

2

m M

ls

−α ) to the

optimum.

But even if your objective function is not globally convex, Alg 2 is an eﬃcient downhill walker, and once it reaches a convex region it will eﬃciently walk into its local minimum.
For completeness, there is a second Wolfe condition,

|∇f (x + αδ) δ| ≤ b|∇f (x) δ| ,

(138)

which states that the gradient magnitude should have decreased suﬃciently. We do not use it much.

4.1.3 The Newton direction

We already discussed the steepest descent direction −G-1∇f (x) if G is a metric tensor. Let’s keep this in mind!

The original Newton method is a method to ﬁnd the root (that is, zero point) of a

function f (x).

In

1D

it

iterates

x

←

x−

f f

(x) (x)

,

that

is,

it

uses

the

gradient

f

to

estimate where the function might cross the x-axis. To ﬁnd an minimum or maximum

50

Maths for Intelligent Systems, Marc Toussaint

of f we want to ﬁnd the root of its gradient. For x ∈ Rn the Newton method iterates

x ← x − ∇2f (x)-1∇f (x) .

(139)

Note that the Newton step δ = −∇2f (x)-1∇f (x) is the solution to

min

f (x) +

∇f (x)

δ

+

1 δ

∇2f (x)δ

.

δ

2

(140)

So the Newton method can also be viewed as 1) compute the 2nd-order Taylor approximation to f at x, and 2) jump to the optimum of this approximation.
Note:

• If f is just a 2nd-order polynomial, the Newton method will jump to the optimum in just one step.

• Unlike the gradient magnitude |∇f (x)|, the magnitude of the Newton step δ is very meaningful. It is scale invariant! If you’d rescale f (trade cents by Euros), δ is unchanged. |δ| is the distance to the optimum of the 2nd-order Taylor.

• Unlike the gradient ∇f (x), the Newton step δ is truely a vector! The vector itself is invariant under coordinate transformations; the coordinates of δ transforms contra-variant, as it is supposed to for vector coordinates.

• The hessian as metric, and the Newton step as steepest descent: Assume that the hessian H = ∇2f (x) is pos-def. Then it fulﬁls all necessary conditions
to deﬁne a scalar product v, w = ij viwjHij, where H plays the role of the metric tensor. If H was the space’s metric, then the steepest descent direction is −H-1∇f (x), which is the Newton direction!

Another way to understand the same: In the 2nd-order Taylor approximation f (x+

δ)

≈

f (x)+∇f (x)

δ

+

1 2

δ

Hδ

the Hessian plays the role

of a

metric tensor.

Or:

we

may think of the function f as being an isometric parabola f (x + δ) ∝ δ, δ , but

we’ve chosen coordinates where v, v = v Hv and the parabola seems squeezed.

Note that this discussion only holds for pos-dev hessian.

A robust Newton method is the core of many solvers, see Algorithm 3. We do backtracking line search along the Newton direction, but with maximal step size α = 1 (the full Newton step).
We can additionally add and adapt damping to gain more robustness. Some notes on the λ:

• In Alg 3, the ﬁrst line chooses λ to ensure that (∇2f (x) + λI) is indeed pos-dev— and a Newton step actually decreases f instead of seeking for a maximum. There would be other options: instead of adding to all eigenvalues we could only set the negative ones to some λ > 0.

Maths for Intelligent Systems, Marc Toussaint

51

Algorithm 3 Newton method

Input: initial x ∈ Rn, functions f (x), ∇f (x), ∇2f (x), tolerance θ, parameters (de-

faults:

+ α

=

1.2,

− α

=

0.5,

+ λ

=

− λ

=

1,

ls = 0.01)

1: initialize stepsize α = 1

2: repeat 3: choose λ > −(minimal eigenvalue of ∇2f (x)) 4: compute δ to solve (∇2f (x) + λI) δ = −∇f (x)

5: while f (x + αδ) > f (x) + ls∇f (x) (αδ) do

6:

α ← −α α

7:

optionally: λ ← +λ λ and recompute d

8: end while

// line search // decrease stepsize // increase damping

9: x ← x + αδ

10:

α ← min{

+ α

α,

1}

11:

optionally: λ ←

− λ

λ

12: until ||αδ||∞ < θ

// step is accepted // increase stepsize // decrease damping

• δ solves the problem

min

∇f (x)

δ

+

1 δ

∇2f (x)δ

+

1 λδ2)

.

δ

2

2

(141)

So, we added a squared potential λδ2 to the local 2nd-order Taylor approximation. This is like introducing a squared penalty for large steps!

• Trust region method: Let’s consider a diﬀerent mathematical program over the step:

min ∇f (x)

δ

+

1 δ

∇2f (x)δ

s.t.

δ2 ≤ β

δ

2

(142)

This problem wants to ﬁnd the minimum of the 2nd-order Taylor (like the Newton step), but constrained to a stepsize no larger than β. This β deﬁnes the trust region: The region in which we trust the 2nd-order Taylor to be a reasonable enough approximation.
Let’s solve this using Lagrange parameters (as we will learn it later): Let’s assume the inequality constraint is active. Then we have

L(δ, λ)

=

∇f (x)

δ

+

1 δ

∇2f (x)δ + λ(δ2

− β)

2

∇δL(δ, λ) = ∇f (x) + δ (∇2f (x) + 2λI)

(143) (144)

Setting this to zero gives the step δ = −(∇2f (x) + 2λI)-1∇f (x).
Therefore, the λ can be viewed as dual variable of a trust region method. There is no analytic relation between β and λ; we can’t determine λ directly from β. We could use a constrained optimization method, like primal-dual Newton, or

52

Maths for Intelligent Systems, Marc Toussaint

Augmented Lagrangian approach to solve for λ and δ. Alternatively, we can always increase λ when the computed steps are too large, and decrease if they are smaller than β—the Augmented Lagrangian update equations could be used to do such an update of λ.
• The λδ2 term can be interpreted as penalizing the velocity (stepsizes) of the algorithm. This is in analogy to a damping, such as “honey pot damping”, in physical dynamic systems. The parameter λ is therefore also called damping parameter. Such a damped (or regularized) least-squares method is also called Levenberg-Marquardt method.
• For λ → ∞ the step direction δ becomes aligned with the plain gradient direction −∇f (x). This shows that for λ → ∞ the Hessian (and metric deformation of the space) becomes less important and instead we’ll walk orthogonal to the iso-lines of the function.
• The λ term makes the δ non-scale-invariant! δ is not anymore a proper vector!

4.1.4 Gauss-Newton: a super important special case

A special case that appears really a lot in intelligent systems is the Gauss-Newton case: Consider an objective function of the form

f (x) = φ(x) φ(x) = φi(x)2
i

(145)

where we call φ(x) the cost features. This is also called a sum-of-squares problem, here a sum of cost feature squares. We have

∂ ∇f (x) = 2 φ(x) φ(x)
∂x

∇2f (x) = 2 ∂ φ(x) ∂x

∂ φ(x) + 2φ(x)
∂x

∂2 ∂x2 φ(x)

(146) (147)

The Gauss-Newton method is the Newton method for f (x) = φ(x) φ(x) while approximating ∇2φ(x) ≈ 0. That is, it computes approximate Newton steps

∂ δ = −( φ(x)

∂ φ(x) + λI)-1 ∂ φ(x) φ(x) .

∂x ∂x

∂x

Note:

(148)

•

The

approximate

Hessian

2

∂ ∂x

φ(x)

∂ ∂x

φ(x)

is

always

semi-pos-def !

Therefore,

no problems arise with negative hessian eigenvalues.

• The approximate Hessian only requires the ﬁrst-order derivatives of the cost features. There is no need for computationally expensive hessians of φ.

Maths for Intelligent Systems, Marc Toussaint

53

Algorithm 4 Newton method – practical version realized in rai

Input: initial x ∈ Rn, functions f (x), ∇f (x), ∇2f (x)

Input: stopping tolerances θx, θf

Input:

parameters

+ α

=

1.2,

− α

=

0.5,

λ0

=

0.01,

+ λ

=

− λ

=

1,

ls = 0.01

Input: maximal stepsize δmax ∈ R, optionally lower/upper bounds x, x ∈ Rn

1: initialize stepsize α = 1, λ = λ0

2: repeat

3: compute smallest eigenvalue σmin of ∇2f (x) + λI

4: if σmin > 0 is suﬃciently posititive then

5:

compute δ to solve (∇2f (x) + λI) δ = −∇f (x)

6: else

7:

Option 1: λ ← 2λ − σmin and goto line 3

// increase regularization

8:

Option 2: δ ← −δmax ∇f (x)/|∇f (x)|

// gradient step of length δmax

9: end if

10: if ||δ||∞ > δmax: δ ← (δmax/||δ||∞) δ

// cap δ length

11: y ← BoundClip(x + αδ, x, x)

12: while f (y) > f (x) + ls∇f (x) (y − x) do

// bound-projected line search

13:

α ← −α α

// decrease stepsize

14:

(unusual option: λ ← +λ λ and goto line 3)

// increase damping

15:

y ← BoundClip(x + αδ, x, x)

16: end while

17:

xold ← x

18:

x←y

// step is accepted

19:

α ← min{

+ α

α,

1}

// increase stepsize

20: (unusual option: λ ← −λ λ)

// decrease damping

21: until ||xold − x||∞ < θx repeatedly, or f (xold) − f (x) < θf repeatedly

22: procedure BoundClip(x, x, x)
23: ∀i : xi ← min{xi, xi} , xi ← max{xi, xi} 24: end procedure

54

Maths for Intelligent Systems, Marc Toussaint

• The objective f (x) can be interpreted as just the Euclidean norm f (φ) = φ φ but pulled back into the x-space. More precisely: Consider a mapping φ : Rn → Rm and a general scalar product ·, · φ in the output space. In diﬀerential geometry there is the notion of a pull-back of a metric, that is, we deﬁne a scalar product
·, · x in the input space as

x, y x = dφ(x), dφ(y) φ

(149)

where dφ is the diﬀerential of φ (a Rm-valued 1-form). Assuming φ-coordinates such that the metric tensor of ·, · φ is Euclidean, we have

∂

∂

x, x x =

dφ(x), dφ(x)

φ

=

φ(x) ∂x

φ(x) ∂x

(150)

and therefore, the approximate Hessian is the pullback of a Euclidean cost feature metric, and x, x x approximates the 2-order polynomial term of f (x), with the non-constant (i.e., Riemannian) pull-back metric ·, · x.

4.1.5 Quasi-Newton & BFGS: approximating the hessian from gradient observations

To apply full Newton methods we need to be able to compute f (x), ∇f (x), and ∇2f (x) for any x. However, sometimes, computing ∇2f (x) is not possible, e.g., because we can’t derive an analytic expression for ∇2f (x), or it would be to expensive to compute the hessian exactly, or even to store it in memory—especially in very high-dimensional spaces. In such cases it makes sense to approximate ∇2f (x) or ∇2f (x)-1 with a low-rank approximation.
Assume we have computed ∇f (x1) and ∇f (x2) at two diﬀerent points x1, x2 ∈ Rn. We deﬁne

y = ∇f (x2) − ∇f (x1) , δ = x2 − x1 .

(151)

From this we may wish to ﬁnd some approximate Hessian matrix H or H-1 that fulﬁls

H δ =! y or δ =! H-1y

(152)

The ﬁrst equation is called secant equation. Here are guesses of H and H-1:

yy H=

or H-1 = δδ

yδ

δy

(153)

Convince yourself that these choices fulﬁl the respective desired relation above. However, these choices are under-determined. There exist many alternative H or H-1 that would be consistent with the observed change in gradient. However, given our understanding of the structure of matrices it is clear that these choices are the lowest rank solutions, namely rank 1.

Maths for Intelligent Systems, Marc Toussaint

55

Broyden-Fletcher-Goldfarb-Shanno (BFGS): An optimization algorithm computes ∇f (xk) at a series of points x0:K . We incrementally update our guess of H-1 with an update equation

H-1 ←

yδ I−

H -1

yδ I−

δδ +,

δy

δy δy

(154)

which is equivalent to (using the Sherman-Morrison formula)

Hδδ H yy

H ←H−

δT Hδ

+ yδ

Note:

(155)

•

If

H -1

is

initially

zero,

this

update

will

assign

H -1

←

δδ δy

,

which

is

the

minimal

rank 1 update we discussed above.

• If H-1 is previously non-zero, the red part “deletes certain dimensions” from

H-1. More precisely, note that

I

−

yδ δy

y = 0, that is, this construction deletes

span{y} from its input space. Therefore, the red part gives zero when multiplied

with y; and it is guaranteed that the resulting H-1 fulﬁls H-1y = δ.

The BFGS algorithms uses this H-1 instead of a precise ∇2f (x)-1 to compute the steps in a Newton method. All we said about line search and Levenberg-Marquardt damping is unchanged. 6
In very high-dimensional spaces we do not want to store H-1 densely. Instead we use a compressed storage for low-rank matrices, e.g., storing vectors {vi} such that H-1 =
i vivi . Limited memory BFGS (L-BFGS) makes this more memory eﬃcient: it limits the rank of the H-1 and thereby the used memory. I do not know the details myself, but I assume that with every update it might aim to delete the lowest eigenvalue to keep the rank constant.

4.1.6 Conjugate Gradient
The Conjugate Gradient Method is a method for solving large linear eqn. systems Ax + b = 0. We only mention its extension for optimizing nonlinear functions f (x).
As above, assume that we evaluted ∇f (x1) and ∇f (x2) at two diﬀerent points x1, x2 ∈ Rn. But now we make one more assumption: The point x2 is the minimum of a line search from x1 along the direction δ1. This latter assumption is quite optimistic: it
6Taken from Peter Blomgren’s lecture slides: terminus.sdsu.edu/SDSU/Math693a_f2013/ Lectures/18/lecture.pdf This is the original Davidon-Fletcher-Powell (DFP) method suggested by W.C. Davidon in 1959. The original paper describing this revolutionary idea – the ﬁrst quasi-Newton method – was not accepted for publication. It later appeared in 1991 in the ﬁrst issue the the SIAM Journal on Optimization.

56

Maths for Intelligent Systems, Marc Toussaint

assumes we did perfect line search. But it gives great information: The iso-lines of f (x) at x2 are tangential to δ1.
In this setting, convince yourself of the following: Ideally each search direction should be orthogonal to the previous one—but not orthogonal in the conventional Euclidean sense, but orthogonal w.r.t. the Hessian H. Two vectors δ and δ are called conjugate w.r.t. a metric H iﬀ d Hd = 0. Therefore, subsequent search directions to be conjugate to each other.
Conjugate gradient descent does the following:

Algorithm 5 Conjugate gradient descent

Input: initial x ∈ Rn, functions f (x), ∇f (x), tolerance θ Output: x

1: initialize descent direction d = g = −∇f (x)

2: repeat

3: α ← argminα f (x + αd) 4: x ← x + αd

// line search

5: g ← g, g = −∇f (x)

// store and compute grad

6:

β ← max

g

(g−g gg

),

0

7: d ← g + βd

// conjugate descent direction

8: until |∆x| < θ

• The equation for β is by Polak-Ribi`ere: On a quadratic function f (x) = x Hx this leads to conjugate search directions, d Hd = 0.
• Intuitively, β > 0 implies that the new descent direction always adds a bit of the old direction. This essentially provides 2nd order information.
• For arbitrary quadratic functions CG converges in n iterations. But this really only works with perfect line search.
4.1.7 Rprop*
Read through Algorithm 6. Notes on this:
• Stepsize adaptation is done in each coordinate separately ! • The algorithm not only ignores |∇f | but also its exact direction! Only the gradient
signs in each coordinate are relevant. Therefore, the step directions may diﬀer up to < 90◦ from −∇f . • It often works surprisingly eﬃcient and robust.

Maths for Intelligent Systems, Marc Toussaint

57

Algorithm 6 Rprop

Input: initial x ∈ Rn, function f (x), ∇f (x), initial stepsize α, tolerance θ

Output: x

1: initialize x = x0, all αi = α, all gi = 0 2: repeat

3: g ← ∇f (x)

4: x ← x

5: for i = 1 : n do

6:

if gigi > 0 then

7:

αi ← 1.2αi

8:

xi ← xi − αi sign(gi)

9:

gi ← gi

10:

else if gigi < 0 then

11:

αi ← 0.5αi

12:

xi ← xi − αi sign(gi)

13:

gi ← 0

14:

else

// same direction as last time // change of direction
// force last case next time

15:

xi ← xi − αi sign(gi)

16:

gi ← gi

17:

end if

18:

optionally: cap αi ∈ [αmin xi, αmax xi]

19: end for

20: until |x − x| < θ for 10 iterations in sequence

• If you like, have a look at: Christian Igel, Marc Toussaint, W. Weishui (2005): Rprop using the natural gradient compared to Levenberg-Marquardt optimization. In Trends and Applications in Constructive Approximation. International Series of Numerical Mathematics, volume 151, 259-272.
4.2 The general optimization problem – a mathematical program
Deﬁnition 4.1. Let x ∈ Rn, f : Rn → R, g : Rn → Rm, h : Rn → Rl. An optimization problem, or mathematical program, is
min f (x)
x
s.t. g(x) ≤ 0 , h(x) = 0
We typically at least assume f, g, h to be diﬀerentiable or smooth. Get an intuition about this problem formulation by considering the following examples. Always discuss where is the optimum, and at the optimum, how the objective f pulls at the point, while the constraints g or h push against it.

58

Maths for Intelligent Systems, Marc Toussaint

Figure 4: 2D example: f (x, y) = −x, pulling constantly to the right; three inequality constraints, two active, one inactive. The “pull/push” vectors fulﬁl the stationarity condition ∇f + λ1∇g1 + λ2∇g2 = 0.
For the following examples, draw the situation and guess, without much maths, where the optimum is:
• A 1D example: x ∈ R, h(x) = sin(x), g(x) = x2/4 − 1, f some non-linear function.
• 2D example: f (x, y) = x (intuition: the objective is constantly pulling to the left), h(x, y) = 0 is some non-linear path in the plane → the optimum is at the leftmost tangent-point of h. Tangent-point means that the tangent of h is vertical. h pushes to the right (always orthogonal to the zero-line of h).
• 2D example: f (x, y) = x, g(x, y) = y2 − x − 1. The zero-line of g is a parabola towards the right. The objective f pulls into this parabola; the optimum is in the ’bottom’ of the parabola, at (−1, 0).
• 2D example: f (x, y) = x, g(x, y) = x2 + y2 − 1. The zero-line of g is a circle. The objective f pulls to the left; the optimum is at the left tangent-point of the circle, at (−1, 0).
• Figure 4

4.3 The KKT conditions

Theorem 4.2 (Karush-Kuhn-Tucker conditions). Given a mathematical program,

x optimal ⇒ ∃λ ∈ Rm, κ ∈ Rl s.t.

m

l

∇f (x) + λi∇gi(x) + κj∇hj(x) = 0

i=1

j=1

∀j : hj(x) = 0 , ∀i : gi(x) ≤ 0

(stationarity) (primal feasibility)

Maths for Intelligent Systems, Marc Toussaint

59

∀i : λi ≥ 0 ∀i : λigi(x) = 0

(dual feasibility) (complementarity)

Note that these are, in general, only necessary conditions. Only in special cases, e.g. convex, these are also suﬃcient.
These conditions should be intuitive in the previous examples:

• The ﬁrst condition describes the “force balance” of the objective pulling and the active constraints pushing back. The existance of dual parameters λ, κ could implicitly be expressed by stating

∇f (x) ∈ span({∇g1..m, ∇h1..l})

(156)

The speciﬁc values of λ and κ tell us, how strongly the constraints push against the objective, e.g., λi|∇gi| is the force excerted by the ith inequality.

• The fourth condition very elegantly describes the logic of inequality constraints being either active (λi > 0, gi = 0) or inactive (λi = 0, gi ≤ 0). Intuitively it says: An inequality can only push at the boundary, where gi = 0, but not inside the feasible region, where gi < 0. The trick of using the equation λigi = 0 to express this logic is beautiful, especially when later we discuss a case which relaxes this strict logic to λigi = −µ for some small µ—which roughly means that inequalities may push a little also inside the feasible region.

• Special case m = l = 0 (no constraints). The ﬁrst condition is just the usual ∇f (x) = 0.

• Discuss the previous examples as special cases; and how the force balance is met.

4.4 Unconstrained problems to tackle a constrained problem
Assume you’d know about basic unconstrained optimization methods (like standard gradient descent or the Newton method) but nothing about constrained optimization methods. How would you solve a constrained problem? Well, I think you’d very quickly have the idea to introduce extra cost terms for the violation of constraints—a million people have had this idea and successfully applied it in practice.
In the following we deﬁne a new cost function F (x), which includes the objective f (x) and some extra terms.
Deﬁnition 4.2 (Log barrier, squared penalty, Lagrangian, Augmented Lagrangian).

Fsp(x; ν, µ) = f (x) + ν hj(x)2 + µ [gi(x) > 0] gi(x)2 (sqr. penalty)

j

i

60

Maths for Intelligent Systems, Marc Toussaint

Figure 5: The function −µ log(−g) (with g on the “x-axis”) for various µ. This is always undeﬁned (“∞”) for g > 0. For µ → 0 this becomes the hard step function.

Flb(x; µ) = f (x) − µ log(−gi(x))
i

L(x, λ, κ) = f (x) + κjhj(x) + λigi(x)

j

i

Lˆ(x) = f (x) + κjhj(x) + λigi(x) +

j

i

+ ν hj(x)2 + µ [gi(x) > 0] gi(x)2

j

i

(log barrier) (Lagrangian)
(Aug. Lag.)

• The squared penalty method is straight-forward if we have an algorithm to minimize F (x). We initialize ν = µ = 1, minimize F (x), then increase ν, µ (multiply with a number > 1) and iterate. For ν, µ → ∞ we retrieve the optimum.
• The log barrier method (see Fig. 5) does exactly the same, except that we decrease µ towards zero (multiply with a numer < 1 in each iteration). Note that we need a feasible initialization x0, because otherwise the barriers are ill-deﬁned! The whole algorithm will keep the temporary solutions always inside the feasible regions (because the barriers push away from the constraints). That’s why it is also called interior point method.
• The Lagrangian is a function L(x, λ, κ) which has the gradient

∇L(x, λ, κ) = ∇f (x) + λi∇gi(x) + κj∇hj(x) .

i

j

(157)

That is, ∇L(x, λ, κ) = 0 is our ﬁrst KKT condition! In that sense, the additional terms in the Lagrangian generate the push forces of the constraints. If we knew the correct λ’s and κ’s beforehand, then we could ﬁnd the optimal x by the unconstrained problem minx L(x, λ, κ) (if this has a unique solution).

Maths for Intelligent Systems, Marc Toussaint

61

• The Augmented Lagrangian Lˆ is a function that includes both, squared penalties, and Lagrangian terms that push proportional to λ, κ. The Augmented Lagrangian method is an iterative algorithm that, while running, ﬁgures out how strongly we need to push to ensure that the ﬁnal solution is exactly on the constraints, where all squared penalties will anyway be zero. It does not need to increase ν, µ and still converges to the correct solution.

4.4.1 Augmented Lagrangian*

This is not a main-stream algorithm, but I like it. See Toussaint (2014).
In the Augmented Lagrangian Lˆ, the solver has two types of knobs to tune: the strenghts of the penalties ν, µ and the strengths of the Lagrangian forces λ, κ. The trick is conceptually easy:

• Initially we set λ, κ = 0 and ν, µ = 1 (or some other constant). In the ﬁrst iteration, the unconstrained solver will ﬁnd x = minx Lˆ(x); the objective f will
typically pull into the penalizations.

• For the second iteration we then choose parameters λ, κ that try to avoid that we will be pulled into penalizations the next time. Let’s update

κj ← κj + 2µhj(x ) , λi ← max(λi + 2µgi(x ), 0).

(158)

Note that 2µhj(x ) is the force (gradient) of the equality penalty at x ; and max(λi + 2µgi(x ), 0) is the force of the inequality constraint at x . What this update does is: it analyzes the forces excerted by the penalties, and translates them to forces excerted by the Lagrange terms in the next iteration. It tries to trade the penalizations for the Lagrange terms.
More rigorously, observe that, if f, g, h are linear and the same constraints are active in two consecutive iterations, then this update will guarantee that all penalty terms are zero in the second iteration, and therefore the solution fulﬁls the ﬁrst KKT condition (Toussaint, 2014). See also the respective exercise.

4.5 The Lagrangian
4.5.1 How the Lagrangian relates to the KKT conditions
The Lagrangian L(x, κ, λ) = f + κ h + λ g has a number of properties that relates it to the KKT conditions:
(i) Requiring a zero-x-gradient, ∇xL = 0, implies the 1st KKT condition. (ii) Requiring a zero-κ-gradient, 0 = ∇κL = h, implies primal feasibility (the 2nd KKT
condition) w.r.t. the equality constraints.

62

Maths for Intelligent Systems, Marc Toussaint

(iii) Requiring that L is maximized w.r.t. λ ≥ 0 is related to the remaining 2nd and 4th KKT conditions:

f (x) if g(x) ≤ 0

max L(x, λ) =

λ≥0

∞ otherwise

(159)

λ = argmax L(x, λ) ⇒
λ≥0

λi = 0

if gi(x) < 0 (160)

0 = ∇λi L(x, λ) = gi(x) otherwise

This implies either (λi = 0 ∧ gi(x) < 0) or gi(x) = 0, which is equivalent to the complementarity and primal feasibility for inequalities.

These three facts show how tightly the Lagrangian is related to the KKT conditions.
To simplify the discussion let us assume only inequality constraints from now on. Fact
(i) tells us that if we minx L(x, λ), we reproduce the 1st KKT condition. Fact (iii) tells us that if we maxλ≥0 L(x, λ), we reproduce the remaining KKT conditions. Therefore, the optimal primal-dual solution (x∗, λ∗) can be characterized as a saddle point of the
Lagrangian. Finding the saddle point can be written in two ways:

Deﬁnition 4.3 (primal and dual problem).

min max L(x, λ)
x λ≥0
max min L(x, λ)
λ≥0 x l(λ) (dual function)

(primal problem) (dual problem)

Convince yourself, using 159, that the ﬁrst expression is indeed the original primal problem minx f (x) s.t. g(x) ≤ 0 .

What can we learn from this? The KKT conditions state that, at an optimum, there exist some λ, κ. This existance statement is not very helpful to actually ﬁnd them. In contrast, the Lagrangian tells us directly how the dual parameters can be found: by maximizing w.r.t. them. This can be exploited in several ways:

4.5.2 Solving mathematical programs analytically, on paper.
Consider the problem min x2 s.t. x1 + x2 = 1 .
x∈R2
We can ﬁnd the solution analytically via the Lagrangian: L(x, κ) = x2 + κ(x1 + x2 − 1)

(161) (162)

Maths for Intelligent Systems, Marc Toussaint

63

0

=

∇xL(x,

κ)

=

2x

+


κ 1   1  

⇒

x1 = x2 = −κ/2

0 = ∇κL(x, κ) = x1 + x2 − 1 = −κ/2 − κ/2 − 1 ⇒ ⇒x1 = x2 = 1/2

κ = −1

(163)
(164) (165)

Here we ﬁrst formulated the Lagrangian. In this context, κ is often called Lagrange multiplier, but I prefer the term dual variable. Then we ﬁnd a saddle point of L by requiring 0 = ∇xL(x, κ), 0 = ∇κL(x, κ). If we want to solve a problem with an inequality constrained, we do the same calculus for both cases: 1) the constraint is active (handled like an equality constrained), and 2) the constrained is inactive. Then we check if the inactive case solution is feasible, or the active case is dual-feasible (λ ≥ 0). Note that if we have m inequality constraints we have to analytically evaluate every combination of constraints being active/inactive—which are 2m cases. This already hints at the fact that a real diﬃculty in solving mathematical programs is to ﬁnd out which inequality constraints are active or inactive. In fact, if we knew this a priori, everything would reduce to an equality constrained problem, which is much easier to solve.

4.5.3 Solving the dual problem, instead of the primal.

In some cases the dual function l(λ) = minx L(x, λ) can analytically be derived. In this case it makes very much sense to try solving the dual problem instead of the primal. First, the dual problem maxλ≥0 l(λ) is guaranteed to be convex even if the primal is non-convex. (The dual function l(λ) is concave, and the constraint λ ≥ 0 convex.) But note that l(λ) is itself deﬁned as the result of a generally non-convex optimization problem minx L(x, λ). Second, the inequality constraints of the dual problem are very simple: just λ ≥ 0. Such inequality constraints are called bound constraints and can be handled with specialized methods.
However, in general minx maxy f (x, y) = maxy minx f (x, y). For example, in discrete domain x, y ∈ {1, 2}, let f (1, 1) = 1, f (1, 2) = 3, f (2, 1) = 4, f (2, 2) = 2, and minx f (x, y) = (1, 2) and maxy f (x, y) = (3, 4). Therefore, the dual problem is in general not equivalent to the primal.
The dual function is, for λ ≥ 0, a lower bound

l(λ) = min L(x, λ) ≤ min f (x) s.t. g(x) ≤ 0 .

x

x

And consequently

(166)

(dual)

max min L(x, λ) ≤ min max L(x, λ)

λ≥0 x

x λ≥0

(primal) (167)

We say strong duality holds iﬀ

max min L(x, λ) = min max L(x, λ)

λ≥0 x

x λ≥0

(168)

64

Maths for Intelligent Systems, Marc Toussaint

If the primal is convex, and there exist an interior point ∃x : ∀i : gi(x) < 0
(which is called Slater condition), then we have strong duality.

(169)

4.5.4 Finding the “saddle point” directly with a primal-dual Newton method.

In basic unconstrained optimization an eﬃcient way to ﬁnd an optimum (minimum or maximum) is to ﬁnd a point where the gradient is zero with a Newton method. At saddle points all gradients are also zero. So, to ﬁnd a saddle point of the Lagrangian we can equally use a Newton method that seeks for roots of the gradient. Note that such a Newton method optimizes in the joint primal-dual space of (x, λ, κ).
In the case of inequalities, the zero-gradients view is over-simpliﬁed: While facts (i) and (ii) characterize a saddle point in terms of zero gradients, fact (iii) makes this more precise to handle the inequality case. For this reason it is actually easier to describe the primal-dual Newton method directly in terms of the KKT conditions: We seek a point (x, λ, κ), with λ ≥ 0, that solves the equation system

∇xf (x) + λ ∂xg + κ ∂xh = 0 h(x) = 0
diag(λ)g(x) + µ1m = 0

(170) (171) (172)

Note that the ﬁrst equation is the 1st KKT, the 2nd is the 2nd KKT w.r.t. equalities, and the third is the approximate 4th KKT with log barrier parameter µ (see below). These three equations reﬂect the saddle point properties (facts (i), (ii), and (iii) above). We deﬁne





∇f (x) + λ ∂g(x) + κ ∂h(x)





r(x,

λ,

κ)

=

 



h(x)

  





 diag(λ) g(x) + µ1m 

(173)

and use the Newton method

x x  




λ  


←

λ − α  
 

∂r(x, λ, κ)-1

r(x, λ)





κ κ  



(174)

to ﬁnd the root r(x, λ, κ) = 0 (α is the stepsize). We have

∇2f (x) +



∂r(x,

λ,

κ)

=

 







i λi∇2gi(x) + j κj ∇2hj (x) ∂h(x)
diag(λ) ∂g(x)

∂g(x) 0
diag(g(x))

∂h(x)   0    0
(175)

where ∂r(x, λ, κ) ∈ R(n+m+l)×(n+m+l). Note that this method uses the hessians ∇2f, ∇2g and ∇2h.

Maths for Intelligent Systems, Marc Toussaint

65

The above formulation allows for a duality gap µ. One could choose µ = 0, but often that is not robust. The beauty is that we can adapt µ on the ﬂy, before each Newton step, so that we do not need a separate outer loop to adapt µ.

Before

computing

a

Newton

step,

we

compute

the

current

duality

measure

µ˜

=

−

1 m

m i=1

λigi(x

Then

we

set

µ

=

1 2

µ˜

to

half

of

this.

In

this

way,

the

Newton

step

will

compute

a

direction

that aims to half the current duality gap. In practise, this leads to good convercence in

a single-loop Newton method. (See also Boyd sec 11.7.3.)

The dual feasibility λi ≥ 0 needs to be handled explicitly by the root ﬁnder – the line search can simply clip steps to stay within the bound constraints.

Typically, the method is called “interior primal-dual Newton”, in which case also the primal feasibility gi ≤ 0 has to be ensured. But I found there are tweaks to make the method also handle infeasible x, including infeasible initializations.

4.5.5 Log Barriers and the Lagrangian

Finally, let’s revisit the log barrier method. In principle it is very simple: For a given µ, we use an unconstrained solver to ﬁnd the minimum x∗(µ) of

F (x; µ) = f (x) − µ log(−gi(x)) .
i

(176)

(This process is also called “centering”.) We then gradually decrease µ to zero, always calling the inner loop to recenter. The generated path of x∗(µ) is called central path.

The method is simple and has very insightful relations to the KKT conditions and the dual problem. For given µ, the optimality condition is

µ ∇F (x; µ) = 0 ⇒ ∇f (x) − i gi(x) ∇gi(x) = 0
⇔ ∇f (x) + λi∇gi(x) = 0 , λigi(x) = −µ
i

(177) (178)

where we deﬁned(!) λi = −µ/gi(x), which guarantees λi ≥ 0 as long as we are in the interior (gi ≤ 0).
So, ∇F (x; µ) = 0 is equivalent to the modiﬁed (=approximate) KKT conditions, where the complemenetarity is relaxed: inequalities may push also inside the feasible region. For µ → 0 we converge to the exact KKT conditions with strict complementarity.
So µ has the interpretation of a relaxation of complementarity. We can derive another interpretation of µ in terms of suboptimality or the duality gap:
Let x∗(µ) = minx F (x; µ) be the central path. At each x∗ we may deﬁne, as above, λi = −µ/gi(x∗). We note that λ ≥ 0 (dual feasible), as well that x∗(µ) minimizes the

66

Maths for Intelligent Systems, Marc Toussaint

Lagrangian L(x, λ) w.r.t. x! This is because,

m

0 = ∇F (x, µ) = ∇f (x) + λi∇gi(x) = ∇L(x, λ) .

(179)

i=1

Therefore, x∗ is actually the solution to minx L(x, λ), which deﬁnes the dual function. We have

m
l(λ) = min L(x, λ) = f (x∗) + λigi(x∗) = f (x∗) − mµ .
x i=1

(180)

(m is simply the count of inequalities.) That is, mµ is the duality gap between the (suboptimal) f (x∗) and l(λ). Further, given that the dual function is a lower bound, l(λ) ≤ p∗, where p∗ = minx f (x) s.t. g(x) ≤ 0 is the optimal primal value, we have

f (x∗) − p∗ ≤ mµ .

(181)

This gives the interpretation of µ as an upper bound on the suboptimality of f (x∗).

4.6 Convex Problems
We do not put much emphasis on discussing convex problems in this lecture. The algorithms we discussed so far equally apply on general non-linear programs as well as on convex problems—of course, only on convex problems we have convergence guarantees, as we can see from the convergence rate analysis of Wolfe steps based on the assumption of positive upper and lower bounds on the Hessian’s eigenvalues.
Nevertheless, we at least deﬁne standard LPs, QPs, etc. Perhaps the most interesting part is the discussion of the Simplex algorithm—not because the algorithms is nice or particularly eﬃcient, but rather because one gains a lot of insights in what actually makes (inequality) constrained problems hard.

4.6.1 Convex sets, functions, problems

Deﬁnition 4.4 (Convex set, convex function). A set X ⊆ V (a subset of some vector space V ) is convex iﬀ

∀x, y ∈ X, a ∈ [0, 1] : ax + (1−a)y ∈ X

(182)

A function is deﬁned

convex

⇔

∀x,

y

∈

n
R

,

a

∈

[0,

1]

:

f (ax + (1−a)y) ≤ a f (x) + (1−a) f (y)

(183)

quasiconvex

⇔

∀x,

y

∈

n
R

,

a

∈

[0,

1]

:

f (ax + (1−a)y) ≤ max{f (x), f (y)}

(184)

Note: quasiconvex ⇔ for any α ∈ R the sublevel set {x|f (x) ≤ α} is convex. Further, I call a function unimodal if it has only one local minimum, which is the global minimum.

Maths for Intelligent Systems, Marc Toussaint

67

Deﬁnition 4.5 (Convex program).
Variant 1: A mathematical program minx f (x) s.t. g(x) ≤ 0, h(x) = 0 is convex if f is convex and the feasible set is convex.
Variant 2: A mathematical program minx f (x) s.t. g(x) ≤ 0, h(x) = 0 is convex if f and every gi are convex and h is linear.
Variant 2 is the stronger and usual deﬁnition. Concerning variant 1, if the feasible set is convex the zero-level sets of all g’s need to be convex and the zero-level sets of h’s needs to be linear. Above these zero levels the g’s and h’s could in principle be abribtrarily non-linear, but these non-linearities are irrelevant for the mathematical program itself. We could replace such g’s and h’s by convex and linear functions and get the same problem.

4.6.2 Linear and quadratic programs

Deﬁnition 4.6 (Linear program (LP), Quadratic program (QP)). Special case mathematical programs are

Linear Program (LP): LP in standard form: Quadratic Program (QP):

min c x s.t. Gx ≤ h, Ax = b
x
min c x s.t. x ≥ 0, Ax = b
x
1 min x Qx + c x s.t. Gx ≤ h, Ax = b , Qpos-def
x2

Rarely, also a Quadratically Constrained QP (QCQP) is considered.

An important example for LP are relaxations of integer linear programs,

min c x
x

s.t.

Ax = b,

xi ∈ {0, 1} ,

(185)

which includes Travelling Salesman, MaxSAT or MAP inference problems. Relaxing such a problem means to instead solve the continuous LP

min c x
x

s.t.

Ax = b,

xi ∈ [0, 1] .

(186)

If one is lucky and the continuous LP problem converges to a fully integer solution, where all xi ∈ {0, 1}, this is also the solution to the integer problem. Typically, the solution of the continuous LP will be partially integer (some values converge to the extreme xi ∈ {0, 1}, while others are inbetween xi ∈ (0, 1)). This continuous valued solution gives a lower bound on the integer problem, and provides very eﬃcient heuristics for backtracking or branch-and-bound search for a fully integer solution.

The standard example for a QP are Support Vector Machines. The primal problem is

n

min ||β||2 + C
β,ξ

ξi s.t. yi(xi β) ≥ 1 − ξi , ξi ≥ 0

i=1

(187)

68

Maths for Intelligent Systems, Marc Toussaint

the dual

nn

n

l(α,

µ)

=

min
β,ξ

L(β,

ξ,

α,

µ)

=

−

1 4

αiαi yiyi xˆi xˆi + αi

i=1 i =1

i=1

max l(α, µ) s.t. 0 ≤ αi ≤ C
α,µ

(See ML lecture 5:13 for a derivation.)

(188) (189)

y

B

A

x
4.6.3 The Simplex Algorithm
Consider an LP. We make the following observations:
• First, in LPs the equality constraints could be resolved simply by introducing new coordinates along the zero-hyperplane of h. Therefore, for the conceptual discussion we neglect equality constraints.
• The objective constantly pulls in the direction −c = −∇f (x).
• If the solution is bounded there need to be some inequality constraints that keep the solution from travelling to ∞ in the −c direction.
• It follows: The solution will always be located at a vertex, that is, an intersection point of several zero-hyperplanes of inequality constraints.
• In fact, we should think of the feasible region as a polytope that is deﬁned by all the zero-hyperplanes of the inequalities. The inside the polytope is the feasible region. The polytope has edges (intersections of two contraint planes), faces, etc. A solution will always be located at a vertex of the polytope; more precisely, there could be a whole set of optimal points (on a face orthogonal to c), but at least one vertex is also optimal.
• An idea for ﬁnding the solution is to walk on the edges of the polytope until an optimal vertex is found. This is the simplex algorithm of Georg Dantzig, 1947. In practise this procedure is done by “pivoting on the simplex tableaux”—but we fully skip such details here.

Maths for Intelligent Systems, Marc Toussaint

69

• The simplex algorithm is often eﬃcient, but in worst case it is exponential in both, n and m! This is hard to make intuitive, because the eﬀects of high dimensions are not intuitive. But roughly, consider that in high dimensions there is a combinatorial number of ways of how constraints may intersect and form edges and vertices.
Here is a view that much more relates to our discussion of the log barrier method: Sitting on an edge/face/vertex is equivalent to temporarily deciding which constraints are active. If we knew which constraints are eventually active, the problem would be solved: all inequalities become equalities or void. (And linear equalities can directly be solved for.) So, jumping along vertices of the polytope is equivalent to sequentially making decisions on which constraints might be active. Note though that there are 2m conﬁgurations of active/non-active constraints. The simplex algorithm therefore walks through this combinatorial space.
Interior point methods do exactly the opposite: Recall that the 4th KKT condition is λigi(x) = 0. The log barrier method (for instance) instead relaxes this hard logic of activ/non-active constraints and ﬁnds in each iteration a solution to the relaxed 4th KKT condition λigi(x) = −µ, which intuitively means that every constraint may be “somewhat active”. In fact, every constraint contributes somewhat to the stationarity condition via the log barrier’s gradients. Thereby interior point methods
• post-pone the hard decisions about active/non-active constraints
• approach the optimal vertex from the inside of the polytope; avoiding the polytope surface (and its hard decisions)
• thereby avoids the need to search through a combinatorial space of constraint activities and instead continuously converges to a decision
• has polynomial worst-case guaranteed
Historically, penalty and barrier methods methods were standard before the Simplex Algorithm. When SA was discovered in the 50ies, it was quickly considered great. But then, later in the 70-80ies, a lot more theory was developed for interior point methods, which now again have become somewhat more popular than the simplex algorithm.

4.6.4 Sequential Quadratic Programming

Just for reference, SQP is another standard approach to solving non-linear mathematical

programs. In each iteration we compute all coeﬃcients of the 2nd order Taylor f (x+δ) ≈

f (x) + ∇f (x)

δ

+

1 2

δ

Hδ

and

1st-order

Taylor

g(x + δ)

≈

g(x) + ∇g(x)

δ

and

then

solve

the QP

min

f (x) + ∇f (x)

δ+

1 δ

∇2f (x)δ

s.t.

g(x) + ∇g(x) δ ≤ 0

δ

2

(190)

70

Maths for Intelligent Systems, Marc Toussaint

The optimal δ∗ of this problem should be seen analogous to the optimal Newton step: If f were a 2nd-order polynomial and g linear, then δ∗ would jump directly to the optimum. However, as this is generally not the case, δ∗ only gives us a very good direction for line search. In SQP, we need to backtrack until we found a feasible point and f decreases suﬃciently.
Solving each QP in the sub-routine requires a constrained solver, which itself might have two nested loops (e.g. using log-barrier or AugLag). In that case, SQP has three nested loops.

4.7 Blackbox & Global Optimization: It’s all about learning

Even if f, g, h are smooth, the solver might not have access to analytic equations or eﬃcient numeric methods to evaluate the gradients or hessians of these. Therefore we distinguish (here neglecting the constraint functions g and h):
Deﬁnition 4.7.

• Blackbox optimization: Only f (x) can be evaluated.

• 1st-order/gradient optimization: Only f (x) and ∇f (x) can be evaluated.

• Quasi-Newton optimization: Only f (x) and ∇f (x) can be evaluated, but the solver does tricks to estimate ∇2f (x). (So this is a special case of 1st-order
optimization.)

• Gauss-Newton type optimization: f is of the special form f (x) = φ(x) φ(x)

and

∂ ∂x

φ(x)

can

be

evaluated.

• 2nd order optimization: f (x), ∇f (x) and ∇2f (x) can be evaluated.

In this lecture I very brieﬂy want to add comments on global blackbox optimization. Global means that we now, for the ﬁrst time, really aim to ﬁnd the global optimum (within some pre-speciﬁed bounded range). In essence, to address such a problem we need to explicitly know what we know about f 7, and an obvious way to do this is to use Bayesian learning.

4.7.1 A sequential decision problem formulation

From now on, let’s neglect constraints and focus on the mathematical program

min f (x)
x 7Cf. the KWIK (knows what it knows) framework.

(191)

Maths for Intelligent Systems, Marc Toussaint

71

for a blackbox function f . The optimization process can be viewed as a Markov Decision Process that describes the interaction of the solver (agent) with the function (environment):

• At step t, Dt = {(xi, yi)}ti=-11 is the data that the solver has collected from previous samples. This Dt is the state of the MDP.
• At step t, the solver may choose a new decision xt about where to sample next.
• Given state Dt and decision xt, the next state is Dt+1 = D ∪ {(xt, f (xt))}, which is a deterministic transition given the function f .
• A solver policy is a mapping π : Dt → xt that maps any state (of knowledge) to a new decision.
• We may deﬁne an optimal solver policy as

π∗ = argmin yT = argmin P (f ) P (DT |π, f ) yT

π

π

f

(192)

where P (DT |π, f ) is deterministic, and P (f ) is a prior over functions.

This objective function cares only about the last value yT sampled by the solver for

a ﬁxed time horizon (budget) T . Alternatively, we may choose objectives

T t=1

yt

or

T t=1

γtyt

for

some

discounting

γ

∈

[0, 1]

The above deﬁned what is an optimal solver! Something we haven’t touched at all before. The transition dynamics of this MDP is deterministic, given f . However, from the perspective of the solver, we do not know f apriori. But we can always compute a posterior belief P (f |Dt) = P (Dt|f ) P (f )/P (Dt). This posterior belief deﬁnes a belief MDP with stochastic transitions

P (Dt+1) =

[Dt+1 = D ∪ {(xt, f (xt))}] π(xt|Dt) P (f |Dt) P (Dt) . (193)

Dt f xt

The belief MDP’s state space is P (Dt) (or equivalently, P (f |Dt), the current belief over f ). This belief MDP is something that the solver can, in principle, forward simulate— it has all information about it. One can prove that, if the solver could solve its own belief MDP (ﬁnd an optimal policy for its belief MDP), then this policy is the optimal solver policy for the original problem given a prior distribution P (f )! So, in principle we not only deﬁned what is an optimal solver policy, but can also provide an algorithm to compute it (Dynamic programming in the belief MDP)! However, this is so expensive to compute that heuristics need to be used in practise.

One aspect we should learn from this discussion: The solver’s optimal decision is based
on its current belief P (f |Dt) over the function. This belief is the Bayesian representation of everything one could possibly have learned about f from the data Dt collected so far. Bayesian Global Optimization methods compute P (f |Dt) in every step and, based on this, use a heuristic to choose the next decision.

72

Maths for Intelligent Systems, Marc Toussaint

4.7.2 Acquisition Functions for Bayesian Global Optimization*

In pratice one typically uses a Gaussian Process representation of P (f |Dt). This means that in every iteration we have an estimate fˆ(x) of the function mean and a variance estimate σˆ(x)2 that describes our uncertainty about the mean estimate. Based on this
we may deﬁne the following acquisition functions

Deﬁnition 4.8. Probability of Improvement (MPI)

αt(x) =

y∗ −∞

N(y|fˆ(x),

σˆ(x))

dy

Expected Improvement (EI)

αt(x) =

y∗ −∞

N(y|fˆ(x),

σˆ(x))

(y∗

−

y)

dy

Upper Conﬁdence Bound (UCB)

(194) (195)

αt(x) = −fˆ(x) + βtσˆ(x)

(196)

Predictive Entropy Search Hern´andez-Lobato et al. (2014)

αt(x) = H[p(x∗|Dt)] − E{p(y|Dt; x)} H[p(x∗|Dt ∪ {(x, y)})]

(197)

= I(x∗, y|Dt) = H[p(y|Dt, x)] − E{p(x∗|Dt)} H[p(y|Dt, x, x∗)]

The last one is special; we’ll discuss it below.
These acquisition functions are heuristics that deﬁne how valuable it is to acquire data from the site x. The solver then makes the decision

xt = argmax αt(x) .
x

(198)

MPI is hardly being used in practise anymore. EI is classical, originating way back in the 50ies or earlier; Jones et al. (1998) gives an overview. UCB received a lot of attention recently due to the underlying bandit theory and bounded regret theorems due to the submodularity. But I think that in practise EI and UCB perform about equally. As UCB is somewhat easier to implement and intuitive.
In all cases, note that the solver policy xt = argmaxx αt(x) requires to internally solve another non-linear optimization problem. However, αt is an analytic function for which we can compute gradients and hessians which ensures every eﬃcient local convergence. But again, xt = argmaxx αt(x) needs to be solved globally —otherwise the solver will also not solve the original problem properly and globally. As a consequence, the optimization of the acquisition function needs to be restarted from many many potential start points close to potential local minima; typically from grid(!) points over the full domain range. The number of grid points is exponential in the problem dimension n. Therefore, this inner loop can be very expensive.

Maths for Intelligent Systems, Marc Toussaint

73

And a subjective note: This all sounds great, but be aware that Gaussian Processes with standard squared-exponential kernels do not generalize much in high dimensions: one roughly needs exponentially many data points to fully cover the domain and reduce belief uncertainty globally, almost as if we were sampling from a grid with grid size equal to the kernel width. So, the whole approach is not magic. It just does what is possible given a belief P (f ). It would be interesting to have much more structured (and heteroscedastic) beliefs speciﬁcally for optimization.
The last acquisition function is called Predictive Entropy Search. This formulation is beautiful: We sample at places x where the (expected) observed value y informs us as much as possible about the optimum x∗ of the function! Formally, this means to maximize the mutual information between y and x∗, in expectation over y|x.

4.7.3 Classical model-based blackbox optimization (non-global)*
A last method very worth mentioning: Classical model-based blackbox optimization simply ﬁts a local polynomial model to the recent data and takes this a basis for search. This is similar to BFGS, but now for the blackbox case where we not even observe gradients. See Algorithm 7.
The local ﬁtting of a polynomial model is again a Machine Learning method. Whether this gives a function approximation for optimization depends on the quality of the data Dt used for this approximation. Classical model-based optimization has interesting heuristics to evaluate the data quality as well as sample new points to improve the data quality. Here is a rough algorithm (following Nodecal et al.’s section on “Derivative-free optimization”):
Some notes:
• Line 4 implement an explicit trust region approach, which hard bound α on the step size.
• Line 5 is like the Wolfe condition. But here, the expected decrease is [f (xˆ) − fˆ(xˆ + δ)] instead of −αδ∇f (x).
• If there is no suﬃcient decrease we may blame it on two reasons: bad data or a too large stepsize.
• Line 10 uses the data determinant as a measure of quality! This is meant in the sense of linear regression on polynomial features. Note that, with data matrix X ∈ Rn×dim(β), βˆls = (X X)-1X y is the optimal regression. The determinant det(X X) or det(X) = det(D) is a measure for well the data supports the regression. If the determinant is zero, the regression problem is ill-deﬁned. The larger the determinant, the lower the variance of the regression estimator.
• Line 11 is an explicit exploration approach: We add a data point solely for the purpose of increasing the data determinant (increasing the data spread). Interest-

74

Maths for Intelligent Systems, Marc Toussaint

Algorithm 7 Classical model-based optimization

1:

Initialize

D

with

at

least

1 2

(n

+

1)(n

+

2)

data

points

2: repeat

3: Compute a regression fˆ(x) = φ2(x) β on D

4: Compute δ = argminδ fˆ(xˆ + δ) s.t. |δ| < α 5: if f (xˆ + δ) < f (xˆ) − ls[f (xˆ) − fˆ(xˆ + δ)] then

6:

Increase the stepsize α

// test suﬃcient decrease

7:

Accept xˆ ← xˆ + δ

8:

Add to data, D ← D ∪ {(xˆ, f (xˆ))}

9: else

// no suﬃcient decrease

10:

if det(D) is too small then

// blame the data quality

11:

Compute x+ = argmaxx det(D ∪ {x }) s.t. |x − x | < α

12:

Add to data, D ← D ∪ {(x+, f (x+))}

13:

else

// blame the stepsize

14:

Decrease the stepsize α

15:

end if

16: end if

17: Perhaps prune the data, e.g., remove argmaxx∈∆ det(D \ {x}) 18: until x converges

ing. Nocedal describes in more detail a geometry-improving procedure to update D.

4.7.4 Evolutionary Algorithms*
There are interesting and theoretically well-grounded evolutionary algorithms for optimization, such as Estimation-of-Distribution Algorithms (EDAs). But generally, don’t use them as ﬁrst choice.

4.8 Examples and Exercises

4.8.1 Convergence proof

a) Given a function f : Rn → R with fmin = minx f (x). Assume that its Hessian— that is, the eigenvalues of ∇2f —are lower bounded by m > 0 and upper bounded by
M > m, with m, M ∈ R. Prove that for any x ∈ Rn it holds

f (x)

−

1 |∇f (x)|2 2m

≤

fmin

≤

f (x)

−

1 2M

|∇f (x)|2

.

Tip: Start with bounding f (x) between the functions with maximal and minimal curvature. Then consider the minima of these bounds. Note, it also follows:

|∇f (x)|2 ≥ 2m(f (x) − fmin) .

Maths for Intelligent Systems, Marc Toussaint

75

b) Consider backtracking line search with Wolfe parameter

ls

≤

1 2

,

and

step

decrease

factor

−α .

First prove that line search terminates the latest when

− α
M

≤α≤

1 M

,

and

then it found a new point y for which

−
f (y) ≤ f (x) − ls α |∇f (x)|2 . M

From this, using the result from a), prove the convergence equation

f (y) − fmin ≤

1 − 2m ls M

− α

(f (x) − fmin) .

4.8.2 Backtracking Line Search
Consider the functions fsq(x) = x Cx ,
fhole(x) = 1 − exp(−x Cx) .

(199) (200)

i−1
with diagonal matrix C and entries C(i, i) = c n−1 , where n is the dimensionality of x.
We choose a conditioning8 c = 10. To plot the function for n = 2, you can use gnuplot calling

set isosamples 50,50 set contour f(x,y) = x*x+10*y*y #f(x,y) = 1 - exp(-x*x-10*y*y) splot [-1:1][-1:1] f(x,y)

a) Implement gradient descent with backtracking, as described on page 42 (Algorithm 2 Plain gradient descent). Test the algorithm on fsq(x) and fhole(x) with start point x0 = (1, 1). To judge the performance, create the following plots:
– The function value over the number of function evaluations. – For n = 2, the function surface including algorithm’s search trajectory. If using gnuplot,
store every evaluated point x and function value f (x) in a line (with n + 1 entries) in a ﬁle ’path.dat’, and plot using
unset contour splot [-3:3][-3:3] f(x,y), ’path.dat’ with lines
8The word “conditioning” generally denotes the ratio of the largest and smallest Eigenvalue of the Hessian.

76

Maths for Intelligent Systems, Marc Toussaint

b) Play around with parameters. How does the performance change for higher dimensions, e.g., n = 100? How does the performance change with ρls (the Wolfe stop criterion)? How does the alternative in step 3 work?
c) Newton step: Modify the algorithm simply by multiplying C-1 to the step. How does that work?
(The Newton direction diverges (is undeﬁned) in the concave part of fhole(x). We’re cheating here when always multiplying with C-1 to get a good direction.)

4.8.3 Gauss-Newton

In x ∈ R2 consider the function f (x) = φ(x) φ(x) ,

 sin(ax1) 

φ(x)

=

sin(acx2)

 

2x1

 

2cx2

The function is plotted above for a = 4 (left) and a = 5 (right, having local minima), and conditioning c = 1. The function is non-convex.

Extend your backtracking method implemented in the last week’s exercise to a Gauss-
Newton method (with constant λ) to solve the unconstrained minimization problem minx f (x) for a random start point in x ∈ [−1, 1]2. Compare the algorithm for a = 4 and a = 5 and conditioning c = 3 with gradient descent.

4.8.4 Robust unconstrained optimization

A ’ﬂattened’ variant of the Rosenbrock function is deﬁned as

f (x)

=

log[1

+

(x2

−

x21)2

+

1 100

(1

−

x2)2]

and has the minimum at x∗ = (1, 1). For reference, the gradient and hessian are

g(x)

:=

1

+

(x2

−

x21)2

+

1 (1
100

−

x2)2

(201)

Maths for Intelligent Systems, Marc Toussaint

1 ∂x1 f (x) = g(x)

− 4(x2 − x21)x1

∂x2 f (x) =

1 g(x)

2(x2

−

x21)

−

2 (1
100

−

x2)

∂x21 f (x) = −

∂x1 f (x)

21 + g(x)

8x21 − 4(x2 − x21)

∂x22 f (x) = −

∂x2 f (x)

21 + g(x)

2 2+
100

1

∂x1 ∂x2 f (x) = − ∂x1 f (x)

∂x2 f (x)

+ g(x)

− 4x1

a) Use gnuplot to display the function copy-and-pasting the following lines:

77
(202) (203) (204) (205) (206)

set isosamples 50,50 set contour f(x,y) = log(1+(y-(x**2))**2 + .01*(1-x)**2 ) - 0.01 splot [-3:3][-3:4] f(x,y)

(The ’-0.01’ ensures that you can see the contour at the optimum.) List and discuss at least three properties of the function (at diﬀerent locations) that may raise problems to naive optimizers.
b) Use x = (−3, 3) as starting point for an optimization algorithm. Try to code an optimization method that uses all ideas mentioned in the lecture. Try to tune it to be eﬃcient on this problem (without cheating, e.g. by choosing a perfect initial stepsize.)

4.8.5 Lagrangian Method of Multipliers

In a previous exercise we deﬁned the “hole function” fhcole(x). Assume conditioning c = 10 and use the Lagrangian Method of Multipliers to solve on paper the following
constrained optimization problem in 2D:

min
x

fhcole(x)

s.t.

h(x) = 0

h(x) = v x − 1

(207) (208)

Near the very end, you won’t be able to proceed until you have special values for v. Go as far as you can without the need for these values.

78

Maths for Intelligent Systems, Marc Toussaint

4.8.6 Equality Constraint Penalties and Augmented Lagrangian

The squared penalty approach to solving a constrained optimization problem minimizes

m

min f (x) + µ
x

hi(x)2 .

i=1

(209)

The Augmented Lagrangian method adds a Lagrangian term and minimizes

m

m

min f (x) + µ
x

hi(x)2 +

λihi(x) .

i=1

i=1

(210)

Assume that we ﬁrst minimize (209) we end up at a minimum x.
Now prove that setting λi = 2µhi(x) will, if we assume that the gradients ∇f (x) and ∇h(x) are (locally) constant, ensure that the minimum of (210) fulﬁlls the constraints h(x) = 0.

4.8.7 Lagrangian and dual function
(Taken roughly from ‘Convex Optimization’, Ex. 5.1) Consider the optimization problem
min x2 + 1 s.t. (x − 2)(x − 4) ≤ 0
x
with variable x ∈ R. a) Derive the optimal solution x∗ and the optimal value p∗ = f (x∗) by hand. b) Write down the Lagrangian L(x, λ). Plot (using gnuplot or so) L(x, λ) over x for various values of λ ≥ 0. Verify the lower bound property minx L(x, λ) ≤ p∗, where p∗ is the optimum value of the primal problem. c) Derive the dual function l(λ) = minx L(x, λ) and plot it (for λ ≥ 0). Derive the dual optimal solution λ∗ = argmaxλ l(λ). Is maxλ l(λ) = p∗ (strong duality)?

4.8.8 Optimize a constrained problem

Consider the following constrained problem

n

min
x

xi s.t. g(x) ≤ 0

i=1

g(x)

=

x


x − 1 

 −x1 

(211) (212)

Maths for Intelligent Systems, Marc Toussaint

79

a) First, assume x ∈ R2 is 2-dimensional, and draw on paper what the problem looks like and where you expect the optimum.

b) Find the optimum analytically using the Lagrangian. Here, assume that you know apriori that all constraints are active! What are the dual parameters λ = (λ1, λ2)?

Note: Assuming that you know a priori which constraints are active is a huge assumption! In real problems, this is the actual hard (and combinatorial) problem. More on this later in the lecture.

c) Implement a simple the Log Barrier Method. Tips:

–

Initialize

x

=

(

1 2

,

1 2

)

and

µ=

1

– First code an inner loop:

– In each iteration, ﬁrst compute the gradient of the log-barrier function. Recall that

F (x; µ) = f (x) − µ log(−gi(x))
i
∇F (x; µ) = ∇f − µ (1/gi(x))∇gi(x)
i

(213) (214)

– Then perform a backtracking line search along −∇F (x, µ). In particular, backtrack if a step goes beyond the barrier (where g(x) ≤ 0 and F (x, µ) = ∞).
– Iterate until convergence; let’s call the result x∗(µ). Further, compute λ∗(m) = −(µ/g1(x), µ/g2(x)) at convergence.
– Decrease µ ← µ/2, recompute x∗(µ) (with the previous x∗ as initialization) and iterate this.

Does x∗ and λ∗ converge to the expected solution?
Note: The path x∗(µ) = argminx F (x; µ) (the optimum in dependence of µ) is called central path.

Comment: Solving problems in the real world involves 2 parts:
1) formulating the problem as an optimization problem (conform to a standard optimization problem category) (→ human)
2) the actual optimization problem (→ algorithm)
These exercises focus on the ﬁrst type, which is just as important as the second, as it enables the use of a wider range of solvers. Exercises from Boyd et al http://www. stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf:

4.8.9 Network ﬂow problem Solve Exercise 4.12 (pdf page 193) from Boyd & Vandenberghe, Convex Optimization.

80

Maths for Intelligent Systems, Marc Toussaint

4.8.10 Minimum fuel optimal control Solve Exercise 4.16 (pdf page 194) from Boyd & Vandenberghe, Convex Optimization.

4.8.11 Reformulating an 1-norm

(This is a subset of Exercise 4.11 (pdf page 193) from Boyd & Vandenberghe.)

Let x ∈ Rn. The optimization problem is minx ||x − b||1, where the 1-norm is deﬁned

as ||z||1 =

n i=1

|zi

|.

Reformulate

this

optimization

problem

as

a

Linear

Program.

4.8.12 Restarts of Local Optima

The following function is essentially the Rastrigin function, but written slightly diﬀerently.

It can be tuned to become uni-modal and is a sum-of-squares problem. For x ∈ R2 we

deﬁne

 sin(ax1) 

f (x) = φ(x) φ(x) ,

φ(x)

=

sin(acx2)

 

2x1

 

2cx2

The function is plotted above for a = 4 (left) and a = 5 (right, having local minima), and conditioning c = 1. The function is non-convex.

Choose a = 6 or larger and implement a random restart method: Repeat initializing x ∼ U([−2, 2]2) uniformlly, followed by a gradient descent (with backtracking line search and monotone convergence).

Restart the method at least 100 times. Count how often the method converges to which local optimum.

4.8.13 GP-UCB Bayesian Optimization
Find an implementation of Gaussian Processes for your language of choice (e.g. python: scikit-learn, or Sheﬃeld/Gpy; octave/matlab: gpml) and implement GP-UCB global

Maths for Intelligent Systems, Marc Toussaint

81

optimization. Test your implementation with diﬀerent hyperparameters (Find the best combination of kernel and its parameters in the GP) on the 2D function deﬁned above.
On the webpage you ﬁnd a starting code to use GP regression in scikit-learn. To install scikit-learn: https://scikit-learn.org/stable/install.html

5 Probabilities & Information
It is beyond the scope of these notes to give a detailed introduction to probability theory. There are excellent books:
• Thomas & Cover • Bishop • MacKay
Instead, we ﬁrst recap very basics of probability theory, that I assume the reader has already seen before. The next section will cover this. Then we focus on speciﬁc topics that, in my opinion, deepen the understanding of the basics, such as the relation between optimization and probabilities, log-probabilities & energies, maxEntropy and maxLikelihood, minimal description length and learning.
5.1 Basics
First, in case you wonder about justiﬁcations of the use of (Bayesian) probabilities versus fuzzy sets or alike, here some pointers to look up: 1) Cox’s theorem, which derives from basic assumptiond about “rationality and consistency” the standard probability axioms; 2) t-norms, which generalize probability and fuzzy calculus; and 3) read about objective vs. subjecte Bayesian probability.
5.1.1 Axioms, deﬁnitions, Bayes rule
Deﬁnition 5.1 (set-theoretic axioms of probabilities). • An experiment can have multiple outcomes; we call the set of possible outcomes sample space or domain S
• A mapping P : A ⊆ S → [0, 1], that maps any subset A ⊆ S to a real number, is called probability measure on S iﬀ – P (A) ≥ 0 for any A ⊆ S (non-negativity) – P ( i Ai) = i P (Ai) if Ai ∩ Aj = ∅ (additivity)

82

Maths for Intelligent Systems, Marc Toussaint

– P (S) = 1 (normalization)
• Implications are: – 0 ≤ P (A) ≤ 1 – P (∅) = 0 – A ⊆ B ⇒ P (A) ≤ P (B) – P (A ∪ B) = P (A) + P (B) − P (A ∩ B) – P (S \ A) = 1 − P (A)

Formally, a random variable X is a mapping X : S → Ω from a measureable space S (that is, a sample space S that has a probability measure P ) to another sample space Ω, which I typically call the domain dom(X) of the random variable. Thereby, the mapping X : S → Ω now also deﬁnes a probability measure over the domain Ω:

P (B ⊆ Ω) = P ({s : X(s) ∈ B})

(215)

In practise we just use the following notations:

Deﬁnition 5.2 (Random Variable). • Let X be a random variable with discrete domain dom(X) = Ω

• P (X = x) ∈ R denotes the speciﬁc probability that X = x for some x ∈ Ω

• P (X) denotes the probability distribution (function over Ω)

• ∀x∈Ω : 0 ≤ P (X = x) ≤ 1
• x∈Ω P (X = x) = 1
• We often use the short hand X P (X) · · · = when summing over possible values of a RV

x∈dom(X) P (X = x) · · ·

If we have two or more random variables, we have

Deﬁnition 5.3 (Joint, marginal, conditional, independence, Bayes’ Theorem). • We denote the joint distribution of two RVs as P (X, Y )

• The marginal is deﬁned as P (X) = Y P (X, Y )

•

The

conditional

is

deﬁned

as

P (X|Y ) =

P (X,Y P (Y )

)

,

which

fulﬁls

∀Y : X P (X|Y ) = 1.

• X is independent of Y iﬀ P (X, Y ) = P (X) P (Y ), or equivalently, P (X|Y ) = P (X).

Maths for Intelligent Systems, Marc Toussaint

83

• The deﬁnition of a conditional implies the product rule

P (X, Y ) = P (X|Y ) P (Y ) = P (Y |X) P (X)

(216)

and Bayes’ Theorem

P (Y |X) P (X) P (X|Y ) =
P (Y )

(217)

The individual terms in Bayes’ Theorem are typically given names:

likelihood · prior posterior =
normalization

(218)

(Sometimes, the normalization is also called evidence.)

• X is conditionally independent of Y given Z iﬀ P (X|Y, Z) = P (X|Z) or P (X, Y |Z) = P (X|Z) P (Y |Z)

5.1.2 Standard discrete distributions

Bernoulli Beta Multinomial Dirichlet

RV x ∈ {0, 1}
µ ∈ [0, 1]
x ∈ {1, .., K}
µ ∈ [0, 1]K , ||µ||1 = 1

parameter µ ∈ [0, 1]
α, β ∈ R+
µ ∈ [0, 1]K , ||µ||1 = 1 α1, .., αK ∈ R+

distribution

Bern(x | µ) = µx(1 − µ)1−x

Beta(µ | a, b)

=

1 B(a,b)

µa−1(1 − µ)b−1

P (x = k | µ) = µk

Dir(µ | α) ∝

K k=1

µαk k−1

Clearly, the Multinomial is a generalization of the Bernoulli, as the Dirichlet is of the

Beta. The mean of the Dirichlet is µi =

αi j αj

,

its

mode

is

µ∗i

=

αi −1 j αj −K

.

The mode

of a distribution p(x) is deﬁned as argmaxx p(x).

5.1.3 Conjugate distributions

Deﬁnition 5.4 (Conjugacy). Let p(D|x) be a likelihood conditional on a RV x. A family C of distributions (i.e., C is a space of distributions, like the space of all Beta distributions) is called conjugate to the likelihood function p(D|x) iﬀ

p(x) ∈ C

⇒

p(x|D)

=

p(D|x) p(x) p(D)

∈

C

.

(219)

The standard conjugates you should know:

84
RV µ µ µ λ Λ (µ, Λ)

Maths for Intelligent Systems, Marc Toussaint

likelihood
Binomial Bin(D | µ)
Multinomial Mult(D | µ)
Gauss N(x | µ, Σ) 1D Gauss N(x | µ, λ-1) nD Gauss N(x | µ, Λ-1) nD Gauss N(x | µ, Λ-1)

conjugate Beta Beta(µ | a, b) Dirichlet Dir(µ | α) Gauss N(µ | µ0, A) Gamma Gam(λ | a, b) Wishart Wish(Λ | W, ν) Gauss-Wishart N(µ | µ0, (βΛ)-1) Wish(Λ | W, ν)

5.1.4 Distributions over continuous domain

Deﬁnition 5.5. Let x be a continuous RV. The probability density function (pdf) p(x) ∈ [0, ∞) deﬁnes the probability

b
P (a ≤ x ≤ b) = p(x) dx ∈ [0, 1]
a

(220)

The cumulative probability distribution F (y) = P (x ≤ y) =

y −∞

dx

p(x)

∈

[0, 1] is the cumulative integral with limy→∞ F (y) = 1

However, I and most others say probability distribution to refer to probability density function.

One comment about integrals. If p(x) is a probability density function and f (x) some arbitrary function, typically one writes

f (x) p(x) dx ,
x

(221)

where dx denotes the (Borel) measure we integrate over. However, some authors (correctly) think of a distribution p(x) as being a measure over the space dom(x) (instead of just a function). So the above notation is actually “double” w.r.t. the measures. So they might (also correctly) write

p(x) f (x) ,
x
and take care that there is exactly one measure to the right of the integral.

(222)

5.1.5 Gaussian

Deﬁnition 5.6. We deﬁne an n-dim Gaussian in normal form as

N(x | µ, Σ) =

1 | 2πΣ | 1/2

1 exp{− (x − µ)
2

Σ-1 (x − µ)}

(223)

Maths for Intelligent Systems, Marc Toussaint

85

with mean µ and covariance matrix Σ. In canonical form we deﬁne

N[x | a, A]

=

exp{−

1 2

a

A-1a}

| 2πA-1 | 1/2

1 exp{− x
2

A x + x a}

(224)

with precision matrix A = Σ-1 and coeﬃcient a = Σ-1µ (and mean µ = A-1a).

Gaussians are used all over—below we explain in what sense they are the probabilistic analogue to a parabola (or a 2nd-order Taylor expansions). The most important properties are:

• Symmetry: N(x | a, A) = N(a | x, A) = N(x − a | 0, A)

• Product: N(x | a, A) N(x | b, B) = N[x | A-1a + B-1b, A-1 + B-1] N(a | b, A + B) N[x | a, A] N[x | b, B] = N[x | a + b, A + B] N(A-1a | B-1b, A-1 + B-1)

• “Propagation”: y N(x | a + F y, A) N(y | b, B) dy = N(x | a + F b, A + F BF )

• Transformation:

N(F x + f | a, A) =

1 |F |

N(x |

F -1(a − f ),

F -1AF -

)

• Marginal & conditional:

N

x y

a b

,

A C

C B

= N(x | a, A) · N(y | b + C A-1(x - a), B − C A-1C)

More Gaussian identities are found at http://ipvs.informatik.uni-stuttgart.de/mlr/marc/
notes/gaussians.pdf

Example 5.1 (ML estimator of the mean of a Gaussian). Assume we have data D = {x1, .., xn}, each xi ∈ Rn, with likelihood

P (D | µ, Σ) = i N(xi | µ, Σ)

1n

argmax
µ

P (D | µ, Σ) =

n

xi
i=1

1n

argmax
Σ

P (D | µ, Σ) =

n

(xi − µ)(xi − µ)
i=1

(225) (226) (227)

Assume we are initially uncertain about µ (but know Σ). We can express this uncertainty using again a Gaussian N[µ | a, A]. Given data we have

P (µ | D) ∝ P (D | µ, Σ) P (µ) = i N(xi | µ, Σ) N[µ | a, A]

(228)

= i N[µ | Σ-1xi, Σ-1] N[µ | a, A] ∝ N[µ | Σ-1 i xi, nΣ-1 + A]

(229)

86

Maths for Intelligent Systems, Marc Toussaint

Note: in the limit A → 0 (uninformative prior) this becomes

1

1

P (µ | D) = N(µ | n

xi,

Σ) n

i

which is consistent with the Maximum Likelihood estimator

(230)

5.1.6 “Particle distribution”

Usually, “particles” are not listed as standard continuous distribution. However I think they should be. They’re heavily used in several contexts, especially as approximating other distributions in Monte Carlo methods and particle ﬁlters.

Deﬁnition 5.7 (Dirac or δ-distribution). In distribution theory it is proper to deﬁne a distribution δ(x) that is the derivative of the Heavyside step function H(x),

∂ δ(x) = H(x) , H(x) = [x ≥ 0] .
∂x

(231)

It is akward to think of δ(x) as a normal function, as it’d be “inﬁnite” at zero. But at least we understand that is has the properties

δ(x) = 0 everywhere except at x = 0 , δ(x) dx = 1 .

(232)

I sometimes call the Dirac distribution also a point particle: it has all its unit “mass” concentrated at zero.

Deﬁnition 5.8 (Particle Distribution). We deﬁne a particle distribution q(x) as a mixture of Diracs,

N
q(x) := wi δ(x − xi) ,
i=1

(233)

which is parameterized by the number N , the locations {xi}Ni=1, xi ∈ Rn, and the normalized weights {wi}Ni=1, wi ∈ R, ||w||1 = 1 of the N particles.

We say that a particle distribution q(x) approximates another distribution p(x) iﬀ for any (smooth) f

f (x) p = f (x)p(x)dx ≈
x

N i=1

wif

(xi)

(234)

Note the generality of this statement! f could be anything, it could be any features of the variable x, like coordinates of x, or squares, or anything. So basically this statement

Maths for Intelligent Systems, Marc Toussaint

87

says, whatever you might like to estimate about p, you can approximate it based on the particles q.
Computing particle approximations of comples (non-analytical, non-tracktable) distributions p is a core challenge in many ﬁelds. The true p could for instance be a distributions over games (action sequences). The approximation q could for instance be samples generated with Monte Carlo Tree Search (MCTS). The tutorial An Introduction to MCMC for Machine Learning www.cs.ubc.ca/~nando/papers/mlintro.pdf gives an excellent introduction. Here are some illustrations of what it means to approximate some p by particles q, taken from this tutorial. The black line is p, histograms illustrate the particles q by showing how many of (uniformly weighted) particles fall into a bin:

(from de Freitas et al.)

5.2 Between probabilities and optimization: neg-log-probabilities, exp-neg-energies, exponential family, Gibbs and Boltzmann

There is a natural relation between probabilities and “energy” (or “error”). Namely, if p(x) denotes a probability for every possible value of x, and E(x) denotes an energy for state x—or an error one assigns to choosing x—then a natural relation is

p(x) = e−E(x) , E(x) = − log p(x) .

(235)

Why is that? First, outside the context of physics it is perfectly fair to just deﬁne axiomatically an energy E(x) as neg-log-probability. But let me try to give some more arguments for why this is a useful deﬁnition.
Let assume we have p(x). We want to ﬁnd a quantity, let’s call it error E(x), which is a function of p(x). Intuitively, if a certain value x1 is more likely than another,

88

Maths for Intelligent Systems, Marc Toussaint

p(x1) > p(x2), then picking x1 should imply less error, E(x1) < E(x2) (Axiom 1). Further, when we have two independent random variables x and y, probabilities are multiplicative, p(x, y) = p(x)p(y). We require axiomatically that error is additive, E(x, y) = E(x) + E(y). From both follows that E needs to be some logarithm of p!

The same argument, now more talking about energy : Assume we have two independent

(physical) systems x and y. p(x, y) = p(x)p(y) is the probability to ﬁnd them in certain

states. We axiomatically require that energy is additive, E(x, y) = E(x) + E(y).

Again, E needs to be some logarithm of p. In the context of physics, what could

be questioned is “why is p(x) a function of E(x) in the ﬁrst place?”. Well, that is

much harder to explain and really is a question about statistical physics. Wikipedia

under keywords “Maxwell-Boltzmann statistics” and “Derivation from microcanonical

ensemble” gives an answer. Essentially the argument is a follows: Given many many

molecules in a gas, each of which can have a diﬀerent energy ei. The total energy

E=

n i=1

ei

must

be

conserved.

What

is the

distribution

over energy levels

that

has

the most microstates? The answer is the Boltzmann distribution. (And why do we,

in nature, ﬁnd energy distributions that have the most microstates? Because these are

most likely.)

Bottom line is: p(x) = e−E(x), probabilities are multiplicative, energies or errors additive.

Let me state some fact just to underline how useful this way of thinking is:

• Given an energy function E(x), its Boltzmann distribution is deﬁned as

p(x) = e−E(x).

(236)

This is sometimes also called Gibbs distribution.
• In machine learning, when data D is given and we have some model β, we typically try to maximize the likelihood p(D|β). This is equivalent to minimizing the neglog-likelihood

L(β) = − log p(D|β) .

(237)

This neg-log-likelihood is a typical measure for error of the model. And this error is additive w.r.t. the data, whereas the likelihood is multiplicative, ﬁtting perfectly to the above discussion.

•

The

Gaussian

distribution

p(x)

∝

exp{−

1 2

||x

− µ||2/σ2}

is

related

to

the

error

E(x)

=

1 2

||x

−

µ||2/σ2,

which

is

nothing

but

the

squared

error

with

the

precision

matrix as metric. That’s why squared error measures (classical regression) and

Gaussian distributions (e.g., Bayesian Ridge regression) are directly related.

A Gaussian is the probabilistic analoque to a parabola.

• The exponential family is deﬁned as

p(x|β) = h(x)g(β) exp{β φ(x)}

(238)

Maths for Intelligent Systems, Marc Toussaint

89

Often h(x) = 1, so let’s neglect this for now. The key point is that the energy is linear in the features φ(x). This is exactly how discriminative functions (for classiﬁcation in Machine learning) are typically formulated.
In the continuous case, the features φ(x) are often chosen as basis polynomials— just as in polynomial regression. Then, β are the coeﬃcients of the energy polynomial and the exponential family is just the probabilistic analogue to the space of polynomials.

• When we have many variables x1, .., xn, the structure of a cost function over these variables can often be expressed as being additive in terms: f (x1, .., xn) = i φi(x∂i) where ∂i denotes the ith group of variables. The respective Boltzmann distribution is a factor graph p(x1, .., xn) ∝ i fi(x∂i) = exp{ i βiφi(x∂i) where ∂i denotes the
So, factor graphs are the probabilistic analoque to additive functions.

• − log p(x) is also the “optimal” coding length you should assign to a symbol x.
Entropy is expected error: H[p] = x −p(x) log p(x) = − log p(x) p(x), where p itself it used to take the expectation.
Assume you use a “wrong” distribution q(x) to decide on the coding length of symbols drawn from p(x). The expected length of an encoding is x p(x)[− log q(x)] ≥ H (p).
The Kullback-Leibler divergence is the diﬀerence:

p(x)

D p q = p(x) log ≥ 0

x

q(x)

(239)

Proof of inequality, using the Jenson inequality:

q(x)

q(x)

− p(x) log ≥ − log p(x) = 0

x

p(x)

x p(x)

(240)

So, my message is that probabilities and error measures are naturally related. However, in the ﬁrst case we typically do inference, in the second we optimize. Let’s discuss the relation between inference and optimization a bit more. For instance, given data D and parameters β, we may deﬁne

Deﬁnition 5.9 (ML, MAP, and Bayes estimate). Given data D and a parameteric model p(D|β), we deﬁne

• Maximum likelihood (ML) parameter estimate: βML := argmaxβ P (D|β)
• Maximum a posteriori (MAP) parameter estimate: βMAP = argmaxβ P (β|D)

90

Maths for Intelligent Systems, Marc Toussaint

• Bayesian parameter estimate: P (β|D) ∝ P (D|β) P (β) used for Bayesian prediction: P (prediction|D) = β P (prediction|β) P (β|D)
Both, the MAP and the ML estimates are really just optimization problems. The Bayesian parameter estimate P (β|D), which can then be used to do fully Bayesian prediction, is in principle diﬀerent. However, in practise also here optimization is a core tool for estimating such distributions if they cannot be given analytically. This is described next.

5.3 Information, Entropie & Kullback-Leibler

Consider the following problem. We have data drawn i.i.d. from p(x) where x ∈ X in some discrete space X. Let’s call every x a word. The problem is to ﬁnd a mapping from words to codes, e.g. binary codes c : X → {0, 1}∗. The optimal solution is in principle simple: Sort all possible words in a list, ordered by p(x) with more likely words going ﬁrst; write all possible binary codes in another list, with increasing code lengths. Match the two lists, and this is the optimal encoding.

Let’s try to get a more analytical grip of this: Let l(x) = |c(x)| be the actual code length assigned to word x, which is an integer value. Let’s deﬁne

q(x) = 1 2−l(x) Z

(241)

with the normalization constrant Z = x 2−l(x). Then we have

p(x)[− log2 q(x)] = − p(x) log 2−l(x) + p(x) log Z

x∈X

x

x

= p(x)l(x) + log Z .
x

(242) (243)

What about log Z? Let l-1(s) be the set of words that have been assigned codes of
length l. There can only be a limited number of words encoded with a given length. For instance, |L-1(1)| must not be greater than 2, |L-1(2)| must not be greater than 4, and |l-1(s)| must not be greater than 2l. We have

∀s :

[l(x) = s] ≤ 2s

x∈X

∀s :

[l(x) = s]2−s ≤ 1

x∈X

∀s :

2−l(x) ≤ 1

x∈X

(244) (245) (246)

Maths for Intelligent Systems, Marc Toussaint

91

However, this way of thinking is ok for separated codes. If such codes would be in a continuous stream of bits you’d never know where a code starts or ends. Preﬁx codes ﬁx this problem by deﬁning a code tree with leaves that clearly deﬁne when a code ends. For preﬁx codes it similarly holds

Z=

2−l(x) ≤ 1 ,

x∈X

which is called Kraft’s inequality. That ﬁnally gives

(247)

p(x)[− log2 q(x)] ≤ p(x)l(x)

x∈X

x

(248)

5.4 The Laplace approximation: A 2nd-order Taylor of log p

Assume we want to estimate some q(x) we cannot express analytically. E.g., q(x) = p(x|D) ∝ P (D|x)p(x) for some awkward likelihood function p(D|x). An example from robotics is: x is stochastically controlled path of a robot. p(x) is a prior distribution over paths that includes how the robot can actually move and some Gaussian prior (squared costs!) over controlls. If the robot is “linear”, p(x) can be expressed nicely and analytically; it if it non-linear, expressing p(x) is already hard. However, p(D|x) might indicate that we do not see collisions on the path—but collisions are a horrible function, usually computed by some black-box collision detection packages that computes distances between convex meshes, perhaps giving gradients but certainly not some analytic function. So q(x) can clearly not be expressed analytically.
One way to approximate q(x) is the Laplace approximation

Deﬁnition 5.10 (Laplace approximation). Given a smooth distribution q(x), we deﬁne its Laplace approximation as

q˜(x) = exp{−E˜(x)} ,

(249)

where E˜(x) is the 2nd-order Taylor expansion

E˜(x)

=

E(x∗)

+

1 (x

−

x∗)

∇2E(x∗)(x

−

x∗)

2

(250)

of the energy E(x) = − log q(x) at the mode

x∗ = argmin E(x) = argmax q(x) .

x

x

(251)

First, we observe that the Laplace approximation is a Gaussian, because its energy is
a parabola. Further, notice that in the Taylor expansion we skipped the linear term. That’s because we are at the mode x∗ where ∇E(x∗) = 0.

92

Maths for Intelligent Systems, Marc Toussaint

The Laplace approximation really is the probabilistic analoque of a local second-order approximation of a function, just as we used it in Newton methods. However, it is deﬁned to be taken speciﬁcally at the mode of the distribution.
Now, computing x∗ is a classical optimization problem x∗ = argminx E(x) which one might ideally solve using Newton methods. These Newton methods anyway compute the local Hessian of E(x) in every step—at the optimum we therefore have the Hessian already, which is then the precision matrix of our Gaussian.
The Laplace approximation is nice, very eﬃcient to use, e.g., in the context of optimal control and robotics. While we can use the expressive power of probability theory to formalize the problem, the Laplace approximation brings us computationally back to eﬃcient optimization methods.

5.5 Variational Inference

Another reduction of inference to optimization is variational inference.

Deﬁnition 5.11 (variational inference). Given a distribution p(x), and a parameterized family of distributions q(x|β), the variational approximation of p(x) is deﬁned as

argmin D q p
q

(252)

5.6 The Fisher information metric: 2nd-order Taylor of the KLD
Recall our notion of steepest descent—it depends on the metric in the space!
Consider the space of probability distributions p(x; β) with parameters β. We think of every p(x; β) as a point in the space and wonder what metric is useful to compare two points p(x; β1) and p(x; β2). Let’s take the KLD TODO : Let p ∈ ΛX , that is, p is a probability distribution over the space X. Further, let θ ∈ Rn and θ → p(θ) is some parameterization of the probability distribution. Then the derivative dθp(θ) ∈ TpΛX is a vector in the tangent space of ΛX . Now, for such vectors, for tangent vectors of the space of probability distributions, there is a generic metric, the Fisher metric: [TODO: move to ’probabilities’ section]
5.7 Examples and Exercises

Note: These exercises are for ’extra credits’. We’ll discuss them on Thu, 21th Jan.

Maths for Intelligent Systems, Marc Toussaint

93

5.7.1 Maximum Entropy and Maximum Likelihood

(These are taken from MacKay’s book Information Theory..., Exercise 22.12 & .13)
a) Assume that a random variable x with discrete domain dom(x) = X comes from a probability distribution of the form

1

d

P (x | w) =

exp

Z (w)

wkfk(x) ,

k=1

where the functions fk(x) are given, and the parameters w ∈ Rd are not known. A

data set D = {xi}ni=1 of likelihood log P (D|w) =

n points x is supplied.

n i=1

log

P

(xi

|w)

that

the

Show by diﬀerentiating the log maximum-likelihood parameters

w∗ = argmaxw log P (D|w) satisfy

P (x | w∗)

fk (x)

=

1 n

n
fk (xi )

x∈X

i=1

where the left-hand sum is over all x, and the right-hand sum is over the data points. A shorthand for this result is that each function-average under the ﬁtted model must equal the function-average found in the data:

fk P (x | w∗) = fk D

b) When confronted by a probability distribution P (x) about which only a few facts are known, the maximum entropy principle (MaxEnt) oﬀers a rule for choosing a distribution that satisﬁes those constraints. According to MaxEnt, you should select the P (x) that maximizes the entropy
H(P ) = − P (x) log P (x)
x
subject to the constraints. Assuming the constraints assert that the averages of certain functions fk(x) are known, i.e.,

fk P (x) = Fk ,

show, by introducing Lagrange multipliers (one for each constraint, including normalization), that the maximum-entropy distribution has the form

1

PMaxEnt(x) = Z exp

wk fk(x)

k

where the parameters Z and wk are set such that the constraints are satisﬁed. And hence the maximum entropy method gives identical results to maximum likelihood ﬁtting of an exponential-family model.

94

Maths for Intelligent Systems, Marc Toussaint

Note: The exercise will take place on Tue, 2nd Feb. Hung will also prepare how much ‘votes’ you collected in the exercises.

5.7.2 Maximum likelihood and KL-divergence

Assume we have a very large data set D = {xi}ni=1 of samples xi ∼ q(x) from some data distribution q(x). Using this data set we can approximate any expectation

n

f q = q(x)f (x) ≈ f (xi) .

x

i=1

Assume we have a parameteric family of distributions p(x|β) and would ﬁnd the Maximum Likelihood (ML) parameter β∗ = argmaxβ p(D|β). Express this ML problem as a KL-divergence minimization.

5.7.3 Laplace Approximation
In the context of so-called “Gaussian Process Classiﬁcation” the following problem arises (we neglect dependence on x here): We have a real-valued RV f ∈ R with prior P (f ) = N(f | µ, σ2). Further we have a Boolean RV y ∈ {0, 1} with conditional probability
ef P (y = 1 | f ) = σ(f ) = 1 + ef . The function σ is called sigmoid funtion, and f is a discriminative value which predicts y = 1 if it is very positive, and y = 0 if it is very negative. The sigmoid function has the property
∂ σ(f ) = σ(f ) (1 − σ(f )) .
∂f
Given that we observed y = 1 we want to compute the posterior P (f | y = 1), which cannot be expressed analytically. Provide the Laplace approximation of this posterior.

(Bonus) As an alternative to the sigmoid function σ(f ), we can use the probit function

φ(z) =

z −∞

N(x|0, 1)

dx

to

deﬁne

the

likelihood

P (y

=

1|f)

=

φ(f ).

Now how can

the posterior P (f | y = 1) be approximated?

5.7.4 Learning = Compression
In a very abstract sense, learning means to model the distribution p(x) for given data D = {xi}ni=1. This is literally the case for unsupervised learning; regression, classiﬁcation

Maths for Intelligent Systems, Marc Toussaint

95

and graphical model learning could be viewed as speciﬁc instances of this where x factores in several random variables, like input and output.
Show in which sense the problem of learning is equivalent to the problem of compression.

5.7.5 A gzip experiment
Get three text ﬁles from the Web, approximately equal length, mostly text (no equations or stuﬀ). Two of them should be in English, the third in Frensh. (Alternatively, perhaps, not sure if it’d work, two of them on a very similar topic, the third on a very diﬀerent.)
How can you use gzip (or some other compression tool) to estimate the mutual information between every pair of ﬁles? How can you ensure some “normalized” measures which do not depend too much on the absolute lengths of the text? Do it and check whether in fact you ﬁnd that two texts are similar while the third is diﬀerent.

(Extra) Lempel-Ziv algorithms (like gzip) need to build a codebook on the ﬂy. How does that ﬁt into the picture?

5.7.6 Maximum Entropy and ML

(These are taken from MacKay’s book Information Theory..., Exercise 22.12 & .13)
a) Assume that a random variable x with discrete domain dom(x) = X comes from a probability distribution of the form

1

d

P (x | w) =

exp

Z (w)

wkfk(x) ,

k=1

(253)

where the functions fk(x) are given, and the parameters w ∈ Rd are not known. A

data set D = {xi}ni=1 of likelihood log P (D|w) =

n points x is supplied.

n i=1

log

P

(xi

|w)

that

the

Show by diﬀerentiating the log maximum-likelihood parameters

w∗ = argmaxw log P (D|w) satisfy

P (x | w∗)

fk (x)

=

1 n

n
fk (xi )

x∈X

i=1

(254)

where the left-hand sum is over all x, and the right-hand sum is over the data points. A shorthand for this result is that each function-average under the ﬁtted model must equal the function-average found in the data:

fk P (x | w∗) = fk D

(255)

96

Maths for Intelligent Systems, Marc Toussaint

b) When confronted by a probability distribution P (x) about which only a few facts are known, the maximum entropy principle (MaxEnt) oﬀers a rule for choosing a distribution that satisﬁes those constraints. According to MaxEnt, you should select the P (x) that maximizes the entropy

H(P ) = − P (x) log P (x)
x

(256)

subject to the constraints. Assuming the constraints assert that the averages of certain functions fk(x) are known, i.e.,

fk P (x) = Fk ,

(257)

show, by introducing Lagrange multipliers (one for each constraint, including normalization), that the maximum-entropy distribution has the form

1

PMaxEnt(x) = Z exp

wk fk(x)

k

(258)

where the parameters Z and wk are set such that the constraints are satisﬁed. And hence the maximum entropy method gives identical results to maximum likelihood ﬁtting of an exponential-family model.

A Gaussian identities

Deﬁnitions We deﬁne a Gaussian over x with mean a and covariance matrix A as the function

1 N(x | a, A) = |2πA|1/2

1 exp{− (x-a)
2

A-1 (x-a)}

(259)

with property N (x | a, A) = N (a| x, A). We also deﬁne the canonical form with precision matrix A as

N[x | a, A]

=

exp{−

1 2

a

A-1a}

|2πA-1|1/2

1 exp{− x
2

A x + x a}

(260)

with properties

N[x | a, A] = N(x | A-1a, A-1) N(x | a, A) = N[x | A-1a, A-1] .

(261) (262)

Non-normalized Gaussian

N(x, a, A) = |2πA|1/2 N(x|a, A)

1 = exp{− (x-a)

A-1 (x-a)}

2

(263) (264)

Maths for Intelligent Systems, Marc Toussaint

97

Matrices [matrix cookbook: http://www.imm.dtu.dk/pubdb/views/edoc_download.php/3274/ pdf/imm3274.pdf]

(A-1 + B-1)-1 = A (A+B)-1 B = B (A+B)-1 A (A-1 − B-1)-1 = A (B-A)-1 B ∂x|Ax| = |Ax| tr(A-x1 ∂xAx) ∂xA-x1 = −A-x1 (∂xAx) A-x1 (A + U BV )-1 = A-1 − A-1U (B-1 + V A-1U )-1V A-1 (A-1 + B-1)-1 = A − A(B + A)-1A (A + J BJ )-1J B = A-1J (B-1 + J A-1J )-1 (A + J BJ )-1A = I − (A + J BJ )-1J BJ

(265) (266) (267) (268) (269) (270) (271) (272)

(269)=Woodbury; (271,272) holds for pos def A and B

Derivatives

∂xN(x|a, A) = N(x|a, A) (−h ) , h := A-1(x-a)

∂θN(x|a, A) = N(x|a, A) ·

−

h

(∂θ x)

+h

(∂θa) −

1 tr(A-1 2

∂θA) +

1 h
2

(∂θ A)h

∂θN[x|a, A] = N[x|a, A]

−

1 x
2

∂θAx +

1 a
2

A-1 ∂θ AA-1 a

+x

∂θ a

−a

A-1 ∂θ a

+

1 2

tr(∂θ

AA-1

)

∂θNx(a, A) = Nx(a, A) ·

1

h

(∂θ x)

+h

(∂θa) +

h 2

(∂θ A)h

Product The product of two Gaussians can be expressed as
N(x | a, A) N(x | b, B) = N[x | A-1a + B-1b, A-1 + B-1] N(a | b, A + B) , = N(x | B(A+B)-1a + A(A+B)-1b, A(A+B)-1B) N(a | b, A + B) , N[x | a, A] N[x | b, B] = N[x | a + b, A + B] N(A-1a | B-1b, A-1 + B-1) = N[x| . . . ] N[A-1a | A(A+B)-1b, A(A+B)-1B] = N[x| . . . ] N[A-1a | (1-B(A+B)-1) b, (1-B(A+B)-1) B] , N(x | a, A) N[x | b, B]

(273) (274)
(275) (276)
(277) (278) (279) (280) (281)

98

Maths for Intelligent Systems, Marc Toussaint

= N[x | A-1a + b, A-1 + B] N(a | B-1b, A + B-1) = N[x| . . . ] N[a | (1-B(A-1+B)-1) b, (1-B(A-1+B)-1) B]

(282) (283)

Convolution x N(x | a, A) N(y − x | b, B) dx = N(y | a + b, A + B)
Division N(x|a, A) N(x|b, B) = N(x|c, C) N(c|b, C + B) C-1c = A-1a − B-1b C-1 = A-1 − B-1 N[x|a, A] N[x|b, B] ∝ N[x|a − b, A − B]

(284)
(285) (286)

Expectations Let x ∼ N(x | a, A),
E{x} g(x) := x N(x | a, A) g(x) dx E{x} x = a , E{x} xx = A + aa E{x} f + F x = f + F a E{x} x x = a a + tr(A) E{x} (x-m) R(x-m) = (a-m) R(a-m) + tr(RA)

(287) (288) (289) (290) (291)

Transformation Linear transformations imply the following identities,

N(x | a, A) = N(x + f | a + f, A) , N(x | a, A) = |F | N(F x | F a, F AF )

N(F x + f | a, A) = 1 N(x | F -1(a − f ), F -1AF - ) |F |

= 1 N[x | F A-1(a − f ), F A-1F ] , |F |

N[F x + f | a, A] =

1 |F |

N[x |

F

(a − Af ),

F

AF ] .

(292) (293) (294) (295)

“Propagation” (propagating a message along a coupling, using eqs (277) and (283), respectively)

y N(x | a + F y, A) N(y | b, B) dy = N(x | a + F b, A + F BF )

(296)

Maths for Intelligent Systems, Marc Toussaint

99

y N(x | a + F y, A) N[y | b, B] dy = N[x | (F - -K)(b + BF -1a), (F - -K)BF -1] , (297)

K = F - B(F - A-1F -1+B)-1

(298)

marginal & conditional:

N(x | a, A) N(y | b + F x, B) = N

x y

b

a +F

a

,

A FA

AF B+FA F

(299)

N

x y

a b

,

A C

C B

= N(x | a, A) · N(y | b + C A-1(x-a), B − C A-1C)

(300)

N[x | a, A] N(y | b + F x, B) = N

x y

a

+ F B-1b B-1b

,

A + F B-1F −B-1F

−F B-1 B-1
(301)

N[x | a, A] N[y | b + F x, B] = N

x y

a

+

F b

B-1b

,

A + F B-1F −F

−F B

(302)

N

x y

a b

,

A C

C B

= N[x | a − CB-1b, A − CB-1C ] · N[y | b − C x, B]

(303)

A D

C B

= |A| |B| = |A| |B| , where

A = A − CB-1D B = B − DA-1C

(304)

A C -1

A-1

−A-1 C B -1

D B = −B-1DA-1

B-1

(305)

=

A-1 −B-1DA-1

−A-1 C B -1 B-1

(306)

pair-wise belief We have a message α(x) = N[x|s, S], transition P (y|x) = N(y|Ax + a, Q), and a message β(y) = N[y|v, V ], what is the belief b(y, x) = α(x)P (y|x)β(y)?

b(y, x) = N[x|s, S] N(y|Ax + a, Q-1) N[y|v, V ]

=N

x y

s 0

,

S 0

0 0

N

x y

A Q-1a Q-1a

,

A Q-1A −Q-1A

∝N

x y

s + A Q-1a v + Q-1a

,

S + A Q-1A −Q-1A

−A Q-1 V + Q-1

−A Q-1 Q-1

(307)

N

x y

0 v

,

0 0

0 V

(308)

(309)

100

Maths for Intelligent Systems, Marc Toussaint

Entropy 1
H(N(a, A)) = log |2πeA| 2

(310)

Kullback-Leibler divergence

p = N(x|a, A) ,

q = N(x|b, B) ,

n = dim(x) ,

p(x) D p q = p(x) log
q(x)
x
(311)

2 D p q = log |B| + tr(B-1A) + (b − a) B-1(b − a) − n |A|

(312)

4 Dsym p q = tr(B-1A) + tr(A-1B) + (b − a) (A-1 + B-1)(b − a) − 2n (313)

λ-divergence 2 Dλ p q = λ D p λp + (1−λ)q + (1−λ) D p (1−λ)p + λq

(314)

For λ = .5: Jensen-Shannon divergence.

Log-likelihoods

1 log N(x|a, A) = −

log|2πA| + (x-a)

A-1 (x-a)

2

1 log N[x|a, A] = −

log|2πA-1| + a A-1a + x Ax − 2x a

2

N(x|b, B) log N(x|a, A) = −D N(b, B) N(a, A) − H(N(b, B))

x

(315) (316) (317)

Mixture of Gaussians Collapsing a MoG into a single Gaussian

argmin D pi N(ai, Ai) N(b, B)

b,B

i

= b = piai , B = pi(Ai + aiai − b b )

i

i

(318) (319)

B 3D geometry basics (for robotics)
This document introduces to some basic geometry, focussing on 3D transformations, and introduces proper conventions for notation. There exist one-to-one implementations of the concepts and equations in libORS.

