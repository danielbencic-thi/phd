IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Typesetting math: 51%

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Conferences > 2021 International Conference...
LIDAR-based Stabilization, Navigation and Localization for UAVs Operating in Dark Indoor Environments
Publisher: IEEE
Cite This
PDF
  << Results   
Matěj Petrlík ; Tomáš Krajník ; Martin Saska
All Authors
View Document
1
Paper
Citation
205
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    Multimedia Material
    I.
    Introduction
    II.
    Related Work
    III.
    2D LIDAR-Based Localization
    IV.
    Scan Matching

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
More Like This
Footnotes

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: Autonomous operation of UAVs in a closed environment requires precise and reliable pose estimate that can stabilize the UAV without using external localization systems su... View more
Metadata
Abstract:
Autonomous operation of UAVs in a closed environment requires precise and reliable pose estimate that can stabilize the UAV without using external localization systems such as GNSS. In this work, we are concerned with estimating the pose from laser scans generated by an inexpensive and lightweight LIDAR. We propose a localization system for lightweight (under 200 g) LIDAR sensors with high reliability in arbitrary environments, where other methods fail. The general nature of the proposed method allows deployment in wide array of applications. Moreover, seamless transitioning between different kinds of environments is possible. The advantage of LIDAR localization is that it is robust to poor illumination, which is often challenging for camera-based solutions in dark indoor environments and in the case of the transition between indoor and outdoor environment. Our approach allows executing tasks in poorly-illuminated indoor locations such as historic buildings and warehouses, as well as in the tight outdoor environment, such as forest, where vision-based approaches fail due to large contrast of the scene, and where large well-equipped UAVs cannot be deployed due to the constrained space.
Published in: 2021 International Conference on Unmanned Aircraft Systems (ICUAS)
Date of Conference: 15-18 June 2021
Date Added to IEEE Xplore : 19 July 2021
ISBN Information:
ISSN Information:
INSPEC Accession Number: 20902139
DOI: 10.1109/ICUAS51884.2021.9476837
Publisher: IEEE
Conference Location: Athens, Greece
Funding Agency:
Contents
Multimedia Material

A video attachment to this work is available at: http://mrs.felk.cvut.cz/icuas2021lidar
SECTION I.
Introduction

Estimating the pose of a UAV is a challenging problem due to limited weight capacity, which implies restricted processing power and lightweight imperfect sensors. The proposed localization system is designed with the limitations of micro-scale UAVs in mind. The method runs online onboard the UAV to provide real-time estimates of the UAV pose which are fed back to the controller to stabilize the UAV. A full 3D space pose estimate can be obtained even using a cheap, small and lightweight 2D LIDAR thanks to the proposed robust data fusion approach. Compared to state-of-the-art visual odometry approaches, our method is primarily targeted for use in compact environments with high density of obstacles such as for example historic buildings and forests, where cameras cannot be used due to the poor lighting conditions. The ability to stabilize a UAV is proven by experimental verification with real UAV platform in real-world conditions.
Fig. 1: - The left image shows our UAV deployed to document the interiors of the chlumin church for the historians. In the right image, the UAV is navigating through a forest.
Fig. 1:

The left image shows our UAV deployed to document the interiors of the chlumin church for the historians. In the right image, the UAV is navigating through a forest.

Show All

The control algorithms of unmanned aerial vehicles (UAVs) are precise enough for deployment in tight, enclosed environments with obstacles, which enables inspection, documentation and monitoring of indoor areas of warehouses, historic buildings, churches, or temples. In particular, the non-linear SE(3) state feedback controller based on the work in [1] and [2] is widely adopted by robotic teams working with UAVs. The authors of [3] propose to use a heterogeneous team consisting of a UGV and UAV to automatize the inventory process in a warehouse. The UGV carries the UAV to a rack of goods, where the UAV takes off and scans the codes of the goods in the rack and then lands on the UGV again which moves to a next rack. In [4] , a team of autonomous UAVs is used to inspect the Saint Nicholas church in Prague and Virgin Mary church in Stern-berk, Czech Republic by applying illumination techniques consisting of a camera and multiple light sources carried by a self-stabilized team of UAVs. The methods of predictive control are used to optimize the angles and ranges of light sources while taking the motion and formation constraints into account.

To reliably navigate through an environment, a precise state estimate is required to follow the desired trajectory successfully while avoiding surrounding obstacles. This work is concerned with localization of the UAV, i.e., obtaining the state estimate from available onboard sensors. The most often used localization technique is based on Global Navigation Satellite System (GNSS) technologies. These techniques require an open environment with satellite visibility and even then provide pose estimate with a standard deviation of several meters.

In GNSS-denied locations (indoor environment, close to large buildings, forests, mines, caves) the UAVs are often localized using an image stream from one or more onboard cameras. Especially visual simultaneous localization and mapping (SLAM) [5] , and visual inertial odometry (VIO) [6] methods have received a lot of attention thanks to the pose error reduction during loop closure and robustness to lost feature tracking respectively. However, cameras require favorable lighting conditions to function optimally [7] , due to the limited dynamic range of the sensor. The worst scenarios for camera-based localization are:

    Dark areas with insufficient illumination, typically churches, ruins, mines, tunnels, or any tasks performed in the night cause underexposure of the whole image.

    Source of light pointing directly into the camera, which happens during outdoor missions when horizontally mounted cameras are pointed into the sun, but can also happen in buildings with windows or bright lighting sources.

    High contrast scenes where part of the image is overexposed and part underexposed, which often occurs in forests - one of our target applications.

The problem of insufficient illumination can be overcome by using structured light or Time-of-Flight cameras. These cameras allow localization in total darkness at the cost of reduced resolution and range, with most of them becoming unusable in outdoor applications due to sunlight [8] .

We propose a solution that provides the pose estimate by aligning laser scans obtained from a small 2D LIDAR. When operating in a tight space, the size of the UAV is limited, so only low-weight payload can be carried. It implies that heavy sensors such as the Velodyne PUCK that provide 3D scans of the surroundings cannot be used. Lightweight LIDARs (Scanse Sweep 1 , SLAMTEC RPLIDAR 2 ) are more suitable for this task, since they can be carried by small UAVs, and are less expensive, which makes them widely used. However, the small form factor also means a lower number of points per scan, lower scanning frequency, lower range, noise with higher variance and more false detections. Most importantly, the sensor provides 2D scans, while the UAV is a 6 DOF robot moving through a 3D environment. As a result, the raw 2D laser scan without compensating the UAV tilt does not represent the surroundings of the UAV accurately when placed in the 3D space, and the full position estimate cannot be obtained without measurements from other sensors. We address these issues in the proposed system by modifying and combining multiple state-of-the-art algorithms so that they work well with the available data.
SECTION II.
Related Work

In [9] the authors present autonomous localization and mapping in GNSS denied environments for UAVs. They use IMU and LIDAR measurements to estimate the position by matching consecutive scans while simultaneously building a map. The current laser scan is matched to the previous one by a customized ICP algorithm to obtain the current position of the UAV. For building the map, Principal Component Analysis (PCA) was used. The method uses spectral decomposition of the covariance matrix of clusters in the laser scan to fit line segments to them. Although the proposed method has a similar motivation as the system proposed in this paper, it has a few shortcomings compared to our approach. The sequential localization that is prone to drift is not corrected by the gradually built map. The map is not scalable to arbitrary environments as it assumes that the geometry of the environment consists of only line features. Moreover, the experimental verification is done by post-processing recorded datasets, which does not guarantee real-time performance.

In contrast to [9] , we present a clear demonstration of applicability to our platform in Section V . Our solution in addition to the tasks solved in [9] contains the fusion of position obtained from localization in a global map, velocity from the displacement of sequential scans and attitude measurements from UAV gyroscopes.

A comprehensive control, localization, navigation, and mapping solution for indoor UAV is introduced in [10] . The controller accepts state estimates from a linear Kalman filter which is fusing IMU accelerations with velocities obtained by computing the optical flow of a downward facing camera. The position estimate can, however, drift over time, since no measurement of absolute position is fused in the Kalman filter. The authors suppress this issue by implementing reactive path planning using potential fields. A global map is built by the FastSLAM particle filter algorithm, which is introduced in [11] . The FastSLAM algorithm was run offline on a desktop computer with the data obtained during the flight. We propose to connect localization, navigation, control, and mapping into one pipeline running online onboard of the UAV. Our approach has the benefit of serving as a low-level no-maintenance layer for higher level algorithms for which it supplies the globally consistent map and the absolute position.

In the manuscript [12] , a SLAM solution aimed at autonomous navigation in forests is presented. The laser scans from LIDAR are processed, clustering is applied, and clusters that are validated by a series of geometric descriptors are classified as features corresponding to trees. These features are matched between successive scans to obtain a translation measurement, which is then fused with IMU estimate in a Kalman filter. The fused position estimate tends to drift so a back-end GraphSLAM [13] algorithm is implemented to detect loop closures with sufficient overlap between the current feature set and the previously visited feature sets. The proposed method is backed by convincing experiments with both simulated dataset, and a real UAV platform. Compared to raw scan matching techniques such as [14] , the feature-based approach is more robust to noise and outliers. Nevertheless, the geometric-based feature detector restricts the use for environments with tree-like features only. Hence the technique cannot be used for a UAV deployed in another environment.

A combination of 2D LIDAR and IMU mounted on a spring is proposed in [15] , where the oscillations of the springs modify the scan plane of the sensor, so that 3D data can be obtained. It is a low-cost alternative to multi-channel 3D LIDARs that scan the environment with multiple spinning beams. However, the weight of 0.5 kg is too heavy compared for the small UAVs considered within this paper. Also, the oscillating mass could potentially destabilize the UAV.

A thorough experimental evaluation of scan registration algorithms for mapping purposes is presented in [16] . The authors designed a custom 3D rotating LIDAR to be able to obtain 3D laser scans of the environment. The original ICP algorithm [17] registers 3D geometrical shapes or points, which is improved in Generalized ICP [18] to allow plane-to-plane registration. In NDT [19] , [20] the points are not registered directly. First, a normal distribution grid map is modeled from one scan, then the second scan is registered into this model. The performance of theses algorithms is measured in terms of Absolute Trajectory Error (ATE) [21] and Mean Map Entropy (MME) [22] . While [16] evaluates the precision of alignment of laser scans or point clouds, which is an important metric, it does not guarantee that the method will be suitable for stabilizing a UAV carrying a lightweight 2D LIDAR.

Our proposed method is more focused on the ability to provide a solution even under non-ideal conditions such as low number of scan points and high outlier/inlier ratio. Furthermore, the method needs to output the estimated motion in real time to achieve stability of the control feedback loop. By addressing these common shortcomings we achieve a general, reliable method suitable for real-world applications, which is verified by numerous experiments in Section V .
SECTION III.
2D LIDAR-Based Localization

The proposed approach is divided into modules forming a pipeline illustrated in Fig. 2 .

Each laser scan Z r a w t produced by the LIDAR mounted on the moving UAV is degraded by noise, outliers, and false detections. These issues are addressed by the Scan processing module before scan matching so that the scans faithfully represent the surroundings.

Traditionally, the scan Z t is aligned to the previous scan Z t − 1 to obtain the displacement Δ x between the two scans. The UAV position cannot be determined reliably by directly integrating the relative displacements of successive scans as it is done by most of the systems proposed in literature, due to a drift accumulated from small errors in each scan matching. Instead of directly integrating the relative displacements, we use the known timestamp of each scan to calculate the UAV velocity x ˙ t = Δ x / Δ t , where Δ t is the time difference between the two scans that are aligned in the Sequential matching module. Simultaneously, Z t is also aligned by the Global matching module to the global map, which is gradually built in the Mapping module, to obtain the global position estimate x t . Both estimates x t and x ˙ t are fused in the Kalman filter to produce the final drift-free position estimate x ^ t .
Fig. 2: - Diagram showing the sequence of modules in the localization pipeline.
Fig. 2:

Diagram showing the sequence of modules in the localization pipeline.

Show All
Fig. 3: - Submodules of the scan processing pipeline, with the raw laser scan $\mathbf{Z}_{t}^{raw}$ on the input and processed scan $\mathbf{Z}_{t}$ on the output. The scan attitude module has to be supplied by UAV roll, pitch, roll angles ($\phi,\theta,\psi$), the ground removal module needs the altitude estimate $\hat{h}_{t}$, and the external removal module requires the last known position $\mathbf{x}_{t-1}$.
Fig. 3:

Submodules of the scan processing pipeline, with the raw laser scan Z r a w t on the input and processed scan Z t on the output. The scan attitude module has to be supplied by UAV roll, pitch, roll angles ( ϕ , θ , ψ ), the ground removal module needs the altitude estimate h ^ t , and the external removal module requires the last known position x t − 1 .

Show All

A. Scan Processing

Each scan Z t is preprocessed before x t and x ˙ t can be estimated. The sequence of operations performed on the laser scan is shown in Fig. 3 . The processed scan is a subset Z t = Z r a w t ∖ ( Z c l o s e t ∪ Z g r o u n d t ∪ Z n o i s e t ∪ Z e x t e r n a l t ) converted from polar to Cartesian coordinates, and transformed by the UAV attitude and altitude.

The subset of close points Z c l o s e t ⊆ Z r a w t contains only points that satisfy the inequality r < R u a v , where r is the range of the point in polar coordinates and R u a v is the radius of the UAV. In the case of the platform used for verification experiments shown in Fig. 1 , the radius of removed close points is R u a v = 0.395 m since the radius of the body frame is 0.275 m plus the radius of the propeller, which is 0.12 m. The points in Z g r o u n d t ⊆ Z r a w t correspond to laser beams that hit the ground. After transforming the points by the UAV altitude and attitude, and converting to Cartesian coordinates, the ground points satisfy p z < h m i n , where h m i n is the ground threshold. Setting h m i n to a higher value allows filtering out sloped terrain. On the other hand, with higher h m i n , more points will be removed from the already quite scarce scan. Another subset Z n o i s e t ⊆ Z r a w t contains the noise points that appear in the scans at random places due to the sensor imperfection. Each point that has less than two neighbors in its radius belongs to this subset. The last subset Z e x t e r n a l t ⊆ Z r a w t contains points outside of the flight area. These points could be highly dynamic and potentially ruin the estimation when there are more dynamic than static points present in the scans, hence it is safer to filter them out.
B. Sequential Matching

The purpose of this module is to align two successive scans Z t and Z t − 1 in a way that minimizes the cost function that will be introduced later in Section IV . The transformation, which has to be applied to acquire such alignment is also the transformation between the UAV poses x t and x t − 1 from which those two scans were obtained. How the two scans are aligned is in detail described in Section IV . The output of the matching method is a rigid transformation in 2D which is a combination of rotation φ and translation [ T x , T y ].

After the scans are aligned, and the final transformation between them is known, the current linear velocity of the UAV is obtained in the following way:
x ˙ t = [ T x T y ] / Δ t , (1)
View Source Right-click on figure for MathML and additional features. \begin{equation*}\dot{\mathbf{x}}_{t}= \begin{bmatrix}T_{x}\\ T_{y}\end{bmatrix}/\Delta t,\tag{1} \end{equation*} where Δ t is the time difference between scans Z t − 1 and Z t obtained from the timestamps which are filled every time a new scan is acquired from the sensor. Similarly, the angular velocity is obtained as
φ ˙ t = φ / Δ t . (2)
View Source Right-click on figure for MathML and additional features. \begin{equation*}\dot{\varphi}_{t}=\varphi/\Delta t.\tag{2}\end{equation*}

C. Global Matching

This module estimates the absolute position of the UAV in the flight area. To achieve this goal, the current preprocessed scan Z t and the latest available map state M t − 1 is required to localize the UAV. The map can either be prepared in advance of built simultaneously during the flight.

The map M t − 1 is cropped around x t − 1 to the radius r c = 1.2 r m a x , with r m a x being the maximum range of the sensor. As can be seen in Fig. 4 , the built map is 3D, with cross-section varying at different heights. To ensure correct alignment, only a vertical slice around the current flight height of the UAV is considered.

The cropping reduces the complexity of correspondence search and also limits the search to a small neighborhood which has the advantage that when there are two places in the map in which the crops are identical (such as corners formed by two walls), there is no ambiguity and the wrong match is not considered.

The actual scan alignment process is the same combination of state-of-the-art methods as in the sequential matching. A detailed explanation resides in Section IV . When scan matching of Z t to \mathbf{M}_{t-1} is finished, the module outputs the transformation matrix \mathbf{T}_{t} . The scan \mathbf{Z}_{t} is then transformed by the transform \mathbf{T}_{t} and current absolute position estimate is extracted from \mathbf{T}_{t} . If the UAV traveled more then 0.5 m since the last map update, \mathbf{Z}_{t} is passed to the mapping module to get integrated into the global map. The resulting estimated absolute position is passed to the Kalman filter to be fused with other measurements. The global matching is run at 1 Hz, which is less than the frequency of sequential matching. The reason is that the global matching estimate should correct the drift accumulated from the sequential matching. The drift after 1 s is negligible, therefore, running the global matching more frequently does not bring any advantage and unnecessarily wastes CPU resources.
D. Mapping

The map is stored in the octree data structure [23] with resolution constrained to r=0.2\, \mathrm{m} . A new scan is integrated into the map every second, adding only points that satisfy \Vert\mathbf{p}_{\mathbf{i}}-\mu(o_{j})\Vert^{2}\geq r for all octree cells o_{j}\in \mathcal{O} with \mu being the center of the cell. When queried for the current state of the map, the module generates a point cloud representation \mathbf{M} with points \mathbf{p}_{j}=\mu(o_{j}) for j=1, \ldots, \vert \mathcal{O}\vert . The mapping module also keeps a dense point cloud composed from all successfully aligned scans that can be used as a side output of UAV missions in an unknown environment to provide an overview for end-users. Such obtained 3D model of the church in Stara Voda can be seen in Fig. 4 .
E. Kalman Filter

The outputs of sequential and global matching are fused as measurements of velocity and position respectively in a linear Kalman filter. Since the components of the UAV position \mathbf{r}=[x, y]^{T} are independent on each other, the estimation can be decoupled for each axis, which decreases the size of the state-space description. The filters for x and y axes have an identical model, and only x-axis estimation will be discussed for brevity.

The state vector for x-axis is defined as \mathbf{x}=(x,\dot{x},\ddot{x}) , and the discrete formulation of the dynamic LTI stochastic system is \begin{align*} \mathbf{x}_{t+1}&=\mathbf{Ax}_{t}+\mathbf{Bu}_{t}+\mathbf{v}_{t},\tag{3}\\ \mathbf{y}_{t}&=\mathbf{Cx}_{t}+\mathbf{Du}_{t}+\mathbf{e}_{t},\tag{4} \end{align*}
View Source Right-click on figure for MathML and additional features. \begin{align*} \mathbf{x}_{t+1}&=\mathbf{Ax}_{t}+\mathbf{Bu}_{t}+\mathbf{v}_{t},\tag{3}\\ \mathbf{y}_{t}&=\mathbf{Cx}_{t}+\mathbf{Du}_{t}+\mathbf{e}_{t},\tag{4} \end{align*} where \mathbf{x}_{t} is the state vector, \mathbf{u}_{t} is the input vector in the sample t . The process noise \mathbf{v}_{t} and measurement noise \mathbf{e}_{t} describe the stochastic part of the system. The noise is drawn from the normal distributions \mathbf{v}_{t}\sim \mathcal{N}(0,\mathbf{Q}) and \mathbf{e}_{t}\sim \mathcal{N}(0,\mathbf{R}) respectively, i.e., zero-mean Gaussian noise is assumed. The rotation around the world-fixed z-axis is estimated using a two-state model {\varphi}=(\varphi,\dot{\varphi}) .

Since the proposed method is developed for UAVs with 2D LIDARs, the altitude or height of the UAV above ground is unobservable from the LIDAR scans only. To estimate the UAV height, another filter with the three-state model \mathbf{z}= (z,\dot{z},\ddot{z}) is used. The gravity-aligned acceleration measured by the IMU drives the prediction step of the filter. When navigating enclosed spaces, the height above terrain is more informative than e.g., above-sea-level altitude. Consequently, the measurements from a downward facing laser rangefinder are fused in the filter as corrections. Additional correction consisting of the rate of change of barometric altitude is applied to the velocity state to mitigate high-innovation changes in state estimate when briefly flying above an obstacle.
SECTION IV.
Scan Matching

The core of the proposed solution is a method that estimates the difference \Delta\mathbf{x} between UAV positions \mathbf{x}_{t_{1}} and \mathbf{x}_{t_{2}} at time t_{1},\ t_{2}\in \mathbb{R}^{+} reliably in arbitrary environment with obstacles. The problem of finding \Delta\mathbf{x} corresponds to finding the rigid 2D transformation between laser scans \mathbf{Z}_{t_{1}} and \mathbf{Z}_{t_{2}} that were taken at positions \mathbf{x}_{t_{1}} and \mathbf{x}_{t_{2}} respectively. This section presents the details of the implemented scan matching algorithm that finds \mathbf{T} (initialized to the identity matrix) by aligning \mathbf{Z}_{t_{2}} to \mathbf{Z}_{t_{1}} as follows:

    Establish correspondences between points from \mathbf{Z}_{t_{2}} and \mathbf{Z}_{t_{1}} such that \begin{equation*}c(\mathbf{q}_{i})=\underset{\mathbf{p}_{j}\in \mathbf{Z}_{t_{2}}}{\arg\!\min}\Vert \mathbf{q}_{i}-\mathbf{p}_{j}\Vert^{2},\tag{5}\end{equation*}
    View Source Right-click on figure for MathML and additional features. \begin{equation*}c(\mathbf{q}_{i})=\underset{\mathbf{p}_{j}\in \mathbf{Z}_{t_{2}}}{\arg\!\min}\Vert \mathbf{q}_{i}-\mathbf{p}_{j}\Vert^{2},\tag{5}\end{equation*} where \mathbf{q}_{i} is the i -th point from the laser scan \mathbf{Z}_{t_{1}} and \mathbf{p}_{j} the j -th point from \mathbf{Z}_{t_{2}} with i\in\{1,\ \ldots,\vert \mathbf{Z}_{t_{1}}\vert\} and j\in\{1,\ldots,\vert \mathbf{Z}_{t_{2}}\vert\} . The operator \vert \mathbf{Z}\vert is the number of points in scan Z.

    Compute the transformation that aligns the laser scans by minimizing the error function. \begin{equation*}E(\mathbf{T})=\sum\limits_{i=1}^{n}\Vert \mathbf{q}_{i}-\mathbf{T}c(\mathbf{q}_{i})\Vert^{2}.\tag{6}\end{equation*}
    View Source Right-click on figure for MathML and additional features. \begin{equation*}E(\mathbf{T})=\sum\limits_{i=1}^{n}\Vert \mathbf{q}_{i}-\mathbf{T}c(\mathbf{q}_{i})\Vert^{2}.\tag{6}\end{equation*}

    Apply the obtained transformation to every \mathrm{p}_{j}\in \mathrm{Z}_{t_{\mathrm{Q}}} .

    Evaluate the quality of alignment and iterate all steps until a stopping condition is met.

Fig. 4: - The 3D model of the stara voda church as scanned by the LIDAR and aligned online onboard the UAV by our scan matching algorithm. The colors represent the height of captured points - red for lowest points, blue for highest points.
Fig. 4:

The 3D model of the stara voda church as scanned by the LIDAR and aligned online onboard the UAV by our scan matching algorithm. The colors represent the height of captured points - red for lowest points, blue for highest points.

Show All

The original ICP algorithm is not suitable for aligning scans with small overlaps and a high number of outliers. These properties are however typical for most of the scans coming from a moving UAV equipped with a slowly rotating LIDAR that outputs a relatively small amount of points. The Scanse Sweep LIDAR used during testing of the stabilization system generates scans at 5 Hz. Each scan contains less than 200 points. The algorithm was thus modified to allow precise and reliable alignment of non-ideal scans. Analysis of each step of the proposed approach follows.
A. Establishing Correspondences

Finding the appropriate correspondences is critical to correct scan alignment. The rate of convergence (number of iterations until reaching minimum), robustness to noise and outliers, and the time complexity all depend on the choice of correspondences.

The information carried by high-density scans is often redundant, and thus can benefit from finding distinctive features that represent well the geometry of the environment with much less data points. The state-of-the-art algorithms [24] , [25] that are tailored for multi-channel LIDARs use these geometric features in correspondence search and minimization of their distance. While feature extraction can provide a unique representation of the environment, with low-density scans it cannot find sufficient amount of features to estimate the transformation between scans reliably. Point-to-point correspondences are more appropriate for a sensor with low sampling rate since the measured point cloud is rather sparse and obtaining unambiguous geometric features is not possible due to insufficient detail of scanned objects. Since the point-to-point correspondence search does not discard any points, no spatial information is lost in the process, and the transformation can still be estimated even in situations where feature-based methods fail.
Fig. 5: - A transformation $\mathbf{T}_{iter}$ is found by combining translation $\mathrm{T}_{x},\ \mathrm{T}_{y}$, from the ICP algorithm, with the rotation $\omega$ obtained from IMRP method. Then the total transformation $\mathbf{T}_{total}$ is updated and applied to $\mathbf{Z}_{t_{2}}^{0}$ to obtain $\mathbf{Z}_{t_{2}}$ which is used to estimate $\mathbf{T}_{iter}$ in the next iteration.
Fig. 5:

A transformation \mathbf{T}_{iter} is found by combining translation \mathrm{T}_{x},\ \mathrm{T}_{y} , from the ICP algorithm, with the rotation \omega obtained from IMRP method. Then the total transformation \mathbf{T}_{total} is updated and applied to \mathbf{Z}_{t_{2}}^{0} to obtain \mathbf{Z}_{t_{2}} which is used to estimate \mathbf{T}_{iter} in the next iteration.

Show All

1) Linear Interpolation

In the original ICP algorithm [17] , as it is used in state-of-the-art literature, the closest point correspondences are used. The correspondence c(\mathbf{p}_{j}) of point \mathbf{p}_{j}\in \mathbf{Z}_{t_{2}} is found as the closest point \mathbf{q}_{i}\in \mathbf{Z}_{t_{1}} in terms of Euclidean distance.

To achieve the accuracy of matching, in sparse point clouds, we propose using the linear interpolation of \mathbf{Z}_{t_{1}} . The neighbor point of \mathbf{q}_{i} that is closer to \mathbf{p}_{i} is selected to be the adjacent point \mathbf{q}_{a} . A virtual correspondence \mathbf{q}_{v} is formed on the closest point of the line segment \overline{\mathbf{q}_{i}\mathbf{q}_{a}} as shown in Fig. 6 .
2) Iterative Matching Range Point

As mentioned in [14] , the Euclidean distance metric leads to fast convergence of the components T_{x},\ T_{y} of the transformation between two scans. However, the convergence of the rotational component \omega is much slower, which is to be expected, as the Euclidean distance metric of two points yields no explicit information about the rotation. As a result, the algorithm tries to minimize the rotational error by translation rather than rotation, thus causing an unnecessary decrease in the rate of convergence of the whole algorithm, which is a problem when the matching is constrained by the real-time requirement.
Fig. 6: - The correspondence search on the line segments. Red line represents the correspondence found by regular ICP, i.e., the closest point $c(\mathbf{p}_{j})=\mathbf{q}_{i}$. Searching for the correspondence on a line segment produces a virtual point $\mathbf{q}_{v}$ lying between the closest point $\mathbf{q}_{i}$ and the adjacent point $\mathbf{q}_{a}$. The correspondence then becomes $c(\mathbf{p}_{j})=\mathbf{q}_{v}$.
Fig. 6:

The correspondence search on the line segments. Red line represents the correspondence found by regular ICP, i.e., the closest point c(\mathbf{p}_{j})=\mathbf{q}_{i} . Searching for the correspondence on a line segment produces a virtual point \mathbf{q}_{v} lying between the closest point \mathbf{q}_{i} and the adjacent point \mathbf{q}_{a} . The correspondence then becomes c(\mathbf{p}_{j})=\mathbf{q}_{v} .

Show All

Similarly as [14] , we use the Iterative matching range point (IMRP) to improve the performance of rotation estimation. The structure of the algorithm is the same as of ICP, and the only difference is that the correspondence search uses a rotation-based metric. The correspondences are searched in the polar coordinates in which the laser scans are natively represented. For every point \mathbf{p}_{j}=(r_{j},\varphi_{j})\in \mathbf{Z}_{t_{1}} , we find the corresponding point \mathbf{q}_{i}=(r_{i},\ \varphi_{i})\in \mathbf{Z}_{t_{2}} that satisfies \varphi_{i}\in[\varphi_{j}-B_{\omega},\ \varphi_{j}+B_{\omega}] and minimizes the metric \Vert r_{j}- r_{i}\Vert^{2} , where B_{\omega} is the maximal expected rotation error. The expected maximal rotation error B_{\omega} is subject to change in every iteration as the scan alignment converges. Since the rotational residual decreases exponentially with the number of iterations, we set B_{\omega}(t)=B_{\omega}(0)e^{-\alpha t} to prevent incorrect correspondences in later iterations. The constant \alpha=0.03 was used in all experiments.
3) Iterative Dual Correspondences

Scan matching based on IMRP converges faster in the rotational component. On the other hand, the convergence of translation is slower than in ICP. To obtain the best possible estimate, we use ICP correspondences for estimating the translation and IMRP correspondences for rotation estimation as illustrated in Fig. 5 . This approach is referred to as Iterative dual correspondences (IDC) in [14] .
4) Outlier Rejection

The found correspondences typically contain a large number of outliers. The points measured from position \mathbf{x}_{t_{1}} that cannot be seen from position \mathbf{x}_{t_{2}} due to moving out of range of the sensor or occlusion do not have a correct correspondence pair and have to be rejected to prevent the incorrect alignment.

Robustness to outliers is improved by incorporating the fraction of inliers into the distance function [26] thus forming a new distance measure Fractional Root Mean Squared Distance (FRMSD) which is defined as \begin{equation*}\text{FRMSD}\, (\mathbf{Z}_{t_{1}},\, \mathbf{Z}_{t_{2}},\, f, c)=\frac{1}{f^{\lambda}}\sqrt{\frac{1}{f\cdot n}\sum\limits_{\mathbf{p}\in \mathbf{Z}_{t_{2}}}\Vert \mathbf{p}-c(\mathbf{p})\Vert^{2}},\tag{7}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}\text{FRMSD}\, (\mathbf{Z}_{t_{1}},\, \mathbf{Z}_{t_{2}},\, f, c)=\frac{1}{f^{\lambda}}\sqrt{\frac{1}{f\cdot n}\sum\limits_{\mathbf{p}\in \mathbf{Z}_{t_{2}}}\Vert \mathbf{p}-c(\mathbf{p})\Vert^{2}},\tag{7}\end{equation*} where f\in[0,1] is the fraction of n points in \mathbf{Z}_{t_{2}} sorted by \Vert \mathbf{p}-c(\mathbf{p})\Vert^{2} that will be used for alignment.

The parameter \lambda depends on the expected outlier density, and according to [26] should be set to \lambda=1.2 for general-purpose scan matching in two dimensions. By fine-tuning \lambda , one can adjust the ratio of false positives to false negatives. A smaller value of \lambda can reject correct correspondences as outliers while on the other hand, higher values result in more outliers classified as inliers.
5) Weighting

Even with the outliers filtered out, correspondences with greater distance still contribute to the estimated transform more than correspondences separated by a smaller gap. The performance of the matching is improved by assigning a weight to each correspondence pair as: \begin{equation*}w(d_{i})=1- \frac{d_{i}}{\max(d_{1,\ldots, n})},\tag{8}\end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*}w(d_{i})=1- \frac{d_{i}}{\max(d_{1,\ldots, n})},\tag{8}\end{equation*} where w(\cdot) is the weighting function and d_{i} the distance of i-th correspondence pair.

B. Computing Transformation

Once the correspondences have been established, we can compute the registration, i.e., the transformation that minimizes the cost function defined as the sum of squared residuals \begin{align*} E(\mathbf{T})=\sum\limits_{i=1}^{n}\left((x_{i}\cos\omega- y_{i}\sin\omega+ T_{x}- x_{i}^{\prime})^{2}\right.\qquad&\\ \left.+(x_{i}\sin\omega+ y_{i}\cos\omega+ T_{y}- y_{i}^{\prime})^{2}\right).&\tag{9} \end{align*}
View Source Right-click on figure for MathML and additional features. \begin{align*} E(\mathbf{T})=\sum\limits_{i=1}^{n}\left((x_{i}\cos\omega- y_{i}\sin\omega+ T_{x}- x_{i}^{\prime})^{2}\right.\qquad&\\ \left.+(x_{i}\sin\omega+ y_{i}\cos\omega+ T_{y}- y_{i}^{\prime})^{2}\right).&\tag{9} \end{align*}

Minimizing E(\mathbf{T}) we get \begin{align*} \omega&= \arctan \frac{S_{xy^{\prime}}- S_{yx^{\prime}}}{S_{xx^{\prime}}+ S_{yy^{\prime}}},\\ T_{x}&= \mu_{x}^{\prime}-(\mu_{x}\cos\omega- \mu_{y}\sin\omega),\\ T_{y}&= \mu_{y}^{\prime}-(\mu_{y}\sin\omega+ \mu_{y}\cos\omega),\tag{10} \end{align*}
View Source Right-click on figure for MathML and additional features. \begin{align*} \omega&= \arctan \frac{S_{xy^{\prime}}- S_{yx^{\prime}}}{S_{xx^{\prime}}+ S_{yy^{\prime}}},\\ T_{x}&= \mu_{x}^{\prime}-(\mu_{x}\cos\omega- \mu_{y}\sin\omega),\\ T_{y}&= \mu_{y}^{\prime}-(\mu_{y}\sin\omega+ \mu_{y}\cos\omega),\tag{10} \end{align*} with \mu being the weighted mean and S_{ab^{\prime}} the weighted covariance defined as \begin{equation*} S_{ab^{\prime}}=\sum\limits_{i=1}^{n}w_{i}(a_{i}-\mu_{a})(b_{i}^{\prime}-\mu_{b}^{\prime}).\tag{11} \end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*} S_{ab^{\prime}}=\sum\limits_{i=1}^{n}w_{i}(a_{i}-\mu_{a})(b_{i}^{\prime}-\mu_{b}^{\prime}).\tag{11} \end{equation*}

C. Evaluation and Iteration

The solution convergence is checked by the difference of FRMSD between the current and previous iteration. Since the error typically decreases exponentially, the difference between the consecutive errors is compared against a close-to-zero constant and if the difference is smaller, the algorithm has converged and further iterations are not necessary. Similarly, when the FRMSD is less than 1 cm the matching does not continue. The time available for matching is set to 50 ms to satisfy the real-time requirement of localization. When the time limit is exceeded the matching will be stopped, regardless of the current error.
Fig. 7: - The left image is stitched from a video with the UAV position snapshot taken every 10 seconds during the outdoor stabilization experiment. The crosses on the right image represent the final map with outliers depicted in red.
Fig. 7:

The left image is stitched from a video with the UAV position snapshot taken every 10 seconds during the outdoor stabilization experiment. The crosses on the right image represent the final map with outliers depicted in red.

Show All

SECTION V.
Experimental Verification
A. Outdoor Stabilization Experiments

The first experimental setup was realized at an outdoor experimental site with obstacles to prove that the localization system can stabilize the UAV even in a difficult environment with a low number of points in the laser scans (mean number of points in each scan was 49 after preprocessing). Although our method was developed primarily for indoor usage, this outdoor experiment proves the robustness of LIDAR-based solution in direct sunlight, which is a situation, where camera-based methods can fail. The Scanse Sweep LIDAR was used in this experiment. The UAV was commanded to follow a trajectory that ends close to the takeoff location to give the localization system the possibility of loop closure. The obtained map and estimated trajectory with GNSS trajectory for comparison are shown in Fig. 7 . The trajectories do not match due to the GNSS estimate drifting a few meters after takeoff. A small but noticeable control error can be observed as the setpoint was not tracked precisely, which could be fixed by increasing the controller position gain.
B. Chlumin Church Evaluation

A dataset containing ground truth pose trajectory was prepared to evaluate the precision of the localization algorithm. The recording of the dataset took place in the Chlumin church. Our experimental UAV platform was equipped with lightweight RPLIDAR A3 sensor, and was flown manually around the inside walls by an experienced pilot. The ground truth was obtained from the Leica Nova MS60 total station 3 that tracks the UAV using a laser beam reflected by a prism carried by the tracked UAV. Fig. 8 shows both sequential and global trajectories compared to the ground truth poses. The evaluation of the performance of the developed localization method was split into sequential performance and global performance, since each step of our system affects the local and global matching differently.
Table I: The absolute trajectory error (ate), heading RMSE (hdg), mean map entropy (mme) measures after modifications, which are described in Section IV, are added to the standard ICP algorithm. The error metrics are calculated with respect to the ground truth trajectory from the tracking total station from the chlumin dataset.
Table I:- The absolute trajectory error (ate), heading RMSE (hdg), mean map entropy (mme) measures after modifications, which are described in Section IV, are added to the standard ICP algorithm. The error metrics are calculated with respect to the ground truth trajectory from the tracking total station from the chlumin dataset.

We quantified the localization performance by the absolute trajectory error (ATE) [21] with respect to the ground truth trajectory obtained from the tracking total station. Fig. 9 depicts the ATE of sequential and global trajectories. Since the ATE represents mainly the translational components of the error (although heading errors can manifest themselves as a wrong translation) we include also the RMSE of the estimated heading. The trajectories of the heading as estimated by sequential and global matching are compared to compass and ground truth in Fig. 10 . Additionally we calculated the map mean entropy (MME) [22] as \begin{equation*} H\, (\mathbf{M})=\frac{1}{\vert \mathbf{M}\vert}\sum\limits_{k=1}^{\vert \mathbf{M}\vert}\frac{1}{2}\ln\vert 2\pi e\Sigma(q_{k})\vert,\tag{12} \end{equation*}
View Source Right-click on figure for MathML and additional features. \begin{equation*} H\, (\mathbf{M})=\frac{1}{\vert \mathbf{M}\vert}\sum\limits_{k=1}^{\vert \mathbf{M}\vert}\frac{1}{2}\ln\vert 2\pi e\Sigma(q_{k})\vert,\tag{12} \end{equation*} where \Sigma(q_{k}) is the sample covariance of mapped points in local radius r around map point q_{k} . The correlation between the ATE and the sharpness of the map represented by the MME can be used to assess position estimate quality of the Stara Voda dataset even in the absence of ground truth. The Table I shows the results of the evaluation of our modified ICP.

C. Stara Voda Church Test Flight

The second indoor experimental verification was performed in the Stara Voda church. We have chosen a larger church with different geometric layout and different sensor (Scanse Sweep) to verify the robustness of the proposed method. Despite no available ground truth, the dataset features a loop closure (the same spot for takeoff and landing) that can be used to evaluate the error at the end of the trajectory. This test flight illustrates the benefit of having a global position estimate that eliminates the drift by closing the loop at the end of the trajectory shown in Fig. 8 . The Euclidean distance between the first and the last point of the trajectory is taken to describe the drift at the end of the trajectory. While the drift of sequential matching was 0.7925 m, the global matching improved the final error to 0.3779 m. The quality of the final map quantified by the MME measure is 0.3293. The higher MME score compared to the best values achieved in the Chlumin dataset ( Table I ) is caused by the more noisy Scanse Sweep sensor and also the takeoff position was cluttered by a lot of people and small objects.
Fig. 8: - The sequential (blue) and global (green) trajectories were obtained during a rectangular flight in the chlumin church (first two plots) and the stara voda church (last two plots). The ground truth trajectory (red) from tracking total station is included in the Chlumin dataset to evaluate the performance of the method. A global map represented by a point cloud was gradually built online. The first and third plots show results of our proposed localization method, the second and fourth plots are obtained using the unmodified state-of-the-art ICP.
Fig. 8:

The sequential (blue) and global (green) trajectories were obtained during a rectangular flight in the chlumin church (first two plots) and the stara voda church (last two plots). The ground truth trajectory (red) from tracking total station is included in the Chlumin dataset to evaluate the performance of the method. A global map represented by a point cloud was gradually built online. The first and third plots show results of our proposed localization method, the second and fourth plots are obtained using the unmodified state-of-the-art ICP.

Show All
Fig. 9: - The ATE of the sequential and global trajectories with respect to ground truth in the Chlumin dataset.
Fig. 9:

The ATE of the sequential and global trajectories with respect to ground truth in the Chlumin dataset .

Show All
Fig. 10: - The sequential estimate of heading trajectory from the Chlumin dataset drifts over time. By aligning scans into global map, we can obtain heading estimate that has lower error than compass.
Fig. 10:

The sequential estimate of heading trajectory from the Chlumin dataset drifts over time. By aligning scans into global map, we can obtain heading estimate that has lower error than compass.

Show All

D. Applications

The proposed system was developed primarily for indoor navigation in churches and cathedrals where the insufficient lighting conditions prevent camera-based localization. The historians are interested in photographs of murals and mosaics that are not reachable from the ground. In addition to the photo documentation, the proposed light-weight solution of the localization based on 2D short-range LIDAR produces 3D models in the form of aligned point clouds from the mapping module.
Fig. 11: - UAV equipped with a lightweight LIDAR navigates through a narrow mining tunnel with sensing degraded by dust (left). A map of the tunnel is gradually built while simultaneously estimating the trajectory of the UAV (right).
Fig. 11:

UAV equipped with a lightweight LIDAR navigates through a narrow mining tunnel with sensing degraded by dust (left). A map of the tunnel is gradually built while simultaneously estimating the trajectory of the UAV (right).

Show All

One of the documentation missions was performed in Saint Anne Church in Stara Voda, which was damaged by Soviet soldiers during the occupation of the Czech Republic. Several photographs of objects of interest were taken for the historians to assess the level of damage and state the necessary restoration works. A virtual model of the church (shown in Fig. 4 ) was captured by the onboard LIDAR.

Furthermore, the modularity of the system allowed easy adaptation and extension with navigation and path planning modules for deployment in DARPA Subterranean challenge. The map in Fig. 11 was acquired during one of many flights through a mining tunnel at the DARPA Subterranean integration exercise.
SECTION VI.
Conclusions

We presented a pose estimation system for UAVs based on the alignment of laser scans from a lightweight 2D laser rangefinder as an alternative to large, heavy, and expensive 3D LIDARs. The system is formed by a pipeline that fuses velocity estimate from sequential scan alignment with a global pose in a gradually built map to obtain a globally consistent smooth estimate, which is used by the controller to stabilize the UAV.

The proposed solution allows autonomous operation in indoor environments with insufficient illumination for camera-based solutions. The large amount of applications where our UAV platforms were deployed in different kinds of environments, and with various sensor configurations were possible thanks to the proposed general and robust system.

Authors
Figures
References
Citations
Keywords
Metrics
Footnotes
   Back to Results   
More Like This
Global path planning in mobile robot using omnidirectional camera

2011 International Conference on Consumer Electronics, Communications and Networks (CECNet)

Published: 2011
Mobile robot localization and path planning using an omnidirectional camera and infrared sensors

2009 IEEE International Conference on Systems, Man and Cybernetics

Published: 2009
Show More
References
1. T. Lee, M. Leok and N. H. McClamroch, "Geometric tracking control of a quadrotor uav on se(3)", CDC , pp. 5420-5425, 2010.
Show in Context View Article Full Text: PDF (1179) Google Scholar
2. D. Mellinger and V. Kumar, "Minimum snap trajectory generation and control for quadrotors", ICRA , pp. 2520-2525, 2011.
Show in Context View Article Full Text: PDF (1980) Google Scholar
3. E. H. C. Harik, F. Guérin, F. Guinand, J. Brethé and H. Pelvillain, "Towards an autonomous warehouse inventory scheme", SSCI , pp. 1-8, 2016.
Show in Context View Article Full Text: PDF (1226) Google Scholar
4. M. Saska, V. Krátký, V. Spurný and T. Báča, "Documentation of dark areas of large historical buildings by a formation of unmanned aerial vehicles using model predictive control", ETFA , pp. 1-8, 2017.
Show in Context View Article Full Text: PDF (9681) Google Scholar
5. C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, et al., "Past present and future of simultaneous localization and mapping: Toward the robust-perception age", IEEE Transactions on Robotics , vol. 32, no. 6, pp. 1309-1332, 2016.
Show in Context View Article Full Text: PDF (812) Google Scholar
6. J. Delmerico and D. Scaramuzza, "A benchmark comparison of monocular visual-inertial odometry algorithms for flying robots", ICRA , pp. 2502-2509, 2018.
Show in Context View Article Full Text: PDF (1166) Google Scholar
7. H. Alismail, M. Kaess, B. Browning and S. Lucey, "Direct visual odometry in low light using binary descriptors", RAL , vol. 2, no. 2, pp. 444-451, 2017.
Show in Context View Article Full Text: PDF (1150) Google Scholar
8. R. Horaud, M. Hansard, G. Evangelidis and C. Ménier, "An overview of depth cameras and range scanners based on time-of-flight technologies", Machine Vision and Applications , vol. 27, no. 7, pp. 1005-1020, 2016.
Show in Context CrossRef Google Scholar
9. R. Opromolla, G. Fasano, G. Rufino, M. Grassi and A. Savvaris, "Lidar-inertial integration for uav localization and mapping in complex environments", ICUAS , pp. 649-656, 2016.
Show in Context View Article Full Text: PDF (478) Google Scholar
10. F. Wang, J.-Q. Cui, B.-M. Chen and T. H. Lee, "A comprehensive uav indoor navigation system based on vision optical flow and laser fastslam", Acta Automatica Sinica , vol. 39, no. 11, pp. 1889-1899, 2013.
Show in Context CrossRef Google Scholar
11. M. Montemerlo and S. Thrun, FastSLAM: A Scalable Method for the Simultaneous Localization and Mapping Problem in Robotics, Springer-Verlag, 2007.
Show in Context Google Scholar
12. J. Q. Cui, S. Lai, X. Dong, P. Liu, B. M. Chen and T. H. Lee, "Autonomous navigation of uav in forest", ICUAS , pp. 726-733, 2014.
Show in Context View Article Full Text: PDF (2481) Google Scholar
13. S. Thrun and M. Montemerlo, "The graph slam algorithm with applications to large-scale mapping of urban structures", The International Journal of Robotics Research , vol. 25, no. 5–6, pp. 403-429, 2006.
Show in Context CrossRef Google Scholar
14. F. Lu and E. Milios, "Robot pose estimation in unknown environments by matching 2d range scans", Journal of Intelligent and Robotic Systems , vol. 18, no. 3, pp. 249-275, 1997.
Show in Context CrossRef Google Scholar
15. M. Bosse, R. Zlot and P. Flick, "Zebedee: Design of a spring-mounted 3-d range sensor with application to mobile mapping", IEEE Transactions on Robotics , vol. 28, no. 5, pp. 1104-1119, 2012.
Show in Context View Article Full Text: PDF (19228) Google Scholar
16. J. Razlaw, D. Droeschel, D. Holz and S. Behnke, "Evaluation of registration methods for sparse 3d laser scans", ECMR , pp. 1-7, 2015.
Show in Context View Article Full Text: PDF (2523) Google Scholar
17. P. J. Besl and N. D. McKay, "A method for registration of 3-d shapes", IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 14, no. 2, pp. 239-256, 1992.
Show in Context View Article Full Text: PDF (1674) Google Scholar
18. A. Segal, D. Hähnel and S. Thrun, Generalized-icp, 2009.
Show in Context CrossRef Google Scholar
19. M. Magnusson, A. Lilienthal and T. Duckett, "Scan registration for autonomous mining vehicles using 3d-ndt", Journal of Field Robotics , vol. 24, no. 10, pp. 803-827, 2007.
Show in Context CrossRef Google Scholar
20. T. Stoyanov, M. Magnusson, H. Andreasson and A. J. Lilienthal, "Fast and accurate scan registration through minimization of the distance between compact 3d ndt representations", The International Journal of Robotics Research , vol. 31, no. 12, pp. 1377-1393, 2012.
Show in Context CrossRef Google Scholar
21. J. Sturm, N. Engelhard, F. Endres, W. Burgard and D. Cremers, "A benchmark for the evaluation of rgb-d slam systems", IROS , 2012.
Show in Context View Article Full Text: PDF (1462) Google Scholar
22. D. Drocschel, J. Stückler and S. Behnke, "Local multi-resolution representation for 6d motion estimation and mapping with a continuously rotating 3d laser scanner", ICRA , pp. 5221-5226, 2014.
Show in Context Google Scholar
23. A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss and W. Burgard, "OctoMap: An efficient probabilistic 3D mapping framework based on octrees", Autonomous Robots , 2013.
Show in Context CrossRef Google Scholar
24. J. Zhang and S. Singh, "Loam: Lidar odometry and mapping in real-time", Robotics: Science and Systems , vol. 2, no. 9, 2014.
Show in Context CrossRef Google Scholar
25. T. Shan, B. Englot, D. Meyers, W. Wang, C. Ratti and D. Rus, Lio-sam: Tightly-coupled lidar inertial odometry via smoothing and mapping, 2020.
Show in Context Google Scholar
26. J. M. Phillips, R. Liu and C. Tomasi, "Outlier robust ICP for minimizing fractional RMSD", CoRR , vol. abs/cs/0606098, 2006.
Show in Context Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
