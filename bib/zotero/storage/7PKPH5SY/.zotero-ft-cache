IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathZoom.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Conferences > 2014 IEEE International Confe...
Multi-sensor fusion for robust autonomous flight in indoor and outdoor environments with a rotorcraft MAV
Publisher: IEEE
Cite This
PDF
  << Results   
Shaojie Shen ; Yash Mulgaonkar ; Nathan Michael ; Vijay Kumar
All Authors
View Document
80
Paper
Citations
3979
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Previous Work
    III.
    Multi-Sensor System Model
    IV.
    Ukf-Based Multi-Sensor Fusion
    V.
    Implementation Details

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
Media
More Like This
Footnotes

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: We present a modular and extensible approach to integrate noisy measurements from multiple heterogeneous sensors that yield either absolute or relative observations at di... View more
Metadata
Abstract:
We present a modular and extensible approach to integrate noisy measurements from multiple heterogeneous sensors that yield either absolute or relative observations at different and varying time intervals, and to provide smooth and globally consistent estimates of position in real time for autonomous flight. We describe the development of algorithms and software architecture for a new 1.9kg MAV platform equipped with an IMU, laser scanner, stereo cameras, pressure altimeter, magnetometer, and a GPS receiver, in which the state estimation and control are performed onboard on an Intel NUC 3 rd generation i3 processor. We illustrate the robustness of our framework in large-scale, indoor-outdoor autonomous aerial navigation experiments involving traversals of over 440 meters at average speeds of 1.5 m/s with winds around 10 mph while entering and exiting buildings.
Published in: 2014 IEEE International Conference on Robotics and Automation (ICRA)
Date of Conference: 31 May-7 June 2014
Date Added to IEEE Xplore : 29 September 2014
Electronic ISBN: 978-1-4799-3685-4
Print ISSN: 1050-4729
INSPEC Accession Number: 14616853
DOI: 10.1109/ICRA.2014.6907588
Publisher: IEEE
Conference Location: Hong Kong, China
Contents
SECTION I.
Introduction

Rotorcraft micro aerial vehicles (MAVs) are ideal platforms for surveillance and search and rescue in confined indoor and outdoor environments due to their small size, superior mobility, and hover capability. In such missions, it is essential that the MAV is capable of autonomous flight to minimize operator workload. Robust state estimation is critical to autonomous flight especially because of the inherently fast dynamics of MAVs. Due to cost and payload constraints, most MAVs are equipped with low cost proprioceptive sensors (e.g. MEMS IMUs) that are incapable for long term state estimation. As such, exteroceptive sensors, such as GPS, cameras, and laser scanners, are usually fused with proprioceptive sensors to improve estimation accuracy.

Besides the well-developed GPS-based navigation technology [1] , [2] . There is recent literature on robust state estimation for autonomous flight in GPS-denied environments using laser scanners [3] , [4] , monocular camera [5] , [6] , stereo cameras [7] , [8] , and RGB-D sensors [9] . However, all these approaches rely on a single exteroceptive sensing modality that is only functional under certain environment conditions. For example, laser-based approaches require structured environments vision based approaches demand sufficient lighting and features, and GPS only works outdoors. This makes them prone to failure in large-scale environments involving indoor-outdoor transitions, in which the environment can change significantly. It is clear that in such scenarios, multiple measurements from GPS, cameras, and lasers may be available, and the fusion of all these measurements yields increased estimator accuracy and robustness. In practice, however, this extra information is either ignored or handled as switching between sensor suites [10] .
Fig. 1. - Our 1.9 kg MAV platform equipped with an IMU, laser scanner, stereo cameras, pressure altimeter, magnetometer, and GPS receiver. All the computation is performed onboard on an intel NUC computer with 3rdgeneration i3 processor.
Fig. 1.

Our 1.9 kg MAV platform equipped with an IMU, laser scanner, stereo cameras, pressure altimeter, magnetometer, and GPS receiver. All the computation is performed onboard on an intel NUC computer with 3rdgeneration i3 processor.

Show All

The main goal of this work is to develop a modular and extensible approach to integrate noisy measurements from multiple heterogeneous sensors that yield either absolute or relative observations at different and varying time intervals, and to provide smooth and globally consistent estimates of position in real time for autonomous flight. The first key contribution, that is central to our work, is a principled approach, building on [11] , to fusing relative measurements by augmenting the vehicle state with copies of previous states to create an augmented state vector for which consistent estimates are obtained and maintained using a filtering framework. A second significant contribution is our Unscented Kalman Filter (UKF) formulation in which the propagation and update steps circumvent the difficulties that result from the semi-definiteness of the covariance matrix for the augmented state. Finally, we demonstrate results with a new experimental platform ( Fig. 1 ) to illustrate the robustness of our framework in large-scale, indoor-outdoor autonomous aerial navigation experiments involving traversals of over 440 meters at average speeds of 1.5 m/s with winds around 10 MPh while entering and exiting two buildings.

Next, we present previous work on which our work is based. In Section III , we outline the modeling framework before presenting the key contributions of UKF-based sensor fusion scheme in Section IV . We bring all the ideas together in our description of the experimental platform and the experimental results in Section VI .
SECTION II.
Previous Work

We are interested in applying constant computation complexity filtering-based approaches, such as nonlinear variants of the Kalman filter, to fuse all available sensor information. We stress that although SLAM-based multi-sensor fusion approaches [12] , [13] yield optimal results, they are computationally expensive for real-time state feedback for the purpose of autonomous control.

While it is straightforward to fuse multiple absolute measurements such as GPS, pressure/laser altimeter in a recursive filtering formulation, the fusion of multiple relative measurements obtained from laser or visual odometry are more involved. It is common to accumulate the relative measurements with the previous state estimates fuse them as pseudo-absolute measurements [5] , [14] . However, such fusion is sub-optimal since the resulting global position and yaw covariance is inconsistently small compared to the actual estimation error. This violates the observability properties [6] , which suggests that such global quantities are in fact unobservable. As such, we develop our method based on state augmentation techniques [11] to properly account for the state uncertainty when applying multiple relative measurements from multiple sensors.

We aim to develop a modular framework that allows easy addition and removal of sensors with minimum coding and mathematical derivation. We note that in the popular EKF-based formulation [5] , [8] , the computation of Jacobians can be problematic for complex systems like MAVs. As such, we employ a loosely coupled, derivative-free Unscented Kalman Filter (UKF) framework [1] . Switching from EKF to UKF poses several challenges, which will be detailed and addressed in Sect. IV-A . [15] is similar to our work. However, the EKF-based estimator in [15] does not support fusion of multiple relative measurements.
SECTION III.
Multi-Sensor System Model

We define vectors in the world and body frames as ( ⋅ ) w and ( ⋅ ) b respectively. For the sake of brevity, we assume that all onboard sensors are calibrated and are attached to the body frame. The main states of the MAV is defined as:
x = [ p w , Φ w , p ˙ b , b b a , b b ω , b w z ] T
View Source Right-click on figure for MathML and additional features. {\rm x}=[{\rm p}^{w}, \Phi^{w},\dot{{\rm p}}^{b}, {\rm b}_{a}^{b}, {\rm b}_{\omega}^{b}, {\rm b}_{z}^{w}]^{{\rm T}} where p w = [ x w , y w , z w ] T is the 3D position in the world frame, Φ w = [ ψ w , θ w , ϕ w ] T is the yaw, pitch, and roll Euler angles that represent the 3-D orientation of the body in the world frame 1 , from which a matrix R w b that represent the rotation of a vector from the body frame to the world frame can be obtained. p ˙ b is the 3D velocity in the body frame. b b a and b b ω are the bias of the accelerometer and gyroscope, both expressed in the body frame. b w z models the bias of the laser and/or pressure altimeter in the world frame.

We consider an IMU-based state propagation model:
u t = v t = x t + 1 = [ a b , ω b ] T [ v a , V ω , v b a , v b ω , v b z ] T f ( x t , u t , v t ) (1)
View Source Right-click on figure for MathML and additional features. \eqalignno{{\rm u}_{t}= & [{\rm a}^{b}, \omega^{b}]{\rm T}\cr {\rm v}_{t}= & [{\rm v}_{a}, {\rm V}_{\omega}, {\rm v}_{{\rm b}_{a}}, {\rm v}_{{\rm b}_{\omega}}, {\rm v}_{{\rm b}_{z}}]{\rm T}\cr {\rm x}_{t+1}= & f({\rm x}_{t}, {\rm u}_{t}, {\rm v}_{t})&\hbox{(1)}} where u is the measurement of linear accelerations and angular velocities from the IMU in the body frame. v t ∼ N ( 0 , D t ) ∈ R m is the process noise. V a and v ω represent additive noise associated with the gyroscope and the accelerometer. v b a v b ω , v b z model the Gaussian random walk of the gyroscope, accelerometer and altimeter bias. The function f ( ⋅ ) is a discretized version of the continuous time dynamical equation [6] .

Exteroceptive sensors are usually used to correct the errors in the state propagation. Following [11] , we consider measurements as either being absolute or relative , depending on the nature of the underlying sensor. We allow an arbitrary number of either absolute or relative measurement models.
A. Absolute Measurements

All absolute measurements can be modeled in the form:
z t + m = h a ( x t + m , n t + m ) (2)
View Source Right-click on figure for MathML and additional features. {\rm z}_{t+m}=h_{a}({\rm x}_{t+m}, {\rm n}_{t+m}) \eqno{\hbox{(2)}} where n t + m ∼ N ( 0 , Q t ) ∈ R p is the measurement noise that can be either additive or not. h a ( ⋅ ) is in general a nonlinear function. An absolute measurement connects the current state with the sensor output. Examples are shown in in Sect. V-B .

B. Relative Measurements

A relative measurement connects the current and the past states with the sensor output, which can be written as:
z t + m = h r ( x t + m , x t , n t + m ) (3)
View Source Right-click on figure for MathML and additional features. {\rm z}_{t+m}=h_{r}({\rm x}_{t+m}, {\rm x}_{t}, {\rm n}_{t+m}) \eqno{\hbox{(3)}}

The formulation accurately models the nature of odometry-like algorithms ( Sect. V-C and Sect. V-D ) as odometry measures the incremental changes between two time instants of the state. We also note that, in order to avoid temporal drifting, most state-of-the-art laser/visual odometry algorithms are keyframe based. As such, we allow multiple future measurement ( m ∈ M , | M | > 1 ) that corresponds to the same past state x t .
SECTION IV.
Ukf-Based Multi-Sensor Fusion

We wish to design a modular sensor-fusion filter that is easily extensible even for inexperienced users. This means that amount of coding and mathematical deviation for the addition/removal of sensors should be minimal. One disadvantage of the popular EKF-based filtering framework is the requirement of computing the Jacobian matrices, which is proven to be difficult and time consuming for a complex MAV system. As such, we employ the derivative-free UKF-based approach [1] . The key of UKF is the approximation of the propagation of Gaussian random vectors through nonlinear functions via the propagation of sigma points. Let x ∼ N ( x ^ , P x x ) ∈ R n and consider the nonlinear function:
y = g ( x ) , (4)
View Source Right-click on figure for MathML and additional features. {\rm y}=g({\rm x}), \eqno{\hbox{(4)}} and let:
X = [ x ^ , x ^ ± ( ( n + λ ) P x x − − − − − − − − − √ ) i ]   f o r   i = 1 , … , n Y i = g ( X i ) , (5)
View Source Right-click on figure for MathML and additional features. \eqalignno{&{\cal X}=\left[\hat{{\rm x}},\hat{{\rm x}}\pm(\sqrt{\left(n+\lambda\right){\rm P}^{{\rm xx}}})_{i}\right]\ {\rm for}\ i=1, \ldots, n\cr &{\cal Y}_{i}=g({\cal X}_{i}),&\hbox{(5)}} where g ( ⋅ ) is a nonlinear function, λ is a UKF parameter. ( ( n + λ ) P x x − − − − − − − − − √ ) i is the i t h column of the square root covariance matrix which is usually computed via Cholesky decomposition. And X are called the sigma points. The mean, covariance of the random vector y , and the cross-covariance between x and y , can be approximated as:
y ^ P y y P y x = ∑ i = 0 2 n w m i Y i = ∑ i = 0 2 n w c i ( Y i − y ^ ) ( Y i − y ^ ) T = ∑ i = 0 2 n w c i ( Y i − y ^ ) ( X i − x ^ ) T (6)
View Source Right-click on figure for MathML and additional features. \eqalignno{\hat{{\rm y}}& =\sum\limits_{i=0}^{2n}w_{i}^{m}{\cal Y}_{i}\cr \displaystyle {\rm P}^{{\rm yy}}& =\sum\limits_{i=0}^{2n}w_{i}^{c}({\cal Y}_{i}-\hat{{\rm y}})({\cal Y}_{i}-\hat{{\rm y}})^{{\rm T}}\cr {\rm P}^{{\rm yx}}& =\sum\limits_{i=0}^{2n}w_{i}^{c}({\cal Y}_{i}-\hat{{\rm y}})({\cal X}_{i}-\hat{{\rm x}})^{{\rm T}}&\hbox{(6)}} where w m i and w c i are weights for the sigma points. This unscented transform can be used to keep track of the covariance in both the state propagation and measurement update, thus avoiding the need of a Jacobian-based covariance approximation.

A. State Augmentation for Multiple Relative Measurements

Since a relative measurement depends both the current and past states, it is a violation of the fundamental assumption in the Kalman filter that the measurement should only depend on the current state. One way to deal with this is through state augmentation [11] , where a copy of the past state is maintained in the filter. Here we present an extension of [11] to handle arbitrary number of relative measurement models with the possiblity that multiple measurements correspond to the same augmented state. Our generic filtering framework allows convenient setup, and facilitates addition and removal of absolute and relative measurement models.

Note that a measurement may not affect all components in the state X . For example, a visual odometry only affects the 6-DOF pose, not the velocity or the bias terms. We define the i t h augmented state as x i ∈ R n i , n i ≤ n . x i is an arbitrary subset of x. We define a binary selection matrix B i of size n i × n , such that x i = B i x . Consider a time instant, there are I augmented states in the filter, along with the covariance:
x ˇ = [ x ^ , x ^ 1 , … , x ^ I ] T P ˇ = ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ P x x P x 1 X ⋮ P x I X P x ⊃ c 1 P x 1 x 1 ⋮ P x I x 1 ⋯ ⋯ ⋱ ⋯ P x x I P x 1 x I ⋮ P x I x I ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ (7)
View Source Right-click on figure for MathML and additional features. \eqalignno{&\check{{\rm x}}=[\hat{{\rm x}},\hat{{\rm x}}_{1}, \ldots,\hat{{\rm x}}_{I}]^{{\rm T}}\cr &\check{{\rm P}}=\left[\matrix{ {\rm P}^{{\rm xx}} & {\rm P}^{{\rm x}\supset {\rm c}_{1}} & \cdots & {\rm P}^{{\rm xx}_{I}}\cr {\rm P}^{{\rm x}_{1^{{\rm X}}}} & {\rm P}^{{\rm x}_{1}{\rm x}_{1}} & \cdots & {\rm P}^{{\rm x}_{1}{\rm x}_{I}}\cr \vdots & \vdots & \ddots & \vdots \cr {\rm P}^{{\rm x}_{I^{{\rm X}}}} & {\rm P}^{{\rm x}_{I}{\rm x}_{1}} & \cdots & {\rm P}^{{\rm x}_{I}{\rm x}_{I}} }\right]&\hbox{(7)}}

The addition of a new augmented state x I + 1 can be done by:
x ˇ + = M + x ˇ , M + = [ I n + Σ I n i B I + 1 ] (8)
View Source Right-click on figure for MathML and additional features. \check{{\rm x}}^{+}={\rm M}^{+}\check{{\rm x}},\quad {\rm M}^{+}=\left[\matrix{{{\rm I}_{n+\Sigma_{I}n_{i}}}\cr {{\rm B}_{I+1}}}\right]\eqno{\hbox{(8)}}

Similarly, the removal of an augmented state x j is given as:
x ˇ − = M − x ˇ , M − = [ I a 0 b × n 0 a × n j 0 b × n j 0 a × b I b ] ,
View Source Right-click on figure for MathML and additional features. \check{{\rm x}}^{-}={\rm M}^{-}\check{{\rm x}}, {\rm M}^{-}=\left[\matrix{ {\rm I}_{a} & 0_{a\times n_{j}} & 0_{a\times b}\cr 0_{b\times n} & 0_{b\times n_{j}} & {\rm I}_{b} }\right], where a = n + ∑ j − 1 i = 1 n i and b = ∑ I i = j + 1 n i . The updated augmented state covariance is given as:
P ˇ ± = M ± P ˇ M ± T .
View Source Right-click on figure for MathML and additional features. \check{{\rm P}}^{\pm}={\rm M}^{\pm}\check{{\rm P}}{\rm M}^{\pm {\rm T}}.

The change of keyframes in an odometry-like measurement model is simply the removal of an augmented state x i followed by the addition of another augmented state with the same B i . Since we allow multiple relative measurements that correspond to the same augmented state, contrast to [11] , augmented states are not deleted after measurement updates ( Sect. IV-D ).

This state augmentation formulation works well in an EKF setting, however, it poses issues when we try to apply it to the UKF. Since the addition of a new augmented state (8) is essentially a copy of the main state. The resulting covariance matrix p ˇ + will not be positive definite, and the Cholesky decomposition (5) for state propagation will fail (non-unique). We now wish to have something that is similar to the Jacobian matrices for EKF, but without explicitly computing the Jacobians.
B. Jacobians for UKF

In [16] , the authors present a new interpretation of the UKF as a Linear Regression Kalman Filter (LRKF). In LRKF, we seek to find the optimal linear approximation y = A x + b + e of the nonlinear function (4) given a weighted discrete (or sigma points (6) ) representation of the distribution N ( x ^ , P x x ) . The objective is to find the regression matrix A and vector b that minimize the linearizion error e:
min A , b ∑ i = 0 2 n w i ( Y i − A X i − b ) ( Y i − A X i − b ) T .
View Source Right-click on figure for MathML and additional features. \min\limits_{{\rm A},{\rm b}}\sum\limits_{i=0}^{2n}w_{i}({\cal Y}_{i}-{\rm A}{\cal X}_{i}-{\rm b})({\cal Y}_{i}-{\rm A}{\cal X}_{i}-{\rm b})^{{\rm T}}.

As shown in [16] , the optimal linear regression is given by:
A = P y x P x x − 1 , b = y ^ − A x ^ (9)
View Source Right-click on figure for MathML and additional features. {\rm A}={\rm P}^{{\rm yx}}{\rm P}^{{\rm xx}^{-1}},\quad {\rm b}=\hat{{\rm y}}-{\rm A}\hat{{\rm x}} \eqno{\hbox{(9)}}

The linear regression matrix A in (9) serves as the linear approximation of the nonlinear fuction (4) . It is similar to the Jacobian in the EKF formulation. As such, the propagation and update steps in UKF can be performed in a similar fashion as EKF.
C. State Propagation

Observing the fact that during state propagation only the main state changes, we start off by partitioning the augmented state and the covariance (7) into:
x ˇ t | t = [ x ^ t | t x ^ i T t | t ] , P ˇ t | t = [ P x x t | t P x I X t | t P x x I t | t P x I x I t | t ] .
View Source Right-click on figure for MathML and additional features. \check{{\rm x}}_{t\vert t}=\left[\matrix{ \hat{{\rm x}}_{t\vert t}\cr \hat{{\rm x}}_{iT_{t\vert t}} }\right],\quad \check{{\rm P}}_{t\vert t}=\left[\matrix{{\rm P}_{t\vert t}^{{\rm xx}} &{\rm P}_{t\vert t}^{{\rm x}{{\rm x}}{\cal I}}\cr {\rm P}_{t\vert t}^{{\rm x}_{{\cal I}^{{\rm X}}}} &{\rm P}_{t \vert t}^{{\rm x}{\cal I}{\rm x}{\cal I}}}\right].

The linear approximation of the nonlinear state propagation (1) , applied on the augmented state (7) , is:
x ˇ t + 1 | t = f ( x ˇ t | t , u t , v t ) = [ F t 0 0 I | I | ] x ˇ t | t + [ J t 0 G t 0 ] [ u t v t ] + b t + e t , (10)
View Source Right-click on figure for MathML and additional features. \eqalignno{\check{{\rm x}}_{t+1\vert t}&=f(\check{{\rm x}}_{t\vert t}, {\rm u}_{t}, {\rm v}_{t})\cr &=\left[\matrix{ {\rm F}_{t} & 0\cr 0 & {\rm I}_{\vert {\cal I}\vert} }\right]\check{{\rm x}}_{t\vert t}+\left[\matrix{ {\rm J}_{t} & {\rm G}_{t}\cr 0 & 0 }\right]\left[\matrix{ {\rm u}_{t}\cr {\rm v}_{t} }\right]+{\rm b}_{t}+{\rm e}_{t}, &\hbox{(10)}} from which we can see that the propagation of the full augmented state is actually unnecessary since the only nontrivial regression matrix corresponds to the main state. We can propagate only the main state x via sigma points generated from P x x t | t and use the UKF Jacobian F t to update the cross-covariance P x x I t | t . Since the covariance matrix of the main state P x ; x t | t is always positive definite, we avoid the Cholesky decomposition failure problem.

Since the process noise is not additive, we augment the main state with the process noise and generate sigma points from:
x ¯ ¯ ¯ t | t = [ x ^ t | t 0 ] , P ¯ ¯ ¯ ¯ t | t = [ P x x t | t D t 0 0 ] . (11)
View Source Right-click on figure for MathML and additional features. \overline{{\rm x}}_{t\vert t}=\left[\matrix{{\hat{{\rm x}}_{t\vert t}}\cr {0}}\right],\quad \overline{{\rm P}}_{t\vert t}=\left[\matrix{{{\rm P}_{t\vert t}^{{\rm xx}}}&0\cr {\rm D}_{t}&0}\right]. {\hbox{(11)}}

The state is then propagated forward by substituting (11) into (1) , (5) and (6) . We obtain x ^ t + 1 | t , the estimated value of x at time t + 1 given the measurements up to t , as well as P x x t + 1 | t and P x x ¯ ¯ ¯ t + 1 | t . Following (9) , we know that:
P x x ¯ ¯ ¯ t + 1 | t P ¯ ¯ ¯ ¯ − 1 t | t = [ F t , G t ] .
View Source Right-click on figure for MathML and additional features. {\rm P}_{t+1}^{{\rm x}\overline{{\rm x}}}{}_{\vert t}\overline{{\rm P}}_{t\vert t}^{-1}=[{\rm F}_{t}, {\rm G}_{t}].

The propagated augmented state and its covariance is updated according to (10):
x ˇ t + 1 | t = [ x ^ t + 1 | t x ^ I t | t ] , P ˇ t + 1 | t = [ P x x t + 1 | t P x I x t | t F t T F t P x x I t | t P x I x I t | t ] . (12)
View Source Right-click on figure for MathML and additional features. \check{{\rm x}}_{t+1\vert t}=\left[\matrix{ \hat{{\rm x}}_{t+1\vert t}\cr \hat{{\rm x}}_{{\cal I}_{t\vert t}} }\right],\quad \check{{\rm P}}_{t+1\vert t}=\left[\matrix{{\rm P}_{t+1\vert t}^{{\rm xx}} &{\rm F}_{t}{\rm P}_{t\vert t}^{{\rm xx}_{{\cal I}}}\cr {\rm P}_{t\vert t}^{{\rm x}_{{\cal I}}{\rm x}}{{\rm F}_{t}}^{\rm T} &{\rm P}_{t\vert t}^{{\rm x}_{{\cal I}}{\rm x}_{\cal I}}}\right].\eqno{\hbox{(12)}}

D. Measurement Update

Let there be m state propagations between two measurements, and we maintain x ˇ t + m | t and P ˇ t + m | t as the newest measurement arrives. Consider a relative measurement (3) that depends on the j t h augmented state, the measurement prediction and its linear regression approximation can be written as:
z ^ t + m | t = = H t + m | t = h r ( x ^ t + m | t , B j T x ^ j t + m | t , n t + m ) H t + m | t X ˇ t + m | t + L t + m n t + m + b t + m + e t + m [ H x t + m | t , 0 , H x j t + m | t , 0 ] .
View Source Right-click on figure for MathML and additional features. \eqalignno{\hat{{\rm z}}_{t+m\vert t}= & h_{r}(\hat{{\rm x}}_{t+m\vert t}, {{\rm B}_{j}}^{{\rm T}}\hat{{\rm x}}_{j_{t+m\vert t}}, {\rm n}_{t+m})\cr = & {\rm H}_{t+m\vert t^{\check{{\rm X}}}t+m\vert t}+{\rm L}_{t+m}{\rm n}_{t+m}+{\rm b}_{t+m}+{\rm e}_{t+m}\cr {\rm H}_{t+m\vert t}= & [{\rm H}_{t+m\vert t}^{{\rm x}}, 0, {\rm H}_{t+m\vert t}^{{\rm x}_{j}}, 0].}

Again, since only the main state and one augmented state are involved in each measurement update, we can construct another augmented state together with the possibly nonadditive measurement noise:
x ` t + m | t = ⎡ ⎣ ⎢ ⎢ x ^ t + m | t x ^ j t + m | t 0 ⎤ ⎦ ⎥ ⎥ , P ` t + m | t = ⎡ ⎣ ⎢ ⎢ ⎢ P x x t + m p x j X t + , 0 m | t 0 P x x j t + m | t P x j X j t + m | t 0 0 0 Q t + m ⎤ ⎦ ⎥ ⎥ ⎥ .
View Source Right-click on figure for MathML and additional features. \grave{{\rm x}}_{t+m\vert t}=\left[\matrix{ \hat{{\rm x}}_{t+m\vert t}\cr \hat{{\rm x}}_{j_{t+m\vert t}}\cr 0 }\right],\grave{{\rm P}}_{t+m\vert t}=\left[\matrix{{\rm P}_{t+m}^{{\rm xx}} &{\rm P}_{t+m\vert t}^{{\rm xx}{j}} &0\cr {\rm p}_{t+,0^{m}\vert t}^{{\rm x}_{j^{{\rm X}}}}& {\rm P}_{t+m\vert t}^{{\rm x}_{j^{{\rm X}}j}}&0\cr 0&0&{\rm Q}_{t+m}}\right].

After the state propagation (12) , P ` t + m | t is guaranteed to be positive definite, thus it is safe to perform sigma point propagation as in (5) and (6) . We obtain z ^ t + m | t , P z z t + m | t , P z x ` t + m | t , and:
P z x ` t + m | t P ` − 1 t + m | t = [ H x t + m | t , H x j t + m | t , L t t + m ] .
View Source Right-click on figure for MathML and additional features. {\rm P}_{t+m}^{{\rm z}\grave{{\rm x}}}{}_{\vert t}\grave{{\rm P}}_{t+m\vert t}^{-1}=\left[{\rm H}_{t+m\vert t}^{{\rm x}}, {\rm H}_{t+m\vert t}^{{\rm x}_{j}}, {\rm L}t_{t+m}\right].

We can apply the measurement update similar to an EKF:
K ˇ t + m = x ˇ t + m | t + m = P ˇ t + m | t + m = P ˇ t + m | t H t + m | t T P z z − 1 t + m | t x ˇ t + m | t + K ˇ t + m ( z t + m − z t + m | t ) P ˇ t + m | t − K ˇ t + m H t + m | t P ˇ t + m | t ,
View Source Right-click on figure for MathML and additional features. \eqalignno{\check{{\rm K}}_{t+m}= & \check{{\rm P}}_{t+m\vert}{}_{t}{\rm H}_{t+m\vert t^{{\rm T}}}{\rm P}_{t+m\vert t}^{{\rm zz}^{-1}}\cr \check{{\rm x}}_{t+m\vert t+m}= & \check{{\rm x}}_{t+m\vert t}+\check{{\rm K}}_{t+m}({\rm z}_{t+m}-{\rm z}_{t+m\vert t})\cr \check{{\rm P}}_{t+m\vert t+m}= & \check{{\rm P}}_{t+m\vert t}-\check{{\rm K}}_{t+m}{\rm H}_{t+m\vert}{}_{t}\check{{\rm P}}_{t+m\vert t},} where z t + m is the actual sensor measurement. Both the main and augmented states will be corrected during measurement update. We note that entries in H t + m | t that correspond to inactive augmented states are zero. This can be utilized to speed up the matrix multiplication.

The fusion of absolute measurements can simply be done by setting x ^ j t + m | t = ∅ and applying the corresponding absolute measurement model (2) .

As shown in Fig. 9 , fusion of multiple relative measurements results in slow growing, but unbounded covariance in the global position and yaw. This is consistent with results in [6] that these global quantities are unobservable.
E. Delayed, Out-of-Order Measurement Update

When fusing multiple measurements, it is possible that the measurements arrive out-of-order to the filter, that is, a measurement that corresponds to an earlier state arrives after the measurement that corresponds to a later state. This violates the Markov assumption of the Kalman filter. Also, due to the sensor processing delay, measurements may lag behind the state propagation.

We address these two issues by storing measurements in a priority queue, where the top of the queue corresponds to the oldest measurement. A pre-defined a maximum allowable sensor delay t d of 100 ms was set for our MAV platform. Newly arrived measurements that correspond to a state older than t d from the current state (generated by state propagation) are directly discarded. After each state propagation, we check the queue and process all measurements in the queue that are older than t d . The priority queue essentially serves as a measurement reordering mechanism ( Fig. 2 ) for all measurements that are not older than t d from the current state. In the filter, we always utilize the most recent IMU measurement to propagat e the state forward. We, however, only propagate the covariance on demand. As illustrated in Fig. 2 , the covariance is only propagated from the time of the last measurement to the current measurement.
F. An Alternative Way for Handling Global Pose Measurements

As the vehicle moves through the environment, global pose measurements from GPS and magnetometer may be available. It is straightforward to fuse the GPS as a global pose measurement and generate the optimal state estimate. However, this may not be the best for real-world applications. A vehicle that operates in a GPS-denied environment may suffer from accumulated drift. When the vehicle gains GPS signal, as illustrated in Fig. 3(a) , there maybe large discrepancies between the GPS measurement and the estimated state ( z 5   − s 5 ) . Directly applying GPS as global measurements will result in undesirable behaviors in both estimation (large linearizion error) and control (sudden pose change).
Fig. 2. - Delayed, out-of-order measurement with priority queue. While ${\rm z}_{4}$ arrives before ${\rm z}_{2}, {\rm z}_{2}$ is first applied to the filter. ${\rm z}_{4}$ is temporary stored in the queue. ${\rm z}_{1}$ is discarded since it is older than $t_{d}$ from the current state. The covariance is only propagated up to time where the most recent measurement is applied to the filter. The state is propagated till the most recent IMU input.
Fig. 2.

Delayed, out-of-order measurement with priority queue. While z 4 arrives before z 2 , z 2 is first applied to the filter. z 4 is temporary stored in the queue. z 1 is discarded since it is older than t d from the current state. The covariance is only propagated up to time where the most recent measurement is applied to the filter. The state is propagated till the most recent IMU input.

Show All

This is not a new problem and it has been studied for ground vehicles [17] under the term of local frame-based navigation. However, [17] assumes that a reasonably accurate local estimate of the vehicle is always available (e.g. wheel odometry). This is not the case for MAVs since the state estimates with only the onboard IMUs drifts away vastly within a few seconds. The major difference between dead reckoning with IMU and wheel odometry is that the former drifts temporally, while the latter only drifts spatially. However, we have relative exteroceptive sensors that are able to produce temporally drift-free estimates. As such, we only need to deal with the case that all relative exteroceptive sensors have failed. Therefore, our goal is to properly transform the global GPS measurement into the local frame to bridge the gap between relative sensor failures.

Consider a pose-only graph SLAM formulation with s k = [ x w k , y w k , ψ w k ] T ∈ Θ being 2D poses. The SLAM module may run at a much lower rate than the UKF-based estimator. We optimize the pose graph given incremental motion constraints d k from laser/visual odometry, spatial loop closure constraints 1 k , and absolute pose constraints z k from GPS:
min Θ { ∑ l = 1 M ∥ h i ( s k − 1 , d k ) − s k ∥ P d k + ∑ k = 1 L ∥ h l ( s k , 1 k ) − s l ( k ) ∥ P 1 k + ∑ k = 1 N ∥ z k − s k ∥ P z k } .
View Source Right-click on figure for MathML and additional features. \eqalignno{&\min_{\Theta}\left\{ \sum\limits_{l=1}^{M}\Vert h_{i}({\rm s}_{k-1}, {\rm d}_{k})-{\rm s}_{k}\Vert_{{\rm P}_{k}^{{\rm d}}}\right.\cr &\left. +\sum\limits_{k=1}^{L}\Vert h_{l}({\rm s}_{k}, 1_{k})-{\rm s}_{l(k)}\Vert_{{\rm P}_{k}^{1}}+\sum\limits_{k=1}^{N}\Vert {\rm z}_{k}-{\rm s}_{k}\Vert_{{\rm P}_{k}^{{\rm z}}}\right\}.}

The optimal pose graph configuration can be found with available solvers [18] , as shown in Fig. 3(b) . The pose graph is disconnected if there are no relative exteroceptive measurements between two nodes. Let two pose graphs be disconnected between k − 1 and k .

The pose graph SLAM provides the transformation between the non-optimized s k − 1 and the SLAM-optimized s + k − 1 state. This transform can be utilized to transform the global GPS measurement to be aligned with s k − 1 :
Δ t − 1 = s k − 1 ⊖ s + k − 1 z − k − 1 = Δ t − 1 ⊕ z k − 1 ,
View Source Right-click on figure for MathML and additional features. \eqalignno{\Delta_{t-1}={\rm s}_{k-1}\ominus {\rm s}_{k-1}^{+}\cr {\rm z}_{k-1}^{-}=\Delta_{t-1}\oplus {\rm z}_{k-1},} where ⊕ and ⊖ are pose compound operations as defined in [19] . The covariance P △ t − 1 of Δ t − 1 and subsequently the covariance P z − t − 1 of z − k − 1 can be computed following [19] . This formulation minimizes the discrepancies between z − k − 1 and s k − 1 , and thus maintains smoothness in the state estimate. The transformed GPS z − k − 1 , is still applied as an absolute measurement to the UKF ( Fig. 4(a) ).

Fig. 3. - In fig. 3(a), GPS signal is regained at $k=5$, resulting in large discrepancies between the measurement z5and the state ${\rm s}_{5}$. Pose graph slam produces a globally consistent graph (fig. 3(b)).
Fig. 3.

In fig. 3(a), GPS signal is regained at k = 5 , resulting in large discrepancies between the measurement z 5 and the state s 5 . Pose graph slam produces a globally consistent graph (fig. 3(b)).

Show All
Fig. 4. - Fig. 4(a) illustrates the alternative GPS fusion, the discrepancy between transformed GPS measurement ${\rm z}_{5}$ and the non-optimized state ${\rm s}_{5}$ is minimized. Fusion of such indirect GPS measurement will lead to smooth state estimate (green dashed line).
Fig. 4.

Fig. 4(a) illustrates the alternative GPS fusion, the discrepancy between transformed GPS measurement z 5 and the non-optimized state s 5 is minimized. Fusion of such indirect GPS measurement will lead to smooth state estimate (green dashed line).

Show All

However, despite the large scale in our field experiments ( Sect. VI ), we hardly find a case where the accumulated drift is large enough to cause issues with direct GPS fusion. In the future, we will seek for even larger scale experiments to verify the necessity of the above local frame-based approach.
SECTION V.
Implementation Details
A. Experimental Platform

The experimental platform shown in Fig. 1 is based on the Pelican quadrotor from Ascending Technologies, GmbH 2 . This platform is natively equipped with an AutoPilot board consisting of an IMU and a user-programmable ARM7 microcontroller. The main computation unit onboard is an Intel NUC with a 1.8 GHz Core i3 processor with 8 GB of RAM and a 120 GB SSD. The sensor suite includes a u-blox LEA-6T GPS module, a Hokuyo UTM-30LX LiDAR and two mvBlueFOX-MLC200w grayscale HDR cameras with fisheye lenses that capture 752 × 480 images at 25 Hz. We use hardware triggering for frame synchronization. The onboard auto exposure controller is fine tuned to enable fast adaption during rapid light condition changes. A 3-D printed laser housing redirects some of the laser beams for altitude measurement. The total mass of the platform is 1.87kg. The entire algorithm is developed in C++ using ROS 3 as the interfacing robotics middleware.
B. Absolute Measurements

Some onboard sensors are capable of producing absolute measurements ( Sect. III-A ), here are their details:

    GPS And Magnetometer:
    z t = ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ( R w b x w t y w t ) ( x ˙ t y ˙ b t ) ψ w t ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ + n t .
    View Source Right-click on figure for MathML and additional features. {\rm z}_{t}=\left[\matrix{\left({{\rm R}_{b}^{w}}_{y_{t}^{w}}^{x_{t}^{w}}\right)\cr\left(_{\dot{y}_{t}^{b}}^{\dot{x}_{t}}\right)\cr\psi_{t}^{w}}\right]+{\rm n}_{t}.

    Laser/Pressure Altimeter:
    z t = z w t + b w z t + n t .
    View Source Right-click on figure for MathML and additional features. {\rm z}_{t}=z_{t}^{w}+{\rm b}_{z_{t}}^{w}+{\rm n}_{t}.

    Pseudo Gravity Vector: If the MAVs is near hover or moving at approximately constant speed, we may say that the accelerometer output provides a pseudo measurement of the gravity vector. Let {\rm g}=[0,0, g]^{{\rm T}} , we have: {\rm z}_{t}={\rm R}_{b}^{w{\rm T}}{\rm g}^{w}+{\rm b}_{a_{t}}^{b}+{\rm n}_{t}.
    View Source Right-click on figure for MathML and additional features. {\rm z}_{t}={\rm R}_{b}^{w{\rm T}}{\rm g}^{w}+{\rm b}_{a_{t}}^{b}+{\rm n}_{t}.

C. Relative Measurement-Laser-Based Odometry

We utilize the laser-based odometry that we developed in our earlier work [4] . Observing that man-made indoor environments mostly contains vertical walls, we can make a 2.5-D environment assumption. With this assumption, we can make use of the onboard roll and pitch estimates to project the laser scanner onto a common ground plane. As such, 2D scan matching can be utilized to estimate the incremental horizontal motion of the vehicle. We keep a local map to avoid drifting while hovering. \eqalignno{{\rm z}_{t+m}=\ominus_{2d}\left[\matrix{ x_{t}^{w}\cr y_{t}^{w}\cr \psi_{t}^{w} }\right]\oplus_{2d}\left[\matrix{ x_{t+m}^{w}\cr y_{t+m}^{w}\cr \psi_{t+m}^{w} }\right]+{\rm n}_{t+m},}
View Source Right-click on figure for MathML and additional features. \eqalignno{{\rm z}_{t+m}=\ominus_{2d}\left[\matrix{ x_{t}^{w}\cr y_{t}^{w}\cr \psi_{t}^{w} }\right]\oplus_{2d}\left[\matrix{ x_{t+m}^{w}\cr y_{t+m}^{w}\cr \psi_{t+m}^{w} }\right]+{\rm n}_{t+m},} where {\rm p}_{2d_{t}}=[x_{t}^{w}, y_{t}^{w}, \psi_{t}^{w}]^{{\rm T}}, \oplus_{2d} and \ominus_{2d} are the 2-D pose compound operations as defined in [19] .

D. Relative Measurement-Visual Odometry

We implemented a classic keyframe-based visual odometry algorithm. Keyframe-based approaches have the benefit of being temporally drift-free. We choose to use light-weight corner features but run the algorithm at a high-rate (25 Hz). Features are tracked across images via KLT tracker. Given a keyframe with a set of triangulated feature points, we run a robust iterative 2D-3D pose estimation [8] to estimate the 6-DOF motion of the vehicle with respect to the keyframe. New keyframes are inserted depending on the distance traveled and the current number of valid 3D points. {\rm z}_{t+m}=\ominus\left[\matrix{ {\rm p}_{t}^{w}\cr \Phi_{t}^{w} }\right]\oplus\left[\matrix{ {\rm p}_{t+m}^{w}\cr \Phi_{t+m}^{w} }\right]+{\rm n}_{t+m}
View Source Right-click on figure for MathML and additional features. {\rm z}_{t+m}=\ominus\left[\matrix{ {\rm p}_{t}^{w}\cr \Phi_{t}^{w} }\right]\oplus\left[\matrix{ {\rm p}_{t+m}^{w}\cr \Phi_{t+m}^{w} }\right]+{\rm n}_{t+m}

Fig. 5. - The MAV maneuvers aggressively with a maximum speed of 3.5 m/s (fig. 5(b)). The horizontal position also compares well with the ground truth with slight drift (fig. 5(a)).
Fig. 5.

The MAV maneuvers aggressively with a maximum speed of 3.5 m/s (fig. 5(b)). The horizontal position also compares well with the ground truth with slight drift (fig. 5(a)).

Show All

E. Feedback Control

To achieve stable flight across different environments with possibly large orientation changes, we choose to use a position tracking controller with a nonlinear error metric [20] . The 100 Hz filter output ( Sect. IV ) is used directly as the feedback for the controller. In our implementation, the attitude controller runs at 1 kHz on the ARM processor on the MAV's AutoPilot board, while the position tracking control operates at 100 Hz on the main computer. We implemented both setpoint trajectory tracking and velocity control to allow flexible operations.
SECTION VI.
Experimental Results

Multiple experiments are conducted to demonstrate the robustness of our system. We begin with an quantitative evaluation in a lab environment equipped with a motion capture systems. We then test our system in two realworld autonomous flight experiments, including an industrial complex and a tree-lined campus.
A. Evaluation of Estimator Performance

We would like to push the limits of our onboard estimator. Therefore, we have a professional pilot to aggressively fly the quadrotor with a 3.5 m/s maximum speed and large attitude of up to 40°. The onboard state estimates are compared the ground truth from the motion capture system. Since there is no GPS measurement indoor, our system relies on a fusion of relative measurements from laser and vision. We do observe occasional laser failure due to large attitude violating the 2.5-D assumption ( Sect. V-C ). However, the multi-sensor filter still tracks the vehicle state throughout ( Fig. 5 ). We do not quantify the absolute pose error since it is unbounded. However, the body frame velocity ( Fig. 5(b) ) compares well with the ground truth with standard deviations of \{0.1021, 0.1185, 0.0755\}^{{\rm T}} (m/s) in x, y, and z, respectively.
Fig. 6. - Images from the onboard camera (figs. 6(a)-6(d)) and an external camera (figs. 6(e)-6(h)). Note the vast variety of environments, including open space, trees, complex building structures, and indoor environments. We highlight the position of the MAV with a red circle. Videos of the experiments are available in the video attachment and at http://mrsl.grasp.upenn.edu/shaojie/icra2014.mp4.
Fig. 6.

Images from the onboard camera (figs. 6(a)-6(d)) and an external camera (figs. 6(e)-6(h)). Note the vast variety of environments, including open space, trees, complex building structures, and indoor environments. We highlight the position of the MAV with a red circle. Videos of the experiments are available in the video attachment and at http://mrsl.grasp.upenn.edu/shaojie/icra2014.mp4 .

Show All
Fig. 7. - Vehicle trajectory aligned with satellite imagery. Different colors indicate different combinations of sensing modalities. ${\rm G}={\rm GPS}, {\rm V}={\rm Vision}$, and ${\rm L}={\rm Laser}$
Fig. 7.

Vehicle trajectory aligned with satellite imagery. Different colors indicate different combinations of sensing modalities. {\rm G}={\rm GPS}, {\rm V}={\rm Vision} , and {\rm L}={\rm Laser}

Show All
Fig. 8. - Sensor availability over time. Note that failures occurred to all sensors. This shows that multi-sensor fusion is a must for this kind of indoor-outdoor missions.
Fig. 8.

Sensor availability over time. Note that failures occurred to all sensors. This shows that multi-sensor fusion is a must for this kind of indoor-outdoor missions.

Show All

B. Autonomous Flight in Large-Scale Indoor and Outdoor Environments

We tested our system in a challenging industrial complex. The testing site spans a variety of environments, including outdoor open space, densely filled trees, cluttered building area, and indoor environments ( Fig. 6 ). The MAV is autonomously controlled using the onboard state estimates. However, a human operator always has the option of sending high level waypoints or velocity commands to the vehicle. The total flight time is approximately 8 minutes, and the vehicle travels 445 meters with an average speed of 1.5 m/s. As shown in the map-aligned trajectory ( Fig. 7 ), during the experiment, frequent sensor failures occurred ( Fig. 8 ), indicating the necessity of multi-sensor fusion. Fig. 9 shows the evolution of covariance as the vehicle flies through a GPS shadowing area. The global x, y and yaw error is bounded by GPS measurement, without which the error will grow unbounded. This matches the observability analysis results. It should be noted that the error on body frame velocity does not grow, regardless of the availability of GPS. The spike in velocity covariance in Fig. 9 is due to the camera facing direct sunlight.
Fig. 9. - Covariance changes as the vehicle flies through a dense building area (between 200s-300s, top of fig. 7, green line). The GPS comes in and out due to building shadowing. The covariance of x, y, and yaw increases as GPS fails and decreases as GPS resumes. Note that the body frame velocity are observable regardless of GPS measurements, and thus its covariance remains small. The spike in the velocity covariance is due to the vehicle directly facing the sun. The X-Y covariance is calculated from the frobenius norm of the covariance submatrix.
Fig. 9.

Covariance changes as the vehicle flies through a dense building area (between 200s-300s, top of fig. 7 , green line). The GPS comes in and out due to building shadowing. The covariance of x, y, and yaw increases as GPS fails and decreases as GPS resumes. Note that the body frame velocity are observable regardless of GPS measurements, and thus its covariance remains small. The spike in the velocity covariance is due to the vehicle directly facing the sun. The X-Y covariance is calculated from the frobenius norm of the covariance submatrix.

Show All
Fig. 10. - Vehicle trajectory overlaid on a satellite map. The vehicle operates in a tree-lined campus environment, where there is high risk of GPS failure during operation.
Fig. 10.

Vehicle trajectory overlaid on a satellite map. The vehicle operates in a tree-lined campus environment, where there is high risk of GPS failure during operation.

Show All

C. Autonomous Flight in Tree-Lined Campus

We also conduct experiments in a tree-lined campus environment, as shown in Fig. 10 . Autonomous flight in this environment is challenging due to nontrivial light condition changes as the vehicle moves in and out of tree shadows. The risk of GPS failure is also very high due to the trees above the vehicle. Laser-based odometry only works when close to buildings. The total trajectory length is 281 meters.
SECTION VII.
Conclusion and Future Work

In this work, we present a modular and extensible approach to integrate noisy measurements from multiple heterogeneous sensors that yield either absolute or relative observations at different and varying time intervals. Our approach generates high rate state estimates in real-time for autonomous flight. The proposed approach runs onboard our new 1.9 kg MAV platform equipped with multiple heterogeneous sensors. We demonstrate the robustness of our framework in large-scale, indoor and outdoor autonomous flight experiments that involves traversal through a industrial complex and a tree-lined campus.
Fig. 11. - Onboard (fig. 11(a)) and external (fig. 11(b)) camera images as the MAV autonomously flies through a tree-lined campus environment. Note the nontrivial light condition.
Fig. 11.

Onboard (fig. 11(a)) and external (fig. 11(b)) camera images as the MAV autonomously flies through a tree-lined campus environment. Note the nontrivial light condition.

Show All

In the near future, we would like to integrate higher level planning and situational awareness on our MAV platform to achieve fully autonomous operation across large-scale complex environments.

Authors
Figures
References
Citations
Keywords
Metrics
Media
Footnotes
   Back to Results   
More Like This
Multi-Timescale Nonlinear Robust Control for a Miniature Helicopter

IEEE Transactions on Aerospace and Electronic Systems

Published: 2010
Multi-timescale nonlinear robust control for a miniature helicopter

2008 American Control Conference

Published: 2008
Show More
References
1. S. J. Julier and J. K. Uhlmann, "A new extension of the kalman filter to nonlinear systems", Proc. of SPIE , vol. 3068, pp. 182-193, July 1997.
Show in Context Google Scholar
2. R. V. D. Merwe, E. A. Wan and S. I. Julier, "Sigma-point kalman filters for nonlinear estimation: Applications to integrated navigation", Proc. of AIAA Guidance Navigation and Controls Conf. , Aug. 2004.
Show in Context CrossRef Google Scholar
3. A. Bachrach, S. Prentice, R. He and N. Roy, "RANGE-robust autonomous navigation in gps-denied environments", J. Field Robotics , vol. 28, no. 5, pp. 644-666, 2011.
Show in Context CrossRef Google Scholar
4. S. Shen, N. Michael and V. Kumar, "Autonomous multi-floor indoor navigation with a computationally constrained MAV", Proc. of the IEEE Intl. Conf. on Robot. and Autom. , pp. 20-25, May 2011.
Show in Context View Article Full Text: PDF (1392) Google Scholar
5. S. Weiss, M. W. Achtelik, S. Lynen, M. Chli and R. Siegwart, "Real-time onboard visual-inertial state estimation and self-calibration of mavs in unknown environments", Proc. of the IEEE Conf. on Robot. and Autom. , pp. 957-964, May 2012.
Show in Context View Article Full Text: PDF (1559) Google Scholar
6. D. G. Kottas, J. A. Hesch, S. L. Bowman and S. I. Roumeliotis, "On the consistency of vision-aided inertial navigation", Proc. of the Intl. Sym. on Exp. Robot. , June 2012.
Show in Context Google Scholar
7. F. Fraundorfer, L. Heng, D. Honegger, G. H. Lee, L. Meier, P. Tan-skanen, et al., "Vision-based autonomous mapping and exploration using a quadrotor MAV", Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots and Syst. , Oct. 2012.
Show in Context CrossRef Google Scholar
8. K. Schmid, T. Tomic, F. Ruess, H. Hirschmuller and M. Suppa, "Stereo vision based indoor/outdoor navigation for flying robots", Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots and Syst. , Nov. 2013.
Show in Context CrossRef Google Scholar
9. A. S. Huang, A. Bachrach, P. Henry, M. Krainin, D. Maturana, D. Fox, et al., "Visual odometry and mapping for autonomous flight using an RGB-D camera", Proc. of the Intl. Sym. of Robot. Research , Aug. 2011.
Show in Context Google Scholar
10. T. Tomic, K. Schmid, P. Lutz, A. Domel, M. Kassecker, E. Mair, et al., "Autonomous UAV: Research platform for indoor and outdoor urban search and rescue", IEEE Robot. Autom. Mag. , vol. 19, no. 3, pp. 46-56, 2012.
Show in Context CrossRef Google Scholar
11. S. I. Roumeliotis and J. W. Burdick, "Stochastic cloning: A generalized framework for processing relative state measurements", Proc. of the IEEE Intl. Conf. on Robot. and Autom. , pp. 1788-1795, May 2002.
Show in Context CrossRef Google Scholar
12. J. Carlson, "Mapping large urban environments with GPS-aided SLAM", July 2010.
Show in Context Google Scholar
13. D. Schleicher, L. M. Bergasa, M. Ocaa, R. Barea and E. Lpez, "Real-time hierarchical GPS aided visual SLAM on urban environments", Proc. of the IEEE Intl. Conf. on Robot. and Autom. , pp. 4381-4386, May 2009.
Show in Context View Article Full Text: PDF (711) Google Scholar
14. S. Shen, Y. Mulgaonkar, N. Michael and V. Kumar, "Vision-based state estimation and trajectory control towards high-speed flight with a quadrotor", Proc. of Robot.: Sci. and Syst. , 2013.
Show in Context Google Scholar
15. S. Lynen, M. W. Achtelik, S. Weiss, M. Chli and R. Siegwart, "A robust and modular multi-sensor fusion approach applied to mav navigation", Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots and Syst. , Nov. 2013.
Show in Context CrossRef Google Scholar
16. T. Lefebvre, H. Bruyninckx and J. D. Schuller, "Comment on “a new method for the nonlinear transformation of means and covariances in filters and estimators”", IEEE Trans. Autom. Control , vol. 47, no. 8, pp. 1406-1409, 2002.
Show in Context View Article Full Text: PDF (221) Google Scholar
17. D. C. Moore, A. S. Huang, M. Walter and E. Olson, "Simultaneous local and global state estimation for robotic navigation", Proc. of the IEEE Intl. Conf. on Robot. and Autom. , pp. 3794-3799, May 2009.
Show in Context View Article Full Text: PDF (1541) Google Scholar
18. R. Kuemmerle, G. Grisetti, H. Strasdat, K. Konolige and W. Burgard, "g2o: A general framework for graph optimizations", Proc. of the IEEE Intl. Conf. on Robot. and Autom. , pp. 3607-3613, May 2011.
Show in Context Google Scholar
19. R. Smith, M. Self and P. Cheeseman, "Estimating uncertain spatial relationships in robotics", Proc. of the IEEE Intl. Conf. on Robot. and Autom. , vol. 4, pp. 850, Mar. 1987.
Show in Context View Article Full Text: PDF (12) Google Scholar
20. T. Lee, M. Leoky and N. McClamroch, "Geometric tracking control of a quadrotor uav on SE(3)", Proc. of the Intl. Conf. on Decision and Control , pp. 5420-5425, Dec. 2010.
Show in Context CrossRef Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
