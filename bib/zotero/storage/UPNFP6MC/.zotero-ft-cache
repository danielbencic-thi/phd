IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathMenu.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Conferences > 2021 IEEE International Confe...
Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad Weather
Publisher: IEEE
Cite This
PDF
  << Results   
Younkwan Lee ; Jihyo Jeon ; Yeongmin Ko ; Byunggwan Jeon ; Moongu Jeon
All Authors
View Document
262
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    INTRODUCTION
    II.
    RELATED WORKS
    III.
    PROPOSED METHOD
    IV.
    Experiments and Evaluation
    V.
    Conclusions

Authors
Figures
References
Keywords
Metrics
Media
More Like This
Footnotes

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: Visual perception in autonomous driving is a crucial part of a vehicle to navigate safely and sustainably in different traffic conditions. However, in bad weather such as... View more
Metadata
Abstract:
Visual perception in autonomous driving is a crucial part of a vehicle to navigate safely and sustainably in different traffic conditions. However, in bad weather such as heavy rain and haze, the performance of visual perception is greatly affected by several degrading effects. Recently, deep learning-based perception methods have addressed multiple degrading effects to reflect real-world bad weather cases but have shown limited success due to 1) high computational costs for deployment on mobile devices and 2) poor relevance between image enhancement and visual perception in terms of the model ability. To solve these issues, we propose a task-driven image enhancement network connected to the high-level vision task, which takes in an image corrupted by bad weather as input. Specifically, we introduce a novel low memory network to reduce most of the layer connections of dense blocks for less memory and computational cost while maintaining high performance. We also introduce a new task-driven training strategy to robustly guide the high-level task model suitable for both high-quality restoration of images and highly accurate perception. Experiment results demonstrate that the proposed method improves the performance among lane and 2D object detection, and depth estimation largely under adverse weather in terms of both low memory and accuracy.
Published in: 2021 IEEE International Conference on Robotics and Automation (ICRA)
Date of Conference: 30 May-5 June 2021
Date Added to IEEE Xplore : 18 October 2021
ISBN Information:
ISSN Information:
INSPEC Accession Number: 21402318
DOI: 10.1109/ICRA48506.2021.9561076
Publisher: IEEE
Conference Location: Xi'an, China
Funding Agency:
Contents
SECTION I.
INTRODUCTION

Autonomous vehicles require comprehensive and accurate visual perception to safely navigate diverse driving conditions with little or no human effort. Currently, visual perception tasks are achieved by deep neural networks (DNN) which have demonstrated impressive performance on a wide range of high-level vision tasks, such as lane detection [1] , [2] , monocular depth estimation [3] [4] – [5] , and scene recognition [6] [7] [8] [9] – [10] . The success of deep convolutional neural networks relies on a large number of high-quality images and a large computational cost on large-scale resource devices. However, existing models do not typically consider the degradations taken from bad weather conditions for training as well as low-resource devices to be deployed on mobile devices. Therefore, one needs to train complex visual degradations caused by bad weather with DNN having lower resource consumption.
Fig. 1.

TuSimple [11] SSIM vs FLOPs. The area of the circle plot represents the parameters of the model as described in parentheses. Compared to three methods (in blue): RESCAN [12] , DeRaindrop [13] and PReNet [14] , our proposed models (in red) are shown to achieve perception- and hardware-friendly performance, while reducing model size.

Show All

To solve this problem, image enhancement has been adopted as a key solution [14] [15] [16] [17] [18] – [19] , enabling it to improve image quality by itself as a preprocessing method within independent components. Benefited from these methods, the latest deep learning-based enhancement models seem almost plausible at first glance. Unfortunately, however, we observe that existing enhancement methods still not suitable for high-level vision tasks in bad weather, and even worsen performance in some cases. Among them, we find its limitations in the following reasons. First, They usually rely on metrics based on the human visual system that are not correlated with visual perception models such as PSNR and SSIM. Thus, when the image is inferred by them, the recovered information is not sufficient or appropriate for high-level tasks. Second, they generally require a lot of computational power despite having to run autonomous driving with resource-limited platforms. Hence they are unaffordable for autonomous driving when integrated into the resource-constrained environments with other perception models.

In this paper, we introduce a novel task-driven image enhancement framework that benefits by exploring the mutual influence between visual perception and enhancement for safe and reliable autonomous driving in bad weather conditions. To this end, our model aims to have perception- and hardware-friendly characteristics against any bad weather situations as end-to-end learning, as shown in Fig. 1 . To the best of our knowledge, this is the first attempt to connect image enhancement and high-level vision for autonomous driving under multiple bad weather conditions. In summary, our work makes following key contributions:

    We propose a universal multiple bad weather removal framework that enables the high-level vision tasks to improve the robustness of existing models without degradation and retraining.

    We develop a task-driven enhancement network for less memory and computational cost, thus it is suitable for the embedded system when building ADAS (Advanced Driver Assistance System) for autonomous vehicles.

    We introduce a novel training strategy that minimizes the detrimental effects of image enhancement while improving the performance of high-level tasks in an end-to-end and task-driven manner.

    We experimentally validate the effectiveness of the proposed method when embeds high-level tasks such as lane detection, monocular depth estimation, and object detection. To our best knowledge, this work is one of few studies to apply the proposed image enhancement module for visual high-level tasks of autonomous driving under bad weather.

SECTION II.
RELATED WORKS
A. Bad Weather Image Enhancement Algorithms

Many models and algorithms have been developed to deal with only one weather condition. For example, [12] [13] – [14] , [20] , [21] have been proposed to recover rain effects, including rain streaks or raindrops. In [22] , desnowing was designed with a multi-scale stacked densely connected CNN for detecting and removing snowflakes from a single snowy image. A few approaches for defogging/dehazing was proposed by non-local prior [23] or image-to-image translation network without relying on the physical scattering model [24] . However, they are not designed and trained for all the bad weather conditions, thus may not guarantee to build safe autonomous driving in bad weather. The above issue of universal bad weather enhancement has been addressed by hybrid all-in-one model [25] [26] – [27] . In [26] , a joint dehazing and deraining CNN was proposed with the classical atmospheric scattering model from the global context of a single image. In [27] , generative adversarial networks were used by relying on task-specific encoders that only process a particular degradation type. Although these all-on-one methods have achieved impressive performance on bad weather image enhancement, most of them were only suited for one specific kinds of perception task, such as object detection [27] or semantic segmentation [26] without studying various high-level tasks. Moreover, their methods are computationally too inefficient for on-device embedding in autonomous vehicles and are not suitable for fast inference. To the best of our knowledge, our work is the first study to provide faster processing time and compressed parameters while being perception- and hardware-friendly to deal with a variety of bad weather conditions.
B. Limitation of High-Level Vision Models

When high-level vision tasks are conducted in bad weather conditions that they often encounter in autonomous driving, image enhancement is usually worked as an independent pre-processing stage, which might be poorly related to the task-specific goal [28] [29] – [30] . Recently, limitation of deep learning-based high-level vision models has been investigated to reveal their inefficiency against bad weather conditions that they operate with image enhancement methods as the independent pre-processing stage. For example, [31] demonstrated that the existing image dehazing methods do not bring much benefit to help the image classification performance based on the analysis of the evaluation metric. Similarly, [18] , [32] showed that existing image deraining models do not much improve the performance of recognition model, or worsened, based on images collected in the real world. To comprehend such problem, some researches [33] [34] – [35] pointed out that visual enhancement works mainly focus on human perception quality [36] , which becomes harmful by visual artifact patterns or noise perturbation.

Nevertheless, there are several studies to overcome the vulnerability of high-level vision models. In [25] , re-formulated atmospheric scattering model that direct reconstructs haze-free images was studied by using an end-to-end learning scheme. In [37] , various factors of image degradation were tackled by analyzing the semantic segmentation networks in autonomous driving scenes. In [26] , [38] , [39] image enhancement and high-level task were jointly designed as an end-to-end learning model, achieving improved performance over both tasks. However, most methods still consider some weather effects such as rain or fog, separately. In addition, their optimization is still not suitable for high-level visual tasks, and there is no consideration of the efficiency of the hardware. As far as we know, our method is the first attempt to propose a recognition- and hardware-friendly framework, taking into account various bad weather conditions.
SECTION III.
PROPOSED METHOD
A. Problem Definition

Here we present the general setting of the problem prior to the illustration of the proposed method. We have a clean image I GT and corresponding bad weather image I X . We define that both images have the same high-level task label Y GT . The bad weather input I X is first fed into the image enhancement network E en and outputs the recovered image I pred , while the last layer before the final output of E en represents f e n l a s t . Subsequently, the recovered image I pred is fed forward through the high-level perception network E ht and outputs the high-level perception result Y pred with the last convolution layer f h t l a s t . The parameters of each network are represented as θ en and θ ht with pre-trained for each task, where θ ht is frozen while optimizing the proposed method. Note that we do not explicitly define a detailed network for high-level task, since our proposed is applicable to arbitrary high-level task baselines. Additionally, the last layers of the two networks mentioned above, f e n l a s t and f h t l a s t , are respectively fed into feature identity extraction network with the learnable parameter ϕ . Fig. 2 shows the overall framework of the networks.
Fig. 2.

Overview of our proposed enhancement framework. Our framework comprises a low memory enhancement network, a task-specific high-level perception network, and a feature identity extraction network. We connect all networks into one pipeline and train in an end-to-end manner.

Show All
Fig. 3.

The detailed structure in the feature identity extraction network (FIE). The random projection shows the connections to 128-d latent feature space from the last flatten representation of each network.

Show All
B. Enhancement Network Architecture

Our network is inspired by DenseNet [40] as a feature encoding network. Feature encoding network has one efficient structure for high-resolution applications at the edge and outperforms existing image enhancement methods by leveraging HarDNet [41] based light-weight block for reducing the concatenation cost. Our enhancement network can be divided into two components: a Harmonic Dense Block (HBlock) for low memory computational cost and a Feature Identity Extraction Module (FIE) with feature fusion high-level perception task.
1) Harmonic Dense Block

To learn the recovery information, inspired by [41] , we model HBlock with depth L layers. While the standard DenseNet passes the gradient from propagated all the layers, it leads to terrible large memory usage and heavy computational cost outweighing the gain. To solve these problems, the output of HBlock with depth- L is acquired through concatenation with L th layer and all the previous odd-ordered layers. We also make the output of all even layers from 2 to L -2 to be removed once the HBlock is finished. Lastly, to adjust the dimension, we set the 32 channels in the last layer of each block. Each layer L has an output channel width k , and the number of its channels is calculated by k × 1.6 n , where n is the maximum value at which the layer l is divided by the integer quotient by 2 m .

Additionally, we employ a bottleneck layer before every 4th convolution layer to further accelerate the parameter efficiency, and set its output channel to c i n c o u t − − − √ , where c in and c out are channels of input and output, respectively. To this end, we propose two versions of the network, each consisting of 71 layers (5 HBlocks) and 33 layers (3 HBlocks). The batch normalization is used after each convolution layer except the last layer. After that, ReLU is applied as an activation function. Finally, in order to achieve more high-quality recovery, a recursive enhancement structure is introduced with a total of 3 stages, which is gradually leading to perception-friendly quality at the final stage. The full description of our enhancement network is shown in Table 1 .
TABLE I The architecture of enhancement network (71 layers). Info is composed of kernel and stride.
2) Feature Identity Extraction Module

The feature identity extraction module (FIE) is designed for correlating information from image enhancement and high-level visual perception features. The FIE is based on 3-layer CNN, which assigns exactly 128-dimensional latent features after the output of flattening the last layer of FIE with random projection instead of dense, as shown in Fig. 3 . This allows unrestricted comparisons through random projection when the final layer output dimensions of FIE are different. Therefore, the FIE connects them by representing the mutual influence between image enhancement and visual perception in a unified framework.
C. Training Strategy

To learn the proposed network, we further integrate both image enhancement network and high-level network via three-stage. Our training strategy is divided into three parts: 1) image enhancement network learning, 2) high-level vision loss calculation, and 3) feature identity learning.
1) Image recovery loss

Existing state-of-the-art methods adopt the pixel-wise loss based on MSE (Mean-Squared error) to train enhancement network. However, the MSE optimization usually produces blurry visual information which results in perceptual unsatisfactory images with over-smooth content. To prevent this, we estimate the successive approximation to the bad weather distribution with the guidance of the Charbonnier penalty function [42] , which is more robust to outliers. The recovery loss is expressed as:
L R ( I p r e d , I G T ) = ∥ ∥ ∥ ( I p r e d − I G T ) 2 ) − − − − − − − − − − − − √ ∥ ∥ ∥ 2 2 + ε 2 , (1)
View Source \begin{equation*}{L_R}\left({{I^{pred}},{I^{GT}}}\right) = \left\| {\sqrt {\left({{I^{pred}}{{\left. { - {I^{GT}}}\right)}^2}}\right)} } \right\|_2^2 + {\varepsilon ^2},\tag{1}\end{equation*} where ε is penalty coefficient and empirically set to 5 × 10 −3 . We take one step further to give rich connectivity between the enhancement network and high-level perception.

2) High-level task loss

We use high-level task loss L HT from a pre-trained high-level vision task network to provide the enhancement network with connectivity that promotes it to be perception-friendly. By default, perception networks for high-level tasks are pre-trained on benchmarks composed of clean images and are frozen while learning the proposed framework. In addition, even if our enhancement network is replaced with another model, it can be replaced without any additional tuning to the coefficients of objective function and retraining of the perception network. As far as we know, this is the first study to run a variety of high-level tasks while dealing with all bad weather, taking a step further in universality. To convey more strong perception-friendly property, we describe the feature identity loss in the next.
3) Feature identity loss

[43] propose to utilize a Euclidean distance which calculates identity information on image pairs, that proves to generate high-quality samples than the standard per-pixel losses. Their idea has been adopted mostly in image generation work, such as super-resolution, translation, and image recovery. Despite the fact, we observed that even when the recognition tasks other than image generation is involved, the identity information is still essential for stable optimization. To give the relevant information in the training process, we propose to use a feature identity loss that leads to the directly related to identity in hypersphere space, defined as:
L F I ( f e n l a s t , f h t l a s t ) = | | F I E ( f e n l a s t ) ˆ − F I E ( f h t l a s t ) | | 2 2 ˆ , (2)
View Source \begin{equation*}{L_{FI}}\left({f_{last}^{en},f_{last}^{ht}}\right) = ||\widehat {FIE\left({f_{last}^{en}}\right)} - \widehat {FIE\left({f_{last}^{ht}}\right)||_2^2},\tag{2}\end{equation*} where F I E ( f e n l a s t ) and F I E ( f h t l a s t ) are the identity features extracted from ( FIE ) for input image I X and recovered image I pred , respectively. F I E ( ⋅ ) ˆ is the identity representation mapped to the hypersphere.

4) Objective Function

Based on the above introduction, we incorporate the above-mentioned losses as an objective function. We optimize the total objective function based on the stage-wise manner and can be trained by the following function:
min { θ e n , ϕ } 1 N ∑ i = 1 N ( L R ( I p r e d i , I G T i ) + α L H T ( Y p r e d i , Y G T i ) + β L F I ( f e n l a s t , f h t l a s t ) ) , (3)
View Source \begin{align*}& \mathop {\min }\limits_{\left\{ {{\theta _{en}},\phi } \right\}} \frac{1}{N}\sum\limits_{i = 1}^N {\left({{L_R}\left({I_i^{pred},I_i^{GT}}\right) + \alpha {L_{HT}}\left({Y_i^{pred},Y_i^{GT}}\right)} \right.} \\ & \qquad \left. { + \beta {L_{FI}}\left({f_{last}^{en},f_{last}^{ht}}\right)}\right),\tag{3}\end{align*} where α and β are trade-off coefficients of the L HT and L FI respectively, θ en and ϕ are learnable parameters from scratch with N samples.

SECTION IV.
Experiments and Evaluation
A. Datasets

In computer vision, few image datasets contain comprehensive bad weather conditions specific to driving situations, and likewise none of the datasets mentioned above are available. In order to create bad weather effects, we adopt rain streak, raindrop, and haze simultaneously on each image in the dataset (2 rain streaks × 1 raindrop × 2 haze effects). For the realistic rain streak effect, we are motivated from [45] and thus create two versions of streak intensities (heavy and light rain) with randomly distributed orientation. For the raindrop effect, we adopt a simulation method from [46] to apply the water drop on the lens to all images. For the haze effect, we employ widely used atmospheric scattering model introduced in [47] , [48] and generate two different hazy images under uniformly randomly chosen atmospheric lights and scattering coefficient as parameters. As a result, we obtain a paired dataset of clean target-bad weather images, where the split indexing of training and testing samples all follow the standard of the existing dataset.

For TuSimple dataset [11] as lane detection benchmark, we take 3,626 images for training and 2,782 images for testing. For monocular depth estimation, we also take 39,810 images for training and 4,424 images for testing from KITTI benchmark [44] . Lastly, object detection evaluation of our model is performed on the RID dataset [18] which contains real bad weather such as rain streaks and densely disrupted haze, it only provides 2,495 samples for testing without synthetic effects.
B. Training Configurations

The used datasets are resized to 1024 × 512 including both training and testing. All the networks are trained from scratch using Adam optimizer for 100 epochs with a total batch size of 8. The learning rate is first initialized to 0.0001 and divided by 5 following milestones at the 30 th , 50 th , and 80 th epochs. The output channel width k is [14] , [16] , [20] , [20] , [40] for the 71-layer and [14] , [16] , [40] for the 33-layer. Additionally, by empirical finding, the coefficient α for high-level tasks is set to 0.01, 0.05, and 0.002 for lane detection, depth estimation, and object detection, respectively. All the experiments are performed by using one NVIDIA TITAN X GPU and one Intel Core i7-6700K CPU based on the PyTorch framework.
TABLE II Quantitative bad weather enhancement evaluations with average PSNR/SSIM on synthetic images. The best results in all methods are marked in bold. Second best are underlined.
TABLE III Quantitative high-level perception evaluations on two datasets (TuSimple, KITTI) and one real-world dataset RID. Best results in each category are in bold. Second best are underlined.
Fig. 4.

Visual comparison of different enhancement results. For the four bad weather images in the first column (a), columns (b-d) show that the enhancement results by state-of-the-art methods, respectively. The proposed method contributes to getting better restoration results in column (e-f). Ground truth is shown in the last column (g). Best viewed on the computer, in color, and zoomed in.

Show All
Fig. 5.

Visualization of object detection results on RID [18] . The yellow bounding box on the bad weather input (a) is ground truth. Other bounding boxes in (b-g) are predicted results. Best viewed on the computer, in color, and zoomed in.

Show All
C. Experimental Configurations

For evaluation of our model, we test the effectiveness of our method on three representative high-level tasks: lane detection, monocular depth estimation, and object detection. For the perception baselines, we employ state-of-the-art baselines: PINet [2] for lane detection, Monodepth2 for monocular depth estimation [5] , and RetinaNet [6] for object detection. We also utilize their pre-trained weights from the publicly available codes.

To quantitatively verify the effectiveness of the proposed method, we employ two types of metrics that measure task performance and image quality. For the image quality, we adopt PSNR and SSIM [49] , which are standard [50] , [51] in image recovery. However, they may become loosely related when it comes to other high-level task purposes [18] , [26] , [31] . Therefore, for the task-specific evaluation, we use standard metrics like the following: accuracy for lane detection, RMSE for monocular estimation, and mAP for object detection. Note that the proposed setting is evaluated without requiring manual data annotation in a comprehensive and fair setting.
TABLE IV Computational cost analysis.
D. Image Enhancement Evaluations

Table 2 shows that our method achieves significant gains in terms of both PSNR and SSIM. As shown in Fig. 4 , qualitative results reveal great effectiveness, while the result by DeRaindrop still contains visible bad weather elements. Consequently, the visual quality enhancement by our methods is significant, while the results by existing methods still contain visible bad weather effects. Moreover, our lightweight model (33-layer) shows PSNR on par with the other three methods. To our best knowledge, our 71-layer model is the only bad weather enhancement method so far that can simultaneously achieve clean visual quality and hardware-friendly property. Such a large gain demonstrates our method generates promising image enhancement results, which are visually more clear.
Fig. 6.

Visualization of monocular depth estimation results on KITTI [44] . The results in (b) is the ground truth extracted from the clean image. Other results in (c-f) are predicted results from bad weather input (a). Best viewed on the computer, in color, and zoomed in.

Show All
Fig. 7.

Visualization of lane detection results on TuSimple [11] . The results in (b) is the ground truth extracted from the clean image. Other results in (c-f) are predicted results from bad weather input (a). Best viewed on the computer, in color, and zoomed in.

Show All
E. High-Level Task Evaluations

To evaluate the effectiveness of our enhancement framework on high-level tasks in bad weather, we further show that the method yields meaningful results. From Table 3 , our method outperforms the high-level perception performances in comparison for different methods using RESCAN, De-Raindrop, and PReNet as a pre-processing. As reported in [18] , most models do not improve over the bad weather input in terms of high-level perception metrics. Instead, our method is observed to improve performance in high-level tasks without deteriorating. This proves that our method represents mutual influence between visual perception and enhancement, thus providing significant help for bad weather capabilities. Fig. 5 , 6 and 7 also show that our proposed method outperforms the existing methods, confirming the perception-friendly ability of the model to bad weather.

Table 4 reports the computational cost of our enhancement network and some state-of-the-art methods. From the results, we can find that our method has less computational overload due to the harmonic dense block. Taking their hardware-friendly ability into account, it is appealing to still maintain perception performance when facing bad weather images.
F. Ablation Studies

To study the contribution of each network in our proposed framework, we alternatively remove it and identify the impact on the high-level perception performance. As can be seen in Table 5 , our method with joint training (c-d) performs better than simple connection (b). Joint training (c) has a slightly lower baseline in Task1 and Task3, which is not surprising since it was not trained with the feature identity network. After applying the FIE (d), our method is the best-ranked approach that significantly outperforms the other three options (a-c), and we are encouraged to observe that the FIE brings the interconnection between the two networks. Finally, it is observed that all the networks in our proposed framework lead to important contribution in the final performance.
TABLE V Ablation Study. HLT refers to the baselines corresponding to the task. T1 to 3 refer to lane detection, monocular depth estimation, and object detection, respectively.
SECTION V.
Conclusions

In this paper, we have proposed a novel task-driven image enhancement framework connected to visual perception for autonomous driving under the presence of bad weather. In particular, we have revealed that the existing methods are not practical for real-world autonomous driving in resource-constrained devices, and have aimed to improve them from two perspectives. First, our method is perception-friendly since it is optimized not only for the human-centric visibility but also for the high-level task models simultaneously. In addition, we developed a low-memory network architecture, focusing on a hardware-friendly ADAS system on the embedded system suitable for autonomous cars. Compared to previous methods, our method has verified improved performance in terms of both perception and hardware for autonomous driving despite bad weather. Future work will focus on modeling bad weather characteristics explicitly to remove artifacts and preserve details more effectively.

Authors
Figures
References
Keywords
Metrics
Media
Footnotes
   Back to Results   
More Like This
Road traffic signs guidance analysis for small navigation vehicle control system

2007 IEEE International Conference on Vehicular Electronics and Safety

Published: 2007
Anomaly modelling in machine learning based navigation system of autonomous vehicles

2020 6th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS)

Published: 2020
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
