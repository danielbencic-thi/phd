IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Typesetting math: 100%

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Conferences > 2019 IEEE/RSJ International C...
FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality
Publisher: IEEE
Cite This
PDF
  << Results    |    Next >  
Winter Guerra ; Ezra Tal ; Varun Murali ; Gilhyun Ryou ; Sertac Karaman
All Authors
View Document
32
Paper
Citations
307
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    System Architecture
    III.
    Exteroceptive Sensor Simulation
    IV.
    Applications
    V.
    Conclusions

Authors
Figures
References
Citations
Keywords
Metrics
More Like This

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: FlightGoggles is a photorealistic sensor simulator for perception-driven robotic vehicles. The key contributions of FlightGoggles are twofold. First, FlightGoggles provid... View more
Metadata
Abstract:
FlightGoggles is a photorealistic sensor simulator for perception-driven robotic vehicles. The key contributions of FlightGoggles are twofold. First, FlightGoggles provides photorealistic exteroceptive sensor simulation using graphics assets generated with photogrammetry. Second, it provides the ability to combine (i) synthetic exteroceptive measurements generated in silico in real time and (ii) vehicle dynamics and proprioceptive measurements generated in motio by vehicle(s) in flight in a motion-capture facility. FlightGoggles is capable of simulating a virtual-reality environment around autonomous vehicle(s) in flight. While a vehicle is in flight in the Flight-Goggles virtual reality environment, exteroceptive sensors are rendered synthetically in real time while all complex dynamics are generated organically through natural interactions of the vehicle. The FlightGoggles framework allows for researchers to accelerate development by circumventing the need to estimate complex and hard-to-model interactions such as aerodynamics, motor mechanics, battery electrochemistry, and behavior of other agents. The ability to perform vehicle-in-the-loop experiments with photorealistic exteroceptive sensor simulation facilitates novel research directions involving, e.g., fast and agile autonomous flight in obstacle-rich environments, safe human interaction, and flexible sensor selection. FlightGoggles has been utilized as the main test for selecting nine teams that will advance in the AlphaPilot autonomous drone racing challenge. We survey approaches and results from the top AlphaPilot teams, which may be of independent interest. FlightGoggles is distributed as open-source software along with the photorealistic graphics assets for several simulation environments, under the MIT license at http://flightgoggles.mit.edu.
Published in: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
Date of Conference: 3-8 Nov. 2019
Date Added to IEEE Xplore : 27 January 2020
ISBN Information:
ISSN Information:
INSPEC Accession Number: 19299180
DOI: 10.1109/IROS40897.2019.8968116
Publisher: IEEE
Conference Location: Macau, China
Contents
SECTION I.
Introduction

Simulation systems have long been an integral part of the development of robotic vehicles. They allow engineers to identify errors early on in the development process, and allow researchers to rapidly prototype and demonstrate their ideas. Despite their success in accelerating development, many researchers view results generated in simulation systems with skepticism, as all simulation systems are abstractions of reality and will disagree with reality at some scale. This skepticism towards results generated exclusively in simulation studies is exemplified by Rodney Brooks’ well-known quote from 1993: “[experiment] simulations are doomed to succeed … [because] simulations cannot be made sufficiently realistic” [1] .

Despite the skepticism towards simulation results, several trends have emerged in recent years that have driven the research community to develop better simulation systems out of necessity. A major driving trend towards realistic simulators stems from the emergence of data-driven algorithmic methods in robotics, for instance, based on machine learning methods that require extensive data. Simulation systems provide not only massive amounts of data but also the labels required for training algorithms. For example, simulation may provide an efficient and safe environment for reinforcement learning [2] , [3] . This driving trend has posed a critical need to develop better, more realistic simulation systems.

Fig. 1: - FlightGoggles renderings of the Abandoned Factory environment, designed for autonomous drone racing. Note the size of the environment and the high level of detail.
Fig. 1:

FlightGoggles renderings of the Abandoned Factory environment, designed for autonomous drone racing. Note the size of the environment and the high level of detail.

Show All

Several enabling trends have also recently emerged that allow for better, more-realistic simulation systems to be developed. The first enabling trend is the development of new computing resources that enable realistic rendering. The rapid evolution of game engine technology, particularly 3D graphics rendering engines, has made available advanced features such as improved material characteristics, real-time reflections, volumetric lighting, and advanced illumination through deferred and raytraced rendering pipelines. Particularly, the maturation of off-the-shelf software packages such as Unreal Engine [4] and Unity [5] , makes them suitable for high-fidelity rendering in applications beyond video games, such as robotics simulation. Simultaneously, next-generation graphics processors simply pack more transistors, and the transistors are better organized for rendering purposes, e.g. , for real-time ray tracing. In addition, they incorporate computation cores that utilize machine learning, for instance, trained with pictures of real environments to generate realistic renderings. This trend is an opportunity to utilize better software and hardware to realize realistic sensor simulations. The second enabling trend stems from the proliferation of motion capture facilities for robotics research, enabling precise tracking of robotic vehicles and humans through various technologies, such as infrared cameras, laser tracking, and ultra-wide band radio. These facilities provide the opportunity to incorporate real motion and behavior of vehicles and humans into the simulation in real time. This trend provides the potential to combine the efficiency, safety, and flexibility of simulation with real-world physics and agent behavior.

Traditionally, simulation systems embody “models” of the vehicles and the environment, which are used to emulate what the vehicles sense, how they move, and how their environment adapts. In this paper, we present two concepts that use “data” to drive realistic simulations. First, we heavily utilize photogrammetry to realistically simulate exteroceptive sensors. For this purpose, we photograph real-world objects, and reconstruct them in the simulation environment. Almost all objects in our simulation environment are, in fact, renderings of real-world objects. This approach allows realistic renderings, as shown in Fig. 1 . Second, we utilize a novel virtual-reality system to realistically embed inertial sensors, vehicles dynamics, and human behavior into the simulation environment. Instead of modeling these effects, we place vehicles and human actors in motion-capture facilities. We acquire the pose of the vehicles and the configuration of the human actors in real time, and create their avatars in the simulation environment. For each autonomous vehicle, its proprioceptive measurements are acquired using on-board sensors, e.g. , inertial measurement units and odometers; while exteroceptive sensors are rendered photorealistically in real time. In addition, the human behavior observed by the vehicles is generated by humans reacting to the simulation. In other words, vehicles embedded in the FlightGoggles simulation system experience real dynamics, real inertial sensing, real human behavior, and synthetic exteroceptive sensor measurements rendered photorealistically effectively by transforming photographs of real-world objects.

The combination of real physics and data-driven exteroceptive sensor simulation that FlightGoggles provides is not achieved in traditional simulation systems. Such systems are typically built around a physics engine that simulates vehicles and the environment based on a “model”, most commonly a system of ordinary or partial differential equations [6] . While these models may accurately exemplify the behavior of a general vehicle or actor, this is not sufficient to ensure that simulation results transfer to the real world. Complicated aspects of vehicle dynamics, e.g. , vibrations and unsteady aerodynamics, and of human behavior may significantly affect results, but can be very challenging to accurately capture in a physics model. In order to generate exteroceptive sensor data, robotics simulators employ a graphics rendering engine in conjunction with the physics engine. A popular example is Gazebo [7] , which lets users select various underlying engines. It is often used in combination with the Robot Operating System (ROS) to enable hardware-in-the-loop simulation. However, Gazebo is generally not capable of photorealistic rendering. Specifically, for unmanned aerial vehicles simulation, two popular simulators that are built on Gazebo are the Hector Quadrotor package [8] and RotorS [9] . Both simulators include vehicle dynamics and exteroceptive sensor models, but lack the capability to render photorealistic camera streams. AirSim, on the other hand, is purposely built on the Unreal rendering engine to enable rendering of photorealistic camera streams from autonomous vehicles, and exemplifies the shift towards using video game rendering engines to improve realism in robotics simulation [10] . However, it is still limited by the fidelity of the underlying physics engine when it comes to vehicle dynamics and inertial measurements.

Simulation offers an alternative to experimental data gathering in addressing the need for extensive labeled data sets, bolstered by the rise of data-driven algorithms for autonomous robotics. Clearly, there are many advantages to this approach, e.g. , cost efficiency, safety, repeatability, and essentially unlimited quantity and diversity. In recent years, several synthetic, or virtual, datasets have appeared in literature. For example, Synthia [11] and Virtual KITTI [12] use Unity to generate photorealistic renders of an urban environment, and ICL-NUIM [13] provides synthetic renderings of an indoor environment based on pre-recorded handheld trajectories. The Blackbird Dataset [14] includes real-world ground truth and inertial measurements of a quad-copter in motion capture, and photorealistic camera imagery rendered in FlightGoggles. The open-source availability of FlightGoggles and its photorealistic assets enables users to straightforwardly generate additional data, including real-time photorealistic renders based on real-world vehicles and actors.

This paper is organized as follows. Section II provides an overview of the FlightGoggles system architecture, including interfacing with real-world vehicles and actors in motion capture facilities. Section III outlines the photogrammetry process and the resulting virtual environment. This section also details the rendering pipeline and the exteroceptive sensor models available. Section IV describes several applications of FlightGoggles, including results of the AlphaPilot qualifications. Finally, Section V concludes with remarks.
SECTION II.
System Architecture

FlightGoggles is based on a modular architecture, as shown in Fig. 2 . This architecture provides the flexibility to tailor functionality for a specific simulation scenario involving real and/or simulated vehicles, and possibly human interactors. As shown in the figure, FlightGoggles’ central component is the Unity game engine. It utilizes position and orientation information to simulate camera imagery and exteroceptive sensors and to detect collisions. Collision checks are performed using mesh colliders, and results are output to be included in the dynamics of simulated vehicles.

FlightGoggles includes a multicopter physics engine for simulation of six degree-of-freedom flight dynamics, inertial measurements, and flight control. The physics model includes motor dynamics, basic vehicle aerodynamics, and IMU bias dynamics and can be integrated using explicit Euler and 4th-order Runge-Kutta algorithms. A detailed description of the simulation is given in an elaborate version of this paper [15] . Additionally, the FlightGoggles API provides a simulation base class that can be used to simulate user-defined vehicle equations of motion and measurement models. Simulation scenarios may also include real-world vehicles through the use of a motion capture system. In this case, Unity simulation of camera images and exteroceptive sensors, and collision detection are based on the real-world vehicle position and orientation. This type of vehicle-in-the-loop simulation can be seen as an extension of customary hardware-in-the-loop configurations. It not only includes the vehicle hardware, but also the actual physics of processes that are challenging to simulate accurately, such as aerodynamics (including effects of turbulent air flows), and inertial measurements subject to vehicle vibrations. FlightGoggles provides the novel combination of real-world vehicle dynamics and proprioceptive measurements, and simulated photorealistic exteroceptive sensor simulation. It allows for real-world physics, flexible exteroceptive sensor configurations, and obstacle-rich environments without the risk of actual collisions. FlightGoggles also allows scenarios involving both humans and vehicles, co-located in simulation but placed in different motion capture rooms, e.g. , for safety.

Fig. 2: - Overview of FlightGoggles system architecture. Pose data of real and simulated vehicles, and human interactors is used by the Unity rendering engine. All dynamics states, control inputs, and sensor outputs of real and simulated vehicles, and human interactors are available through the FlightGoggles API.
Fig. 2:

Overview of FlightGoggles system architecture. Pose data of real and simulated vehicles, and human interactors is used by the Unity rendering engine. All dynamics states, control inputs, and sensor outputs of real and simulated vehicles, and human interactors are available through the FlightGoggles API.

Show All

Dynamics states, control inputs, and sensor outputs of real and simulated vehicles, and human interactors are available to the user through the FlightGoggles API. In order to enable message passing between FlightGoggles nodes and the API, the framework can be used with either ROS [16] or LCM [17] . The FlightGoggles simulator can be run headlessly on an Amazon Web Services (AWS) cloud instance to enable real-time simulation on systems with limited hardware.

Dynamic elements, such as moving obstacles, lights, vehicles, and human actors, can be added and animated in the environment in real time. Using these added elements, users can change environment lighting or simulate complicated human-vehicle, vehicle-vehicle, and vehicle-object interactions in the virtual environment. In Section IV , we describe an use case involving a dynamic human actor. In this scenario, skeleton tracking motion capture data is used to render a 3D model of the human in the virtual FlightGoggles environment. The resulting render is observed in real time by a virtual camera attached to a quadcopter in real-world flight in a different motion capture room, as shown in Fig. 6 .
SECTION III.
Exteroceptive Sensor Simulation

This section describes the creation of the environment using photogrammetry, lists the features of the render pipeline, and describes each of the exteroceptive sensor models.

FlightGoggles provides a simulation environment with exceptional visual fidelity. Its high level of photorealism is achieved using 84 unique 3D models captured from real-world objects using photogrammetry, as can be seen in Fig. 3 . The resulting environment is comprised of over 40 million triangles and 1,050 object instances.
A. Photorealistic Sensor Simulation using Photogrammetry

Photogrammetry is the process in which multiple photographs of a real-world object from different viewpoints are used to efficiently construct a realistic high-resolution 3D model for use in virtual environments. This technique has two major advantages when compared to traditional 3D modeling techniques. Firstly, it requires virtually no manual modeling and texturing. The elimination of these time-consuming and artistically demanding processes enables the creation of many high-resolution assets in a relatively short time and at a more moderate cost. Secondly, the resulting renderings are based directly on real-world data, i.e. , photographs. Consequently, the simulation includes a photorealistic representation of the real-world object that is being modeled, which may be critical in robotics applications. Due to its advantages over traditional modeling methods, photogrammetry is already widely used in the video game industry; however, its application towards photorealistic robotics simulation, as introduced in FlightGoggles, is novel.

    Photogrammetry asset capture pipeline: Photogrammetry was used to create 84 unique open-source 3D assets for the FlightGoggles environment. These assets are based on thousands of high-resolution digital photographs of real-world objects and environmental elements, such as walls and floors. The digital images were first color-balanced, and then combined to reconstruct object meshes using the GPU-based reconstruction software Reality Capture [18] . After this step, the raw object meshes were manually cleaned to remove reconstruction artifacts. Mesh baking was performed to generate base color, normal, height and ambient occlusion maps for each object; which are then combined into one high-definition surface material in Unity3D. For a detailed overview of a typical photogrammetry capture workflow, we refer the reader to [19] .

    HD render pipeline: Fig. 3 shows several 3D assets that were generated using the process described above. The Figure also shows examples of real-world reference imagery that was used in the photogrammetry process to construct these assets. To achieve photorealistic RGB camera rendering, FlightGoggles uses the Unity Game Engine High Definition Render Pipeline (HDRP) [20] . Using HDRP, cameras rendered in FlightGoggles have characteristics similar to those of real-world cameras including motion blur, lens dirt, bloom, real-time reflections, and precomputed ray-traced indirect lighting. Additional camera characteristics such as chromatic aberration, vignetting, lens distortion, and depth of field can be enabled in the simulation environment.

B. Performance Optimizations and System Requirements

Extensive performance and memory optimizations were performed to ensure that FlightGoggles is able to run on a wide spectrum of GPU rendering hardware with ≥ 2GB of video random access memory (VRAM). Additionally, Flight-Goggles VRAM and GPU computation requirements can be reduced further by user-selectable quality profiles based on three major settings: real-time reflections, maximum object texture resolution, and maximum level of detail ( i.e. polygon count).

    Mesh level of detail: For each object mesh in the environment, three meshes with different levels of detail (LOD), i.e. , polygon count and texture resolution, were generated: low, medium, and high. For meshes with lower levels of detail, textures were downsampled using subsampling and subsequent smoothing. During simulation, the real-time render pipeline improves render performance by selecting the appropriate level of detail object mesh and texture based on the size of the object mesh in camera image space. GPU VRAM usage can be decreased further by limiting the maximum level of detail across all meshes, through the user-selectable quality profiles.

    Pre-baked ray-traced lighting: In order to reduce runtime computation, all direct and indirect lighting, ambient occlusions, and shadow details from static light sources are pre-baked via NVIDIA RTX ray tracing. The resulting static lightmaps are layered onto object meshes in the environment. An NVIDIA Quadro RTX 8000 GPU was used to precompute the ray-traced lighting for each lighting condition in the Abandoned Warehouse environment, resulting in an average bake time of 45 minutes per lighting arrangement.

    Render batching: Flightgoggles uses platform-specific render batching to increase rendering performance by reducing individual GPU draw calls. On Windows-based systems supporting DirectX11, FlightGoggles leverages the experimental Unity3D Scriptable Render Pipeline dynamic batcher, which drastically reduces GPU draw calls for all static and dynamic objects in the environment. On Linux and MacOS systems, FlightGoggles statically batches all static meshes in the environment. Static batching drastically increases rendering performance, but also increases VRAM usage as all meshes must be combined and pre-loaded onto the GPU memory at run time. If necessary, the latter increase can be addressed by reducing the VRAM usage through the user-selectable quality profiles.

    Dynamic clock scaling: FlightGoggles provides optional dynamic clock scaling to guarantee a nominal camera frame rate in simulation time, even on rendering hardware that is incapable of achieving reliable real-time frame rates. When automatic clock scaling is enabled, FlightGoggles monitors the frame rate of the renderer output and dynamically adjusts the ROS simulation time rate to achieve the desired nominal frame rate in simulation time. Since the built-in ROS time framework is used, changes in time rate do not affect the relative timing of client nodes, which alleviates non-deterministic timing issues across simulation runs.

Fig. 3: - Object photographs that were used for photogrammetry and corresponding rendered assets in FlightGoggles.
Fig. 3:

Object photographs that were used for photogrammetry and corresponding rendered assets in FlightGoggles.

Show All

C. Exteroceptive Sensor Models

FlightGoggles is capable of high-fidelity simulation of various types of exteroceptive sensors, such as RGB-D cameras, time-of-flight distance sensors, and infrared radiation (IR) beacon sensors. Default noise characteristics, and intrinsic and extrinsic parameters are based on real sensor specifications, and can easily be adjusted. Moreover, users can instantiate multiple instances of each sensor type. This capability allows quick prototyping and evaluation of distinct exteroceptive sensor arrangements.

    Camera: The default camera model provided by FlightGoggles is a perfect, i.e. , distortion-free, projection model with optional motion blur, lens dirt, auto-exposure, and bloom. Major camera parameters, such as field of view, image resolution, and stereo baseline, are exposed in the FlightGoggles API. The camera extrinsics T b c where b is the vehicle fixed body frame and c is the camera frame can also be changed in real time.

    Infrared beacon sensor: An IR beacon sensor model is included to facilitate the quick development of guidance, navigation, and control algorithms. This sensor provides image-space u, v measurements of IR beacons in the camera field of view. The beacons can be placed at static locations in the environment or on moving objects. Using real-time ray-casting from each RGB camera, simulated IR beacon measurements are tested for occlusion before being included in the IR sensor output. Fig. 4 shows a visual representation of the sensor output.

    Time-of-flight range sensor: FlightGoggles is able to simulate (multi-point) time-of-flight range sensors using ray casts in any specified direction. In the default vehicle configuration, a downward-facing single-point range finder for height estimation is provided. The noise characteristics of this sensor are similar to the commercially available LightWare SF11/B laser altimeter [21] .

Fig. 4: - Rendered camera view (faded) with IR marker locations overlayed. The unprocessed measurements and marker IDs from the simulated IR beacon sensor are indicated in red. The measurements are verified by comparison to image-space reprojections of ground-truth IR marker locations, which are indicated in green. Note that IR markers can be arbitrarily placed by the user, including on dynamic objects.
Fig. 4:

Rendered camera view (faded) with IR marker locations overlayed. The unprocessed measurements and marker IDs from the simulated IR beacon sensor are indicated in red. The measurements are verified by comparison to image-space reprojections of ground-truth IR marker locations, which are indicated in green. Note that IR markers can be arbitrarily placed by the user, including on dynamic objects.

Show All

SECTION IV.
Applications

In this section, we discuss current and potential FlightGoggles applications, such as human-vehicle interaction, active sensor selection, multi-agent systems, and visual inertial navigation research for fast and agile vehicles [14] , [22] , [23] . We also describe the simulation stage of the AlphaPilot challenge, in which teams used FlightGoggles to compete in a simulated drone race [24] .
A. Aircraft-in-the-Loop High-Speed Flight using Visual Inertial Odometry

Camera-IMU sensor packages are widely used in both commercial and research applications, because of their relatively low cost and low weight. Particularly in GPS-denied environments, cameras may be essential for effective state estimation. Visual inertial odometry (VIO) algorithms combine camera images with pre-integrated IMU measurements to estimate the vehicle state. While these algorithms are often critical for safe navigation, it is challenging to verify their performance in varying conditions. Environment variables, e.g. , lighting and object placement, and camera properties may significantly affect performance, but generally cannot easily be varied in reality. Moreover, obstacle-rich environments may increase the risk of collisions, especially in high-speed flight, increasing the cost of extensive experiments.

Fig. 5: - Visual features tracked using a typical visual inertial odometry pipeline on FlightGoggles simulated camera imagery.
Fig. 5:

Visual features tracked using a typical visual inertial odometry pipeline on FlightGoggles simulated camera imagery.

Show All

There are several examples of research in state estimation that relied on the FlightGoggles framework. In previous work, it was shown that the estimation error of a VIO algorithm remains similar when replacing data from a real on-board camera with simulated camera imagery from FlightGoggles [22] . Fig. 5 shows tracking of virtual visual features during a VIO flight in FlightGoggles. In [23] , the FlightGoggles simulation system enabled experimentation with various environment configurations to show the efficiency of a perception-aware planning algorithm. For these experiments, the quadcopter was tracking a perception-aware trajectory using a state-of-the-art controller [25] , while relying on a pose estimate from VIO based on the virtual camera in FlightGoggles and real-world inertial measurements from the vehicle in flight.
B. Interactions with Dynamic Actors

FlightGoggles is able to render dynamic actors, e.g. , humans or vehicles, in real time from real-world models with ground-truth movement. Fig. 6 gives an overview of a simulation scenario involving a human actor. In this scenario, the human is rendered in real time based on skeleton tracking motion capture data, while a quadcopter is simultaneously flying in a separate motion capture room. While both dynamic actors ( i.e. human and quadcopter) are physically in separate spaces, they are both in the same virtual Flight-Goggles environment. Consequently, both actors are visible to each other and can interact through simulated camera imagery. This imagery can for example be displayed on virtual reality headsets, or used in vision-based autonomy algorithms. FlightGoggles provides the capability to simulate these realistic and versatile human-vehicle interactions in an inherently safe manner.

Fig. 6: - A dynamic human actor in the FlightGoggles virtual environment is rendered in real time, based on skeleton tracking data of a human in a motion capture suit with markers.
Fig. 6:

A dynamic human actor in the FlightGoggles virtual environment is rendered in real time, based on skeleton tracking data of a human in a motion capture suit with markers.

Show All

C. AlphaPilot Challenge

The AlphaPilot challenge [24] is an autonomous drone racing challenge organized by Lockheed Martin, NVIDIA, and the Drone Racing League (DRL). The challenge is split into two stages. A simulation phase open to the general public, and a real-world phase in which teams compete against each other by programming fully-autonomous racing drones built by the DRL. During the simulation phase, the FlightGoggles simulation framework was used as the main qualifying test for selecting nine teams that would progress to the next stage of the AlphaPilot challenge. To complete the test, contestants had to submit code to autonomously race a simulated quadcopter with simulated sensors through the 11-gate race track shown in Fig. 7 . Test details were revealed to all contestants on February 14th, 2019 and final submissions were due on March 20th, 2019.

    Challenge outline: The purpose of the AlphaPilot simulation challenge was for teams to demonstrate their autonomous guidance, navigation, and control capability in a realistic simulation environment. The participants’ aim was to complete the track as fast as possible using a simulated quadcopter based on the FlightGoggles multicopter dynamics model. To accomplish this, measurements from four simulated sensors were provided: (stereo) cameras, IMU, downward-facing time-of-flight range sensor, and infrared gate beacons. Through the FlightGoggles ROS API, autonomous systems could obtain sensor measurements and provide collective thrust and attitude rate inputs to the quadcopter low-level acro/rate mode controller.The race track was located in the FlightGoggles Abandoned Factory environment and consisted of 11 gates. To successfully complete the entire track, the quadcopter had to pass through all the gates in order. The final score was calculated as score = 10 · gates – time where gates is the number of gates passed in order and time is the time taken in seconds to reach the final gate. If the final gate was not reached within the race time limit or the quadcopter collided with an environment object, a score of zero was recorded. To discourage memorization of the course, the exact gate locations were subject to random unknown perturbations. These perturbations were large enough to require adapting the vehicle trajectory, but did not change the track layout in a fundamental way. The final score for each team was the average of their five highest scores over an evaluation set of 25 perturbed courses that was kept unknown to the teams. For development and verification of their algorithms, participants were provided with the nominal gate locations, as well as another set of 25 perturbed courses with identically distributed gate locations.

    Fig. 7: - Overhead visualization of speed profiles (in $ms^{-1}$) and crash locations for top 20 AlphaPilot teams across all 25 runs. Nominal gate locations are numbered in track order and marked with boxes. Note that most crashes occur near gates, obstacles, or immediately after takeoff.
    Fig. 7:

    Overhead visualization of speed profiles (in m s − 1 ) and crash locations for top 20 AlphaPilot teams across all 25 runs. Nominal gate locations are numbered in track order and marked with boxes. Note that most crashes occur near gates, obstacles, or immediately after takeoff.

    Show All

    FlightGoggles sensor usage: Table I shows the usage of provided sensors, the algorithm choices, and the final and five highest scores for the 20 top teams (sorted by final score). All of these 20 teams used both the simulated IMU sensor and the infrared beacon sensors. Several teams chose to also incorporate the camera and the time-of-flight range sensor.

    Algorithm choices: The contestants were tasked with developing guidance, navigation, and control algorithms. Table I tabulates the general estimation, planning, and control approaches used for each team alongside the sensor choices and their scores. Of the top 20 teams, only one used an end-to-end learning-based method. The other 19 teams relied on more traditional pipelines (estimation, planning, and control) to complete the challenge. One of those teams used learning to determine the pose of the camera from the image. For state estimation, all but one team used a filtering algorithm such as the extended Kalman filter, unscented Kalman filter, particle filter, or the Madgwick filter with the other team using a smoothing based technique. The teams that chose to use a visual inertial odometry algorithm opted to use off-the-shelf solutions for state estimation. The most common methods used for planning involved visual servo using infrared beacons or polynomial trajectory planning. Other methods used for planning either used manually-defined waypoints or used sampling-based techniques for building trajectory libraries. Five of the 19 teams that used model-based techniques also incorporated some form of perception awareness in their planning algorithms. The predominant methods for control were linear control techniques and model predictive control. Additionally, geometric and backstepping control methods were used.

    Analysis of trajectories: Fig. 7 shows the resulting trajectory speed profiles. To visualize the speed along the trajectories, we discretized the horizontal plane and colored each grid cell on a logarithmic scale according to the average of the local speeds. From the figure, we can observe that most teams chose to slow down for the sharp turns at gates 2 and 7. We can also observe that in general the average speed around gates is lower than at other portions of the environment, which can be attributed to the need to ‘search’ for the next gate. Fig. 7 also shows the crash locations of all the failed attempts. We observe that many of the crash locations are in the vicinity of the gates, which may be caused by widespread use of visual-servo-based techniques combined with the fact the infrared gate beacons are more likely to leave the camera field of view at close range.

    Individual performance of top teams: Given that the final scoring function for the competition only included the five best scoring runs, teams were encouraged to take significant risk to improve their top scores. Consequently, 75% of the contestants failed to complete the course in at least half of their 25 runs. Only one team completed the entire course in all of their 25 runs. Notably, this team also achieved very consistent scores across all runs. While their average score across all runs ranks among the highest of all teams; their final score based on the five best runs is ranked significantly lower, showing that risk-taking strategies are indeed rewarded.

Table I - Sensor Usage, Algorithm Choices, and Final and Five Highest Scores In Alphapilot Simulation Challenge.
Table I

Sensor Usage, Algorithm Choices, and Final and Five Highest Scores In Alphapilot Simulation Challenge.

Show All

SECTION V.
Conclusions

This paper introduced FlightGoggles, a new modular framework for realistic simulation to aid robotics testing and development. FlightGoggles is enabled by photogrammetry and virtual reality technologies. Heavy utilization of photogrammetry helps provide realistic simulation of camera sensors. Virtual reality allows for integration of real vehicle motion and human behavior acquired in motion capture facilities directly into the simulation system. FlightGoggles is being actively utilized by a community of robotics researchers. In particular, FlightGoggles has served as the main test for selecting the contestants for the AlphaPilot autonomous drone racing challenge. This paper also presented a survey of approaches and results from the simulation challenge.

Authors
Figures
References
Citations
Keywords
Metrics
   Back to Results    |    Next >  
More Like This
Integration of mobile robot into virtual reality testbed

Proceedings of the 1998 IEEE International Conference on Control Applications (Cat. No.98CH36104)

Published: 1998
Design of mobile robot teleoperation system based on virtual reality

2015 3rd International Conference on Control, Engineering & Information Technology (CEIT)

Published: 2015
Show More
References
1. R. A. Brooks and M. J. Mataric, "Real robots real learning problems", Robot learning , pp. 193-213, 1993.
Show in Context CrossRef Google Scholar
2. H. Chiu, V. Murali, R. Villamil, G. D. Kessler, S. Samarasekera and R. Kumar, "Augmented reality driving using semantic geo-registration", IEEE Conference on Virtual Reality and 3D User Interfaces (VR) , pp. 423-430, 2018.
Show in Context View Article Full Text: PDF (3324) Google Scholar
3. J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, et al., "Sim-to-real: Learning agile locomotion for quadruped robots", arXiv preprint arXiv:1804.10332 , 2018.
Show in Context CrossRef Google Scholar
4. Unreal Engine, February 2019, [online] Available: https://www.unrealengine.com/.
Show in Context Google Scholar
5. Unity3d Game Engine, February 2019, [online] Available: https://unity3d.com/.
Show in Context Google Scholar
6. T. Erez, Y. Tassa and E. Todorov, "Simulation tools for model-based robotics: Comparison of Bullet Havok MuJoCo ODE and Physx", IEEE International Conference on Robotics and Automation (ICRA) , pp. 4397-4404, 2015.
Show in Context View Article Full Text: PDF (1935) Google Scholar
7. N. Koenig and A. Howard, "Design and use paradigms for Gazebo an open-source multi-robot simulator", IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pp. 2149-2154, 2004.
Show in Context View Article Full Text: PDF (512) Google Scholar
8. J. Meyer, A. Sendobry, S. Kohlbrecher, U. Klingauf and O. Von Stryk, "Comprehensive simulation of quadrotor UAVs using ROS and Gazebo", International Conference on Simulation Modeling and Programming for Autonomous Robots , pp. 400-411, 2012.
Show in Context CrossRef Google Scholar
9. F. Furrer, M. Burri, M. Achtelik and R. Siegwart, "RotorS: A modular Gazebo MAV simulator framework", Robot Operating System (ROS) , pp. 595-625, 2016.
Show in Context Google Scholar
10. S. Shah, D. Dey, C. Lovett and A. Kapoor, "Airsim: High-fidelity visual and physical simulation for autonomous vehicles", Field and Service Robotics , pp. 621-635, 2018.
Show in Context CrossRef Google Scholar
11. G. Ros, L. Sellart, J. Materzynska, D. Vazquez and A. M. Lopez, "The Synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes", IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 3234-3243, 2016.
Show in Context View Article Full Text: PDF (1527) Google Scholar
12. A. Gaidon, Q. Wang, Y. Cabon and E. Vig, "Virtual worlds as proxy for multi-object tracking analysis", CVPR , 2016.
Show in Context Google Scholar
13. A. Handa, T. Whelan, J. McDonald and A. Davison, "A benchmark for RGB-D visual odometry 3D reconstruction and SLAM", IEEE Intl. Conf. on Robotics and Automation , May 2014.
Show in Context View Article Full Text: PDF (1251) Google Scholar
14. A. Antonini, W. Guerra, V. Murali, T. Sayre-McCord and S. Karaman, "The Blackbird dataset: A large-scale dataset for UAV perception in aggressive flight", International Symposium on Experimental Robotics (ISER) , 2018.
Show in Context Google Scholar
15. W. Guerra, E. Tal, V. Murali, G. Ryou and S. Karaman, "Flightgoggles: Photorealistic sensor simulation for perception-driven robotics using photogrammetry and virtual reality", arXiv:1905.11377 [cs.RO] , 2019.
Show in Context Google Scholar
16. M. Quigley, K. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs, et al., "ROS: an open-source robot operating system", ICRA Workshop on Open Source Software , 2009.
Show in Context Google Scholar
17. A. S. Huang, E. Olson and D. C. Moore, "LCM: Lightweight communications and marshalling", IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pp. 4057-4062, 2010.
Show in Context View Article Full Text: PDF (918) Google Scholar
18. Reality Capture, February 2019, [online] Available: https://www.capturingreality.com/Product.
Show in Context Google Scholar
19. S. Lachambre, S. Lagarde and C. Jover, Unity photogrammetry workflow, 2017, [online] Available: https://unity3d.com/files/solutions/photogrammetry/Unity-Photogrammetry-Workflow 2017-07 v2.pdf.
Show in Context Google Scholar
20. High Definition Render Pipeline overview, July 2019, [online] Available: https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@6.9/manual/index.html.
Show in Context Google Scholar
21. LightWare SF11/B Laser Range Finder, February 2019, [online] Available: http://documents.lightware.co.za/SF11%20-%20Laser%20Altimeter%20Manual%20-%20Rev% 208.pdf.
Show in Context Google Scholar
22. T. Sayre-McCord, W. Guerra, A. Antonini, J. Arneberg, A. Brown, G. Cavalheiro, et al., "Visual-inertial navigation algorithm development using photorealistic camera simulation in the loop", IEEE International Conference on Robotics and Automation (ICRA) , pp. 2566-2573, 2018.
Show in Context View Article Full Text: PDF (1602) Google Scholar
23. V. Murali, I. Spasojevic, W. Guerra and S. Karaman, "Perception-aware trajectory generation for aggressive quadrotor flight using differential flatness", American Control Conference (ACC) , 2019.
Show in Context CrossRef Google Scholar
24. AlphaPilot – Lockheed Martin AI Drone Racing Innovation Challenge, February 2019, [online] Available: https://www.herox.com/alphapilot.
Show in Context Google Scholar
25. E. Tal and S. Karaman, "Accurate tracking of aggressive quadrotor trajectories using incremental nonlinear dynamic inversion and differential flatness", IEEE Conference on Decision and Control (CDC) , pp. 4282-4288, 2018.
Show in Context View Article Full Text: PDF (1885) Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
