Choset-79066 book February 23, 2005 13:40

Principles of
Robot Motion

Choset-79066 book February 23, 2005 13:40
Intelligent Robotics and Autonomous Agents Ronald C. Arkin, editor
Behavior-Based Robotics, Ronald C. Arkin, 1998 Robot Shaping: An Experiment in Behavior Engineering, Marco Dorigo and Marco Colombetti, 1998 Layered Learning in Multiagent Systems: A Winning Approach to Robotic Soccer, Peter Stone, 2000 Evolutionary Robotics: The Biology, Intelligence, and Technology of Self-Organizing Machines, Stefano Nolfi and Dario Floreano, 2000 Reasoning about Rational Agents, Michael Wooldridge, 2000 Introduction to AI Robotics, Robin R. Murphy, 2000 Strategic Negotiation in Multiagent Environments, Sarit Kraus, 2001 Mechanics of Robotic Manipulation, Matthew T. Mason, 2001 Designing Sociable Robots, Cynthia L. Breazeal, 2001 Introduction to Autonomous Mobile Robots, Roland Siegwart and Illah R. Nourbakhsh, 2004 Principles of Robot Motion: Theory, Algorithms, and Implementations, Howie Choset, Kevin Lynch, Seth Hutchinson, George Kantor, Wolfram Burgard, Lydia Kavraki and Sebastian Thrun, 2005

Choset-79066 book February 23, 2005 13:40

Principles of
Robot Motion
Theory, Algorithms, and Implementation

Howie Choset, Kevin Lynch, Seth Hutchinson, George Kantor, Wolfram Burgard, Lydia Kavraki,
and Sebastian Thrun
A Bradford Book The MIT Press
Cambridge, Massachusetts London, England

Choset-79066 book February 23, 2005 13:40

©2005 Massachusetts Institute of Technology All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.
MIT Press books may be purchased at special quantity discounts for business or sales promotional use. For information, please email special sales@mitpress.mit.edu or write to Special Sales Department, The MIT Press, 5 Cambridge Center, Cambridge, MA 02142.
This book was set in LATEX2e by Interactive Composition Corporation and was printed and bound in the United States of America.

Library of Congress Cataloging-in-Publication Data

Principles of robot motion : theory, algorithms, and implementation/Howie Choset [et al.].

p. cm. (Intelligent robotics and autonomous agents)

“A Bradford book.”

Includes bibliographical references and index.

ISN 0-262-03327-5 (alk. paper)

1. Robots—Motion. I. Choset, Howie M. II. Series.

TJ211.4.P75 2004

629.8 92—dc22

2004044906

10 9 8 7 6 5 4 3 2 1

Choset-79066 book February 23, 2005 13:40
To our families

Choset-79066 book February 23, 2005 13:40

Contents

Foreword xv Preface xvii Acknowledgments xxi

1 Introduction 1 1.1 Overview of Concepts in Motion Planning 9 1.2 Overview of the Book 12 1.3 Mathematical Style 13

2 Bug Algorithms 17

2.1 Bug1 and Bug2 17

2.2 Tangent Bug 23

2.3 Implementation 30

2.3.1 2.3.2
2.3.3

What Information: The Tangent Line 31 How to Infer Information with Sensors: Distance and Gradient 32 How to Process Sensor Information: Continuation Methods 35

3 Conﬁguration Space 39 3.1 Specifying a Robot’s Conﬁguration 40 3.2 Obstacles and the Conﬁguration Space 43 3.2.1 Circular Mobile Robot 43 3.2.2 Two-Joint Planar Arm 45 3.3 The Dimension of the Conﬁguration Space 47

Choset-79066 book February 23, 2005 13:40

viii

Contents

3.4 The Topology of the Conﬁguration Space 50

3.4.1 3.4.2 3.4.3 3.4.4

Homeomorphisms and Diffeomorphisms 51 Differentiable Manifolds 55 Connectedness and Compactness 58 Not All Conﬁguration Spaces Are Manifolds 59

3.5 Embeddings of Manifolds in Rn 59

3.5.1 Matrix Representations of Rigid-Body Conﬁguration 60

3.6 Parameterizations of S O(3) 66

3.7 Example Conﬁguration Spaces 68

3.8 Transforming Conﬁguration and Velocity Representations 69

4 Potential Functions 77

4.1 Additive Attractive/Repulsive Potential 80

4.2 Gradient Descent 84

4.3 Computing Distance for Implementation in the Plane 85

4.3.1 4.3.2

Mobile Robot Implementation 86 Brushﬁre Algorithm: A Method to Compute Distance on a Grid 86

4.4 Local Minima Problem 89

4.5 Wave-Front Planner 90

4.6 Navigation Potential Functions 93

4.6.1 Sphere-Space 93 4.6.2 Star-Space 96

4.7 Potential Functions in Non-Euclidean Spaces 99

4.7.1
4.7.2 4.7.3

Relationship between Forces in the Workspace and Conﬁguration Space 100 Potential Functions for Rigid-Body Robots 101 Path Planning for Articulated Bodies 104

5 Roadmaps 107 5.1 Visibility Maps: The Visibility Graph 110 5.1.1 Visibility Graph Deﬁnition 110 5.1.2 Visibility Graph Construction 113 5.2 Deformation Retracts: Generalized Voronoi Diagram 117 5.2.1 GVD Deﬁnition 118

Choset-79066 book February 23, 2005 13:40

Contents

ix

5.2.2 5.2.3 5.2.4
5.2.5

GVD Roadmap Properties 119 Deformation Retract Deﬁnition 121 GVD Dimension: The Preimage Theorem and Critical Points 123 Construction of the GVD 126

5.3 Retract-like Structures: The Generalized Voronoi Graph 129

5.3.1 5.3.2 5.3.3

GVG Dimension: Transversality 130 Retract-like Structure Connectivity 133 Lyapunov Control: Sensor-Based Construction of the HGVG 136

5.4 Piecewise Retracts: The Rod-Hierarchical Generalized Voronoi Graph 138

5.5 Silhouette Methods 141

5.5.1 Canny’s Roadmap Algorithm 142 5.5.2 Opportunistic Path Planner 151

6 Cell Decompositions 161

6.1 Trapezoidal Decomposition 162

6.2 Morse Cell Decompositions 168

6.2.1 6.2.2 6.2.3
6.2.4 6.2.5

Boustrophedon Decomposition 169 Morse Decomposition Deﬁnition 170 Examples of Morse Decomposition: Variable Slice 172 Sensor-Based Coverage 178 Complexity of Coverage 182

6.3 Visibility-Based Decompositions for Pursuit/Evasion 187

7 Sampling-Based Algorithms 197
7.1 Probabilistic Roadmaps 202 7.1.1 Basic PRM 203 7.1.2 A Practical Implementation of Basic PRM 208 7.1.3 PRM Sampling Strategies 216 7.1.4 PRM Connection Strategies 225
7.2 Single-Query Sampling-Based Planners 227 7.2.1 Expansive-Spaces Trees 230

Choset-79066 book February 23, 2005 13:40

x

Contents

7.2.2 Rapidly-Exploring Random Trees 233 7.2.3 Connection Strategies and the SBL Planner 238

7.3 Integration of Planners: Sampling-Based Roadmap of Trees 238

7.4 Analysis of PRM 242

7.4.1 7.4.2 7.4.3

PRM Operating in Rd 243 ( , α, β)-Expansiveness 246 Abstract Path Tiling 250

7.5 Beyond Basic Path Planning 253

7.5.1 7.5.2 7.5.3 7.5.4 7.5.5 7.5.6

Control-Based Planning 253 Multiple Robots 254 Manipulation Planning 257 Assembly Planning 259 Flexible Objects 260 Biological Applications 262

8 Kalman Filtering 269

8.1 Probabilistic Estimation 270

8.2 Linear Kalman Filtering 272

8.2.1 8.2.2 8.2.3 8.2.4 8.2.5 8.2.6 8.2.7

Overview 273 A Simple Observer 274 Observing with Probability Distributions 277 The Kalman Filter 282 Kalman Filter Summary 284 Example: Kalman Filter for Dead Reckoning 285 Observability in Linear Systems 287

8.3 Extended Kalman Filter 289

8.3.1 8.3.2 8.3.3

EKF for Range and Bearing Localization 290 Data Association 292 EKF for Range-Only Localization 294

8.4 Kalman Filter for SLAM 294

8.4.1 Simple SLAM 294 8.4.2 Range and Bearing SLAM 296

9 Bayesian Methods 301 9.1 Localization 301 9.1.1 The Basic Idea of Probabilistic Localization 302

Choset-79066 book February 23, 2005 13:40

Contents

xi

9.1.2
9.1.3 9.1.4 9.1.5

Probabilistic Localization as Recursive Bayesian Filtering 304 Derivation of Probabilistic Localization 308 Representations of the Posterior 310 Sensor Models 322

9.2 Mapping 328

9.2.1 9.2.2

Mapping with Known Locations of the Robot 328 Bayesian Simultaneous Localization and Mapping 337

10 Robot Dynamics 349 10.1 Lagrangian Dynamics 349 10.2 Standard Forms for Dynamics 353 10.3 Velocity Constraints 357 10.4 Dynamics of a Rigid Body 361 10.4.1 Planar Rotation 362 10.4.2 Spatial Rotation 363

11 Trajectory Planning 373
11.1 Preliminaries 374 11.2 Decoupled Trajectory Planning 374
11.2.1 Zero Inertia Points 378 11.2.2 Global Time-Optimal Trajectory Planning 384 11.3 Direct Trajectory Planning 384 11.3.1 Optimal Control 385 11.3.2 Nonlinear Optimization 389 11.3.3 Grid-Based Search 392

12 Nonholonomic and Underactuated Systems 401
12.1 Preliminaries 402 12.1.1 Tangent Spaces and Vector Fields 405 12.1.2 Distributions and Constraints 407 12.1.3 Lie Brackets 409
12.2 Control Systems 414

Choset-79066 book February 23, 2005 13:40

xii

Contents

12.3 Controllability 416

12.3.1 Local Accessibility and Controllability 419 12.3.2 Global Controllability 422

12.4 Simple Mechanical Control Systems 424

12.4.1 12.4.2 12.4.3

Simpliﬁed Controllability Tests 425 Kinematic Reductions for Motion Planning 434 Simple Mechanical Systems with Nonholonomic Constraints 438

12.5 Motion Planning 440

12.5.1 Optimal Control 440 12.5.2 Steering Chained-Form Systems Using
Sinusoids 444 12.5.3 Nonlinear Optimization 445 12.5.4 Gradient Methods for Driftless Systems 446 12.5.5 Differentially Flat Systems 447 12.5.6 Cars and Cars Pulling Trailers 450 12.5.7 Kinematic Reductions of Mechanical Systems 462 12.5.8 Other Approaches 465

A Mathematical Notation 473

B Basic Set Deﬁnitions 475

C Topology and Metric Spaces 478 C.1 Topology 478 C.2 Metric Spaces 479 C.3 Normed and Inner Product Spaces 480 C.4 Continuous Functions 481 C.5 Jacobians and Gradients 483

D Curve Tracing 487 D.1 Implicit Function Theorem 487 D.2 Newton-Raphson Convergence Theorem 488

E Representations of Orientation 489 E.1 Euler Angles 489 E.2 Roll, Pitch, and Yaw Angles 491

Choset-79066 book February 23, 2005 13:40

Contents

xiii

E.3 Axis-Angle Parameterization 492 E.4 Quaternions 494

F Polyhedral Robots in Polyhedral Worlds 499
F.1 Representing Polygons in Two Dimensions 499 F.2 Intersection Tests for Polygons 502 F.3 Conﬁguration Space Obstacles in Q = R2:
The Star Algorithm 507 F.4 Conﬁguration Space Obstacles in Q = S E(2) 508 F.5 Computing Distances between Polytopes in R2 and R3 509

G Analysis of Algorithms and Complexity Classes 513 G.1 Running Time 513 G.2 Complexity Theory 515 G.3 Completeness 520

H Graph Representation and Basic Search 521
H.1 Graphs 521 H.2 A∗ Algorithm 527
H.2.1 Basic Notation and Assumptions 530 H.2.2 Discussion: Completeness, Efﬁciency,
and Optimality 531 H.2.3 Greedy-Search and Dijkstra’s Algorithm 532 H.2.4 Example of A∗ on a Grid 533 H.2.5 Nonoptimistic Example 535 H.3 D∗ Algorithm 536
H.4 Optimal Plans 546

I Statistics Primer 547 I.1 Distributions and Densities 548 I.2 Expected Values and Covariances I.3 Multivariate Gaussian Distributions

550 551

J Linear Systems and Control 552 J.1 State Space Representation 552 J.2 Stability 554

Choset-79066 book February 23, 2005 13:40

xiv

Contents

J.3 LTI Control Systems 557 J.4 Observing LTI Systems 559 J.5 Discrete Time Systems 562
J.5.1 Stability 562 J.5.2 Controllability and Observability 563
Bibliography 565
Index 597

Choset-79066 book February 23, 2005 13:40
Foreword
THIS IMPRESSIVE book is the result of a serious undertaking of distinguished motion planning researchers led by Howie Choset. Over the years, motion planning has become a major research theme in robotics. The goal is to enable robots to automatically compute their motions from high-level descriptions of tasks and models acquired through sensing. This goal has recently become even more crucial. On the one hand, robotics has expanded from a largely dominant focus on industrial manufacturing into areas where tasks are less repetitive and environments less structured, for instance, medical surgery, ocean and space exploration, assistance to the elderly, and search-and-rescue. In these areas, it is impractical to explicitly program the robots for each new goal. On the other hand, the need for automatic motion-planning capabilities has expanded outside the realm of robotics, into domains such as computer animation (e.g., to generate motions of avatars), computer-aided design (e.g., to test that a product can be assembled or serviced), veriﬁcation of building codes (e.g., to check access of key facilities to the disabled), exploration of virtual environments (to help the user navigate in geometric models made of tens of millions of triangles), or even computational biology (to help analyze important molecular motions, like folding and binding). Today, progress in motion planning is increasingly motivated by these new applications.
By confronting novel and difﬁcult problems, researchers have made considerable progress in recent years. Not only have faster and more robust algorithms been developed and tested, but the range of motion-planning problems has continuously expanded. In the ’80s and part of the ’90s, ﬁnding collision-free paths was the main or only goal. Today, while obstacle avoidance remains a key issue, other important constraints are considered as well, for instance, visibility, coverage, kinodynamic, optimality, equilibrium, and uncertainty constraints. These constraints make problems more interesting and lead to more useful algorithms. In addition, while research in motion planning used to be neatly divided between theory and practice, this distinction has now largely disappeared. Most recent contributions to the ﬁeld combine effective

Choset-79066 book February 23, 2005 13:40

xvi

Foreword

algorithms tested on signiﬁcant problems, along with some formal guarantees of performance.
Although journal and conference papers in motion planning have proliferated, there has not been any comprehensive reference text in more than a decade. This book ﬁlls this gap in outstanding fashion. It covers both the early foundations of the ﬁeld and the recent theoretical and practical progress that has been made. It beautifully demonstrates how the enduring contributions of early researchers in the ﬁeld, like Lozano-Perez (conﬁguration space) and Reif (complexity of motion planning), have led to a rich and vibrant research area, with ramiﬁcations that were unsuspected only a decade ago.
I am usually suspicious of books in which chapters have been written by different authors. But, to my good surprise, this book is more than a standard textbook. The fact that seven authors collaborated on this book attests to the diversity of the research going on in motion planning and the excitement associated with each research topic. Simultaneously, the authors have done excellent work in providing a uniﬁed presentation of the core concepts and methodologies, and thus the book can be used as a textbook. This book will serve well the growing community of students, researchers, and engineers interested in motion planning.
Jean-Claude Latombe Stanford, California

Choset-79066 book February 23, 2005 13:40
Preface
PEOPLE HAVE always dreamed of building intelligent machines to perform tasks. Today, these machines are called robots, derived from the Czech word robota meaning servitude or drudgery. Robots have inspired the imagination of many, appearing in mythology, literature, and popular movies. Some popular robotic characters include Robby the Robot, R2D2 and C3P0, Golem, Pushpack, Wanky and Fanny, Gundam and Lt. Cmdr. Data. Just like their literary counterparts, robots can take on many forms and constructing them involves addressing many challenges in engineering, computer science, cognitive science, language, and so on. Regardless of the form of the robot or the task it must perform, robots must maneuver through our world. This book is about automatic planning of robot motions. However, the approaches developed in this book are not limited to robots: recently, they have been used for “designing” pharmaceutical drugs, planning routes on circuit boards, and directing digital actors in the graphics world.
The robot motion ﬁeld and its applications have become incredibly broad, and this is why the book has seven co-authors. This type of book requires a broad spectrum of expertise. However, it should be stressed that this is indeed a textbook and not a collection of independent chapters put together in an edited volume. Each author participated in writing each of the chapters and all of the chapters are integrated with each other.
This book is aimed at the advanced undergraduate or new graduate student interested in robot motion, and it may be read in a variety of ways. Our goal in writing in this book is threefold: to create an updated textbook and reference for robot motion, to make the fundamental mathematics behind robot motion accessible to the novice, and to stress implementation relating low-level details to high-level algorithmic concepts.
Since the robot motion ﬁeld is indeed broad, this book cannot cover all the topics, nor do we believe that any book can contain exhaustive coverage on robot motion. We do, however, point the reader to Jean-Claude Latombe’s Robot Motion Planning [262].

Choset-79066 book February 23, 2005 13:40

xviii

Preface

Latombe’s book was one of the ﬁrst text and reference books aimed at the motionplanning community and it certainly was a guide for us when writing this book. In the decade since Latombe’s book was published, there have been great advances in the motion-planning ﬁeld, particularly in probabilistic methods, mechanical systems, and sensor-based planning, so we intended to create a text with these new advances. However, there are many topics not included in our text that are included in his, including assembly planning, geometric methods in dealing with uncertainty, multiple moving obstacles, approximate cell decompositions, and obstacle representations.
We also believe that concepts from control theory and statistical reasoning have gained greater relevance to robot motion. Therefore, we have included an appendix brieﬂy reviewing linear control systems which serves as background for our presentation on Kalman ﬁltering. Our description of Kalman ﬁltering differs from others in that it relies on a rich geometric foundation. We present a comprehensive description of Bayesian-based approaches. Concepts from mechanics and dynamics have also had great impact on robot motion. We have included a chapter on dynamics which serves as a basis for our description of trajectory planning and planning for underactuated robots.
This book can be read from cover to cover. In doing so, there are four logical components to the book: geometric motion planning approaches (chapters 2 through 6), probabilistic methods (chapters 7, 8, and 9), mechanical control systems (chapters 10, 11, and 12), and the appendices. Covering the entire book could require a full year course. However, not all of the topics in this book need be covered for a course on robot motion. For semester-long courses, the following themes are suggested:

Theme Path Planning Mobile Robotics Mechanical Control Systems Position Estimation

Chapter and Appendix Sequence 3, 4, G, 5, 7, and 6 2, H, 3, 4, 5, D, and 6 3, 10, 11, and 12 I, J, 8, and 9

The algorithms and approaches presented in this book are based on geometry and thus rest on a solid mathematical basis. Beyond anything superﬁcial, in order to understand the many motion-planning algorithms, one must understand these mathematical underpinnings. One of the goals of this book is to make mathematical concepts more accessible to students of computer science and engineering. In this book, we introduce the intuition behind new mathematical concepts on an “as needed” basis to understand both how and why certain motion planning algorithms work. Some salient

Choset-79066 book February 23, 2005 13:40

Preface

xix

concepts are formally deﬁned in each chapter and the appendices contain overviews of some basic topics in more detail. The idea here is that the reader can develop an understanding of motion planning algorithms without getting bogged down by mathematical details, but can turn to them in the appendices when necessary. It is our hope that the reader will gain enough new knowledge in algebra, graph theory, geometry, topology, probability, ﬁltering, and so on, to be able to read the state of the art literature in robot motion.
We discuss implementation issues and it is important to note that such issues are not mere details, but pose deep theoretical problems as well. In chapters 2, 4, 5, and 6, we discuss speciﬁc issues on how to integrate range sensor information into a planner. The Kalman Filtering (chapter 8) and Bayesian-based (chapter 9) approaches have been widely used in the robot motion ﬁeld to deal with positioning and sensor uncertainty. Finally, we discuss in chapters 11 and 12 issues involving kinematic and dynamic contraints that real robots experience.
We have also included pseudocode for many of the algorithms presented throughout the book. In appendix H, we have included a discussion of graph search with detailed examples to enable the novice to implement some standard graph search approaches, with applicability well beyond robot motion. Finally, at the end of each chapter, we present problems that stress implementation.

Choset-79066 book February 23, 2005 13:40
Acknowledgments
WE FIRST and foremost want to thank our students, who were incredibly supportive of us when writing this book. We would like to thank the members of the Biorobotics/ Sensor Based Planning Lab at Carnegie Mellon, especially Ji Yeong Lee; the Laboratory for Intelligent Mechanical Systems at Northwestern; the robotics group in the Beckman Institute at the University of Illinois, Urbana Champaign; the Physical and Biological Computing Group at Rice, especially Andrew Ladd and Erion Plaku; the Lab for Autonomous Intelligent Systems at the University of Freiburg, especially Dirk Ha¨hnel and Cyrill Stachniss; and the Stanford and Carnegie Mellon Learning Labs, for their contributions and efforts for this book.
We thank Alfred Anthony Rizzi and Matt Mason for their inspiration and support, and Ken Goldberg and Jean-Claude Latombe for their input and advice. For input in the form of ﬁgures or feedback on drafts, we thank Ercan Acar, Srinivas Akella, Nancy Amato, Serkan Apaydin, Prasad Atkar, Denise Bafman, Devin Balkcom, Francesco Bullo, Joel Burdick, Prasun Choudhury, Cynthia Cobb, Dave Conner, Al Costa, Frank Dellaert, Bruce Donald, Dieter Fox, Bob Grabowski, Aaron Greenﬁeld, David Hsu, Pekka Isto, James Kuffner, Christian Laugier, Jean-Paul Laumond, Steve LaValle, Brad Lisien, Julie Nord, Jim Ostrowski, Nancy Pollard, Cedric Pradalier, Ionnis Rekleitis, Elie Shammas, Thierry Simeon, Sarjun Skaff, and M. Dick Tsuyuki.
We are also indebted to the many students who helped debug this text for us. Portions of this text were used in Carnegie Mellon’s Sensor Based Motion Planning Course, Carnegie Mellon’s Mechanical Control Systems reading group, Northwestern’s ME 450 Geometry in Robotics, University of Illinois’ ECE 450 Advanced Robotic Planning, and University of Freiburg’s Autonomous Systems course.

Choset-79066 book February 22, 2005 17:31
1 Introduction
SOME OF the most signiﬁcant challenges confronting autonomous robotics lie in the area of automatic motion planning. The goal is to be able to specify a task in a highlevel language and have the robot automatically compile this speciﬁcation into a set of low-level motion primitives, or feedback controllers, to accomplish the task. The prototypical task is to ﬁnd a path for a robot, whether it is a robot arm, a mobile robot, or a magically free-ﬂying piano, from one conﬁguration to another while avoiding obstacles. From this early piano mover’s problem, motion planning has evolved to address a huge number of variations on the problem, allowing applications in areas such as animation of digital characters, surgical planning, automatic veriﬁcation of factory layouts, mapping of unexplored environments, navigation of changing environments, assembly sequencing, and drug design. New applications bring new considerations that must be addressed in the design of motion planning algorithms.
Since actions in the physical world are subject to physical laws, uncertainty, and geometric constraints, the design and analysis of motion planning algorithms raises a unique combination of questions in mechanics, control theory, computational and differential geometry, and computer science. The impact of automatic motion planning, therefore, goes beyond its obvious utility in applications. The possibility of building computer-controlled mechanical systems that can sense, plan their own motions, and execute them has contributed to the development of our math and science base by asking fundamental theoretical questions that otherwise might never have been posed.
This book addresses the theory and practice of robot motion planning, with an eye toward applications. To focus the discussion, and to point out some of the important concepts in motion planning, let’s ﬁrst look at a few motivating examples.

Choset-79066 book February 22, 2005 17:31

2

1 Introduction

Piano Mover’s Problem
The classic path planning problem is the piano mover’s problem [373]. Given a threedimensional rigid body, for example a polyhedron, and a known set of obstacles, the problem is to ﬁnd a collision-free path for the omnidirectional free-ﬂying body from a start conﬁguration to a goal conﬁguration. The obstacles are assumed to be stationary and perfectly known, and execution of the planned path is exact. This is called ofﬂine planning, because planning is ﬁnished in advance of execution. Variations on this problem are the sofa mover’s problem, where the body moves in a plane among planar obstacles, and the generalized mover’s problem, where the robot may consist of a set of rigid bodies linked at joints, e.g., a robot arm.
The key problem is to make sure no point on the robot hits an obstacle, so we need a way to represent the location of all the points on the robot. This representation is the conﬁguration of the robot, and the conﬁguration space is the space of all conﬁgurations the robot can achieve. An example of a conﬁguration is the set of joint angles for a robot arm or the one orientation and two position variables for a sofa in the plane. The conﬁguration space is generally non-Euclidean, meaning that it does not look like an n-dimensional Euclidean space Rn. The dimension of the conﬁguration space is equal to the number of independent variables in the representation of the conﬁguration, also known as the degrees of freedom (DOF). The piano has six degrees of freedom: three to represent the position (x-y-z) and three to represent the orientation (roll-pitch-yaw). The problem is to ﬁnd a curve in the conﬁguration space that connects the start and goal points and avoids all conﬁguration space obstacles that arise due to obstacles in the space.

The Mini AERCam
NASA’s Johnson Space Center is developing the Mini AERCam, or Autonomous Extravehicular Robotic Camera, for visual inspection tasks in space (ﬁgure 1.1). It is a free-ﬂying robot equipped with twelve cold gas thrusters, allowing it to generate a force and torque in any direction. When operating in autonomous mode, it must be able to navigate in a potentially complex three-dimensional environment. In this respect the problem is similar to the piano mover’s problem. Since we have to apply thrusts to cause motion, however, we need to plan not only the path the robot is to follow, but also the speed along the path. This is called a trajectory, and the thruster inputs are determined by the dynamics of the robot. In the piano mover’s problem, we only worried about geometric or kinematic issues.

Choset-79066 book February 22, 2005 17:31

1 Introduction

3

Figure 1.1 NASA’s Mini AERCam free-ﬂying video inspection robot.

(a)

(b)

Figure 1.2 (a) The CyCab. (b) The Segway Human Transporter.

Personal Transport Vehicles
Small personal transport vehicles may become a primary means of transportation in pedestrian-ﬁlled urban environments where the size, speed, noise, and pollution of automobiles is undesirable. One concept is the CyCab [355], a small vehicle designed by a consortium of institutions in France to transport up to two people at speeds up to 30 km/h (ﬁgure 1.2a). Another concept is the Segway HT, designed to carry a single rider at speeds up to 20 km/h (ﬁgure 1.2b).

Choset-79066 book February 22, 2005 17:31

4

1 Introduction

To simplify control of vehicles in crowded environments, one capability under study is automatic parallel parking. The rider would initiate the parallel-parking procedure, and the onboard computer would take over from there. Such systems will soon be commercially available in automobiles. On the surface, this problem sounds like the sofa mover’s problem, since both involve a body moving in the plane among obstacles. The difference is that cars and the vehicles in ﬁgure 1.2 cannot instantaneously slide sideways like the sofa. The velocity constraint preventing instantaneous sideways motion is called a nonholonomic constraint, and the motion planner must take this constraint into account. Systems without velocity constraints, such as the sofa, are omnidirectional in the conﬁguration space.

Museum Tour Guides
In 1997, a mobile robot named RHINO served as a fully autonomous tour-guide at the Deutsches Museum Bonn (ﬁgure 1.3). RHINO was able to lead museum visitors from one exhibit to the next by calculating a path using a stored map of the museum. Because the perfect execution model of the piano mover’s problem is unrealistic in this setting, RHINO had to be able to localize itself by comparing its sensor readings to its stored

Figure 1.3 RHINO, the interactive mobile tour-guide robot.

Choset-79066 book February 22, 2005 17:31

1 Introduction

5

Figure 1.4 The Mars rover Sojourner. http://mars.jpl.nasa.gov/MPF/rover/sojourner.html.
map. To deal with uncertainty and changes in the environment, RHINO employed a sensor-based planning approach, interleaving sensing, planning, and action.
Planetary Exploration
One of the most exciting successes in robot deployment was a mobile robot, called Sojourner (ﬁgure 1.4), which landed on Mars on July 4, 1997. Sojourner provided upclose images of Martian terrain surrounding the lander. Sojourner did not move very far from the lander and was able to rely on motion plans generated ofﬂine on Earth and uploaded. Sojourner was followed by its fantastically successful cousins, Spirit and Opportunity, rovers that landed on Mars in January 2004 and have provided a treasure trove of scientiﬁc data. In the future, robots will explore larger areas and thus will require signiﬁcantly more autonomy. Beyond navigation capability, such robots will have to be able to generate a map of the environment using sensor information. Mapping an unknown space with a robot that experiences positioning error is an especially challenging “chicken and egg” problem—without a map the robot cannot determine its own position, and without knowledge about its own position the robot cannot compute the map. This problem is often called simultaneous localization and mapping or simply SLAM.
Demining
Mine ﬁelds stiﬂe economic development and result in tragic injuries and deaths each year. As recently as 1994, 2.5 million mines were placed worldwide while only 100,000 were removed.

Choset-79066 book February 22, 2005 17:31

6

1 Introduction

Robots can play a key role in quickly and safely demining an area. The crucial ﬁrst step is ﬁnding the mines. In demining, a robot must pass a mine-detecting sensor over all points in the region that might conceal a mine. To do this, the robot must traverse a carefully planned path through the target region. The robot requires a coverage path planner to ﬁnd a motion that passes the sensor over every point in the ﬁeld. If the planner is guaranteed to ﬁnd a path that covers every point in the ﬁeld when such a path exists, then we call the planner complete. Completeness is obviously a crucial requirement for this task.
Coverage has other applications including ﬂoor cleaning [116], lawn mowing [198], unexploded ordnance hunting [260], and harvesting [341]. In all of these applications, the robot must simultaneously localize itself to ensure complete coverage.

Fixed-base Robot Arms in Industry
In highly structured spaces, ﬁxed-base robot arms perform a variety of tasks, including assembly, welding, and painting. In painting, for example, the robot must deposit a uniform coating over all points on a target surface (ﬁgure 1.5). This coverage problem presents new challenges because (1) ensuring equal paint deposition is a more severe requirement than mere coverage, (2) the surface is not usually ﬂat, and (3) the robot must properly coordinate its internal degrees of freedom to drive the paint atomizer over the surface.
Industrial robot installations are clearly driven by economic factors, so there is a high priority on minimizing task execution time. This motivates motion planners that return time-optimal motion plans. Other kinds of tasks may beneﬁt from other kinds of optimality, such as energy- or fuel-optimality for mobile robots.

Figure 1.5 ABB painting robot named Tobe.

Choset-79066 book February 22, 2005 17:31

1 Introduction

7

Figure 1.6 The Hirose active cord.

Figure 1.7 The Carnegie Mellon snake robot Holt mounted on a mobile base.
Snake Robots for Urban Search and Rescue
When a robot has more degrees of freedom than required to complete its task, the robot is called redundant. When a robot has many extra degrees of freedom, then it is called hyper-redundant. These robots have multidimensional non-Euclidean conﬁguration spaces. Hyper-redundant serial mechanisms look like elephant trunks or snakes (ﬁgures 1.6 and 1.7), and they can use their extra degrees of freedom to thread through tightly packed volumes to reach locations inaccessible to humans and conventional machines. These robots may be particularly well-suited to urban search and rescue, where it is of paramount importance to locate survivors in densely packed rubble as quickly and safely as possible.
Robots in Surgery
Robots are increasingly used in surgery applications. In noninvasive stereotactic radiosurgery, high-energy radiation beams are cross-ﬁred at brain tumors. In certain cases,

Choset-79066 book February 22, 2005 17:31
8

1 Introduction

Figure 1.8 The motions of a digital actor are computed automatically. (Courtesy of J.C. Latombe)
these beams are delivered with high accuracy, using six degrees of freedom robotic arms (e.g., the CyberKnife system [1] ). Robots are also used in invasive procedures. They often enhance the surgeon’s ability to perform technically precise maneuvers. For example, the da Vinci Surgical System [2] can assist in advanced surgical techniques such as cutting and suturing. The ZEUS System [3] can assist in the control of blunt retractors, graspers, and stabilizers. Clearly, as robotics advances, more and more of these systems will be developed to improve our healthcare.
Digital Actors
Algorithms developed for motion planning or sensor interpretation are not just for robots anymore. In the entertainment industry, motion planning has found a wide variety of applications in the generation of motion for digital actors, opening the way to exciting scenarios in video games, animation, and virtual environments (ﬁgure 1.8).
Drug Design
An important problem in drug design and the study of disease is understanding how a protein folds to its native or most stable conﬁguration. By considering the protein as an articulated linkage (ﬁgure 1.9), researchers are using motion planning to identify likely folding pathways from a long straight chain to a tightly folded conﬁguration. In pharmaceutical drug design, proteins are combined with smaller molecules to form complexes that are vital for the prevention and cure of disease. Motion planning

Choset-79066 book March 15, 2005 10:36

1.1 Overview of Concepts in Motion Planning

9

Figure 1.9 A molecule represented as an articulated linkage.
methods are used to analyze molecular binding motions, allowing the automated testing of drugs before they are synthesized in the laboratory.
1.1 Overview of Concepts in Motion Planning
The previous examples touched on a number of ways to characterize the motion planning problem and the algorithm used to address it. Here we summarize some of the important concepts. Our characterization of a motion planner is according to the task it addresses, properties of the robot solving the task, and properties of the algorithm.1 We focus on topics that are covered in this book (table 1.1).
Task
The most important characterization of a motion planner is according to the problem it solves. This book considers four tasks: navigation, coverage, localization, and mapping. Navigation is the problem of ﬁnding a collision-free motion for the robot system from one conﬁguration (or state) to another. The robot could be a robot arm, a mobile robot, or something else. Coverage is the problem of passing a sensor or tool over all points in a space, such as in demining or painting. Localization is the problem of using a map to interpret sensor data to determine the conﬁguration of the robot. Mapping is the problem of exploring and sensing an unknown environment
1. This classiﬁcation into three categories is somewhat arbitrary but will be convenient for introduction.

Choset-79066 book February 22, 2005 17:31

10

1 Introduction

Task

Robot

Algorithm

Navigate Conﬁguration space, degree of freedom Optimal/nonoptimal motions

Map

Kinematic/dynamic

Computational complexity

Cover Omnidirectional or Localize motion constraints

Completeness (resolution, probabilistic)
Online/ofﬂine

Sensor-based/world model

Table 1.1 Some of the concepts covered in this book.

to construct a representation that is useful for navigation, coverage, or localization. Localization and mapping can be combined, as in SLAM.
There are a number of interesting motion planning tasks not covered in detail in this book, such as navigation among moving obstacles, manipulation and grasp planning, assembly planning, and coordination of multiple robots. Nonetheless, algorithms in this book can be adapted to those problems.

Properties of the Robot
The form of an effective motion planner depends heavily on properties of the robot solving the task. For example, the robot and the environment determine the number of degrees of freedom of the system and the shape of the conﬁguration space. Once we understand the robot’s conﬁguration space, we can ask if the robot is free to move instantaneously in any direction in its conﬁguration space (in the absence of obstacles). If so, we call the robot omnidirectional. If the robot is subject to velocity constraints, such as a car that cannot translate sideways, both the constraint and the robot are called nonholonomic. Finally, the robot could be modeled using kinematic equations, with velocities as controls, or using dynamic equations of motion, with forces as controls.

Properties of the Algorithm
Once the task and the robot system is deﬁned, we can choose between algorithms based on how they solve the problem. For example, does the planner ﬁnd motions that are optimal in some way, such as in length, execution time, or energy consumption? Or does it simply ﬁnd a solution satisfying the constraints? In addition to the quality of the output of the planner, we can ask questions about the computational complexity of

Choset-79066 book February 22, 2005 17:31

1.1 Overview of Concepts in Motion Planning

11

the planner. Are the memory requirements and running time of the algorithm constant, polynomial, or exponential in the “size” of the problem description? The size of the input could be the number of degrees of freedom of the robot system, the amount of memory needed to describe the robot and the obstacles in the environment, etc., and the complexity can be deﬁned in terms of the worst case or the average case. If we expect to scale up the size of the inputs, a planner is often only considered practical if it runs in time polynomial or better in the inputs. When a polynomial time algorithm has been found for a problem that previously could only be solved in exponential time, some key insight into the problem has typically been gained.
Some planners are complete, meaning that they will always ﬁnd a solution to the motion planning problem when one exists or indicate failure in ﬁnite time. This is a very powerful and desirable property. For the motion planning problem, as the number of degrees of freedom increases, complete solutions may become computationally intractable. Therefore, we can seek weaker forms of completeness. One such form is resolution completeness. It means that if a solution exists at a given resolution of discretization, the planner will ﬁnd it. Another weaker form of completeness is probabilistic completeness. It means that the probability of ﬁnding a solution (if one exists) converges to 1 as time goes to inﬁnity.
Optimality, completeness, and computational complexity naturally trade off with each other. We must be willing to accept increased computational complexity if we demand optimal motion plans or completeness from our planner.
We say a planner is ofﬂine if it constructs the plan in advance, based on a known model of the environment, and then hands the plan off to an executor. The planner is online if it incrementally constructs the plan while the robot is executing. In this case, the planner can be sensor-based, meaning that it interleaves sensing, computation, and action. The distinction between ofﬂine algorithms and online sensor-based algorithms can be somewhat murky; if an ofﬂine planner runs quickly enough, for example, then it can be used in a feedback loop to continually replan when new sensor data updates the environment model. The primary distinction is computation time, and practically speaking, algorithms are often designed and discussed with this distinction in mind. A similar issue arises in control theory when attempting to distinguish between feedforward control (commands based on a reference trajectory and dynamic model) and feedback control (commands based on error from the desired trajectory), as techniques like model predictive control essentially use fast feedforward control generation in a closed loop. In this book we will not discuss the low-level feedback controllers needed to actually implement robot motions, but we will assume they are available.

Choset-79066 book February 22, 2005 17:31

12

1 Introduction

1.2 Overview of the Book
Chapter 2 dives right into a class of simple and intuitive “Bug” algorithms requiring a minimum of mathematical background to implement and analyze. The task is to navigate a point mobile robot to a known goal location in a plane ﬁlled with unknown static obstacles. The Bug algorithms are sensor-based—the robot uses a contact sensor or a range sensor to determine when it is touching or approaching an obstacle, as well as odometry or other sensing to know its current position in the plane. It has two basic motion primitives, moving in a straight line and following a boundary, and it switches between these based on sensor data. These simple algorithms guarantee that the robot will arrive at the goal if it is reachable.
To move beyond simple point robots, in chapter 3 we study the conﬁguration space of more general robot systems, including rigid bodies and robot arms. The mathematical foundations in this chapter allow us to view general path planning problems as ﬁnding paths through conﬁguration space. We study the dimension (degrees of freedom), topology, and parameterizations of non-Euclidean conﬁguration spaces, as well as representations of these conﬁguration spaces as surfaces embedded in higherdimensional Euclidean spaces. The forward kinematic map is introduced to relate one choice of conﬁguration variables to another. The differential of this map, often called the Jacobian, is used to relate the velocites in the two coordinate systems. Material in this chapter is referenced throughout the remainder of the book.
Chapter 4 describes a class of navigation algorithms based on artiﬁcial potential functions. In this approach we set up a virtual potential ﬁeld in the conﬁguration space to make obstacles repulsive and the goal conﬁguration attractive to the robot. The robot then simply follows the downhill gradient of the artiﬁcial potential. For some navigation problems, it is possible to design the potential ﬁeld to ensure that following the gradient will always take the robot to the goal. If calculating such a potential ﬁeld is difﬁcult or impossible, we can instead use one that is easy to caclulate but may have the undesirable property of local minima, locations where the robot gets “stuck.” In this case, we can simply use the potential ﬁeld to guide a search-based planner. Potential ﬁelds can be generated ofﬂine, using a model of the environment, or calculated in real-time using current sensor readings. Purely reactive gradient-following potential ﬁeld approaches always run the risk of getting stuck in local minima, however.
In chapter 5, we introduce more concise representations of the robot’s free space that a planner can use to plan paths between two conﬁgurations. These structures are called roadmaps. A planner can also use a roadmap to explore an unknown space. By using sensors to incrementally construct the roadmap, the robot can then use the roadmap for future navigation problems. This chapter describes several roadmaps

Choset-79066 book February 22, 2005 17:31

1.3 Mathematical Style

13

including the visibility graph, the generalized Voronoi diagram, and Canny’s original roadmap. Chapter 6 describes an alternative representation of the free space called a cell decomposition which consists of a set of cells of the free space and a graph of cells with connections between adjacent cells. A cell decomposition is useful for coverage tasks, and it can be computed ofﬂine or incrementally using sensor data.
Constructing complete and exact roadmaps of an environment is generally quite computationally complex. Therefore, chapter 7 develops sampling-based algorithms that trade completeness guarantees for a reduction of the running time of the planner. This chapter highlights recent work in probabilistic roadmaps, expansive-spaces trees, and rapidly-exploring random trees and the broad range of motion planning problems to which they are applicable.
Probabilistic reasoning can also address the problems of sensor uncertainty and positioning error that plague mobile robot deployment. We can model these uncertainties and errors as probability distributions and use Kalman ﬁltering (chapter 8) and Bayesian estimation (chapter 9) to address localization, mapping, and SLAM problems.
Just as the description of conﬁguration space in chapter 3 provides many of the kinematic and geometric tools used in path planning, the description of second-order robot dynamics in chapter 10 is necessary for feasible trajectory planning, i.e., ﬁnding motions parameterized by time. We can then pose time- and energy-optimal trajectory planning problems for dynamic systems subject to actuator limits, as described in chapter 11.
Chapter 11 assumes that the robot has an actuator for each degree of freedom. In chapter 12 we remove that assumption and consider robot systems subject to nonholonomic (velocity) constraints and/or acceleration constraints due to missing actuators, or underactuation. We study the reachable states for such systems, i.e., controllability, using tools from differential geometry. The chapter ends by describing planners that ﬁnd motions for systems such as cars, cars pulling trailers, and spacecraft or robot arms with missing actuators.

1.3 Mathematical Style
Our goal is to present topics in an intuitive manner while helping the reader appreciate the deeper mathematical concepts. Often we suppress mathematical rigor, however, when intuition is sufﬁcient. In many places proofs of theorems are omitted, and the reader is referred to the original papers. For the most part, mathematical concepts are introduced as they are needed. Supplementary mathematical material is deferred to the appendices to allow the reader to focus on the main concepts of the chapter.

Choset-79066 book February 22, 2005 17:31
14 c (0)

1 Introduction
QO1 c (1)

Figure 1.10 A path is a curve in the free conﬁguration space Qfree connecting c(0) to c(1).
Throughout this book, robots are assumed to operate in a planar (R2) or threedimensional (R3) ambient space, sometimes called the workspace W. This workspace will often contain obstacles; let WOi be the ith obstacle. The free workspace is the set of points Wfree = W\ i WOi where the \ is a subtraction operator.
Motion planning, however, does not usually occur in the workspace. Instead, it occurs in the conﬁguration space Q (also called C-space), the set of all robot conﬁgurations. We will use the notation R(q) to denote the set of points of the ambient space occupied by the robot at conﬁguration q. An obstacle in the conﬁguration space corresponds to conﬁgurations of the robot that intersect an obstacle in the workspace, i.e., QOi = {q | R(q) WOi = ∅}. Now we can deﬁne the free conﬁguration space as Qfree = Q\ i QOi . We sometimes simply refer to “free space” when the meaning is unambiguous.
In this book we make a distinction between path planning and motion planning. A path is a continuous curve on the conﬁguration space. It is represented by a continuous function that maps some path parameter, usually taken to be in the unit interval [0, 1], to a curve in Qfree (ﬁgure 1.10). The choice of unit interval is arbitrary; any parameterization would sufﬁce. The solution to the path planning problem is a continuous function c ∈ C0 (see appendix C for a deﬁnition of continuous functions) such that
(1.1) c : [0, 1] → Q where c(0) = qstart, c(1) = qgoal and c(s) ∈ Qfree ∀s ∈ [0, 1].
When the path is parameterized by time t, then c(t) is a trajectory, and velocities and accelerations can be computed by taking the ﬁrst and second derivatives with

Choset-79066 book February 22, 2005 17:31

1.3 Mathematical Style

15

respect to time. This means that c should be at least twice-differentiable, i.e., in the class C2. Finding a feasible trajectory is called trajectory planning or motion planning.
In this book, conﬁguration, velocity, and force vectors will be written as column vectors when they are involved in any matrix algebra. For example, a conﬁguration q ∈ Rn will be written in coordinates as q = [q1, q2, . . . , qn]T . When the vector will not be used in any computation, we may simply refer to it as a tuple of coordinates, e.g., q = (q1, q2, . . . , qn), without bothering to make it a column vector.

Choset-79066 book February 22, 2005 17:34
2 Bug Algorithms
EVEN A simple planner can present interesting and difﬁcult issues. The Bug1 and Bug2 algorithms [301] are among the earliest and simplest sensor-based planners with provable guarantees. These algorithms assume the robot is a point operating in the plane with a contact sensor or a zero range sensor to detect obstacles. When the robot has a ﬁnite range (nonzero range) sensor, then the Tangent Bug algorithm [217] is a Bug derivative that can use that sensor information to ﬁnd shorter paths to the goal. The Bug and Bug-like algorithms are straightforward to implement; moreover, a simple analysis shows that their success is guaranteed, when possible. These algorithms require two behaviors: move on a straight line and follow a boundary. To handle boundary-following, we introduce a curve-tracing technique based on the implicit function theorem at the end of this chapter. This technique is general to following any path, but we focus on following a boundary at a ﬁxed distance.
2.1 Bug1 and Bug2
Perhaps the most straight forward path planning approach is to move toward the goal, unless an obstacle is encountered, in which case, circumnavigate the obstacle until motion toward the goal is once again allowable. Essentially, the Bug1 algorithm formalizes the “common sense” idea of moving toward the goal and going around obstacles. The robot is assumed to be a point with perfect positioning (no positioning error) with a contact sensor that can detect an obstacle boundary if the point robot “touches” it. The robot can also measure the distance d(x, y) between any two points x

Choset-79066 book February 22, 2005 17:34

18

2 Bug Algorithms

and y. Finally, assume that the workspace is bounded. Let Br (x) denote a ball of radius r centered on x, i.e., Br (x) = {y ∈ R2 | d(x, y) < r }. The fact that the workspace is bounded implies that for all x ∈ W, there exists an r < ∞ such that W ⊂ Br (x).
The start and goal are labeled qstart and qgoal, respectively. Let q0L = qstart and the m-line be the line segment that connects qiL to qgoal. Initially, i = 0. The Bug1 algorithm exhibits two behaviors: motion-to-goal and boundary-following. During
motion-to-goal, the robot moves along the m-line toward qgoal until it either encounters the goal or an obstacle. If the robot encounters an obstacle, let q1H be the point where the robot ﬁrst encounters an obstacle and call this point a hit point. The robot then circumnavigates the obstacle until it returns to q1H . Then, the robot determines the closest point to the goal on the perimeter of the obstacle and traverses to this point. This point is called a leave point and is labeled q1L . From q1L , the robot heads straight toward the goal again, i.e., it reinvokes the motion-to-goal behavior. If the line that connects q1L and the goal intersects the current obstacle, then there is no path to the goal; note that this intersection would occur immediately “after” leaving q1L . Otherwise, the index i is incremented and this procedure is then repeated for qiL and qiH until the goal is reached or the planner determines that the robot cannot reach the goal (ﬁgures 2.1, 2.2). Finally, if the line to the goal “grazes” an obstacle, the robot
need not invoke a boundary following behavior, but rather continues onward toward
the goal. See algorithm 1 for a description of the Bug1 approach.
qgoal

q1L q2H WO1

q2L WO2

q1H qstart Figure 2.1 The Bug1 algorithm successfully ﬁnds the goal.

Choset-79066 book February 22, 2005 17:34

2.1 Bug1 and Bug2

19

qgoal q1L

qstart

q1H

Figure 2.2 The Bug1 algorithm reports the goal is unreachable.

Algorithm 1 Bug1 Algorithm

Input: A point robot with a tactile sensor

Output: A path to the qgoal or a conclusion no such path exists

1: while Forever do

2: repeat

3:

From qiL−1, move toward qgoal.

4: until qgoal is reached or an obstacle is encountered at qiH .

5: if Goal is reached then

6:

Exit.

7: end if

8: repeat

9: Follow the obstacle boundary.
10: until qgoal is reached or qiH is re-encountered. 11: Determine the point qiL on the perimeter that has the shortest distance to the goal. 12: Go to qiL .
13: if the robot were to move toward the goal then

14:

Conclude qgoal is not reachable and exit.

15: end if

16: end while

Like its Bug1 sibling, the Bug2 algorithm exhibits two behaviors: motion-to-goal and boundary-following. During motion-to-goal, the robot moves toward the goal on the m-line; however, in Bug2 the m-line connects qstart and qgoal, and thus remains ﬁxed. The boundary-following behavior is invoked if the robot encounters an obstacle,

Choset-79066 book February 22, 2005 17:34

20

2 Bug Algorithms

qgoal

WO1

q2L

q2H q1L

WO2

q1H qstart

qgoal

q1H
qstart
Figure 2.3 (Top) The Bug2 algorithm ﬁnds a path to the goal. (Bottom) The Bug2 algorithm reports failure.
but this behavior is different from that of Bug1. For Bug2, the robot circumnavigates the obstacle until it reaches a new point on the m-line closer to the goal than the initial point of contact with the obstacle. At this time, the robot proceeds toward the goal, repeating this process if it encounters an object. If the robot re-encounters the original departure point from the m-line, then there is no path to the goal (ﬁgures 2.3, 2.4). Let x ∈ Wfree ⊂ R2 be the current position of the robot, i = 1, and q0L be the start location. See algorithm 2 for a description of the Bug2 approach.
At ﬁrst glance, it seems that Bug2 is a more effective algorithm than Bug1 because the robot does not have to entirely circumnavigate the obstacles; however, this is not always the case. This can be seen by comparing the lengths of the paths found by the two algorithms. For Bug1, when the ith obstacle is encountered, the robot completely circumnavigates the boundary, and then returns to the leave point. In the worst case, the robot must traverse half the perimeter, pi , of the obstacle to reach this leave point.

Choset-79066 book February 22, 2005 17:34

2.1 Bug1 and Bug2

21

qstart

q1H

q1L

q2L

q2H

qgoal

Figure 2.4 Bug2 Algorithm.

Moreover, in the worst case, the robot encounters all n obstacles. If there are no obstacles, the robot must traverse a distance of length d(qstart, qgoal). Thus, we obtain

(2.1)

n
LBug1 ≤ d(qstart, qgoal) + 1.5 pi .
i =1

For Bug2, the path length is a bit more complicated. Suppose that the line through qstart and qgoal intersects the i th obstacle ni times. Then, there are at most ni leave points for this obstacle, since the robot may only leave the obstacle when it returns to a point on this line. It is easy to see that half of these intersection points are not valid leave points because they lie on the “wrong side” of the obstacle, i.e., moving toward the goal would cause a collision. In the worst case, the robot will traverse nearly the entire perimeter of the obstacle for each leave point. Thus, we obtain

(2.2)

L Bug2

≤ d(qstart, qgoal) +

1 2

n i =1

ni pi .

Naturally, (2.2) is an upper-bound because the summation is over all of the obstacles

as opposed to over the set of obstacles that are encountered by the robot.

Choset-79066 book February 22, 2005 17:34

22

2 Bug Algorithms

Algorithm 2 Bug2 Algorithm

Input: A point robot with a tactile sensor

Output: A path to qgoal or a conclusion no such path exists

1: while True do

2: repeat

3:

From qiL−1, move toward qgoal along m-line.

4: until

qgoal is reached or an obstacle is encountered at hit point qiH . 5: Turn left (or right).

6: repeat

7: Follow boundary

8: until

9: qgoal is reached or

10: qiH is re-encountered or

11: m-line is re-encountered at a point m such that

12:

m = qiH (robot did not reach the hit point),

13:

d(m, qgoal) < d(m, qiH ) (robot is closer), and

14:

If robot moves toward goal, it would not hit the obstacle

15: Let qiL+1 = m

16: Increment i

17: end while

A casual examination of (2.1) and (2.2) shows that LBug2 can be arbitrarily longer than LBug1. This can be achieved by constructing an obstacle whose boundary has many intersections with the m-line. Thus, as the “complexity” of the obstacle increases, it becomes increasingly likely that Bug1 could outperform Bug2 (ﬁgure 2.4).
In fact, Bug1 and Bug2 illustrate two basic approaches to search problems. For each obstacle that it encounters, Bug1 performs an exhaustive search to ﬁnd the optimal leave point. This requires that Bug1 traverse the entire perimeter of the obstacle, but having done so, it is certain to have found the optimal leave point. In contrast, Bug2 uses an opportunistic approach. When Bug2 ﬁnds a leave point that is better than any it has seen before, it commits to that leave point. Such an algorithm is also called greedy, since it opts for the ﬁrst promising option that is found. When the obstacles are simple, the greedy approach of Bug2 gives a quick payoff, but when the obstacles are complex, the more conservative approach of Bug1 often yields better performance.

Choset-79066 book February 22, 2005 17:34

2.2 Tangent Bug

23

2.2 Tangent Bug

Tangent Bug [216] serves as an improvement to the Bug2 algorithm in that it determines a shorter path to the goal using a range sensor with a 360 degree inﬁnite orientation resolution. Sometimes orientation is called azimuth. We model this range sensor with the raw distance function ρ : R2 × S1 → R. Consider a point robot situated at x ∈ R2 with rays radially emanating from it. For each θ ∈ S1, the value ρ(x, θ) is the distance to the closest obstacle along the ray from x at an angle θ . More formally,

(2.3)

ρ(x, θ ) = min d(x, x + λ[cos θ, sin θ ]T ),
λ∈[0, ∞]
such that x + λ[cos θ, sin θ ]T ∈ WOi .
i

Note that there are inﬁnitely many θ ∈ S1 and hence the inﬁnite resolution. This

assumption is approximated with a ﬁnite number of range sensors situated along the

circumference of a circular mobile robot which we have modeled as a point.

Since real sensors have limited range, we deﬁne the saturated raw distance function, denoted ρR : R2 × S1 → R, which takes on the same values as ρ when the obstacle is within sensing range, and has a value of inﬁnity when the ray lengths are greater

than the sensing range, R, meaning that the obstacles are outside the sensing range.

More formally,

ρR(x, θ) =

ρ(x, θ), ∞,

if ρ(x, θ ) < R otherwise.

The Tangent Bug planner assumes that the robot can detect discontinuities in ρR as depicted in ﬁgure 2.5. For a ﬁxed x ∈ R2, an interval of continuity is deﬁned to be a connected set of points x + ρ(x, θ )[cos θ, sin θ ]T on the boundary of the free space where ρR(x, θ ) is ﬁnite and continuous with respect to θ .
The endpoints of these intervals occur where ρR(x, θ ) loses continuity, either as a result of one obstacle blocking another or the sensor reaching its range limit. The endpoints are denoted Oi . Figure 2.6 contains an example where ρR loses continuity. The points O1, O2, O3, O5, O6, O7, and O8 correspond to losses of continuity associated with obstacles blocking other portions of Wfree; note the rays are tangent to the obstacles here. The point O4 is a discontinuity because the obstacle boundary falls out of range of the sensor. The sets of points on the boundary of the free space
between O1 and O2, O3 and O4, O5 and O6, O7 and O8 are the intervals of continuity. Just like the other Bugs, Tangent Bug iterates between two behaviors: motion-
to-goal and boundary-following. However, these behaviors are different than in the
Bug1 and Bug2 approaches. Although motion-to-goal directs the robot to the goal,

Choset-79066 book February 22, 2005 17:34

24

2 Bug Algorithms

WO4

WO5
x

WO3 WO2

WO1

Figure 2.5 The thin lines are values of the raw distance function, ρR(x, θ ), for a ﬁxed x ∈ R2, and the thick lines indicate discontinuities, which arise either because an obstacle occludes another or the sensing range is reached. Note that the segments terminating in free space represent inﬁnitely long rays.
this behavior may have a phase where the robot follows the boundary. Likewise, the boundary-following behavior may have a phase where the robot does not follow the boundary.
The robot initially invokes the motion-to-goal behavior, which itself has two parts. First, the robot moves in a straight line toward the goal until it senses an obstacle R units away and directly between it and the goal. This means that a line segment connecting the robot and goal must intersect an interval of continuity. For example, in ﬁgure 2.7, WO2 is within sensing range, but does not block the goal, but WO1 does. When the robot initially senses an obstacle, the circle of radius R becomes tangent to the obstacle. Immediately after, this tangent point splits into two Oi ’s, which are the endpoints of the interval. If the obstacle is in front of the robot, then this interval intersects the segment connecting the robot and the goal.
The robot then moves toward the Oi that maximally decreases a heuristic distance to the goal. An example of a heuristic distance is the sum d(x, Oi ) +d( Oi , qgoal). (The heuristic distance can be more complicated when factoring in available information

Choset-79066 book February 22, 2005 17:34

2.2 Tangent Bug

25

qgoal

O2 O3

x

O4

O8

O5

O1

O6

O7

Figure 2.6 The points of discontinuity of ρR(x, θ ) correspond to points Oi on the obstacles. The thick solid curves represent connected components of the range of ρR(x, θ ), i.e., the intervals of continuity. In this example, the robot, to the best of its sensing range, believes there is a straight-line path to the goal.
qgoal
O2 WO1
O1 O3
O4 WO2

Figure 2.7 The vertical represents the path of the robot and the dotted circle its sensing range. Currently, the robot is located at the “top” of the line segment. The points Oi represent the points of discontinuity of the saturated raw distance function. Note that the robot passes by WO2.

Choset-79066 book February 22, 2005 17:34

26 O1

2 Bug Algorithms O1

WO1

x

qgoal

O2

O3

WO2

WO1

x

O2

O3

WO2

qgoal

O4

O4

Figure 2.8 (Left) The planner selects O2 as a subgoal for the robot. (Right) The planner selects O4 as a subgoal for the robot. Note the line segment between O4 and qgoal cuts through the obstacle.

with regard to the obstacles.) In ﬁgure 2.8 (left), the robot sees WO1 and drives to O2 because i = 2 minimizes d(x, Oi ) + d( Oi , qgoal). When the robot is located at x, it cannot know that WO2 blocks the path from O2 to the goal. In ﬁgure 2.8 (right), when the robot is located at x but the goal is different, it has enough sensor information to conclude that WO2 indeed blocks a path from O2 to the goal, and therefore drives toward O4. So, even though driving toward O2 may initially minimize d(x, Oi ) + d( Oi , qgoal) more than driving toward O4, the planner effectively assigns an inﬁnite cost to d( O2, qgoal) because it has enough information to conclude that any path through O2 will be suboptimal.
The set {Oi } is continuously updated as the robot moves toward a particular Oi , which can be seen in ﬁgure 2.9. When t = 1, the robot has not sensed the obstacle, hence the robot moves toward the goal. When t = 2, the robot initially senses the obstacle, depicted by a thick solid curve. The robot continues to move toward the goal, but off to the side of the obstacle heading toward the discontinuity in ρ. For t = 3 and t = 4, the robot senses more of the obstacle and continues to decrease distance toward the goal while hugging the boundary.
The robot undergoes the motion-to-goal behavior until it can no longer decrease the heuristic distance to the goal. Put differently, it ﬁnds a point that is like a local minimum of d(·, Oi ) + d( Oi , qgoal) restricted to the path that motion-to-goal dictates.
When the robot switches to boundary-following, it ﬁnds the point M on the sensed portion of the obstacle which has the shortest distance on the obstacle to the goal. Note that if the sensor range is zero, then M is the same as the hit point from the Bug1 and Bug2 algorithms. This sensed obstacle is also called the followed obstacle. We make a distinction between the followed obstacle and the blocking obstacle. Let x be

Choset-79066 book February 22, 2005 17:34

2.2 Tangent Bug

27

t =1

t =2

t =4

t =4

Figure 2.9 Demonstration of motion-to-goal behavior for a robot with a ﬁnite sensor range moving toward a goal which is “above” the light gray obstacle.

WO1

qgoal M

WO2

Figure 2.10 The workspace is the same as in ﬁgure 2.7. The solid and dashed segments represent the path generated by motion-to-goal and the dotted path represents the boundaryfollowing path. Note that M is the “local minimum” point.
the current position of the robot. The blocking obstacle is the closest obstacle within sensor range that intersects the segment (1 − λ)x + λqgoal ∀λ ∈ [0, 1]. Initially, the blocking obstacle and the followed obstacle are the same.
Now the robot moves in the same direction as if it were in the motion-to-goal behavior. It continuously moves toward the Oi on the followed obstacle in the chosen direction (ﬁgure 2.10). While undergoing this motion, the planner also updates two values: dfollowed and dreach. The value dfollowed is the shortest distance between the boundary which had been sensed and the goal. Let be all of the points within

Choset-79066 book February 22, 2005 17:34

28

2 Bug Algorithms

line of sight of x with range R that are on the followed obstacle WO f , i.e., = {y ∈ ∂WO f : λx + (1 − λ) y ∈ Qfree ∀λ ∈ [0, 1]}. The value dreach is the
distance between the goal and the closest point on the followed obstacle that is within line of sight of the robot, i.e.,

dreach

=

min
c∈

d (qgoal ,

c).

When dreach < dfollowed, the robot terminates the boundary-following behavior. Let T be the point where a circle centered at x of radius R intersects the segment
that connects x and qgoal. This is the point on the periphery of the sensing range that is closest to the goal when the robot is located at x. Starting with x = qstart and dleave = d(qstart, qgoal), see algorithm 3.

Algorithm 3 Tangent Bug Algorithm

Input: A point robot with a range sensor

Output: A path to the qgoal or a conclusion no such path exists

1: while True do

2: repeat

3:

Continuously move toward the point n ∈ {T, Oi } which minimizes d(x, n) +

d(n, qgoal)

4: until

the goal is encountered or

The direction that minimizes d(x, n) + d(n, qgoal) begins to increase d(x, qgoal), i.e., the robot detects a “local minimum” of d(·, qgoal).

5: Chose a boundary following direction which continues in the same direction as the

most recent motion-to-goal direction.

6: repeat

7:

Continuously update dreach, dfollowed, and {Oi }.

8:

Continuously moves toward n ∈ {Oi } that is in the chosen boundary direction.

9: until

The goal is reached.

The robot completes a cycle around the obstacle in which case the goal cannot be achieved.

dreach < dfollowed 10: end while

Choset-79066 book February 22, 2005 17:34

2.2 Tangent Bug

29

L3

qstart

H1 D1

H2 D2

M3 H3

M4 H4

L4 qgoal

Figure 2.11 The path generated by Tangent Bug with zero sensor range. The dashed lines correspond to the motion-to-goal behavior and the dotted lines correspond to boundary-following.

L3

H1 qstart

D1

H2

sw3 M3

D2

H3

L4 sw 4 H4 M4
qgoal

Figure 2.12 Path generated by Tangent Bug with ﬁnite sensor range. The dashed lines correspond to the motion-to-goal behavior and the dotted lines correspond to boundary-following. The dashed-dotted circles correspond to the sensor range of the robot.
See ﬁgures 2.11, 2.12 for example runs. Figure 2.11 contains a path for a robot with zero sensor range. Here the robot invokes a motion-to-goal behavior until it encounters the ﬁrst obstacle at hit point H1. Unlike Bug1 and Bug2, encountering a hit point does not change the behavior mode for the robot. The robot continues with the motion-to-goal behavior by turning right and following the boundary of the ﬁrst obstacle. The robot turned right because that direction minimized its heuristic distance to the goal. The robot departs this boundary at a depart point D1. The robot

Choset-79066 book February 22, 2005 17:34
30

2 Bug Algorithms

S

H1

T

D1

H2

D2

H3

d3

Figure 2.13 Path generated by Tangent Bug with inﬁnite sensor range. The dashed-lines correspond to the motion-to-goal behavior and there is no boundary-following.

continues with the motion-to-goal behavior, maneuvering around a second obstacle, until it encounters the third obstacle at H3. The robot turns left and continues to invoke the motion-to-goal behavior until it reaches M3, a minimum point. Now, the planner invokes the boundary-following behavior until the robot reaches L3. Note that since we have zero sensing range, dreach is the distance between the robot and the goal. The procedure continues until the robot reaches the goal. Only at Mi and Li does the robot switch between behaviors. Figures 2.12, 2.13 contain examples where the robot has a ﬁnite and inﬁninte sensing ranges, respectively.

2.3 Implementation
Essentially, the bug algorithms have two behaviors: drive toward a point and follow an obstacle. The ﬁrst behavior is simply a form of gradient descent of d(·, n) where n is either qgoal or an Oi . The second behavior, boundary-following, presents a challenge because the obstacle boundary is not known a priori. Therefore, the robot planner must rely on sensor information to determine the path. However, we must concede that the full path to the goal will not be determined from one sensor reading: the sensing range of the robot may be limited and the robot may not be able to “see” the entire world from one vantage point. So, the robot planner has to be incremental. We must determine ﬁrst what information the robot requires and then where the robot should move to acquire more information. This is indeed the challenge of sensor-based planning. Ideally, we would like this approach to be reactive with sensory information

Choset-79066 book February 22, 2005 17:34

2.3 Implementation

31

feeding into a simple algorithm that outputs translational and rotational velocity for the robot.
There are three questions: What information does the robot require to circumnavigate the obstacle? How does the robot infer this information from its sensor data? How does the robot use this information to determine (locally) a path?
2.3.1 What Information: The Tangent Line
If the obstacle were ﬂat, such as a long wall in a corridor, then following the obstacle is trivial: simply move parallel to the obstacle. This is readily implemented using a sensing system that can determine the obstacle’s surface normal n(x), and hence a direction parallel to its surface. However, the world is not necessarily populated with ﬂat obstacles; many have nonzero curvature. The robot can follow a path that is consistently orthogonal to the surface normal; this direction can be written as n(x)⊥ and the resulting path satisﬁes c˙(t) = v where v is a basis vector in (n (c (t)))⊥. The sign of v is based on the “previous” direction of c˙.
Consistently determining the surface normal can be quite challenging and therefore for implementation, we can assume that obstacles are “locally ﬂat.” This means the sensing system determines the surface normal, the robot moves orthogonal to this normal for a short distance, and then the process repeats. In a sense, the robot determines the sequence of short straight-line segments to follow, based on sensor information.
This ﬂat line, loosely speaking, is the tangent (ﬁgure 2.14). It is a linear approximation of the curve at the point where the tangent intersects the curve. The tangent

Offset Curve

Tangent

WOi W*

x

D(x)

Figure 2.14 The solid curve is the offset curve. The dashed line represents the tangent to the offset curve at x.

Choset-79066 book February 22, 2005 17:34

32

2 Bug Algorithms

can also be viewed as a ﬁrst-order approximation to the function that describes the

curve. Let c : [0, 1] → Wfree be the function that deﬁnes a path. Let x = c(s0) for

a

s0

∈

[0, 1].

The tangent

at

x

is

dc ds

s=s0 . The tangent space

can

be

viewed

as a

line

whose basis vector is

dc ds

s=s0 , i.e.,

α

dc ds

s=s0

α∈R

.

2.3.2 How to Infer Information with Sensors: Distance and Gradient
The next step is to infer the tangent from sensor data. Instead of thinking of the robot as a point in the plane, let’s think of it as a circular base which has a ﬁne array of tactile sensors radially distributed along its circumference (ﬁgure 2.15). When the robot contacts an obstacle, the direction from the contacted sensor to the robot’s center approximates the surface normal. With this information, the robot can determine a sequence of tangents to follow the obstacle.
Unfortunately, using a tactile sensor to prescribe a path requires the robot to collide with obstacles, which endangers the obstacles and the robot. Instead, the robot should follow a path at a safe distance W∗ ∈ R from the nearest obstacle. Such a path is called

Tactile Ring

Robot

n(t)

Obstacle Figure 2.15 A ﬁne-resolution tactile sensor.

Choset-79066 book February 22, 2005 17:34

2.3 Implementation

33

WO4

D(x) Robot
x

WO3

WO1

WO2

Figure 2.16 The global minimum of the rays determines the distance to the closest obstacle; the gradient points in a direction away from the obstacle along the ray.

an offset curve [381]. Let D(x) be the distance from x to the closest obstacle, i.e.,

(2.4) D(x) = minc∈ i WOi d(x, c).

To measure this distance with a mobile robot equipped with an onboard range sensing

ring, we use the raw distance function again. However, instead of looking for dis-

continuities, we look for the global minimum. In other words, D(x) = mins ρ(x, s)

(ﬁgure 2.16).

We will need to use the gradient of distance. In general, the gradient is a vector

that points in the direction that maximally increases the value of a function. See

appendix C.5 for more details. Typically, the ith component of the gradient vector is

the partial derivative of the function with respect to its ith coordinate. In the plane,

∇ D(x)

=

[

∂

D(x ∂ x1

)

∂

D(x ∂ x2

)

]T

which

points

in

the

direction

that

increases

distance

the

most. Finally, the gradient is the unit direction associated with the smallest value of

the raw distance function. Since the raw distance function seemingly approximates a

sensing system with individual range sensing elements radially distributed around the

perimeter of the robot, an algorithm deﬁned in terms of D can often be implemented

using realistic sensors.

There are many choices for range sensors; here, we investigate the use of ultrasonic

sensors (ﬁgure 2.17), which are commonly found on mobile robots. Conventional

ultrasonic sensors measure distance using time of ﬂight. When the speed of sound

Choset-79066 book February 22, 2005 17:34
34

2 Bug Algorithms

Figure 2.17 The disk on the right is the standard Polaroid ultrasonic transducer found on many mobile robots; the circuitry on the left drives the transducer.

Figure 2.18 Beam pattern for the Polaroid transducer.

in air is constant, the time that the ultrasound requires to leave the transducer, strike

an obstacle, and return is proportional to the distance to the point of reﬂection on

the obstacle [113]. This obstacle, however, can be located anywhere along the angu-

lar spread of the sonar sensor’s beam pattern (ﬁgure 2.18). Therefore, the distance

information that sonars provide is fairly accurate in depth, but not in azimuth. The

beam pattern can be approximated with a cone (ﬁgure 2.19). For the commonly used

Polaroid transducer, the arcbase is 22.5 degrees. When the reading of the sensor is d,

the

point

of

reﬂection

can

be

anywhere

along

the

arc

base

of

length

2π d22.5 360

.

Choset-79066 book February 22, 2005 17:34

2.3 Implementation

35

Obstacle d
Point Sensor

Sensor Measurement Axis Beam Pattern
Robot

Figure 2.19 Centerline model.

Initially, assume that the echo originates from the center of the sonar cone. We acknowledge that this is a naive model, and we term this the centerline model (ﬁgure 2.19). The ultrasonic sensor with the smallest reading approximates the global minimum of the raw distance function, and hence D(x). The direction that this sensor is facing approximates the negated gradient −∇ D(x) because this sensor faces the closest obstacle. The tangent is then the line orthogonal to the direction associated with the smallest sensor reading.

2.3.3

How to Process Sensor Information: Continuation Methods
The tangent to the offset curve is (∇ D(x))⊥, the line orthogonal to ∇ D(x) (ﬁgure 2.14). The vector ∇ D(x) points in the direction that maximally increases distance; likewise, the vector −∇ D(x) points in the direction that maximally decreases distance; they both point along the same line, but in opposite directions. Therefore, the vector (∇ D(x))⊥ points in the direction that locally maintains distance; it is perpendicular to both ∇ D(x) and −∇ D(x). This would be the tangent of the offset curve which maintains distance to the nearby obstacle.
Another way to see why (∇ D(x))⊥ is the tangent is to look at the deﬁnition of the offset curve. For a safety distance W∗, we can deﬁne the offset curve implicitly as the set of points where G(x) = D(x) − W∗ maps to zero. The set of nonzero points (or vectors) that map to zero is called the null space of a map. For a curve implicitly deﬁned by G, the tangent space at a point x is the null space of DG(x), the Jacobian of G [410]. In general, the i, jth component of the Jacobian matrix is the partial derivative of the ith component function with respect to the jth coordinate and thus the Jacobian

Choset-79066 book February 22, 2005 17:34

36

2 Bug Algorithms

is a mapping between tangent spaces. Since in this case, G is a real-valued function (i = 1), the Jacobian is just a row vector D D(x). Here, we are reusing the symbol D. The reader is forced to use context to determine if D means distance or differential.
In Euclidean spaces, the ith component of a single-row Jacobian equals the ith component of the gradient and thus ∇ D(x) = ( D D(x))T . Therefore, since the tangent space is the null space of D D(x), the tangent for boundary-following in the plane is the line orthogonal to ∇ D(x), i.e., (∇ D(x))⊥, and can be derived from sensor information.
Using distance information, the robot can determine the tangent direction to the offset curve. If the obstacles are ﬂat, then the offset curve is also ﬂat, and simply following the tangent is sufﬁcient to follow the boundary of an unknown obstacle. Consider, instead, an obstacle with curvature. We can, however, assume that the obstacle is locally ﬂat. The robot can then move along the tangent for a short distance, but since the obstacle has curvature, the robot will not follow the offset curve, i.e., it will “fall off” of the offset curve. To reaccess the offset curve, the robot moves either toward or away from the obstacle until it reaches the safety distance W∗. In doing so, the robot is moving along a line deﬁned by ∇ D(x), which can be derived from sensor information.
Essentially, the robot is performing a numerical procedure of prediction and correction. The robot uses the tangent to locally predict the shape of the offset curve and then invokes a correction procedure once the tangent approximation is not valid. Note that the robot does not explicitly trace the path but instead “hovers” around it, resulting in a sampling of the path, not the path itself (ﬁgure 2.20).
A numerical tracing procedure can be posed as one which traces the roots of the expression G(x) = 0, where in this case G(x) = D(x) − W∗. Numerical curvetracing techniques rest on the implicit function theorem [9, 232, 307] which locally deﬁnes a curve that is implicitly deﬁned by a map G : Y × R → Y . Speciﬁcally, the roots of G locally deﬁne a curve parameterized by λ ∈ R. See appendix D for a formal deﬁnition.

Figure 2.20 The dashed line is the actual path, but the robot follows the thin black lines, predicting and correcting along the path. The black circles are samples along the path.

Choset-79066 book February 22, 2005 17:34

2.3 Implementation

37

For boundary following at a safety distance W∗, the function G( y, λ) = D( y, λ) − W∗ implicitly deﬁnes the offset curve. Note that the λ-coordinate corresponds to a tangent direction and the y-coordinates to the line or hyperplane orthogonal to the tangent. Let Y denote this hyperplane and DY G be the matrix formed by taking the derivative of G(x) = D(x) − W∗ = 0 with respect to the y-coordinates. It takes the form DY G(x) = DY D(x) where DY denotes the differential with respect to the y-coordinates. If DY G( y, λ) is surjective at x = (λ, y)T , then the implicit function theorem states that the roots of G( y, λ) locally deﬁne a curve that follows the boundary at a distance W∗ as λ is varied, i.e., y(λ).
By numerically tracing the roots of G, we can locally construct a path. While there are a number of curve-tracing techniques [232], let us consider an adaptation of a common predictor-corrector scheme. Assume that the robot is located at a point x which is a ﬁxed distance W∗ away from the boundary. The robot takes a “small” step,
λ, in the λ-direction (i.e., the tangent to the local path). In general, this prediction step takes the robot off the offset path. Next, a correction method is used to bring the robot back onto the offset path. If λ is small, then the local path will intersect a correcting plane, which is a plane orthogonal to the λ-direction at a distance λ away from the origin.
The correction step ﬁnds the location where the offset path intersects the correcting plane and is an application of the Newton convergence theorem [232]. See appendix D.2 for a more formal deﬁnition of this theorem. The Newton convergence theorem also requires that DY G( y, λ) be full rank at every ( y, λ) in a neighborhood of the offset path. This is true because for G(x) = D(x) − W∗, [0 DY G( y, λ)]T = DG( y, λ). Since DG( y, λ) is full rank, so must be DY G( y, λ) on the offset curve. Since the set of nonsingular matrices is an open set, we know there is a neighborhood around each ( y, λ) in the offset path where DG( y, λ) is full rank and hence we can use the iterative Newton method to implement the corrector step. If yh and λh are the hth estimates of y and λ, the h + 1st iteration is deﬁned as
(2.5) yh+1 = yh − ( DY G)−1 G( yh, λh),
where DY G is evaluated at ( yh, λh). Note that since we are working in a Euclidean space, we can determine DY G solely from distance gradient, and hence, sensor information.

Problems
1. Prove that D(x) is the global minimum of ρ(x, s) with respect to s. 2. What are the tradeoffs between the Bug1 and Bug2 algorithms?

Choset-79066 book February 22, 2005 17:34

38

2 Bug Algorithms

3. Extend the Bug1 and Bug2 algorithms to a two-link manipulator.
4. What is the difference between the Tangent Bug algorithm with zero range detector and Bug2? Draw examples.
5. What are the differences between the path in ﬁgure 2.11 and the paths that Bug1 and Bug2 would have generated?
6. The Bug algorithms also assume the planner knows the location of the goal and the robot has perfect positioning. Redesign one of the Bug algorithms to relax the assumption of perfect positioning. Feel free to introduce a new type of “reasonable” sensor (not a highresolution Global Positioning System).
7. In the Bug1 algorithm, prove or disprove that the robot does not encounter any obstacle that does not intersect the disk of radius d(qstart, qgoal) centered at qgoal.
8. What assumptions do the Bug1, Bug2, and Tangent Bug algorithms make on robot localization, both in position and orientation?
9. Prove the completeness of the Tangent Bug algorithm.
10. Adapt the Tangent Bug algorithm so that it has a limited ﬁeld of view sensor, i.e., it does not have a 360 degree ﬁeld of view range sensor.
11. Write out DY G for boundary following in the planar case. 12. Let G1(x) = D(x) + 1 and let G2(x) = D(x) + 2. Why are their Jacobians the same? 13. Let G(x, y) = y3 + y − x2. Write out a y as a function of x in an interval about the origin
for the curve deﬁned by G(x, y) = 0.

Choset-79066 book February 22, 2005 17:37
3 Conﬁguration Space
TO CREATE motion plans for robots, we must be able to specify the position of the robot. More speciﬁcally, we must be able to give a speciﬁcation of the location of every point on the robot, since we need to ensure that no point on the robot collides with an obstacle. This raises some fundamental questions: How much information is required to completely specify the position of every point on the robot? How should this information be represented? What are the mathematical properties of these representations? How can obstacles in the robot’s world be taken into consideration while planning the path of a robot?
In this chapter, we begin to address these questions. We ﬁrst discuss exactly what is meant by a conﬁguration of a robot and introduce the concept of the conﬁguration space, one of the most important concepts in robot motion planning. We then brieﬂy discuss how obstacles in the robot’s environment restrict the set of admissible paths. We then begin a more rigorous investigation of the properties of the conﬁguration space, including its dimension, how it sometimes can be represented by a differentiable manifold, and how manifolds can be represented by embeddings and parameterizations. We conclude the chapter by discussing mappings between different representations of the conﬁguration, and the Jacobian of these mappings, which relates velocities in the different representations.

Choset-79066 book February 22, 2005 17:37

40

3 Conﬁguration Space

3.1 Specifying a Robot’s Conﬁguration
To make our discussion more precise, we introduce the following deﬁnitions. The conﬁguration of a robot system is a complete speciﬁcation of the position of every point of that system. The conﬁguration space, or C-space, of the robot system is the space of all possible conﬁgurations of the system. Thus a conﬁguration is simply a point in this abstract conﬁguration space. Throughout the text, we use q to denote a conﬁguration and Q to denote the conﬁguration space.1 The number of degrees of freedom of a robot system is the dimension of the conﬁguration space, or the minimum number of parameters needed to specify the conﬁguration.
To illustrate these deﬁnitions, consider a circular mobile robot that can translate without rotating in the plane. A simple way to represent the robot’s conﬁguration is to specify the location of its center, (x, y), relative to some ﬁxed coordinate frame. If we know the radius r of the robot, we can easily determine from the conﬁguration q = (x, y) the set of points occupied by the robot. We will use the notation R(q) to denote this set. When we deﬁne the conﬁguration as q = (x, y), we have
R(x, y) = {(x , y ) | (x − x )2 + ( y − y )2 ≤ r 2},
and we see that these two parameters, x and y, are sufﬁcient to completely determine the conﬁguration of the circular robot. Therefore, for the circular mobile robot, we can represent the conﬁguration space by R2 once we have chosen a coordinate frame in the plane.
Robots move in a two- or three-dimensional Euclidean ambient space, represented by R2 or R3, respectively. We sometimes refer to this ambient space as the workspace. Other times we have a more speciﬁc meaning for “workspace.” For example, for a robot arm, often we call the workspace the set of points of the ambient space reachable by a point on the hand or end effector (see ﬁgure 3.3). For the translating mobile robot described above, the workspace and the conﬁguration space are both two-dimensional Euclidean spaces, but it is important to keep in mind that these are different spaces. This becomes clear when we consider even slightly more complicated robots, as we see next.
Consider a two-joint planar robot arm, as shown in ﬁgure 3.1. A point on the ﬁrst link of the arm is pinned, so that the only possible motion of the ﬁrst link is a rotation about this joint. Likewise, the base of the second link is pinned to a point at the end of the ﬁrst link, and the only possible motion of the second link is a rotation about this joint. Therefore, if we specify the parameters θ1 and θ2, as shown in ﬁgure 3.1, we
1. While q is used almost universally to denote a conﬁguration, the conﬁguration space is sometimes denoted by C, particularly in the path-planning community.

Choset-79066 book February 22, 2005 17:37

3.1 Specifying a Robot’s Conﬁguration

41

End effector

Link 1

Link 2 q2

q1

Figure 3.1 The angles θ1 and θ2 specify the conﬁguration of the two-joint robot.
have speciﬁed the conﬁguration of the arm. For now we will assume no joint limits, so the two links can move over each other.
Each joint angle θi corresponds to a point on the unit circle S1, and the conﬁguration space is S1 × S1 = T 2, the two-dimensional torus. It is common to picture a torus as the surface of a doughnut because a torus has a natural embedding in R3, just as a circle S1 has a natural embedding in R2. By cutting the torus along the θ1 = 0 and θ2 = 0 curves, we can ﬂatten the torus onto the plane, as shown in ﬁgure 3.2. With this planar representation, we are identifying points on S1 by points in the interval [0, 2π ) ⊂ R. While this representation covers all points in S1, the interval [0, 2π), being a subset of the real line, does not naturally wrap around like S1, so there is a discontinuity in the representation. As we discuss in section 3.4, this is because S1 is topologically different from any interval of R.
We deﬁne the workspace of the two-joint manipulator to be the reachable points by the end effector. The workspace for our two-joint manipulator is an annulus (ﬁgure 3.3), which is a subset of R2. All points in the interior of the annulus are reachable in two ways, with the arm in a right-arm and a left-arm conﬁguration, sometimes called elbow-up and elbow-down. Therefore, the position of the end effector is not a valid conﬁguration (not a complete description of the location of all points of the robot), so the annulus is not a conﬁguration space for this robot.
So we have seen that the conﬁguration spaces of both the translating mobile robot and the two-joint manipulator are two-dimensional, but they are quite different. The torus T 2 is doughnut-shaped with ﬁnite area, while R2 is ﬂat with inﬁnite area. We delve further into these sorts of differences when we discuss topology in section 3.4.

Choset-79066 book March 22, 2005 9:59

42 q2

q1 (a)

2π R

P

q2 q1
GF PR
(b)

q2 0F 0

G

q1

2π

(c)

Figure 3.2 (a) A two-joint manipulator. (b) The conﬁguration of the robot is represented as a point on the toral conﬁguration space. (c) The torus can be cut and ﬂattened onto the plane. This planar representation has “wraparound” features where the edge F R is connected to G P, etc.

Figure 3.3 The workspace for this two-joint manipulator is an annulus, a disk with a smaller disk removed from it. Note that all points in the interior of the annulus are reachable with a right-arm conﬁguration and a left-arm conﬁguration.

Choset-79066 book February 22, 2005 17:37

3.2 Obstacles and the Conﬁguration Space

43

3.2 Obstacles and the Conﬁguration Space

Equipped with our understanding of conﬁgurations and of conﬁguration spaces, we can deﬁne the path-planning problem to be that of determining a continuous mapping, c : [0, 1] → Q, such that no conﬁguration in the path causes a collision between the robot and an obstacle. It is useful to deﬁne explicitly the set of conﬁgurations for which such a collision occurs. We deﬁne a conﬁguration space obstacle QOi to be the set of conﬁgurations at which the robot intersects an obstacle WOi in the workspace, i.e.,

QOi = {q ∈ Q | R(q) WOi = ∅}.

The free space or free conﬁguration space Qfree is the set of conﬁgurations at which the robot does not intersect any obstacle, i.e.,

Qfree = Q\

QOi .

i

With this notation, we deﬁne a free path to be a continuous mapping c : [0, 1] →

Qfree, and a semifree path to be a continuous mapping c : [0, 1] → cl(Qfree), in which cl(Qfree) denotes the closure of Qfree. A free path does not allow contact between the

robot and obstacles, while a semifree path allows the robot to contact the boundary

of an obstacle. We assume that Qfree is open unless otherwise noted. We now examine how obstacles in the workspace can be mapped into the conﬁg-

uration space for the robots that we discussed above.

3.2.1 Circular Mobile Robot
Consider the circular mobile robot in an environment with a single polygonal obstacle in the workspace, as shown in ﬁgure 3.4. In ﬁgure 3.4(b), we slide the robot around the obstacle to ﬁnd the constraints the obstacle places on the conﬁguration of the robot, i.e., the possible locations of the robot’s reference point. We have chosen to use the center of the robot, but could easily choose another point. Figure 3.4(c) shows the resulting obstacle in the conﬁguration space. Motion planning for the circular robot in ﬁgure 3.4(a) is now equivalent to motion planning for a point in the conﬁguration space, as shown in ﬁgure 3.4(c).
Figure 3.5 shows three mobile robots of different radii in the same environment. In each case, the robot is trying to ﬁnd a path from one conﬁguration to another. To transform the workspace obstacles into conﬁguration obstacles, we “grow” the polygon outward and the walls inward. The problem is now to ﬁnd a path for the point robot in the conﬁguration space. We see that the growing process has disconnected the free conﬁguration space Qfree for the largest robot, showing that there is no solution for this robot.

Choset-79066 book February 22, 2005 17:37
44

3 Conﬁguration Space

(a)

(b)

(c)

Figure 3.4 (a) The circular mobile robot approaches the workspace obstacle. (b) By sliding the mobile robot around the obstacle and keeping track of the curve traced out by the reference point, we construct the conﬁguration space obstacle. (c) Motion planning for the robot in the workspace representation in (a) has been transformed into motion planning for a point robot in the conﬁguration space.

Workspace

C-space

(a)

(b)

(c)

Figure 3.5 The top row shows the workspace and the bottom row shows the conﬁguration space for (a) a point mobile robot, (b) a circular mobile robot, and (c) a larger circular mobile robot.

Although the example in ﬁgure 3.5 is quite simple, the main point is that it is easier to think about points moving around than bodies with volume. Keep in mind that although both the workspace and the conﬁguration space for this system can be represented by R2, and the obstacles appear to simply “grow” in this example, the conﬁguration space and workspace are different spaces, and the transformation from workspace obstacles to conﬁguration space obstacles is not always so simple.

Choset-79066 book February 22, 2005 17:37

3.2 Obstacles and the Conﬁguration Space

45

For example, appendix F discusses how to generate conﬁguration space obstacles for a polygon that translates and rotates among polygonal obstacles in the plane. The two-joint arm example is examined next.
3.2.2 Two-Joint Planar Arm
For the case of the circular mobile robot in a world populated with polygonal obstacles, it is easy to compute conﬁguration space obstacles. When the robot is even slightly more complex, it becomes much more difﬁcult to do so. For this reason, grid-based representations of the conﬁguration space are sometimes used. Consider the case of the two-joint planar arm, for which Q = T 2. We can deﬁne a grid on the surface of the torus, and for each point on this grid we can perform a fairly simple test to see if the corresponding conﬁguration causes a collision between the arm and any obstacle in the workspace. If we let each grid point be represented by a pixel, we can visualize the conﬁguration space obstacle by “coloring” pixels appropriately. This method was used to obtain ﬁgures 3.6, 3.7, and 3.8.2 In each of the ﬁgures, the image on the left depicts a two-joint arm in a planar workspace, while the image on the right depicts the conﬁguration space. In each case, the arm on the left is depicted in several conﬁgurations, and these are indicated in the conﬁguration spaces on the right.

Figure 3.6 (Left) A path for the two-joint manipulator through its workspace, where the start and goal conﬁgurations are darkened. (Right) The path in the conﬁguration space.
2. These ﬁgures were obtained using the applet at http://ford.ieor.berkeley.edu/cspace/.

Choset-79066 book February 22, 2005 17:37
46

3 Conﬁguration Space

Figure 3.7 (Left) The workspace for a two-joint manipulator where the start and goal conﬁgurations are shown. (Right) The conﬁguration space shows there is no free path between the start and goal conﬁgurations.
Figure 3.8 (Left) The workspace for a two-joint manipulator where the start and goal conﬁgurations are darkened. (Right) The path shows the wraparound of the planar representation of the conﬁguration space.
While pictures such as those in ﬁgures 3.6, 3.7, and 3.8 are useful for visualizing conﬁguration space obstacles, they are not sufﬁcient for planning collision-free motions. The reason for this is that the grid only encodes collision information for the discrete set of points lying on the grid. A path includes not only grid points, but also the points on the curves that connect the grid points. One possible remedy for this problem is to “thicken” the robot when we test it at a grid point, so that if the thickened

Choset-79066 book February 22, 2005 17:37

3.3 The Dimension of the Conﬁguration Space

47

robot is collision-free, then paths to adjacent grid points are also collision-free.3 We could also choose to ignore this problem by choosing a grid resolution that is “high enough.”

3.3 The Dimension of the Conﬁguration Space
In our introduction to conﬁguration space above, we restricted our attention to twodimensional conﬁguration spaces that are easy to visualize. For each example, it was fairly straightforward to conclude that there were only two degrees of freedom: for a translating robot, the conﬁguration was speciﬁed by a point in the familiar Euclidean plane, while for the two-joint arm the two joint angles gave a complete speciﬁcation of the arm’s position. In this section we determine the number of degrees of freedom of more complex systems by considering constraints on the motions of individual points of the systems.
As a ﬁrst example, suppose the robot is a point that can move in the plane. The conﬁguration can be given by two coordinates, typically q = (x, y) ∈ Q = R2, once we have chosen a reference coordinate frame ﬁxed somewhere in space. Thus the robot has two degrees of freedom; the conﬁguration space is two-dimensional.
Now consider a system consisting of three point robots, A, B, and C, that are free to move in the plane. Since the points can move independently, in order to specify the conﬁguration of the system we need to specify the conﬁguration of each of A, B, and C. By simply generalizing the case for a single point, we see that a conﬁguration for this system can be given by the six coordinates xA, yA, xB, yB, xC , and yC (assuming that the points can overlap). The system has six degrees of freedom, and in this case we have Q = R6.
Real robots are typically modeled as a set of rigid bodies connected by joints (or a single rigid body for the case of most mobile robots), not a set of points that are free to move independently. So, suppose now that the robot is a planar rigid body that can both translate and rotate in the plane. Deﬁne A, B, and C to be three distinct points that are ﬁxed to the body. To place the body in the plane, we are ﬁrst free to choose the position of A by choosing its coordinates (xA, yA). Now we wish to choose the coordinates of B, (xB, yB), but the rigidity of the body requires that this point maintain a constant distance d( A, B) from A:
d( A, B) = (xA − xB)2 + (yA − yB)2.
3. This approach is called conservative, as a motion planner using this approach will never ﬁnd an incorrect solution, but it might miss solutions when they exist. As a result, the planner can only be resolution complete, not complete.

Choset-79066 book February 22, 2005 17:37

48

3 Conﬁguration Space

This equation constrains B to lie somewhere on a circle of radius d( A, B) centered at (xA, yA), and our only freedom in placing B is the angle θ from A to B.
Now when we try to choose coordinates (xC , yC ) for C, we see that our choice is subject to two constraints:
d( A, C) = (x A − xC )2 + ( yA − yC )2 d( B, C) = (xB − xC )2 + ( yB − yC )2
In other words, C has already been placed for us. In fact, every point on the body has been placed once we have chosen (xA, yA, θ ), making this a good representation of the conﬁguration. The body has three degrees of freedom, and its conﬁguration space is R2 × S1.
Each of the distance constraints above is an example of a holonomic constraint. A holonomic constraint is one that can be expressed purely as a function g of the conﬁguration variables (and possibly time), i.e., of the form
g(q, t) = 0.
Each linearly independent holonomic constraint on a system reduces the dimension of the system’s conﬁguration space by one. Thus a system described by n coordinates subject to m independent holonomic constraints has an (n − m)-dimensional conﬁguration space. In this case, we normally attempt to represent the conﬁguration space by a smaller set of n − m coordinates subject to no constraints, e.g., the coordinates (xA, yA, θ ) for the planar body above.
Nonholonomic constraints are velocity constraints of the form
g(q, q˙, t) = 0
which do not reduce the dimension of the conﬁguration space. Nonholonomic constraints are left to chapter 12.
We can apply the counting method above to determine the number of degrees of freedom of a three-dimensional rigid body. Choose three noncollinear points on the body, A, B, C. The coordinates (xA, yA, z A) of point A are ﬁrst chosen arbitrarily. After ﬁxing A, the distance constraint from A forces B to lie on the two-dimensional surface of a sphere centered at A. After both A and B are ﬁxed, the distance constraints from A and B force C to lie on a one-dimensional circle about the axis formed by A and B. Once this point is chosen, all other points on the body are ﬁxed. Thus the conﬁguration of a rigid body in space can be described by nine coordinates subject to three constraints, yielding a six-dimensional conﬁguration space.
We have already seen that a rigid body moving in a plane has three degrees of freedom, but we can arrive at this same conclusion if we imagine a spatial rigid body

Choset-79066 book February 22, 2005 17:37

3.3 The Dimension of the Conﬁguration Space

49

with six degrees of freedom with a set of constraints that restricts it to a plane. Choose this book as an example, using three corners of the back cover as points A, B and C. The book can be conﬁned to a plane (e.g., the plane of a tabletop) using the three holonomic constraints

z A = zB = zC = 0.

The two-joint planar arm can be shown to have two degrees of freedom by this (somewhat indirect) counting method. Each of the two links can be thought of as a rigid spatial body with six degrees of freedom. Six constraints restrict the bodies to a plane (three for each link), two constraints restrict a point on the ﬁrst link (the ﬁrst joint) to be at a ﬁxed location in the plane, and once the angle of the ﬁrst link is chosen, two constraints restrict a point on the second link (the second joint) to be at a ﬁxed location. Therefore we have (12 coordinates) − (10 constraints) = 2 degrees of freedom.
Of course we usually count the degrees of freedom of an open-chain jointed robot, also known as a serial mechanism, by adding the degrees of freedom at each joint. Common joints with one degree of freedom are revolute (R) joints, joints which rotate about an axis, and prismatic (P) joints, joints which allow translational motion along an axis. Our two-joint robot is sometimes called an RR or 2R robot, indicating that both joints are revolute. An RP robot, on the other hand, has a revolute joint followed by a prismatic joint. Another common joint is a spherical (ball-and-socket) joint, which has three degrees of freedom.
A closed-chain robot, also known as a parallel mechanism, is one where the links form one or more closed loops. If the mechanism has k links, then one is designated as a stationary “ground” link, and k − 1 links are movable. To determine the number of degrees of freedom, note that each movable link has N degrees of freedom, where N = 6 for a spatial mechanism and N = 3 for a planar mechanism. Therefore the system has N (k − 1) degrees of freedom before the joints are taken into account. Now each of the n joints between the links places N − fi constraints on the feasible motions of the links, where fi is the number of degrees of freedom at joint i (e.g., fi = 1 for a revolute joint, and fi = 3 for a spherical joint). Therefore, the mobility M of the mechanism, or its number of degrees of freedom, is given by

(3.1)

n
M = N (k − 1) − ( N − fi )
i =1 n
= N (k − n − 1) + fi .
i =1

Choset-79066 book February 22, 2005 17:37

50

3 Conﬁguration Space

E

F

C

B

A

D

Figure 3.9 A planar mechanism with six links (A through F), seven revolute joints, and one degree of freedom.
This is known as Gru¨bler’s formula for closed chains, and it is only valid if the constraints due to the joints are independent. In the planar mechanism of ﬁgure 3.9, there are seven joints, each with one degree of freedom, and six links, yielding a mobility of 3(6 − 7 − 1) + 7 = 1.
As an application of the ideas in this section, determine the number of degrees of freedom of your arm by adding the degrees of freedom at your shoulder, elbow, and wrist. To test your answer, place your palm ﬂat down on a table with your elbow bent. Without moving your torso or your palm, you should ﬁnd that it is still possible to move your arm. This internal freedom means that your arm is redundant with respect to the task of positioning your hand (or a rigid body grasped by your hand) in space; an inﬁnity of arm conﬁgurations puts your hand in the same place.4 How many internal degrees of freedom do you have? How many holonomic constraints were placed on your arm’s conﬁguration when you ﬁxed your hand’s position and orientation? The sum of the number of constraints and internal degrees of freedom is the number of degrees of freedom of your (unconstrained) arm, and you should ﬁnd that your arm has more than six degrees of freedom. A robot is said to be hyper-redundant with respect to a task if it has many more degrees of freedom than required for the task. (There is no strict deﬁnition of “many” here.)

3.4 The Topology of the Conﬁguration Space
Now that we understand how to determine the dimension of a conﬁguration space, we can begin to explore its topology and geometry, each of which plays a vital role in developing and analyzing motion-planning algorithms. Some basic concepts from topology are discussed in appendixes B and C.
4. Provided your arm is away from its joint limits.

Choset-79066 book February 22, 2005 17:37

3.4 The Topology of the Conﬁguration Space

51

Figure 3.10 The surfaces of the coffee mug and the torus are topologically equivalent.
Topology is a branch of mathematics that considers properties of objects that do not change when the objects are subjected to arbitrary continuous transformations, such as stretching or bending. For this reason, topology is sometimes referred to as “rubber sheet geometry.” Imagine a polygon drawn on a rubber sheet. As the sheet is stretched in various directions, the polygon’s shape changes; however, certain properties of the polygon do not change. For example, points that are inside the polygon do not move to the outside of the polygon simply because the sheet is stretched.
Two spaces are topologically different if cutting or pasting is required to turn one into the other, as cutting and pasting are not continuous transformations. For example, the conﬁguration spaces of the circular mobile robot (R2) and the two-joint planar arm (T 2) are topologically different. If we imagine T 2 as the surface of a rubber doughnut, we see that no matter how we stretch or deform the doughnut (without tearing it), the doughnut will always have a hole in it. Also, if we imagine R2 as an inﬁnite rubber sheet, there is no way to stretch it (without tearing it) such that a hole will appear in the sheet. To a topologist, all rubber doughnuts are the same, regardless of how they are stretched or deformed (ﬁgure 3.10). Likewise, all rubber sheet versions of R2 are the same.
One reason that we care about the topology of conﬁguration space is that it will affect our representation of the space. Another reason is that if we can derive a pathplanning algorithm for one kind of topological space, then that algorithm may carry over to other spaces that are topologically equivalent (see, e.g., chapter 4, section 4.6).
Since topology is concerned with properties that are preserved under continuous transformations, we begin our study of the topology of conﬁguration spaces by describing two types of continuous transformations: homeomorphisms and diffeomorphisms. Appendix C provides an introduction to differentiable transformations.
3.4.1 Homeomorphisms and Diffeomorphisms
A mapping φ : S → T is a rule that places elements of S into correspondence with elements of T . We respectively deﬁne the image of S under φ and the preimage

Choset-79066 book February 22, 2005 17:37

52

3 Conﬁguration Space

f(s)

f(s)

f(s)

T

T

T

S

s

Surjection

S

s

Injection

S

s

Bijection

f(s)

f(s)

f(s)

T

T

T

S

s

Discontinuous bijection (isomorphism)

S

s

Homeomorphism

S

s

Diffeomorphism

Figure 3.11 Representative ways of looking at surjective, injective, and bijective mappings. Bijections may become homeomorphisms or diffeomorphisms if they are sufﬁciently differentiable.

of T by

φ(S) = {φ(s) | s ∈ S} and φ−1(T ) = {s | φ(s) ∈ T }.

If φ(S) = T (i.e., every element of T is covered by the mapping) then we say that

φ is surjective or onto. If φ puts each element of T into correspondence with at most one element of S, i.e., for any t ∈ T , φ−1(t) consists of at most one element in S,

then we say that φ is injective (one-to-one). If φ is injective, then when s1 = s2 we

have φ(s1) = φ(s2) for s1, s2 ∈ S. Maps that are both surjective and injective are

said to be bijective. Figure 3.11 illustrates these deﬁnitions. As another example, the

map

sin

:

(−

π 2

,

π 2

)

→ (−1,

1) is bijective, whereas sin : R → [−1,

1] is only surjective.

Bijective maps have the property that their inverse exists at all points in the range T ,

and thus they allow us to move easily back and forth between the two spaces S and T .

In our case, we will use bijective maps to move back and forth between conﬁguration

spaces (whose geometry can be quite complicated) and Euclidean spaces.

We will consider two important classes of bijective mappings.

DEFINITION 3.4.1 If φ : S → T is a bijection, and both φ and φ−1 are continuous, then φ is a homeomorphism. When such a φ exists, S and T are said to be homeomorphic.

Choset-79066 book February 22, 2005 17:37

3.4 The Topology of the Conﬁguration Space

53

Circle

Ellipse

Racetrack

Figure 3.12 A circle, an ellipse, and a racetrack.

A mapping φ : U → V is said to be smooth if all partial derivatives of φ, of all orders, are well deﬁned (i.e., φ is of class C∞). With the notion of smoothness, we
deﬁne a second type of bijection.

DEFINITION 3.4.2 A smooth map φ : U → V is a diffeomorphism if φ is bijective and φ−1 is smooth. When such a φ exists, U and V are said to be diffeomorphic.

The condition for diffeomorphisms (smoothness) is stronger that the condition for homeomorphisms (continuity), and thus all diffeomorphisms are homeomorphisms.
To illustrate these ideas, consider three one-dimensional surfaces: a circle, denoted by Mc; an ellipse, denoted by Me; and a “racetrack,” denoted by Mr . The racetrack consists of two half-circles connected by straight lines (ﬁgure 3.12). We deﬁne these shapes mathematically as

(3.2) Mc = {(x, y) | fc(x, y) = x2 + y2 − 1 = 0}

(3.3)

Me

= {(x,

y)

|

fe(x, y) =

x2 4

+ y2 − 1 = 0}

(3.4) Mr = {(x, y) | fr (x, y) = 0}

with



(3.5)

fr (x ,

y)

=

 ( y  ( y

x −1 + 1)2 + x2 − 1)2 + x2

−1 −1

: : :

−1

≤ y ≤ 1, x y < −1 y>1

>

0

.

x +1

: −1 ≤ y ≤ 1, x < 0

Note that these surfaces are implicitly deﬁned as being the set of points that satisfy some equation f (x, y) = 0.
In some ways, these three surfaces are similar. For example, they are all simple, closed curves in the plane; all of fc(x, y), fe(x, y), and fr (x, y) are continuous. In other ways, they seem quite different. For example, both fc(x, y) and fe(x, y) are

Choset-79066 book February 22, 2005 17:37

54

3 Conﬁguration Space

continuously differentiable, while fr (x, y) is not. We can more precisely state the similarities and differences between these surfaces using the concepts of homeomor-

phism and diffeomorphism. In particular, it can be shown that Mc, Me, and Mr are all homeomorphic to each other. For example, the map φ : Me → Mc given by

φ(x, y) =

x ,
x2 + y2

T
y
x2 + y2

is a homeomorphism. For this choice of φ, both φ and φ−1 are smooth, and therefore, Mc is diffeomorphic
to Me. Neither Mc nor Me is diffeomorphic to Mr , however. This is because fr (x, y) is not continuously differentiable, while both fc(x, y) and fe(x, y) are. This can be seen by examining the curvatures of the circle, ellipse, and racetrack. For the circle, the curvature is constant (and thus continuous), and for the ellipse, curvature is continuous. For the racetrack, there are discontinuities in curvature (at the points (−1, 1), (−1, −1), (1, 1), (1, −1)), and therefore there is no smooth mapping from either the circle or the ellipse to the racetrack.
We are often concerned only with the local properties of conﬁguration spaces. Local properties are deﬁned on neighborhoods. For metric spaces5, neighborhoods are most easily deﬁned in terms of open balls. For a point p of some manifold M, we deﬁne an open ball of radius by

B ( p) = { p ∈ M | d( p, p ) < },

where d is a metric on M.6 A neighborhood of a point p ∈ M is any subset U ⊆ M with p ∈ U such that for every p ∈ U, there exists an open ball B ( p ) ⊂ U. Any open ball is itself a neighborhood. The open disk in the plane is an example. For the point (x0, y0) in the plane, an open ball deﬁned by the Euclidean metric is
B (x0, y0) = {(x, y) | (x − x0)2 + ( y − y0)2 < 2}.

We say that S is locally diffeomorphic (resp. locally homeomorphic) to T if for each p ∈ S there exists a diffeomorphism (resp. homeomorphism) f from S to T on some neighborhood U with p ∈ U.
The sphere presents a familiar example of these concepts. At any point on the sphere, there exists a neighborhood of that point that is diffeomorphic to the plane. It is not surprising that people once believed the world was ﬂat — they were only looking at their neighborhoods!

5. A metric space is a space equipped with a distance metric. See appendix C. 6. One can deﬁne all topological properties, including neighborhoods, without resorting to the use of metrics, but for our purposes, it will be easier to assume a metric on the conﬁguration space and exploit the metric properties.

Choset-79066 book February 22, 2005 17:37

3.4 The Topology of the Conﬁguration Space

55

Let us now reﬂect on the two examples from the beginning of this chapter. For the

circular mobile robot, the workspace and the conﬁguration space are diffeomorphic.

This is easy to see, since both are copies of R2. In this case, the identity map φ(x) = x

is a perfectly ﬁne global diffeomorphism between the workspace and conﬁguration

space. In contrast, the two-joint manipulator has a conﬁguration space that is T 2, the

torus. The torus T 2 is not diffeomorphic to R2, but it is locally diffeomorphic. If the

revolute joints in the two-joint manipulator have lower and upper limits, θi < θi < θiu, so that they cannot perform a complete revolution, however, then the conﬁguration

space of the two-joint manipulator becomes an open subset of the torus, which is

diffeomorphic to R2 (globally). This follows from the fact that each joint angle lies

in an open interval of R1, and we can “stretch” that open interval to cover the line.

An

example

of

such

a

stretching

function

is

tan

:

(

−

π 2

,

π 2

)

→ R.

3.4.2 Differentiable Manifolds
For all of the conﬁguration spaces that we have seen so far, we have been able to uniquely specify a conﬁguration by n parameters, where n is the dimension of the conﬁguration space (two for the planar two-joint arm, three for a polygon in the plane, etc.). The reason that we could do so was that these conﬁguration spaces were all “locally like” n -dimensional Euclidean spaces. Such spaces, called manifolds, are a central topic of topology.

DEFINITION 3.4.3 (Manifold) A set S is a k-dimensional manifold if it is locally homeomorphic to Rk, meaning that each point in S possesses a neighborhood that is homeomorphic to an open set in Rk.
While a general k-dimensional manifold is locally homeomorphic to Rk, the conﬁguration spaces that we will consider are locally diffeomorphic to Rk, an even stronger relationship. In fact, when we parameterized conﬁgurations in section 3.1, we were merely constructing diffeomorphisms from conﬁguration spaces to R2. If we construct enough of these diffeomorphisms (so that every conﬁguration in Q is in the domain of at least one of them), and if these diffeomorphisms are compatible with one another (an idea that we will formalize shortly), then this set of diffeomorphisms together with the conﬁguration space deﬁne a differentiable manifold. We now make these ideas more precise.
DEFINITION 3.4.4 (Chart) A pair (U, φ), such that U is an open set in a kdimensional manifold and φ is a diffeomorphism from U to some open set in Rk, is called a chart.

Choset-79066 book February 22, 2005 17:37

56

3 Conﬁguration Space

The use of the term chart is analogous to its use in cartography, since the subset

U is “charted” onto Rk in much the same way that cartographers chart subsets of

the globe onto a plane when creating maps. Charts are sometimes referred to as

coordinate systems because each point in the set U is assigned a set of coordinates in a Euclidean space [410]. The inverse diffeomorphism, φ−1 : Rk → U , is referred to

as a parameterization of the manifold.

As an example, consider the one-dimensional manifold S1 = {x = (x1, x2) ∈ R2 | x12 + x22 = 1}. For any point x ∈ S1 we can deﬁne a neighborhood that is diffeomorphic to R. For example, consider the upper portion of the circle, U1 = {x ∈ S1 | x2 > 0}.
The chart φ1 : U1 → (0, 1) is given by φ1(x) = x1, and thus x1 can be used to

deﬁne a local coordinate system for the upper semicircle. In the other direction, the

upper portion of the circle can be parameterized by z ∈ (0, 1) ⊂ R, with φ1−1(z) =

(z,

(1

−

z)

1 2

),

which

maps

the

open

unit

interval

to

the

upper

semicircle.

But

S1

is

not

globally diffeomorphic to R1; we cannot ﬁnd a single chart whose domain includes

all of S1.

We have already used this terminology in section 3.1, when we referred to θ1, θ2 as parameters that represent a conﬁguration of the two-joint arm. Recall that (θ1, θ2) ∈ R2, and that when considered as a representation of the conﬁguration, they deﬁne a

point in T 2, the conﬁguration space, which is a manifold. We now see that in sec-

tion 3.1, when we represented a conﬁguration of the planar arm by the pair (θ1, θ2), we were in fact creating a chart from a subset of the conﬁguration space to a subset of R2.

A single mapping from T 2 to R2 shown in ﬁgure 3.2 encounters continuity prob-

lems at θi = {0, 2π }. For many interesting conﬁguration spaces, it will be the case

that we cannot construct a single chart whose domain contains the entire conﬁguration

space. In these cases, we construct a collection of charts that cover the conﬁguration

space. We are not free to choose these charts arbitrarily; any two charts in this col-

lection must be compatible for parts of the manifold on which their domains overlap. Two charts with such compatibility are said to be C∞-related (ﬁgure 3.13).

DEFINITION 3.4.5 (C∞-related) Let (U, φ) and (V , ψ) be two charts on a kdimensional manifold. Let X be the image of U ∩ V under φ, and Y be the image of U ∩ V under ψ, i.e.,
X = {φ(x) ∈ Rk | x ∈ U ∩ V } Y = {ψ( y) ∈ Rk | y ∈ U ∩ V }.
These two charts are said to be C∞-related if both of the composite functions
ψ ◦ φ−1 : X → Y, φ ◦ ψ−1 : Y → X
are C∞.

Choset-79066 book February 22, 2005 17:37

3.4 The Topology of the Conﬁguration Space

57

f

U

V

f ° y-1

X

y ° f-1

Y

y

Figure 3.13 The charts (U, φ) and (V, ψ) map open sets on the k-dimensional manifold to open sets in Rk.

If two charts are C∞-related, we can switch back and forth between them in a smooth
way when their domains overlap. This idea will be made more concrete in the example of S1 below.
A set of charts that are C∞-related, and whose domains cover the entire conﬁguration space Q, form an atlas for Q. An atlas is sometimes referred to as the differentiable structure for Q. Together, the atlas and Q comprise a differentiable manifold. There are other ways to deﬁne differentiable manifolds, as we will see in sec-
tion 3.5. As an example, consider again the one-dimensional manifold S1. Above, we deﬁned
a single chart, (U1, φ1). If we deﬁne three more charts, we can construct an atlas for S1. These four charts are given by

U1 = {x ∈ S1 | x2 > 0}, U2 = {x ∈ S1 | x2 < 0}, U3 = {x ∈ S1 | x1 > 0}, U4 = {x ∈ S1 | x1 < 0},

φ1(x) = x1 φ2(x) = x1 φ3(x) = x2 φ4(x) = x2.

The corresponding parameterizations are given by φi−1 : (−1, 1) → Ui , with

φ1−1(z) = (z, 1 − z2) φ2−1(z) = (z, z2 − 1) φ3−1(z) = (1 − z2, z) φ4−1(z) = (z2 − 1, z).

It is clear that the Ui cover S1, so to verify that these charts form an atlas it is only necessary to show that they are C∞-related (ﬁgure 3.14). Note that

Choset-79066 book February 22, 2005 17:37

58

3 Conﬁguration Space

U1

U4

U3

U2 Figure 3.14 Four charts covering the circle S1.

U1 ∩ U2 = U3 ∩ U4 = ∅, so we need only check the four pairs of composite maps:

φ1 ◦ φ3−1 : (0, 1) → (0, 1), φ1 ◦ φ4−1 : (0, 1) → (−1, 0), φ2 ◦ φ3−1 : (−1, 0) → (0, 1), φ2 ◦ φ4−1 : (−1, 0) → (−1, 0),

φ3 ◦ φ1−1 : (0, 1) → (0, 1) φ4 ◦ φ1−1 : (−1, 0) → (0, 1) φ3 ◦ φ2−1 : (0, 1) → (−1, 0) φ4 ◦ φ2−1 : (−1, 0) → (−1, 0).

In

each

case,

φi

◦

φ

−1 j

(z)

is

smooth

on

each

of

the

open

unit

intervals

that

deﬁne

the

domains for the composite mappings given above. For example, φ1 ◦ φ3−1(z) = 1 − z2.

This collection of four charts is not minimal; it is straightforward to ﬁnd two charts

to cover S1 (see problem 9).

3.4.3 Connectedness and Compactness
We say that a manifold is path-connected, or just connected, if there exists a path between any two points of the manifold.7 All of the obstacle-free conﬁguration spaces that we will consider in this text, e.g., Rn, Sn, and T n, are connected. The presence of obstacles, however, can disconnect the free conﬁguration space Qfree. In this case, the free conﬁguration space is broken into a set of connected components, the maximal connected subspaces. In ﬁgure 3.5(c), for example, obstacles break the mobile robot’s free conﬁguration space into two connected components. There can be no solution to a motion-planning problem if qstart and qgoal do not lie in the same connected component of Qfree.
7. For more general spaces, the concepts of path-connectedness and connectedness are not equivalent, but for a manifold they are the same. More generally, a space is connected if there is no continuous mapping from the space to a discrete set of more than one element.

Choset-79066 book February 22, 2005 17:37

3.5 Embeddings of Manifolds in Rn

59

A space is compact8 if it resembles a closed, bounded subset of Rn. A space is closed if it includes all of its limit points. As examples, the half-open interval [0, 1) ⊂ R is bounded but not compact, while the closed interval [0, 1] is bounded and compact. The space Rn is not bounded and therefore not compact. The spaces Sn and T n are both compact, as they can be expressed as closed and bounded subsets of Euclidean spaces. The unit circle S1, e.g., can be expressed as a closed and bounded subset of R2.
In conﬁguration spaces with obstacles or joint limits, the modeling of the obstacles may affect whether the space is compact or not. For a revolute joint subject to joint limits, the set of joint conﬁgurations is compact if the joint is allowed to hit the limits, but not compact if the joint can only approach the limits arbitrarily closely.
The product of compact conﬁguration spaces is also compact. For a noncompact space M1 × M2, if M1 is compact, then it is called the compact factor of the space. Compact and noncompact spaces cannot be diffeomorphic.
3.4.4 Not All Conﬁguration Spaces Are Manifolds
We are focusing on conﬁguration spaces that are manifolds, and more speciﬁcally differentiable manifolds, but it is important to keep in mind that not all conﬁguration spaces are manifolds. As a simple example, the closed unit square [0, 1] × [0, 1] ⊂ R2 is not a manifold, but a manifold with boundary obtained by pasting the onedimensional boundary on the two-dimensional open set (0, 1) × (0, 1). Also, some parallel mechanisms with one degree of freedom have conﬁgurations from which there are two distinct possible motion directions (i.e., the conﬁguration space is a self-intersecting ﬁgure eight). It is beyond the scope of this chapter to discuss other types of conﬁguration spaces, but be aware: if you cannot show it to be a manifold, it may not be!

3.5 Embeddings of Manifolds in Rn
Although a k-dimensional manifold can be represented using as few as k parameters, we have seen above that doing so may require multiple charts. An alternative is to use a representation with “extra” numbers, subject to constraints, to achieve a single global representation. As an example, S1 is a one-dimensional manifold that we can
8. In topology, a space is deﬁned to be compact if every open cover of the space admits a ﬁnite subcover, but we will not use these concepts here.

Choset-79066 book February 22, 2005 17:37

60

3 Conﬁguration Space

represent as S1 = {(x, y) | x2 + y2 = 1}; we “embed” S1 in R2. The fact that we cannot ﬁnd a single chart for all of S1 tells us that we cannot embed S1 in R1. Likewise, although the torus T 2 is a two-dimensional manifold, it is not possible to embed the torus in R2, which is why we typically illustrate the torus as a doughnut shape in R3.
The manifolds S1 and T 2 can be viewed as submanifolds of R2 and R3, respectively. Submanifolds are smooth subsets of an ambient space that inherit the differentiability properties of the ambient space. Submanifolds are often created by a smooth set of equality constraints on Rn, as we see in the example of the circle S1 above. Any differentiable manifold can be viewed as an embedded submanifold of Rn for large enough n .
When we are confronted with a conﬁguration space that does not permit a single global coordinate chart, we are faced with a choice. We can either use a single set of parameters and suffer the consequences of singularities and discontinuities in the representation, use multiple charts to construct an atlas, or use a single global representation by embedding the conﬁguration space in a higher-dimensional space. One advantage of the last approach is that the representation can facilitate other operations. Important examples are representations of orientation using complex numbers and quaternions (see appendix E) and matrix representations for the conﬁguration of a rigid body in the plane or in space, as discussed next.
3.5.1 Matrix Representations of Rigid-Body Conﬁguration
It is often convenient to represent the position and orientation of a rigid body using an m × m matrix of real numbers. The m2 entries of this matrix must satisfy a number of smooth equality constraints, making the manifold of such matrices a submanifold of Rm2 . One advantage of such a representation is that these matrices can be multiplied to get another matrix in the manifold. More precisely, these matrices form a group with the group operation of matrix multiplication.9 Simple matrix multiplication can be used to change the reference frame of a representation or to rotate and translate a conﬁguration.
We describe the orientation of a rigid body in n-dimensional space (n = 2 or 3) by the matrix groups S O(n), and the position and orientation by the matrix groups S E(n). After describing these representations abstractly, we look in detail at examples
9. In fact, the matrix representations in this section are Lie groups, as (1) they are differentiable manifolds which are also groups, (2) the group operation is C∞, and (3) the mapping from an element of the group to its inverse is C∞.

Choset-79066 book February 22, 2005 17:37

3.5 Embeddings of Manifolds in Rn

61

zs

~z ~y

ys

xs

~x

Figure 3.15 The rotation matrix for a body is obtained by expressing the unit vectors x˜, y˜, and z˜ of the body x-y-z frame in a stationary frame xs-ys-zs.
of the use of S E(n) for representing the conﬁguration of a body, for changing the reference frame of the representation, and for translating and rotating a conﬁguration.

Orientation: S O(2) and S O(3)

Figure 3.15 shows a rigid body with a frame x-y-z attached to it. Our representation

of the orientation of the body will be as a 3 × 3 matrix







x˜1 y˜1 z˜1

R11 R12 R13

R =  x˜2 y˜2 z˜2  =  R21 R22 R23  ∈ S O(3),

x˜3 y˜3 z˜3

R31 R32 R33

where x˜ = [x˜1, x˜2, x˜3]T is the unit vector in the body x-direction expressed in a stationary coordinate frame xs-ys-zs. The vectors y˜ and z˜ are deﬁned similarly (see ﬁgure 3.15).
The matrix R is often called the rotation matrix representation of the orientation. This representation uses nine numbers to represent the three angular degrees of freedom, so there are six independent constraints on the matrix entries: each column (and row) is a unit vector,

||x˜|| = ||y˜|| = ||z˜|| = 1,

yielding three constraints, and the columns (and rows) are orthogonal to each other, x˜ T y˜ = y˜ T z˜ = z˜T x˜ = 0,

Choset-79066 book February 22, 2005 17:37

62

3 Conﬁguration Space

yielding three more constraints. Because we are assuming right-handed frames,10 the determinant of R is +1. Matrices satisfying these conditions belong to the special orthogonal group of 3 × 3 matrices S O(3). “Special” refers to the fact that the determinant is +1, not −1.
In the planar case, R is the 2 × 2 matrix

R=

x˜ 1 x˜ 2

y˜ 1 y˜ 2

=

cos θ sin θ

−sinθ cos θ

∈ S O(2),

where θ is the orientation of the x-y frame relative to the xs-ys frame. Generalizing, orientations in n-dimensional space can be written

S O(n) = {R ∈ Rn×n | R RT = I, det( R) = 1},

where I is the identity matrix. This implies the relation RT = R−1.

Position and Orientation: S E(2) and S E(3)
Figure 3.16 shows a rigid body with an attached x-y-z coordinate frame relative to a stationary frame xs-ys-zs. Let p = [ p1, p2, p3]T ∈ R3 be the vector from the origin of the stationary frame to the body frame, as measured in the stationary frame, and let R ∈ S O(3) be the rotation matrix as described above, as if the body frame were translated back to the stationary frame. Then we represent the position and orientation of the body frame relative to the stationary frame as the 4 × 4 transform matrix
T = R p ∈ S E(3), 01
where the bottom row consists of three zeros and a one. (These “extra” numbers will be needed to allow us to perform matrix multiplications, as we will see shortly.) Since R and p both have three degrees of freedom, the conﬁguration of a rigid body in three-space has six degrees of freedom, as we discovered earlier in the chapter.
Generalizing, the position and orientation of a rigid body in n-dimensional space can be written as an element of the special Euclidean group S E(n):
S E(n) ≡ S O(n) Rn , 01
where the bottom row consists of n zeros and a one.
10. To make a right-handed frame, point straight ahead with your right index ﬁnger, point your middle ﬁnger 90 degrees to the left, and stick your thumb straight up. Your index ﬁnger is pointing in the +x direction, your middle ﬁnger is pointing in the +y direction, and your thumb is pointing in the +z direction.

Choset-79066 book February 22, 2005 17:37

3.5 Embeddings of Manifolds in Rn

63

zs

~z

~y

~x p
ys

xs

Figure 3.16 The body frame x-y-z relative to a stationary world frame xs-ys-zs.

Uses of the Matrix Representations The matrix groups S O(n) and S E(n) can be used to 1. represent rigid-body conﬁgurations,

2. change the reference frame for the representation of a conﬁguration or a point, and

3. displace (move) a conﬁguration or a point.

When the matrix is used for representing a conﬁguration, we often call it a frame.

When it is used for displacement or coordinate change, we often call it a transform.

The various uses are best demonstrated by example.

Figure 3.17 shows three coordinate frames on a regular grid of unit spacing. These

frames are conﬁned to a plane with their z-axes pointing out of the page. Let TAB be

the conﬁguration of frame B relative to frame A, and let TBC be the conﬁguration of

frame C relative to frame B. It is clear from the ﬁgure that

 −1 0 0 −2 

 0 1 0 −4 

TAB =

RAB 0

pAB 1

= 

0 0

−1 0

0 1

0 0



,

TBC

=



−1 0

0 0

0 1

−1 0



.

0 00 1

000 1

From these, we can ﬁnd TAC , the frame C relative to the frame A, by performing a change of reference frame on TBC . This involves premutliplying by TAB, based on

Choset-79066 book February 22, 2005 17:37

64

3 Conﬁguration Space

~xB B ~yB

~xC

~yA

~yC C

A ~xA

w

Figure 3.17 Three frames in a plane with their z-axes pointing out of the page.

the rule for coordinate transformations that the second subscript of the matrix on the left cancels with the ﬁrst subscript of the matrix on the right, if they are the same subscript. In other words,

TAB TBC = TAB TBC = TAC .

We ﬁnd that

 −1

0 0 −2   0 1 0 −4 

 0

−1

0

 2

TAC = 

0 0

−1 0

0 1

0 0





−1 0

0 0

0 1

−1 0



=



1 0

0 0

0 1

1 0



,

0 00 1 000 1

0 001

which we can verify by inspection. The representation of the point w in the coordinates of frame C is written wC . From
ﬁgure 3.17, we can see that the coordinates of w in C are [−2, 1, 0]T . To facilitate matrix multiplications, however, we will express points in homogeneous coordinates by appending a 1 to the end of the vector, i.e.,

wC = [−2, 1, 0, 1]T .

To ﬁnd the representation of the point w in other frames, we use a modiﬁcation of the subscript canceling rule to get

TBC wC = TBC wC = w B = [−3, 1, 0, 1]T

and

TAB TBC wC = TAC wC = wA = [1, −1, 0, 1]T ,

which can be veriﬁed by inspection. Elements of S E(n) can also be used to displace a point. For example, TABwA does
not satisfy the subscript canceling rule, and the result is not simply a representation

Choset-79066 book February 22, 2005 17:37

3.5 Embeddings of Manifolds in Rn

65

w'

BA

A

w

(a)

(b)

(c)

(d)

Figure 3.18 Displacing a point by the transformation TAB. (a) The frames A and B and the point w. (b) Rotating frame A to the orientation of frame B, carrying the point w along with it. (c) Translating frame A to the location of frame B, carrying w along with it. (d) The ﬁnal point wA = TAB wA.

of the point w in a new frame. Instead, the point w is rotated about the origin of the frame A by RAB (expressed in the A frame), and then translated by pAB in the A frame. This is the same motion required to take frame A to frame B. The result is

wA = TAB wA = [−3, 1, 0, 1]T ,

the location of the transformed point in the frame A. This transformation is shown graphically in ﬁgure 3.18.
Finally, we can use elements of S E(3) to displace frames, not just points. For example, given a frame B represented by TAB relative to frame A, and a transform T1 ∈ S E(3), then

TAB = TAB T1 =

RAB R1 0

RAB p1 + pAB 1

is the representation of the transformed frame B relative to A after rotating B about its origin by R1 (expressed in the B frame) and then translating by p1 in the original B frame (before it was rotated). On the other hand,

TAB = T1TAB =

R1 RAB 0

R1 pAB + p1 1

is the representation of the transformed frame B relative to A after rotating B about the origin of A by R1 (expressed in the A frame) and then translating by p1 in the A frame. Note that TAB and TAB are generally different, as matrix multiplication is not commutative.
If we consider frame B to be attached to a moving body, we call T1 a body-frame transformation if it is multiplied on the right, as the rotation and translation are expressed relative to the body frame B. If A is a stationary world frame, we call T1 a world-frame transformation if it is multiplied on the left, as the rotation and translation

Choset-79066 book February 22, 2005 17:37

66

3 Conﬁguration Space

B A

A B'

B'' A

Figure 3.19 (a) The initial frame B relative to A. (b) B is obtained by rotating about B and then translating in the original yB-direction. (c) B is obtained by rotating about A and then translating in the yA-direction.

are expressed relative to the ﬁxed A frame. An example is shown in ﬁgure 3.19 for

 −1 0 0 −2 

 −1

 000

TAB = 

0 0

−1 0

0 1

0 0



T1 = 

0 0

−1 0

0 1

1 0



,

0 00 1

0 001

giving

 1

0

0

−2 

TAB

=

TAB T1

=



0 0

1 0

0 1

−1 0



000 1





1002

TAB

=

T1 TA B

=



0 0

1 0

0 1

1 0



.

0001

Applying n world-frame transformations yields TAB = Tn . . . T2T1TAB, while n body-frame transformations yields TAB = TAB T1T2 . . . Tn.

3.6 Parameterizations of S O(3)
We have seen that the nine elements Ri j of a rotation matrix R ∈ S O(3) are subject to six constraints, leaving three rotational degrees of freedom. Thus, we expect that S O(3) can be locally parameterized using three variables. Euler angles are a common parameterization. However, just as we see we cannot ﬁnd a global parameterization for a circle with a single variable, we cannot build a global parameterization of S O(3) with Euler angles.
Given two coordinate frames F0 and F1, we can specify the orientation of frame F1 relative to frame F0 by three angles (φ, θ, ψ), known as Z-Y-Z Euler angles. These Euler angles are deﬁned by three successive rotations as follows. Initially, the two

Choset-79066 book February 22, 2005 17:37

3.6 Parameterizations of S O(3)

z0, za

zb, z1

67
y1 ya, yb

x0

xa

xb

x1

Figure 3.20 Euler angle representation.

frames are coincident. Rotate F0 about the z-axis by the angle φ to obtain frame Fa. Next, rotate frame Fa about its y-axis by the angle θ to obtain frame Fb. Finally, rotate frame Fb about its z-axis by the angle ψ to obtain frame F1. This is illustrated in ﬁgure 3.20.
The corresponding rotation matrix R can thus be generated by successive multiplication of rotation matrices that deﬁne rotations about coordinate axes,

(3.6) (3.7) (3.8)

R = Rz,φ Ry,θ Rz,ψ





cφ −sφ 0

cθ

=  sφ cφ 0   0





0 sθ cψ −sψ 0

1 0   sψ cψ 0 

0 01 
cφcθ cψ − sφsψ =  sφcθ cψ + cφsψ

−sθ 0 cθ 0 0 1 
−cφcθ sψ − sφcψ cφsθ −sφcθ sψ + cφcψ sφsθ  .

−sθ cψ

sθ sψ

cθ

Note that successive rotation matrices are multiplied on the right, as successive rotations are deﬁned about axes in the changing “body” frame.
Parameterization of S O(3) using Euler angles, along with some other representations of S O(3), are described in detail in appendix E.

Choset-79066 book February 22, 2005 17:37

68

3 Conﬁguration Space

3.7 Example Conﬁguration Spaces
In most cases, we can model robots as rigid bodies, articulated chains, or combinations of these two. Some common robots and representations of their conﬁguration spaces are given in table 3.1.
When designing a motion planner, it is often important to understand the underlying structure of the robot’s conﬁguration space. In particular, we note the following.
S1 × S1 × · · · × S1 (n times) = T n, the n-dimensional torus
S1 × S1 × · · · × S1 (n times) = Sn, the n-dimensional sphere in Rn+1
S1 × S1 × S1 = S O(3)
S E(2) = R3
S E(3) = R6 It is sometimes important to know whether a manifold is compact. The manifolds Sn, T n, and S O(n) are all compact, as are all of their direct products. The manifolds Rn and S E(n) are not compact, and therefore Rn × M is not compact, regardless of whether or not the manifold M is compact. Despite their differences, all of these conﬁguration spaces have an important similarity. When equipped with an atlas, each is a differentiable manifold. In particular, R1 and S O(2) are one-dimensional manifolds;
R2, S2 and T 2 are two-dimensional manifolds;

Type of robot Mobile robot translating in the plane Mobile robot translating and rotating in the plane Rigid body translating in the three-space A spacecraft An n-joint revolute arm A planar mobile robot with an attached n-joint arm

Representation of Q R2
S E(2) or R2 × S1
R3 S E(3) or R3 × S O(3)
Tn S E(2) × T n

Table 3.1 Some common robots and their conﬁguration spaces.

Choset-79066 book February 22, 2005 17:37

3.8 Transforming Conﬁguration and Velocity Representations

69

R3, S E(2) and S O(3) are three-dimensional manifolds;
R6, T 6 and S E(3) are six-dimensional manifolds. Thus, for example, all of R3, S E(2), and S O(3) can be represented locally by a set of three coordinates.

3.8 Transforming Conﬁguration and Velocity Representations

We often need to transform from one representation of the conﬁguration of a robot

q ∈ Q to some other representation x ∈ M. A common example occurs when q

represents the joint angles of a robot arm and x represents the conﬁguration of the

end effector as a rigid body in the ambient space. The representation x is more

convenient when planning manipulation tasks in the world, but control of the robot

arm is more easily expressed in q variables, so we need an easy way of switching back

and forth. It is often the case that Q and M are not homeomorphic; the dimension of

the two spaces may not even be equal.

Using the robot arm as inspiration, we deﬁne the forward kinematics map φ : Q → M and the inverse kinematics map φ−1 : M → Q. These maps may not be homeo-

morphisms even if the dimensions of Q and M are equal. As the robot system moves,

the

time

derivative

x˙

=

dx dt

is

related

to

the

time

derivative

q˙

=

dq dt

by

x˙

=

∂φ ∂q

q˙

=

J (q)q˙,

where J is the Jacobian of the map φ, also known as the differential Dφ (see appendix C). The Jacobian is also useful for transforming forces expressed in one set of coordinates to another (see chapter 4, section 4.7, and chapter 10).

EXAMPLE 3.8.1 The 2R robot arm of ﬁgure 3.21 has link lengths L1 and L2. Its conﬁguration space is Q = T 2, and we represent the conﬁguration by the two joint angles q = [θ1, θ2]T . The endpoint of the hand in the Cartesian space is x = [x1, x2]T ∈ M ⊂ R2. In this case, the dimensions of Q and M are equal, but they are not homeomorphic. The forward kinematics map φ : Q → M is

φ(q) =

φ1 (q ) φ2 (q )

=

L1 cos θ1 + L2 cos(θ1 + θ2) L1 sin θ1 + L2 sin(θ1 + θ2)

.

The inverse kinematics map φ−1 is one-to-two at most points of M, meaning that the robot can be chosen to be in either the right-arm or left-arm conﬁguration. The inverse kinematics of the 2R arm is most easily found geometrically using the law of cosines and is left for problem 20.

Choset-79066 book February 22, 2005 17:37

70

3 Conﬁguration Space

(x1, x2) L2

q2

L1

q1

Figure 3.21 The 2R robot arm and the velocity at its endpoint.

The Jacobian of the forward kinematics map is

J (q)

=

∂φ ∂q

=

∂ φ1 ∂ θ1
∂ φ2

∂ φ1 ∂ θ2
∂ φ2

∂θ1 ∂θ2

=

−L1 sin θ1 − L2 sin(θ1 + θ2) L1 cos θ1 + L2 cos(θ1 + θ2)

−L2 sin(θ1 + θ2) L2 cos(θ1 + θ2)

.

Plugging in L1 = L2 = 1, θ1 = π/4, θ2 = π/2, and q˙ = [1, 0]T , as shown in

ﬁgure 3.21, we ﬁnd that

√√

x˙ = J (q)q˙ =

−2 0

−√2/2 − 2/2

√

1 = −2,

0

0

matching the motion seen in the ﬁgure. When sin θ2 = 0, the Jacobian J (q) loses rank, and the robot is said to be in a
singular conﬁguration. In this case, the two-dimensional set of joint velocities q˙ maps to a one-dimensional set of endpoint velocities x˙ — instantaneous endpoint motion is impossible in one direction.

EXAMPLE 3.8.2 A polygon moving in the plane is represented by the conﬁguration q = [q1, q2, q3]T ∈ Q = R2 × S1, where (q1, q2) gives the position of a reference frame Fp attached to the polygon relative to a world frame F , and q3 gives the

Choset-79066 book February 22, 2005 17:37

Problems

71 (x1, x2)

r

Fp

q3

(q1, q2)

F
Figure 3.22 The point on the polygon is at r in the polygon frame Fp and x in the world frame F.

orientation of Fp relative to F (see ﬁgure 3.22). A point is ﬁxed on the polygon at r = [r1, r2]T in the polygon frame Fp, and let x = [x1, x2]T ∈ M = R2 be the
position of this point in the plane. Then the forward kinematics mapping is

x1 = φ(q) = q1 + cos q3 −sinq3 r1 ,

x2

q2

sin q3 cos q3 r2

where we recognize the 2 × 2 rotation matrix. The inverse map φ−1 in this example is one-to-many, as the dimension of Q is greater than the dimension of M. The velocities x˙ and q˙ are related by the Jacobian

J (q)

=

∂φ ∂q

=

∂ φ1 ∂ q1
∂ φ2

∂ q1

∂ φ1 ∂ q2
∂ φ2 ∂ q2

∂ φ1 ∂ q3
∂ φ3 ∂ q2

=

1 0

0 1

−r1 sin q3 − r2 cos q3 r1 cos q3 − r2 sin q3

.

Problems
1. Invent your own nontrivial robot system. It could consist of one or more robot arms, mobile platforms, conveyor belts, ﬁxed obstacles, movable objects, etc. Describe the conﬁguration space mathematically. Explain whether or not the conﬁguration space is compact, and if not, describe the compact factors. Describe the connected components of the conﬁguration space. Draw a rough picture of your robot system.

Choset-79066 book February 22, 2005 17:37

72

3 Conﬁguration Space

2. Give the dimension of the conﬁguration spaces of the following systems. Explain your answers.
(a) Two mobile robots rotating and translating in the plane. (b) Two translating and rotating planar mobile robots tied together by a rope. (c) Two translating and rotating planar mobile robots connected rigidly by a bar. (d) The two arms of a single person (with torso stationary) holding on ﬁrmly to a car’s
steering wheel. (e) A train on train tracks. What if we include the wheel angles? (The wheels roll without
slipping.) (f) A spacecraft with a 6R robot arm. (g) The end effector of the 6R robot arm of a spacecraft. (h) Your legs as you pedal a bicycle (remaining seated with feet ﬁxed to the pedals). (i) A sheet of paper.
3. Describe the Bug2 algorithm for a two-joint manipulator. What are the critical differences between the Bug2 algorithm for a mobile base and the two-joint manipulator? What does a straight line mean in the arm’s conﬁguration space? Can Bug2 be made to work for the two-joint arm?
4. Prove the conﬁguration space obstacle of a convex mobile robot translating in a plane with a convex obstacle is convex.
5. Prove the union operator propagates from the workspace to the conﬁguration space. That is, the union of two conﬁguration space obstacles is the conﬁguration space obstacle of the union of two workspace obstacles. In other words, assuming Q is a conﬁguration space operator, show that
Q(WOi WO j ) = QOi QO j .
6. How many degrees of freedom does a rigid body in n-space have? How many of them are rotational? Prove these two ways: (a) using the method of choosing a number of points on the body and sequentially adding their independent degrees of freedom until each point on the body is ﬁxed, and (b) using the deﬁnitions of S E(n) and S O(n).
7. Use cardboard and pushpins to create a closed chain with four links, a four-bar mechanism. One of these bars is considered stationary, or ﬁxed to the ground. Going around the loop, the link lengths between joints are 6.5 (the ground link), 3.0, 1.5, and 3.0 inches (or centimeters) in length. Poke a hole at the midpoint of the 1.5 inch link and trace the path that the hole traces. Describe a good representation of the conﬁguration space of the linkage.
8. Give a homeomorphism from the racetrack to the ellipse in ﬁgure 3.12.
9. Find two charts for the unit circle S1 and prove they form an atlas.

Choset-79066 book February 22, 2005 17:37

Problems

73

10. Explain why the latitude-longitude chart we often place on the Earth is not a global parameterization. Find two charts for the sphere and prove that they form an atlas.
11. The set of right-arm and left-arm conﬁgurations of the 2R manipulator in ﬁgure 3.3 each give an annulus of reachable positions by the end effector, neither of which is diffeomorphic to the robot’s conﬁguration space. Consider the right-arm and left-arm workspaces as two separate annuluses, and describe how they can be glued together to make a single space that is a valid representation of the conﬁguration space. Comment on the topology of this glued space.
12. For the 2R manipulator of ﬁgure 3.7, how many connected components of free conﬁguration space are there? Copy the ﬁgure, color each of the connected components a different color, and give a drawing of the robot in each of these connected components.
13. Show that compact and noncompact spaces are never diffeomorphic.
14. Find a diffeomorphism from any open interval (a, b) ∈ R to the whole real line R.
15. Give an implicit constraint equation f (x, y, z) = 0 that embeds a torus in R3.
16. For T ∈ S E(3) consisting of the rotation matrix R ∈ S O(3) and the translation p ∈ R3, ﬁnd the inverse transform T −1, so that T T −1 = T −1T = I. Your answer should not contain any matrix inverses.
17. Consider two three-dimensional frames aligned with each other, called A and B. Rotate B 90 degrees about the x-axis of A, then rotate again by 90 degrees about the y-axis of A, then move the origin of B by three units in the z-direction of A. (Make sure you rotate in the right direction! Use the right-hand rule: thumb points along the positive axis, ﬁngers curl in the direction of positive rotation.) Give the matrix TAB ∈ S E(3) describing the frame B relative to the frame A. Consider the point xB = [4, 3, 1, 1]T in homogeneous coordinates in frame B. What is the point xA (expressed in the frame A)? Consider the point yA = [1, 2, 3, 1]T in homogeneous coordinates in frame A, and perform the transformation TAB . Where is the new point yA?
18. Write a program to calculate the conﬁguration space for a convex polygonal robot translating in an environment with convex obstacles. The program should read in from a ﬁle a counterclockwise list of vertices representing the robot, where (0, 0) is the robot reference point. From a second ﬁle, the program should read in a set of obstacles in the workspace. The user enters an orientation for the robot and the program calculates the conﬁguration space obstacles (see, e.g., appendix F). Display the conﬁguration space for different orientations of the robot to show that translating paths between two points may exist for some orientations of the robot, but not for others.
19. Write a program to display the conﬁguration space for a 2R manipulator in a polygonal environment. The program should read in a ﬁle containing the location of the base of the robot, the length of the links, and the lists of vertices representing the obstacles. The

Choset-79066 book February 22, 2005 17:37

74

3 Conﬁguration Space

L3

q3

L2

u1

q2 q1

L1

fixed base

Figure 3.23 A 3R planar robot with a frame attached to the end effector.
program should check for collision at 1 degree increments for each joint and create a plot similar to that shown in ﬁgure 3.7.
20. Find the inverse kinematics φ−1 mapping the end-effector coordinates x to the joint coordinates q for the 2R robot arm in example 3.8.1. Note that for most reachable points x, there will be two solutions, corresponding to the right- and left-arm conﬁgurations. Your solution will likely make use of the two-argument arctangent atan2(x2, x1), which returns the unique angle in [−π, π ) to the point (x1, x2) in the plane, as well as the law of cosines a2 = b2 + c2 − 2bc cos A, where a, b, and c are the lengths of the three edges of a triangle and A is the angle opposite to edge a. Solve for general L1, L2 (do not plug in numbers).
21. Give the forward kinematics φ for the planar 3R arm shown in ﬁgure 3.23, from joint angles q to the position and orientation of the end effector frame in the plane. Find the manipulator Jacobian.
22. For the problem above, show that the forward kinematics mapping is injective, surjective, bijective, or none of these, when viewed as a mapping from T 3 to R2 × S1. Find a “large” set of joint angle ranges U ⊂ T 3 and a set of end-effector conﬁgurations V ⊂ R2 × S1 for which the mapping is a diffeomorphism.
23. The topology of S E(2) is equivalent to R2 × S O(2). Let’s represent an element of R2 × S O(2) by (x, R), where x ∈ R2, R ∈ S O(2). As we have seen, we can make S E(2) a group by giving it a group operation, namely, matrix multiplication. We can also make

Choset-79066 book February 22, 2005 17:37

Problems

75

R2 × S O(2) a group by using the direct product structure to deﬁne composition of two elements:
(x1, R1)(x2, R2) = (x1 + x2, R1 R2) ∈ R2 × S O(2)
We are using vector addition as the group operation on R2 and matrix multiplication on S O(2). With this group operation, is R2 × S O(2) commutative? Is S E(2) commutative? The spaces S E(2) and R2 × S O(2) are topologically equivalent, but are they equivalent as groups?

Choset-79066 book February 22, 2005 17:43
4 Potential Functions

HAVING SEEN the difﬁculty of explicitly representing the conﬁguration space, an alter-

native is to develop search algorithms that incrementally “explore” free space while

searching for a path. Already, we have seen that the Bug algorithms maneuver through

free space without constructing the conﬁguration space, but the Bug algorithms are

limited to simple two-dimensional conﬁguration spaces. Therefore, this chapter intro-

duces navigation planners that apply to a richer class of robots and produce a greater

variety of paths than the Bug methods, i.e., they apply to a more general class of

conﬁguration spaces, including those that are multidimensional and non-Euclidean.

A potential function is a differentiable real-valued function U : Rm → R. The value

of a potential function can be viewed as energy and hence the gradient of the potential

is force. The gradient

is a vector ∇U (q)

=

DU (q )T

=

[

∂U ∂ q1

(q

),

...,

∂U ∂ qm

(

q

)

]T

which

points in the direction that locally maximally increases U . See appendix C.5 for a

more rigorous deﬁnition of the gradient. We use the gradient to deﬁne a vector ﬁeld,

which assigns a vector to each point on a manifold. A gradient vector ﬁeld, as its name

suggests, assigns the gradient of some function to each point. When U is energy, the

gradient vector ﬁeld has the property that work done along any closed path is zero.

The potential function approach directs a robot as if it were a particle moving

in a gradient vector ﬁeld. Gradients can be intuitively viewed as forces acting on a

positively charged particle robot which is attracted to the negatively charged goal.

Obstacles also have a positive charge which forms a repulsive force directing the robot

away from obstacles. The combination of repulsive and attractive forces hopefully

directs the robot from the start location to the goal location while avoiding obstacles

(ﬁgure 4.1).

Choset-79066 book February 22, 2005 17:43

78
+
qstart

4 Potential Functions

+++++++

+

+

+

+

+

+++++++

+

+

+

+

qgoal

Figure 4.1 The negative charge attracts the robot and the positive charge repels it, resulting in a path, denoted by the dashed line, around the obstacle and to the goal.

Note that in this chapter, we mainly deal with ﬁrst-order systems (i.e., we ignore dynamics), so we view the gradients as velocity vectors instead of force vectors. Potential functions can be viewed as a landscape where the robots move from a “high-value” state to a “low-value” state. The robot follows a path “downhill” by following the negated gradient of the potential function. Following such a path is called gradient descent, i.e.,

c˙(t) = −∇U (c(t)).

The robot terminates motion when it reaches a point where the gradient vanishes, i.e., it has reached a q∗ where ∇U (q∗) = 0. Such a point q∗ is called a critical point of U . The point q∗ is either a maximum, minimum, or a saddle point (ﬁgure 4.2).

One can look at the second derivative to determine the type of critical point. For

real-valued functions, this second derivative is the Hessian matrix

 ∂2U  ∂q...12

... ...

∂2U 

∂

q1 ∂
...

qn



.

∂2U ∂ q1 ∂ qn

···

∂2U ∂ qn2

When the Hessian is nonsingular at q∗, the critical point at q∗ is non-degenerate,

implying that the critical point is isolated [173]. When the Hessian is positive-deﬁnite,

the critical point is a local minimum; when the Hessian is negative-deﬁnite, then

Choset-79066 book February 22, 2005 17:43

4 Potential Functions

79

(Maximum)

(Saddle)

(Minimum)

Figure 4.2 Different types of critical points: (Top) Graphs of functions. (Bottom) Gradients of functions.

the critical point is a local maximum. Generally, we consider potential functions whose Hessians are nonsingular, i.e., those which only have isolated critical points. This also means that the potential function is never ﬂat.
For gradient descent methods, we do not have to compute the Hessian because the robot generically terminates its motion at a local minimum, not at a local maximum or a saddle point. Since gradient descent decreases U, the robot cannot arrive at a local maximum, unless of course the robot starts at a maximum. Since we assume that the function is never ﬂat, the set of maxima contains just isolated points, and the likelihood of starting at one is practically zero. However, even if the robot starts at a maximum, any perturbation of the robot position frees the robot, allowing the gradient vector ﬁeld to induce motion onto the robot. Arriving at a saddle point is also unlikely, because they are unstable as well. Local minima, on the other hand, are stable because after any perturbation from a minimum, gradient descent returns the robot to the minimum. Therefore, the only critical point where the robot can generically terminate is a local minimum. Hopefully this is where the goal is located. See ﬁgure 4.3 for an example of a conﬁguration space with its corresponding potential function, along with its energy surface landscape and gradient vector ﬁeld.

Choset-79066 book February 22, 2005 17:43

80

4 Potential Functions

qstart qgoal

(a)

(b)

(c)

(d)

Figure 4.3 (a) A conﬁguration space with three circular obstacles bounded by a circle. (b) Potential function energy surface. (c) Contour plot for energy surface. (d) Gradient vectors for potential function.

There are many potential functions other than the attractive/repulsive potential. Many of these potential functions are efﬁcient to compute and can be computed online [234]. Unfortunately, they all suffer one problem—the existence of local minima not corresponding to the goal. This problem means that potential functions may lead the robot to a point which is not the goal; in other words, many potential functions do not lead to complete path planners. Two classes of approaches address this problem: the ﬁrst class augments the potential ﬁeld with a search-based planner, and the second deﬁnes a potential function with one local minimum, called a navigation function [239]. Although complete (or resolution complete), both methods require full knowledge of the conﬁguration space prior to the planning event.
Finally, unless otherwise stated, the algorithms presented in this chapter apply to spaces of arbitrary dimension, even though the ﬁgures are drawn in two dimensions. Also, we include some discussion of implementation on a mobile base operating in the plane (i.e., a point in a two-dimensional Euclidean conﬁguration space).

4.1 Additive Attractive/Repulsive Potential
The simplest potential function in Qfree is the attractive/repulsive potential. The intuition behind the attractive/repulsive potential is straightforward: the goal attracts the robot while the obstacles repel it. The sum of these effects draws the robot to the goal while deﬂecting it from obstacles. The potential function can be constructed as the sum of attractive and repulsive potentials
U (q) = Uatt(q) + Urep(q).

Choset-79066 book February 22, 2005 17:43

4.1 Additive Attractive/Repulsive Potential

81

The Attractive Potential

There are several criteria that the potential ﬁeld Uatt should satisfy. First, Uatt should

be monotonically increasing with distance from qgoal. The simplest choice is the conic potential, measuring a scaled distance to the goal, i.e., U (q) = ζ d(q, qgoal). The ζ is

a parameter used to scale the effect of the attractive potential. The attractive gradient

is

∇U (q )

=

d

(q

ζ , qgoal

)

(

q

− qgoal).

The

gradient

vector

points

away

from

the

goal

with

magnitude ζ at all points of the conﬁguration space except the goal, where it is

undeﬁned. Starting from any point other than the goal, by following the negated

gradient, a path is traced toward the goal.

When numerically implementing this method, gradient descent may have “chatter-

ing” problems since there is a discontinuity in the attractive gradient at the origin. For

this reason, we would prefer a potential function that is continuously differentiable,

such that the magnitude of the attractive gradient decreases as the robot approaches

qgoal. The simplest such potential function is one that grows quadratically with the

distance to qgoal, e.g.,

Uatt (q )

=

1 ζ d2(q, 2

qgoal),

with the gradient

(4.1)

∇Uatt(q) = ∇

1 2

ζ

d

2

(

q

,

qgoal)

,

=

1 2

ζ

∇

d

2

(q

,

qgoal)

,

= ζ (q − qgoal),

which is a vector based at q, points away from qgoal, and has a magnitude proportional to the distance from q to qgoal. The farther away q is from qgoal, the bigger the magnitude of the vector. In other words, when the robot is far away from the goal, the robot quickly approaches it; when the robot is close to the goal, the robot slowly approaches it. This feature is useful for mobile robots because it reduces “overshoot” of the goal (resulting from step quantization).
In ﬁgure 4.4(a), the goal is in the center and the gradient vectors for various points are drawn. Figure 4.4(b) contains a contour plot for Uatt; each solid circle corresponds to a set of points q where Uatt(q) is constant. Finally, ﬁgure 4.4(c) plots the graph of the attractive potential.
Note that while the gradient ∇Uatt(q) converges linearly to zero as q approaches qgoal (which is a desirable property), it grows without bound as q moves away from qgoal. If qstart is far from qgoal, this may produce a desired velocity that is too large. For this reason, we may choose to combine the quadratic and conic potentials so that the

Choset-79066 book February 22, 2005 17:43
82

4 Potential Functions

(a)

(b)

(c)

Figure 4.4 (a) Attractive gradient vector ﬁeld. (b) Attractive potential isocontours. (c) Graph of the attractive potential.

(4.2) (4.3)

conic potential attracts the robot when it is very distant from qgoal and the quadratic potential attracts the robot when it is near qgoal. Of course it is necessary that the gradient be deﬁned at the boundary between the conic and quadratic portions. Such a ﬁeld can be deﬁned by

Uatt(q) =

1 2

ζ

d

2

(

q

,

qgoal)

,

dg∗oalζ d(q,

qgoal)

−

1 2

ζ

(dg∗oal

)2

,

d(q, qgoal) ≤ dg∗oal, d(q, qgoal) > dg∗oal.

and in this case we have

 ζ (q − qgoal),

∇Uatt(q) = 

, dg∗oalζ (q−qgoal)
d (q , qgoal )

d(q, qgoal) ≤ dg∗oal, d(q, qgoal) > dg∗oal,

where dg∗oal is the threshold distance from the goal where the planner switches between conic and quadratic potentials. The gradient is well deﬁned at the boundary of the two ﬁelds since at the boundary where d(q, qgoal) = dg∗oal, the gradient of the quadratic potential is equal to the gradient of the conic potential, ∇Uatt(q) = ζ (q − qgoal).

The Repulsive Potential
A repulsive potential keeps the robot away from an obstacle. The strength of the repulsive force depends upon the robot’s proximity to the an obstacle. The closer the robot is to an obstacle, the stronger the repulsive force should be. Therefore, the

Choset-79066 book February 22, 2005 17:43

4.1 Additive Attractive/Repulsive Potential

83

Obstacle Q*

Figure 4.5 The repulsive gradient operates only in a domain near the obstacle.

(4.4)

repulsive potential is usually deﬁned in terms of distance to the closest obstacle D(q), i.e.,

Urep(q) =

2

1 2

η

1 D(q)

−

1 Q∗

,

0,

D(q) ≤ Q∗, D(q) > Q∗,

whose gradient is

(4.5)

∇Urep(q) =

η

1 Q∗

−

1 D(q)

1 D2(q

)

∇

D

(q

),

0,

D(q) ≤ Q∗, D(q) > Q∗,

where the Q∗ ∈ R factor allows the robot to ignore obstacles sufﬁciently far away from it and the η can be viewed as a gain on the repulsive gradient. These scalars are usually determined by trial and error. (See ﬁgure 4.5.)
When numerically implementing this solution, a path may form that oscillates around points that are two-way equidistant from obstacles, i.e., points where D is nonsmooth. To avoid these oscillations, instead of deﬁning the repulsive potential function in terms of distance to the closest obstacle, the repulsive potential function is redeﬁned in terms of distances to individual obstacles where di (q) is the distance to obstacle QOi , i.e.,

(4.6) di (q) = min d(q, c).
c∈QOi
Note that the min operator returns the smallest d(q, c) for all points c in QOi .

