IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/jax/element/mml/optable/BasicLatin.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
ADVANCED SEARCH
Journals & Magazines > IEEE Transactions on Robotics > Volume: 36 Issue: 2
Approximate Optimal Motion Planning to Avoid Unknown Moving Avoidance Regions
Publisher: IEEE
Cite This
PDF
Patryk Deptula ; Hsi-Yuan Chen ; Ryan A. Licitra ; Joel A. Rosenfeld ; Warren E. Dixon
All Authors
5
Paper
Citations
1325
Full
Text Views

    Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Problem Formulation
    III.
    Value Function Approximation
    IV.
    Online Learning
    V.
    Stability Analysis

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
Media
Footnotes
Abstract:
In this article, an infinite-horizon optimal regulation problem is considered for a control-affine nonlinear autonomous agent subject to input constraints in the presence of dynamic avoidance regions. A local model-based approximate dynamic programming method is implemented to approximate the value function in a local neighborhood of the agent. By performing local approximations, prior knowledge of the locations of avoidance regions is not required. To alleviate the a priori knowledge of the number of avoidance regions in the operating domain, an extension is provided that modifies the value function approximation. The developed feedback-based motion planning strategy guarantees uniformly ultimately bounded convergence of the approximated control policy to the optimal policy while also ensuring the agent remains outside avoidance regions. Simulations are included to demonstrate the preliminary development for a kinematic unicycle and generic nonlinear system. Results from three experiments are also presented to illustrate the performance of the developed method, where a quadcopter achieves approximate optimal regulation while avoiding three mobile obstacles. To demonstrate the developed method, known avoidance regions are used in the first experiment, unknown avoidance regions are used in the second experiment, and an unknown time-varying obstacle directed by a remote pilot is included in the third experiment.
Published in: IEEE Transactions on Robotics ( Volume: 36 , Issue: 2 , April 2020 )
Page(s): 414 - 430
Date of Publication: 10 December 2019
ISSN Information:
INSPEC Accession Number: 19585503
DOI: 10.1109/TRO.2019.2955321
Publisher: IEEE
Funding Agency:
SECTION I.
Introduction

Many challenges exist for real-time navigation in uncertain environments. To operate safely in an uncertain environment, an autonomous agent must identify and react to possible collisions. In practice, challenges come from limitations in computational resources, sensing, communication, and mobility. Hence, robot navigation, motion planning, and path planning continues to be an active research area (cf.,  [1] and references therein).

Because motion and path-planning strategies need to account for environmental factors with various uncertainties, they can be divided into two groups—global and local approaches  [2] . Global planners seek the best trajectory by using models of the entire environment, are computed before a mission begins, and tend to provide high-level plans (cf.,  [3] – [4] [5] [6] [7] ). Local planners (sometimes referred to as reactive methods) plan only a few time steps forward based on limited knowledge using sensory data; hence, they have the advantage of providing optimal feedback if the agent is forced off of its original path, but they may need to be recomputed online (cf.,  [7] – [8] [9] [10] ). Since complex operating conditions present significant navigation, guidance, and control challenges (i.e., agents’ dynamics, obstacles, disturbances, or even faults), online feedback-based control/guidance algorithms with online learning and adaptation capabilities are essential for replanning and execution in dynamically changing and uncertain environments. Constrained optimization methods can be leveraged to generate guidance/control laws for agents operating in complex environments. However, agents often exhibit nonlinear dynamics and navigate in environments with uncertain dynamics or constraints, which makes the determination of analytical solutions to constrained optimization problems difficult. Traditional guidance/control solutions exploit numerical methods to generate approximate optimal solutions. For instance, approaches may use pseudoscpectral methods, they may solve the Hamilton–Jocobi–Bellman (HJB) equation offline via discretization and interpolation, or viscosity solutions can be solved offline before a mission begins (cf.,  [3] , [11] – [12] [13] [14] ). Such results may provide performance guarantees; however, numerical nonlinear optimization problems are typically computationally expensive (often preventing real-time implementation), especially as the dimension of the system increases. Generally, numerical methods are unable to consider uncertainty in the dynamics or environment, and are ill suited for dynamically changing environments because new guidance/control solutions would need to be recalculated offline in the event of a change in the environment. Such challenges motivate the use of approximate optimal control methods that use parametric function approximation techniques capable of approximating the solution to the HJB online (cf.,  [15] – [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] ).

Further complicating the task of optimal motion planning are agent actuator constraints and state constraints (e.g., static or mobile avoidance regions) often present en route to an objective. Certain avoidance regions may remain undiscovered until they fall into a given detection range. The concept of avoidance control was introduced in  [7] for two-player pursuit-evasion games. However, results such as  [9] , [10] , and [27] – [29] have used navigation functions for low-level control with collision avoidance in applications, such as multiagent systems. Other results, such as  [30] – [31] [32] have considered collision avoidance in multiagent systems with limited sensing by using bounded avoidance functions in the controller which are only active when agents are within a defined sensing radius. The results in  [9] and [28] – [32] do not consider optimal controllers, and in certain cases do not consider control constraints. Compared to such results which do not consider optimality, work such as  [33] utilizes unbounded avoidance functions to explicitly compute optimal controllers for cooperative avoidance for multiagent systems. Moreover, results such as  [34] – [35] [36] , develop sets of feasible states along with safe controllers using reachability methods such as  [14] by developing differential games between two players. Moreover, results such as  [37] – [38] [39] [40] [41] approach collision avoidance problems through the use of collision cones in conjunction with other methods based on engagement geometry between two point objects. In such works, dynamically moving objects are modeled by quadric surfaces and collision conditions are derived for dynamic inversion-based avoidance strategies between agents. Despite the progress, the results in  [33] rely on explicitly computed controllers, which are unknown when the optimal value function is unknown, and while results such as  [37] – [38] [39] [40] establish a framework for providing collision cones, they are still combined with methods which may not necessarily be optimal, cf.,  [41] . However, although results such as  [14] and [34] – [36] provide optimality guarantees, they rely on numerical techniques, which tend to be computationally intensive, and need to be resolved when conditions change.

Over the last several years, model predictive control (MPC) has gained attention for its capability to solve finite horizon optimal control problems in real-time (cf.,  [8] , [42] – [43] [44] [45] ). Moreover, MPC has been applied in a plethora of optimization problems; MPC is known for handling complex problems, such as of multiobjective problems, point-to-point trajectory generation problems, and collision avoidance (cf.,  [8] , [42] – [43] [44] [45] ). Specifically, works such as  [8] consider multiobjective MPC frameworks for autonomous underwater vehicles with different prioritized objectives where the main objective is path convergence, while the secondary objective is different (i.e., speed assignment, which can be sacrificed at times in lieu of better performance on path convergence), or the objective is purely trajectory generation, such as  [42] and  [43] , where the goal is point-to-point trajectory generation (i.e., offline multiagent trajectory generation or trajectory generation for constrained linearized agent models). Unlike, the aforementioned MPC results, results such as  [44] and  [45] take advantage of MPC's ability for fast optimization to combine it with other methods when considering collision avoidance problems. Although MPC has shown to be effective in motion/path planning and obstacle avoidance problems, the system dynamics are generally considered to be discretized and at each time-step, a finite horizon optimal control problem needs to be solved where a sequence of control inputs is generated. Even in the absence of obstacles, MPC methods generally do not yield an optimal policy over the complete trajectory since new solutions need to be recomputed at the end of each time horizon. Specifically, limited horizon methods, such as MPC, often require linear dynamics (cf.,  [42] , [43] ) or at least known dynamics (cf.,  [8] , [42] – [43] [44] [45] ). Since in practice, the environment and agents are prone to uncertainties, motivation exists to use parametric methods, such as neural-networks (NNs), to approximate optimal controllers online in continuous state nonlinear systems.

In recent years, approximate dynamic programming (ADP) has been successfully used in deterministic autonomous control-affine systems to solve optimal control problems  [15] – [16] [17] [18] , [46] , [47] . By utilizing parametric approximation methods, ADP methods approximate the value function, which is the solution to the HJB and is used to compute the online forward-in-time optimal policy. Input constraints are considered in  [19] – [20] [21] by using a nonquadratic cost function  [48] to yield a bounded approximate optimal controller.

For general nonlinear systems, generic basis functions, such as Gaussian radial basis functions, polynomials, or universal kernel functions are used to approximate the value function. One limitation of these generic approximation methods is that they only ensure an approximation over a compact neighborhood of the origin. Once outside the compact set, the approximation error tends to either grow or decay depending on the selected functions. Consequently, in the absence of domain knowledge, a large number of basis functions, and hence, a large number of unknown parameters, are required for value function approximation. A recent advancement in ADP utilizes computationally efficient state-following (StaF) kernel basis functions for local approximation of the value function around the current state, thereby reducing the number of basis functions required for sufficient value function approximation  [22] , [49] – [50] [51] . The authors in  [49] utilized the StaF approximation method to develop an approximate optimal online path planner with static obstacle avoidance. However, the development in  [49] used a transitioning controller which switched between the approximate controller and a robust controller when the obstacles where sensed.

Inspired by advances in  [22] – [26] , [49] , and [50] , an approximate local optimal feedback-based motion planner is developed in this article that considers input and state constraints with mobile avoidance regions. The developed method differs from numerical approaches, such as  [15] – [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] , or MPC approaches, such as  [42] and  [43] , because this article provides an online closed-loop feedback controller with computational efficiency provided by the local StaF approximation method. Moreover, the agent's trajectory is not computed offline, but instead the agent adjusts its trajectory online when it encounters an obstacle. Compared to works such as  [9] and [28] – [32] , which do not consider optimality, the controller designed in this article is based on an optimal control formulation that provides an approximate optimal control solution. In addition, unlike  [49] and other path planners, this method tackles the challenge of avoiding dynamic avoidance regions within the control strategy without switching between controllers. Since the StaF method uses local approximations, it does not require knowledge of uncertainties in the state space outside an approximation window. Local approximations of the StaF kernel method can be applied when an agent is approaching avoidance regions represented as ( n − 1 ) -spheres, not known a priori , in addition to state and system constraints. Because the avoidance regions become coupled with the agent in the HJB, their respective states must be incorporated when approximating the value function. Hence, a basis is given for each region which is zero outside of the sensing radius but is active when the avoidance region is sensed. In applications, such as station keeping of marine craft (e.g.,  [52] ), knowledge of the weights for an avoidance region may provide useful information, as the approximation of the value function can be improved every time the region is encountered. To prevent collision, a penalizing term is added to the cost function which guarantees that the agent stays outside of the avoidance regions. A Lyapunov-based stability analysis is presented and guarantees uniformly ultimately bounded convergence while also ensuring that the agent remains outside of the avoidance regions. This work extends from the preliminary results in  [53] . Unlike the preliminary work in  [53] , this article provides a unique value function representation and approximation, the actor update law is modified, and a more detailed stability analysis is included. The significance of this work over  [53] , is the mathematical development that considers an uncertain number of avoidance regions by transforming the autonomous value function approximation into a nonautonomous approximation. Because time does not lie on a compact set, it cannot be used in the StaF NNs, a transformation is performed so that a bounded signal of time is leveraged in the NNs. Moreover, experimental validations are presented to illustrate the performance of the developed path planning strategy.
Notation

In the following development, R denotes the set of real numbers, R n and R n × m denote the sets of real n -vectors and n × m matrices, and R ≥ a and R > a denote the sets of real numbers greater than or equal to a and strictly greater than a , respectively, where a ∈ R . The n × n identity matrix, column vector of ones of dimension j , and the zeros matrix or dimension m × n are denoted by I n , 1 j , and 0 m × n , respectively; hence, if n = 1 , 0 m × n reduces to a vector of zeros. The partial derivative of k with respect to the state x is denoted by ∇ k ( x , y , … ) , while the transpose of a matrix or vector is denoted by ( ⋅ ) T . For a vector ξ ∈ R m , the notation Tanh ( ξ ) ∈ R m and sgn ( ξ ) ∈ R m are defined as Tanh ( ξ ) ≜ [ tanh ( ξ i ) , … , tanh ( ξ m ) ] T and sgn ( ξ ) ≜ [ sgn ( ξ i ) , … , sgn ( ξ m ) ] T , respectively, where tanh ( ⋅ ) denotes the hyperbolic tangent function and sgn ( ⋅ ) denotes the signum function. The notation U [ a , b ] 1 n × 1 denotes a n-dimensional vector selected from a uniform distribution on [ a , b ] , and 1 n × m denotes a n × m matrix of ones.
SECTION II.
Problem Formulation

Consider an autonomous agent with control-affine nonlinear dynamics given by
x ˙ ( t ) = f ( x ( t ) ) + g ( x ( t ) ) u ( t ) (1)
View Source \begin{equation*} \dot{x}(t)=f\left(x(t)\right)+g\left(x(t)\right)u(t) \tag{1} \end{equation*} for all t ∈ R ≥ t 0 , where x : R ≥ t 0 → R n denotes the state, f : R n → R n denotes the drift dynamics, g : R n → R n × m denotes the control effectiveness, u : R t ≥ t 0 → R m denotes the control input, and t 0 ∈ R ≥ 0 denotes the initial time. In addition, consider dynamic avoidance regions with nonlinear dynamics given by
z ˙ i ( t ) = h i ( z i ( t ) ) (2)
View Source \begin{equation*} \dot{z}_{i}(t)=h_{i}\left(z_{i}(t)\right) \tag{2} \end{equation*} for all t ∈ R ≥ t 0 , where z i : R t ≥ t 0 → R n denotes the state of the center of the i th avoidance region and h i : R n → R n denotes the drift dynamics for the i th zone in M ≜ { 1 , 2 , … , M } , where M is the set of avoidance regions in the state space R n . 1 The dynamics in (2) are modeled as autonomous and isolated systems to facilitate the control problem formulation. The representation of the dynamics in (2) would require that complete knowledge of the dynamics over the entire operating domain are used. However, motivated by real systems where agents may only have local sensing, it is desired to only consider the zone inside a detection radius. Therefore, to alleviate the need for the HJB to require knowledge of the avoidance region dynamics outside of the agents’ ability to sense the obstacles, the avoidance regions are represented as \begin{equation*} \dot{z}_{i}(t)=\mathscr {F}_{i}\left(x(t),z_{i}(t)\right)h_{i}\left(z_{i}(t)\right) \tag{3} \end{equation*}
View Source \begin{equation*} \dot{z}_{i}(t)=\mathscr {F}_{i}\left(x(t),z_{i}(t)\right)h_{i}\left(z_{i}(t)\right) \tag{3} \end{equation*} for all t\in \mathbb {R}_{\geq t_{0}}. In (3) , \mathscr {F}_{i}:\mathbb {R}^{n}\times \mathbb {R}^{n}\rightarrow [0,1] is a smooth transition function that satisfies \mathscr {F}_{i}(x,z_{i})=0 for \Vert x-z_{i}\Vert >r_{d} and \mathscr {F}_{i}(x,z_{i})=1 for \Vert x-z_{i}\Vert \leq \bar{r} , where r_{d}\in \mathbb {R}_{>0} denotes the detection radius of the system in (1) , and \bar{r}\in (r_{a},r_{d}) where r_{a}\in \mathbb {R}_{>0} denotes the radius of the avoidance region. From the agent's perspective, the dynamics of the obstacles do not affect the agent outside of the sensing radius.

Remark 1:

In application, a standard practice is to enforce a minimum avoidance radius to ensure safety  [30] , [31] . In addition, the detection radius r_{d} and avoidance radius r_{s} depend on the system parameters such as the maximum agent velocity limits.

Assumption 1:

The number of dynamic avoidance regions M is known; however, the locations of the states of each region is unknown until it is within the sensing radius of the agent. Section VII presents an approach to alleviate Assumption 1 .

Assumption 2:

The drift dynamics f , h_{i} , and control effectiveness g are locally Lipschitz continuous, and g is bounded such that 0< \Vert g(x(t))\Vert \leq \overline{g} for all x\in \mathbb {R}^{n} and all t\in \mathbb {R}_{\geq t_{0}} where \overline{g}\in \mathbb {R}_{>0} . Furthermore, f(0)=0 , and \nabla f:\mathbb {R}^{n}\rightarrow \mathbb {R}^{n}\times \mathbb {R}^{n} is continuous.

Assumption 3:

The equilibrium points z_{i}^{e} for the obstacles given by the dynamics in (3) lie outside of a ball of radius r_{d} centered at the origin. That is, the origin is sufficiently clear of obstacles. Furthermore, obstacles do not trap the agent, meaning the obstacles do not completely barricade the agent in the sense that the agent has a free, unblocked, path to the goal location. Moreover, the agent is assumed to be sufficiently agile to be able to outmaneuver the moving obstacles. Specifically, the obstacle velocities must be appropriately equal or less than the agent for the agent to have capability to avoid the obstacle in general.

Remark 2:

Assumption 3 limits pathological scenarios where obstacle avoidance is not possible. Specifically, scenarios may arise where obstacles move faster than the agent. In such scenarios, it may be infeasible for agents using this method, or other existing approaches, to avoid the obstacle without colliding. However, given an upper bound on the obstacles velocities, the sensing radius can be sized large enough for the agent to respond accordingly.

Remark 3:

To facilitate the development, let d:\mathbb {R}^{n}\times \mathbb {R}^{n}\rightarrow \mathbb {R} denote a distance metric defined as d(v,w)\triangleq \Vert v-w\Vert for v,w\in \mathbb {R}^{n} . Moreover, the centers of the avoidance regions, shown in Fig. 1 , are augmented with the following. 2

Fig. 1.

Augmented regions around each avoidance region.

Show All

    The total detection set is defined as \mathcal {D}=\cup _{i\in \mathcal {M}}\mathcal {D}_{i}, where \begin{align*} \mathcal {D}_{i}=\left\lbrace x\in \mathbb {R}^{n}\;|\;d\left(x,z_{i}\right)\leq r_{d}\right\rbrace . \end{align*}
    View Source \begin{align*} \mathcal {D}_{i}=\left\lbrace x\in \mathbb {R}^{n}\;|\;d\left(x,z_{i}\right)\leq r_{d}\right\rbrace . \end{align*}

    The total conflict set is defined as \mathcal {W}=\cup _{i\in \mathcal {M}}\mathcal {W}_{i}, where \begin{align*} \mathcal {W}_{i}=\left\lbrace x\in \mathbb {R}^{n}\;|\;r_{a}< d\left(x,z_{i}\right)\leq \overline{r}\right\rbrace . \end{align*}
    View Source \begin{align*} \mathcal {W}_{i}=\left\lbrace x\in \mathbb {R}^{n}\;|\;r_{a}< d\left(x,z_{i}\right)\leq \overline{r}\right\rbrace . \end{align*}

    The total avoidance set is \Omega =\cup _{i\in \mathcal {M}}\Omega _{i}, where each local avoidance region is \begin{align*} \Omega _{i}=\left\lbrace x\in \mathbb {R}^{n}\;|\;d\left(x,z_{i}\right)\leq r_{a}\right\rbrace . \end{align*}
    View Source \begin{align*} \Omega _{i}=\left\lbrace x\in \mathbb {R}^{n}\;|\;d\left(x,z_{i}\right)\leq r_{a}\right\rbrace . \end{align*}

Furthermore, the avoidance region and agent dynamics can be combined to form the following system: \begin{equation*} \dot{\zeta }(t)=F\left(\zeta (t)\right)+G\left(\zeta (t)\right)u(t) \tag{4} \end{equation*}
View Source \begin{equation*} \dot{\zeta }(t)=F\left(\zeta (t)\right)+G\left(\zeta (t)\right)u(t) \tag{4} \end{equation*} for all t\in \mathbb {R}_{\geq t_{0}}, where \zeta =[x^{T},z_{1}^{T},\ldots,z_{M}^{T}]^{T}\in \mathbb {R}^{\mathcal {N}} , \mathcal {N}=(M+1)n and \begin{align*} F\left(\zeta \right)=\left[\begin{array}{c}f(x)\\ \mathscr {F}_{1}\left(x,z_{1}\right)h_{1}\left(z_{1}\right)\\ \vdots \\ \mathscr {F}_{M}\left(x,z_{M}\right)h_{M}\left(z_{M}\right) \end{array}\right]\qquad G\left(\zeta \right)=\left[\begin{array}{c}g(x)\\ 0_{Mn\times m} \end{array}\right]. \end{align*}
View Source \begin{align*} F\left(\zeta \right)=\left[\begin{array}{c}f(x)\\ \mathscr {F}_{1}\left(x,z_{1}\right)h_{1}\left(z_{1}\right)\\ \vdots \\ \mathscr {F}_{M}\left(x,z_{M}\right)h_{M}\left(z_{M}\right) \end{array}\right]\qquad G\left(\zeta \right)=\left[\begin{array}{c}g(x)\\ 0_{Mn\times m} \end{array}\right]. \end{align*}

The goal is to simultaneously design and implement a controller u which minimizes the cost function \begin{equation*} J\left(\zeta,u\right)\triangleq \int _{t_{0}}^{\infty }r\left(\zeta \left(\tau \right),u\left(\tau \right)\right)d\tau \tag{5} \end{equation*}
View Source \begin{equation*} J\left(\zeta,u\right)\triangleq \int _{t_{0}}^{\infty }r\left(\zeta \left(\tau \right),u\left(\tau \right)\right)d\tau \tag{5} \end{equation*} subject to (4) while obeying \sup _{t}(u_{i})\leq \mu _{\text{sat}} \forall i=1,\ldots,m , where \mu _{\text{sat}}\in \mathbb {R}_{>0} is the control effort saturation limit. In (5) , r:\mathbb {R}^{\mathcal {N}}\times \mathbb {R}^{m}\rightarrow [0,\infty ] is the instantaneous cost defined as \begin{equation*} r\left(\zeta,u\right)=Q_{x}(x)+\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)Q_{z}\left(z_{i}\right)+\Psi (u)+P\left(\zeta \right) \tag{6} \end{equation*}
View Source \begin{equation*} r\left(\zeta,u\right)=Q_{x}(x)+\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)Q_{z}\left(z_{i}\right)+\Psi (u)+P\left(\zeta \right) \tag{6} \end{equation*} where Q_{x},\;Q_{z}:\mathbb {R}^{n}\rightarrow \mathbb {R}_{\geq 0} are user-defined positive definite functions that penalize the agent and obstacle states. The Q_{z}(z_{i}) term in (6) only influences the cost when the obstacles are sensed. The smooth scheduling function s_{i}:\mathbb {R}^{n}\times \mathbb {R}^{n}\rightarrow [0,1] that allows the avoidance region states in the detection radius to be penalized, satisfies s_{i}=0 for \Vert x-z_{i}\Vert >r_{d} and s_{i}=1 for \Vert x-z_{i}\Vert \leq \bar{r} . In (6) , \Psi :\mathbb {R}^{m}\rightarrow \mathbb {R} is a positive definite function penalizing the control input u, defined as \begin{equation*} \Psi (u)\triangleq 2\sum _{i=1}^{m}\left[\int _{0}^{u_{i}}\left(\mu _{\text{sat}}r_{i}\tanh ^{-1}\left(\frac{\xi _{u_{i}}}{\mu _{\text{sat}}}\right)\right)d\xi _{u_{i}}\right] \tag{7} \end{equation*}
View Source \begin{equation*} \Psi (u)\triangleq 2\sum _{i=1}^{m}\left[\int _{0}^{u_{i}}\left(\mu _{\text{sat}}r_{i}\tanh ^{-1}\left(\frac{\xi _{u_{i}}}{\mu _{\text{sat}}}\right)\right)d\xi _{u_{i}}\right] \tag{7} \end{equation*} where u_{i} is the i{\text{th}} element of the control u , \xi _{u_{i}} is an integration variable, and r_{i} is the diagonal elements which make up the symmetric positive definite weighting matrix R\in \mathbb {R}^{m\times m} where R\triangleq \operatorname{diag}\lbrace \underline{\overline{R}}\rbrace, and \underline{\overline{R}}\triangleq [r_{1},\ldots,r_{m}]\in \mathbb {R}^{1\times m} [19] , [21] , [48] . The selection of the input penalizing function in (7) is motivated such that a bounded form of control policy can be derived from the HJB  [48] . Moreover, \tanh (\cdot) is used in (7) because it is a continuous one-to-one real-analytic function, \tanh (0_{m})=0_{m} , and \tanh ^{-1}(\frac{\xi _{u_{i}}}{\mu _{\text{sat}}}) is monotonically increasing. The function P:\mathbb {R}^{\mathcal {N}}\rightarrow \mathbb {R} in (6) , called the avoidance penalty function, is a positive semidefinite compactly supported function defined as \begin{equation*} P\left(\zeta \right)\triangleq \sum _{i=1}^{M}\left(\min \left\lbrace 0,\frac{d\left(x,z_{i}\right)^{2}-r_{d}^{2}}{\left(d\left(x,z_{i}\right)^{2}-r_{a}^{2}\right)^{2}}\right\rbrace \right)^{2}. \tag{8} \end{equation*}
View Source \begin{equation*} P\left(\zeta \right)\triangleq \sum _{i=1}^{M}\left(\min \left\lbrace 0,\frac{d\left(x,z_{i}\right)^{2}-r_{d}^{2}}{\left(d\left(x,z_{i}\right)^{2}-r_{a}^{2}\right)^{2}}\right\rbrace \right)^{2}. \tag{8} \end{equation*}

Remark 4:

The avoidance penalty function in (8) is zero outside of the compact set \mathcal {D}, and yields an infinite penalty when \Vert x-z_{i}\Vert =r_{a} for any i\in \mathcal {M}. Other penalty/avoidance functions can be used; see  [33] for a generalization of avoidance functions. The avoidance penalty function in (8) modifies the one found in  [33] , which studies a generalization of avoidance penalty functions. Since the term in the denominator has quartic growth compared to only quadratic growth, the function in (8) is scaled differently compared to the one found in  [33] . Other growth factors can also be used which affect the rate at which the agent penalizes the avoidance regions once it detects them.

Assumption 4:

There exist constants \underline{q}_{x},\;\overline{q}_{x},\;\underline{q}_{z},\;\overline{q}_{z}\in \mathbb {R}_{>0} such that \underline{q}_{x}\Vert x\Vert ^{2}\leq Q_{x}(x)\leq \overline{q}_{x}\Vert x\Vert ^{2} for all x\in \mathbb {R}^{n} , and \underline{q}_{z}\Vert z_{i}\Vert ^{2}\leq Q_{z}(z_{i})\leq \overline{q}_{z}\Vert z_{i}\Vert ^{2} for all z_{i}\in \mathbb {R}^{n} and i\in \mathcal {M} .

The infinite-horizon scalar value function for the optimal value function, denoted by V^{*}:\mathbb {R}^{\mathcal {N}}\rightarrow \mathbb {R}_{\geq 0}, is expressed as \begin{equation*} V^{*}\left(\zeta \right)=\min _{u\left(\tau \right)\in U|\tau \in \mathbb {R}_{\geq t}}\int _{t}^{\infty }r\left(\zeta \left(\tau \right),u\left(\tau \right)\right)d\tau \tag{9} \end{equation*}
View Source \begin{equation*} V^{*}\left(\zeta \right)=\min _{u\left(\tau \right)\in U|\tau \in \mathbb {R}_{\geq t}}\int _{t}^{\infty }r\left(\zeta \left(\tau \right),u\left(\tau \right)\right)d\tau \tag{9} \end{equation*} where U\subset \mathbb {R}^{m} denotes the set of admissible inputs. For the stationary solution, the HJB equation, which characterizes the optimal value function is given by \begin{align*} 0 =&\frac{\partial V^{*}\left(\zeta \right)}{\partial \zeta }\left(F\left(\zeta \right)+G\left(\zeta \right)u^{*}\left(\zeta \right)\right)+r\left(\zeta,u^{*}\left(\zeta \right)\right)\\ =&\frac{\partial V^{*}\left(\zeta \right)}{\partial x}\left(f(x)+g(x)u^{*}\left(\zeta \right)\right) \\ & +\sum _{i=1}^{M}\frac{\partial V^{*}\left(\zeta \right)}{\partial z_{i}}\left(\mathscr {F}_{i}\left(x,z_{i}\right)h_{i}\left(z_{i}\right)\right)+r\left(\zeta,u^{*}\left(\zeta \right)\right)\tag{10} \end{align*}
View Source \begin{align*} 0 =&\frac{\partial V^{*}\left(\zeta \right)}{\partial \zeta }\left(F\left(\zeta \right)+G\left(\zeta \right)u^{*}\left(\zeta \right)\right)+r\left(\zeta,u^{*}\left(\zeta \right)\right)\\ =&\frac{\partial V^{*}\left(\zeta \right)}{\partial x}\left(f(x)+g(x)u^{*}\left(\zeta \right)\right) \\ & +\sum _{i=1}^{M}\frac{\partial V^{*}\left(\zeta \right)}{\partial z_{i}}\left(\mathscr {F}_{i}\left(x,z_{i}\right)h_{i}\left(z_{i}\right)\right)+r\left(\zeta,u^{*}\left(\zeta \right)\right)\tag{10} \end{align*} with the condition V^{*}(0)=0 , where u^{*}:\mathbb {R}^{\mathcal {N}}\rightarrow \mathbb {R}^{m} is the optimal control policy. Taking the partial derivative of (10) with respect to u^{*}(\zeta) , setting it to zero (i.e., u^{*}(\zeta) is the minimizing argument) and solving for u^{*}(\zeta) results in \begin{align*} u^{*}\left(\zeta \right) & =-\mu _{\text{sat}}\text{Tanh}\bigg (\frac{R^{-1}G\left(\zeta \right)^{T}}{2\mu _{\text{sat}}}\left(\nabla V^{*}\left(\zeta \right)\right)^{T}\bigg). \tag{11} \end{align*}
View Source \begin{align*} u^{*}\left(\zeta \right) & =-\mu _{\text{sat}}\text{Tanh}\bigg (\frac{R^{-1}G\left(\zeta \right)^{T}}{2\mu _{\text{sat}}}\left(\nabla V^{*}\left(\zeta \right)\right)^{T}\bigg). \tag{11} \end{align*} The HJB in (10) uses both the agent and avoidance region dynamics. 3 However, because each avoidance region is modeled as in (3) , the terms that include them are zero when the regions are not detected; hence, they do not affect the HJB. Furthermore, the analytical expression in (11) requires knowledge of the optimal value function. However, the analytical solution for the HJB, i.e., the value function, is not feasible to compute in general cases. Therefore, an approximation is sought using a neural network approach.

SECTION III.
Value Function Approximation

Recent developments in ADP have resulted in computationally efficient StaF kernels to approximate the value function  [22] . To facilitate the development let \chi \subset \mathbb {R}^{\mathcal {N}} be a compact set, with x and all z_{i} in the interior of \chi . Based on the StaF method in  [22] and  [50] , after adding and subtracting a bounded avoidance function P_{a}(\zeta), the optimal value function and controller can be approximated as \begin{align*} V^{*}(y) =&P_{a}(y)+W(y)^{T}\sigma \left(y,c\left(\zeta \right)\right)+\epsilon \left(\zeta,y\right)\tag{12}\\ u^{*}(y) =&-\mu _{\text{sat}}\operatorname{Tanh}\bigg (\frac{R^{-1}G(y)^{T}}{2\mu _{\text{sat}}} \\ & \times \Big (\nabla P_{a}(y)^{T}+\nabla \sigma \left(y,c\left(\zeta \right)\right)^{T}W\left(\zeta \right) \\ & +\nabla W\left(\zeta \right)^{T}\sigma \left(y,c\left(\zeta \right)\right)+\nabla \epsilon \left(y,\zeta \right)^{T}\Big)\bigg) \tag{13} \end{align*}
View Source \begin{align*} V^{*}(y) =&P_{a}(y)+W(y)^{T}\sigma \left(y,c\left(\zeta \right)\right)+\epsilon \left(\zeta,y\right)\tag{12}\\ u^{*}(y) =&-\mu _{\text{sat}}\operatorname{Tanh}\bigg (\frac{R^{-1}G(y)^{T}}{2\mu _{\text{sat}}} \\ & \times \Big (\nabla P_{a}(y)^{T}+\nabla \sigma \left(y,c\left(\zeta \right)\right)^{T}W\left(\zeta \right) \\ & +\nabla W\left(\zeta \right)^{T}\sigma \left(y,c\left(\zeta \right)\right)+\nabla \epsilon \left(y,\zeta \right)^{T}\Big)\bigg) \tag{13} \end{align*} where c(\zeta)\in (\overline{B_{r}(\zeta)})^{L} are centers around the current concatenated state \zeta , L\in Z_{>0} is the number of centers, and y\in \overline{B_{r}(\zeta)} where \overline{B_{r}(\zeta)} is a small compact set around the current state \zeta \in \chi . In (12) , W:\chi \rightarrow \mathbb {R}^{L} is the continuously differentiable ideal StaF weight function that changes with the state dependent centers, \epsilon :\chi \rightarrow \mathbb {R} is the continuously differentiable bounded function reconstruction error, and \sigma :\chi \rightarrow \mathbb {R}^{L} is a concatenated vector of StaF basis functions such that \begin{equation*} \sigma \left(\zeta,c\left(\zeta \right)\right)=\left[\begin{array}{c}\sigma _{0}\left(x,c_{0}(x)\right)\\ s_{1}\left(x,z_{1}\right)\sigma _{1}\left(z_{1},c_{1}\left(z_{1}\right)\right)\\ \vdots \\ s_{M}\left(x,z_{M}\right)\sigma _{M}\left(z_{M},c_{M}\left(z_{M}\right)\right) \end{array}\right] \tag{14} \end{equation*}
View Source \begin{equation*} \sigma \left(\zeta,c\left(\zeta \right)\right)=\left[\begin{array}{c}\sigma _{0}\left(x,c_{0}(x)\right)\\ s_{1}\left(x,z_{1}\right)\sigma _{1}\left(z_{1},c_{1}\left(z_{1}\right)\right)\\ \vdots \\ s_{M}\left(x,z_{M}\right)\sigma _{M}\left(z_{M},c_{M}\left(z_{M}\right)\right) \end{array}\right] \tag{14} \end{equation*} where \sigma _{0}(x,c_{0}(x)):\mathbb {R}^{n}\rightarrow \mathbb {R}^{P_{x}} and \sigma _{i}(z_{i},c_{i}(z_{i})):\mathbb {R}^{n}\rightarrow \mathbb {R}^{P_{z_{i}}} for i\in \mathcal {M} are strictly positive definite, continuously differentiable StaF kernel function vectors, c_{i}:\mathbb {R}^{n}\rightarrow \mathbb {R}^{n} for i\in \lbrace 0,1,\ldots,M\rbrace are state-dependent centers, and the dimension of the concatenated vector of StaF basis functions \sigma is L=P_{x}+\sum _{i=1}^{M}P_{z_{i}} . The formation of the vector of basis functions in (14) allows for certain weights of the approximation to be constant when the agent and no-entry zones are not in the detection regions. This formulation introduces a sparse-like approach because the basis functions that correlate to the no-entry zones are off due to the scheduling function s_{i}, when they are outside of the detection regions. Hence, approximation of the value function is only influenced by the no-entry zones when they are in the detection regions \mathcal {D}_{i}. However, the optimal value function and controller are not known in general; therefore, approximations \hat{V}:\mathbb {R}^{\mathcal {N}}\times \mathbb {R}^{\mathcal {N}}\times \mathbb {R}^{L}\rightarrow \mathbb {R} and \hat{u}:\mathbb {R}^{\mathcal {N}}\times \mathbb {R}^{\mathcal {N}}\times \mathbb {R}^{L}\rightarrow \mathbb {R}^{m} are used where \begin{align*} \hat{V}\big (y,\zeta,\hat{W}_{c}\big) \triangleq& P_{a}(y)+\hat{W}_{c}^{T}\sigma \left(y,c\left(\zeta \right)\right)\tag{15}\\ \hat{u}\big (y,\zeta,\hat{W}_{a}\big) \triangleq& -\mu _{\text{sat}}\operatorname{Tanh}\bigg (\frac{R^{-1}G(y)^{T}}{2\mu _{\text{sat}}} \\ & \times \left(\nabla \sigma \left(y,c\left(\zeta \right)\right)^{T}\hat{W}_{a}+\nabla P_{a}^{T}(y)\right)\bigg). \tag{16} \end{align*}
View Source \begin{align*} \hat{V}\big (y,\zeta,\hat{W}_{c}\big) \triangleq& P_{a}(y)+\hat{W}_{c}^{T}\sigma \left(y,c\left(\zeta \right)\right)\tag{15}\\ \hat{u}\big (y,\zeta,\hat{W}_{a}\big) \triangleq& -\mu _{\text{sat}}\operatorname{Tanh}\bigg (\frac{R^{-1}G(y)^{T}}{2\mu _{\text{sat}}} \\ & \times \left(\nabla \sigma \left(y,c\left(\zeta \right)\right)^{T}\hat{W}_{a}+\nabla P_{a}^{T}(y)\right)\bigg). \tag{16} \end{align*} In (15) and (16) , \hat{V} and \hat{u} are evaluated at a point y\in B_{r}(\zeta) using StaF kernels centered at \zeta , while \hat{W}_{c},\;\hat{W}_{a}\in \mathbb {R}^{L} are the weight estimates for the ideal weight vector W . In actor-critic architectures, the estimates \hat{V} and \hat{u} replace the optimal value function V^{*} and optimal policy u^{*} in (10) to form a residual error \delta :\mathbb {R}^{\mathcal {N}}\times \mathbb {R}^{\mathcal {N}}\times \mathbb {R}^{L}\times \mathbb {R}^{L}\rightarrow \mathbb {R} known as the Bellman error (BE), which is defined as \begin{align*} &\delta \big (y,\zeta,\hat{W}_{c},\hat{W}_{a}\big)\triangleq \nabla \hat{V}\big (y,\zeta,\hat{W}_{c}\big)\Big (F(y) \\ &\quad+G(y)\hat{u}\big (y,\zeta,\hat{W}_{a}\big)\Big)+r\left(y,\hat{u}\big (y,\zeta,\hat{W}_{a}\big)\right). \tag{17} \end{align*}
View Source \begin{align*} &\delta \big (y,\zeta,\hat{W}_{c},\hat{W}_{a}\big)\triangleq \nabla \hat{V}\big (y,\zeta,\hat{W}_{c}\big)\Big (F(y) \\ &\quad+G(y)\hat{u}\big (y,\zeta,\hat{W}_{a}\big)\Big)+r\left(y,\hat{u}\big (y,\zeta,\hat{W}_{a}\big)\right). \tag{17} \end{align*} The aim of the actor and critic is to find a set of weights which minimize the BE for all \zeta \in \mathbb {R}^{\mathcal {\mathcal {N}}} .

Remark 5:

Unlike the function P, which is not finite when \Vert x-z_{i}\Vert =r_{a}, for any i\in \mathcal {M}, the function P_{a} satisfies P_{a}=0 when x,\;z_{i}\notin \mathcal {D}_{i} for each i\in \mathcal {M}, and for all 0\leq P(\zeta)\leq \overline{P}_{a}, and \Vert \nabla P_{a}(\zeta)\Vert \leq \overline{\Vert \nabla P_{a}\Vert } for all \zeta \in \mathbb {R}^{\mathcal {N}}. An example of P_{a}(\zeta) includes P_{a}(\zeta)\triangleq \sum _{i=1}^{M}P_{a,i}(x,z_{i}) where P_{a,i}\triangleq (\min \lbrace 0,\frac{\Vert x-z_{i}\Vert ^{2}-r_{d}^{2}}{(\Vert x-z_{i}\Vert ^{2}-r_{a}^{2})^{2}+r_{\varepsilon }}\rbrace)^{2} for r_{\varepsilon }\in \mathbb {R}_{>0}, or see  [30] – [31] [32] for other examples of bounded avoidance functions.

SECTION IV.
Online Learning

To implement the approximations online, at a given time instance t , the BE \delta _{t}:\mathbb {R}_{\geq 0}\rightarrow \mathbb {R} is evaluated as \begin{align*} \delta _{t} & (t)\triangleq \delta \big (\zeta (t),\zeta (t),\hat{W}_{c}(t),\hat{W}_{a}(t)\big) \tag{18} \end{align*}
View Source \begin{align*} \delta _{t} & (t)\triangleq \delta \big (\zeta (t),\zeta (t),\hat{W}_{c}(t),\hat{W}_{a}(t)\big) \tag{18} \end{align*} where \zeta denotes the state of the system in (4) starting at initial time t_{0} with initial condition \zeta _{0} , while \hat{W}_{c}(t) and \hat{W}_{a}(t) denote the critic weight and actor weight estimates at time t , respectively. The controller which influences the state x(t)\subset \zeta (t) is \begin{equation*} u(t)=\hat{u}\big (\zeta (t),\zeta (t),\hat{W}_{a}(t)\big). \tag{19} \end{equation*}
View Source \begin{equation*} u(t)=\hat{u}\big (\zeta (t),\zeta (t),\hat{W}_{a}(t)\big). \tag{19} \end{equation*}

Simulation of experience is used to learn online by extrapolating the BE to unexplored areas of the state space  [22] , [23] . Off-policy trajectories \lbrace x_{k}\;:\;\mathbb {R}^{n}\times \mathbb {R}_{\geq 0}\rightarrow \mathbb {R}^{n}\rbrace _{k=1}^{N} are selected by the critic such that each x_{k} maps the current state x(t) to a point x_{k}(x(t),t)\in B_{r}(x(t)). The extrapolated BE \delta _{k}:\mathbb {R}_{\geq 0}\rightarrow \mathbb {R} for each \zeta _{k} takes the form \begin{equation*} \delta _{k}(t)=\hat{W}_{c}^{T}(t)\omega _{k}(t)+\omega _{Pk}(t)+r\left(\zeta _{k}(t),\hat{u}_{k}(t)\right) \tag{20} \end{equation*}
View Source \begin{equation*} \delta _{k}(t)=\hat{W}_{c}^{T}(t)\omega _{k}(t)+\omega _{Pk}(t)+r\left(\zeta _{k}(t),\hat{u}_{k}(t)\right) \tag{20} \end{equation*} where \zeta _{k}=[\begin{array}{cc}x_{k}^{T}, & Z(t)\end{array}]^{T} \begin{align*} \omega _{Pk}(t)\triangleq& \nabla P_{a}\left(\zeta _{k}(t)\right)\bigg (F\left(\zeta _{k}(t)\right)\\ &+G\left(\zeta _{k}(t)\right)\hat{u}\left(\zeta _{k}(t),\zeta (t),\hat{W}_{a}(t)\right)\bigg)\\ \omega _{k}(t)\triangleq& \nabla \sigma \left(\zeta _{k}(t),c\left(\zeta (t)\right)\right)\bigg (F\left(\zeta _{k}(t)\right)\\ &+G\left(\zeta _{k}(t)\right)\hat{u}\left(\zeta _{k}(t),\zeta (t),\hat{W}_{a}(t)\right)\bigg) \end{align*}
View Source \begin{align*} \omega _{Pk}(t)\triangleq& \nabla P_{a}\left(\zeta _{k}(t)\right)\bigg (F\left(\zeta _{k}(t)\right)\\ &+G\left(\zeta _{k}(t)\right)\hat{u}\left(\zeta _{k}(t),\zeta (t),\hat{W}_{a}(t)\right)\bigg)\\ \omega _{k}(t)\triangleq& \nabla \sigma \left(\zeta _{k}(t),c\left(\zeta (t)\right)\right)\bigg (F\left(\zeta _{k}(t)\right)\\ &+G\left(\zeta _{k}(t)\right)\hat{u}\left(\zeta _{k}(t),\zeta (t),\hat{W}_{a}(t)\right)\bigg) \end{align*} and the extrapolated policies are \begin{align*} \hat{u}_{k}(t) \triangleq& -\mu _{\text{sat}}\operatorname{Tanh}\bigg (\frac{R^{-1}G\left(\zeta _{k}(t)\right)}{2\mu _{\text{sat}}} \\ & \times \left(\nabla \sigma \big (\zeta _{k}(t),c\left(\zeta (t)\right)\big)^{T}\hat{W}_{a}(t)+\nabla P_{a}^{T}\left(\zeta _{k}(t)\right)\right)\bigg). \tag{21} \end{align*}
View Source \begin{align*} \hat{u}_{k}(t) \triangleq& -\mu _{\text{sat}}\operatorname{Tanh}\bigg (\frac{R^{-1}G\left(\zeta _{k}(t)\right)}{2\mu _{\text{sat}}} \\ & \times \left(\nabla \sigma \big (\zeta _{k}(t),c\left(\zeta (t)\right)\big)^{T}\hat{W}_{a}(t)+\nabla P_{a}^{T}\left(\zeta _{k}(t)\right)\right)\bigg). \tag{21} \end{align*} The concurrent learning-based least squares update laws are designed as \begin{align*} \dot{\hat{W}}_{c}(t) =&-\Gamma (t)\left (\frac{k_{c1}\omega (t)}{\rho (t)}\delta (t)+\frac{k_{c2}}{N}\sum _{k=1}^{N}\frac{\omega _{k}(t)}{\rho _{k}(t)}\delta _{k}(t)\right)\tag{22}\\ \dot{\Gamma }(t) =&\beta \Gamma (t)-k_{c1}\Gamma (t)\frac{\omega (t)\omega ^{T}(t)}{\rho ^{2}(t)}\Gamma (t) \\ & -\frac{k_{c2}}{N}\Gamma (t)\sum _{k=1}^{N}\frac{\omega _{k}(t)\omega _{k}^{T}(t)}{\rho _{k}^{2}(t)}\Gamma (t),\quad \Gamma \left(t_{0}\right)=\Gamma _{0}. \tag{23} \end{align*}
View Source \begin{align*} \dot{\hat{W}}_{c}(t) =&-\Gamma (t)\left (\frac{k_{c1}\omega (t)}{\rho (t)}\delta (t)+\frac{k_{c2}}{N}\sum _{k=1}^{N}\frac{\omega _{k}(t)}{\rho _{k}(t)}\delta _{k}(t)\right)\tag{22}\\ \dot{\Gamma }(t) =&\beta \Gamma (t)-k_{c1}\Gamma (t)\frac{\omega (t)\omega ^{T}(t)}{\rho ^{2}(t)}\Gamma (t) \\ & -\frac{k_{c2}}{N}\Gamma (t)\sum _{k=1}^{N}\frac{\omega _{k}(t)\omega _{k}^{T}(t)}{\rho _{k}^{2}(t)}\Gamma (t),\quad \Gamma \left(t_{0}\right)=\Gamma _{0}. \tag{23} \end{align*} Furthermore, in (22) and (23) \rho (t)\triangleq 1+\gamma _{1}\omega (t)^{T}\omega (t), \rho _{k}(t)\triangleq 1+\gamma _{1}\omega _{k}(t)^{T}\omega _{k}(t) are normalizing factors, k_{c1,}k_{c2},\gamma _{1}\in \mathbb {R}_{>0} are adaptation gains, \beta \in \mathbb {R}_{>0} is a forgetting factor, and \begin{align*} \omega (t)\triangleq \nabla \sigma \big (\zeta (t),c\left(\zeta (t)\right)\big)\Big (F\left(\zeta (t)\right)\\ +G\left(\zeta (t)\right)\hat{u}\big (\zeta (t),\zeta (t),\hat{W}_{a}(t)\big)\Big). \end{align*}
View Source \begin{align*} \omega (t)\triangleq \nabla \sigma \big (\zeta (t),c\left(\zeta (t)\right)\big)\Big (F\left(\zeta (t)\right)\\ +G\left(\zeta (t)\right)\hat{u}\big (\zeta (t),\zeta (t),\hat{W}_{a}(t)\big)\Big). \end{align*} The policy weights are updated to follow the critic weights using the actor update law designed as \begin{align*} \dot{\hat{W}}_{a}(t) & =-\Gamma _{a}\Bigg (k_{a1}\left(\hat{W}_{a}(t)-\hat{W}_{c}(t)\right)+k_{a2}\hat{W}_{a}(t) \\ &\qquad\qquad\qquad +k_{c1}G_{a1}(t)\frac{\omega ^{T}(t)}{\rho (t)}\hat{W}_{c}(t) \\ & \qquad\qquad\qquad \left.+\frac{k_{c2}}{N}\sum _{k=1}^{N}G_{a1,k}(t)\frac{\omega _{k}^{T}(t)}{\rho _{k}(t)}\hat{W}_{c}(t)\right) \tag{24} \end{align*}
View Source \begin{align*} \dot{\hat{W}}_{a}(t) & =-\Gamma _{a}\Bigg (k_{a1}\left(\hat{W}_{a}(t)-\hat{W}_{c}(t)\right)+k_{a2}\hat{W}_{a}(t) \\ &\qquad\qquad\qquad +k_{c1}G_{a1}(t)\frac{\omega ^{T}(t)}{\rho (t)}\hat{W}_{c}(t) \\ & \qquad\qquad\qquad \left.+\frac{k_{c2}}{N}\sum _{k=1}^{N}G_{a1,k}(t)\frac{\omega _{k}^{T}(t)}{\rho _{k}(t)}\hat{W}_{c}(t)\right) \tag{24} \end{align*} where k_{a1},k_{a2}\in \mathbb {R}_{>0} are adaptation gains, \Gamma _{a}\in \mathbb {R}^{L\times L} is a positive definite constant matrix, and \begin{align*} G_{a1}(t) \triangleq &\mu _{\text{sat}}\nabla \sigma \left(\zeta (t),c\left(\zeta (t)\right)\right)G\left(\zeta (t)\right)\\ & \times \left(\operatorname{Tanh}\left (\frac{1}{k_{u}}\hat{\bar{D}}(t)\right)-\operatorname{Tanh}\left (\frac{R^{-1}}{2\mu _{\text{sat}}}\hat{\bar{D}}(t)\right)\right)\\ G_{a1,k}(t) \triangleq& \mu _{\text{sat}}\nabla \sigma \left (\zeta _{k}(t),c\left(\zeta (t)\right)\right)G (\zeta _{k}(t))\\ & \times \left(\operatorname{Tanh}\left (\frac{1}{k_{u}}\hat{\bar{D}}_{k}(t)\right)-\operatorname{Tanh}\left (\frac{R^{-1}}{2\mu _{\text{sat}}}\hat{\bar{D}}_{k}(t)\right)\right) \end{align*}
View Source \begin{align*} G_{a1}(t) \triangleq &\mu _{\text{sat}}\nabla \sigma \left(\zeta (t),c\left(\zeta (t)\right)\right)G\left(\zeta (t)\right)\\ & \times \left(\operatorname{Tanh}\left (\frac{1}{k_{u}}\hat{\bar{D}}(t)\right)-\operatorname{Tanh}\left (\frac{R^{-1}}{2\mu _{\text{sat}}}\hat{\bar{D}}(t)\right)\right)\\ G_{a1,k}(t) \triangleq& \mu _{\text{sat}}\nabla \sigma \left (\zeta _{k}(t),c\left(\zeta (t)\right)\right)G (\zeta _{k}(t))\\ & \times \left(\operatorname{Tanh}\left (\frac{1}{k_{u}}\hat{\bar{D}}_{k}(t)\right)-\operatorname{Tanh}\left (\frac{R^{-1}}{2\mu _{\text{sat}}}\hat{\bar{D}}_{k}(t)\right)\right) \end{align*} where k_{u}\in \mathbb {R}_{>0} is a constant, \hat{\bar{D}}(t)\triangleq G^{T}(\zeta (t))(\nabla \sigma ^{T}(\zeta (t), c(\zeta (t)))\hat{W}_{a}(t)+\nabla P_{a}^{T}(\zeta (t))), and \hat{\bar{D}}_{k}(t)\triangleq G^{T}(\zeta _{k}(t)) (\nabla \sigma ^{T} (\zeta _{k}(t),c(\zeta (t)))\hat{W}_{a}(t)+\nabla P_{a}^{T}(\zeta _{k}(t))). Similar to the preliminary work in  [53] , a projection-based update law for the actor weight estimates can be used to simplify the stability analysis. In such a case, (24) would become \dot{\hat{W}}_{a}(t)=\operatorname{\text{proj}}\lbrace -\Gamma _{a}k_{a1}(\hat{W}_{a}(t)-\hat{W}_{c}(t))\rbrace , where \operatorname{\text{proj}}\lbrace \cdot \rbrace denotes a smooth projection operator which bounds the weight estimates, see [54, Ch. 4] for details of the projection operator.

Remark 6:

Rather than extrapolating the entire state vector of the system, as designed in  [22] , [23] , and [51] , only the controlled states, i.e., the agent's states, are extrapolated to perform simulation of experience. Compared to experience replay results such as  [21] , which record a history stack of prior input–output pairs, the simulation of experience approach in this result only uses extrapolated states within a time-varying neighborhood of the current agent state. This is motivated by the StaF approximation method, which only provides a sufficient approximation of the value function a neighborhood of the current agent state.

SECTION V.
Stability Analysis

For notational brevity, time dependence of functions are henceforth suppressed. Define \tilde{W}_{c}\triangleq W-\hat{W}_{c} and \tilde{W}_{a}\triangleq W- \hat{W}_{a} as the weight estimation errors, and let \overline{\Vert (\cdot)\Vert }\triangleq \sup _{\pi \in B_{\xi }}\Vert (\cdot)\Vert, where B_{\xi }\subset \chi \times \mathbb {R}^{L}\times \mathbb {R}^{L} is a compact set. Then, the BEs in (18) and (20) can be expressed as \begin{align*} \delta _{t} & =-\omega ^{T}\tilde{W}_{c}+G_{a1}^{T}\tilde{W}_{a}+G_{a2}^{T}\tilde{W}_{a}+\Delta \left(\zeta \right)\\ \delta _{k} & =-\omega _{k}^{T}\tilde{W}_{c}+G_{a1,k}^{T}\tilde{W}_{a}+G_{a2,k}^{T}\tilde{W}_{a}+\Delta _{k}\left(\zeta \right). \end{align*}
View Source \begin{align*} \delta _{t} & =-\omega ^{T}\tilde{W}_{c}+G_{a1}^{T}\tilde{W}_{a}+G_{a2}^{T}\tilde{W}_{a}+\Delta \left(\zeta \right)\\ \delta _{k} & =-\omega _{k}^{T}\tilde{W}_{c}+G_{a1,k}^{T}\tilde{W}_{a}+G_{a2,k}^{T}\tilde{W}_{a}+\Delta _{k}\left(\zeta \right). \end{align*} The terms G_{a2} and G_{a2k} are defined as G_{a2}\triangleq \mu _{\text{sat}}\nabla \sigma G (\operatorname{sgn}(\hat{\bar{D}})-\operatorname{Tanh}(\frac{1}{k_{u}}\hat{\bar{D}})) and G_{a2,k}\triangleq \mu _{\text{sat}}\nabla \sigma _{k}G_{k}(\operatorname{sgn}(\hat{\bar{D}}_{k})-\operatorname{Tanh}(\frac{1}{k_{u}}\hat{\bar{D}}_{k})). The functions \Delta,\;\Delta _{k}:\mathbb {R}^{\mathcal {N}}\rightarrow \mathbb {R} are uniformly bounded over \chi such that the residual bounds \overline{\Vert \Delta \Vert },\;\overline{\Vert \Delta _{k}\Vert } decrease with decreasing \overline{\Vert \nabla W\Vert } and \overline{\Vert \nabla \epsilon \Vert }. 4

To facilitate the analysis, the system states x and selected states x_{k} are assumed to satisfy the following inequalities.

Assumption 5:

There exists constants T\in \mathbb {R}_{>0} and \underline{c}_{1},\underline{c}_{2},\underline{c}_{3}\in \mathbb {R}_{\geq 0} , such that \begin{align*} \underline{c}_{1}I_{L} & \leq \frac{1}{N}\sum _{k=1}^{N}\frac{\omega _{k}(t)\omega _{k}^{T}(t)}{\rho _{k}^{2}(t)}\\ \underline{c}_{2}I_{L} & \leq \int _{t}^{t+T}\left(\frac{1}{N}\sum _{k=1}^{N}\frac{\omega _{k}\left(\tau \right)\omega _{k}^{T}\left(\tau \right)}{\rho _{k}^{2}\left(\tau \right)}\right)d\tau \qquad \;\forall t\in \mathbb {R}_{\geq t_{0}}\\ \underline{c}_{3}I_{L} & \leq \int _{t}^{t+T}\left(\frac{\omega \left(\tau \right)\omega ^{T}\left(\tau \right)}{\rho ^{2}\left(\tau \right)}\right)d\tau \qquad \;\forall t\in \mathbb {R}_{\geq t_{0}} \end{align*}
View Source \begin{align*} \underline{c}_{1}I_{L} & \leq \frac{1}{N}\sum _{k=1}^{N}\frac{\omega _{k}(t)\omega _{k}^{T}(t)}{\rho _{k}^{2}(t)}\\ \underline{c}_{2}I_{L} & \leq \int _{t}^{t+T}\left(\frac{1}{N}\sum _{k=1}^{N}\frac{\omega _{k}\left(\tau \right)\omega _{k}^{T}\left(\tau \right)}{\rho _{k}^{2}\left(\tau \right)}\right)d\tau \qquad \;\forall t\in \mathbb {R}_{\geq t_{0}}\\ \underline{c}_{3}I_{L} & \leq \int _{t}^{t+T}\left(\frac{\omega \left(\tau \right)\omega ^{T}\left(\tau \right)}{\rho ^{2}\left(\tau \right)}\right)d\tau \qquad \;\forall t\in \mathbb {R}_{\geq t_{0}} \end{align*} where at least one of the constants \underline{c}_{1} , \underline{c}_{2} , or \underline{c}_{3} is strictly positive  [22] .

In general, \underline{c}_{1} can be made strictly positive by sampling redundant data, i.e, choosing N\gg L, and \underline{c}_{2} can be made strictly positive by sampling extrapolated trajectories at a high frequency. Generally, \underline{c}_{3} is strictly positive provided the system is persistently excited (PE), which is a strong assumption that cannot be verified online. Since only one constant has to be strictly positive, \omega _{k} can be selected such that \underline{c}_{1}>0 or \underline{c}_{2}>0 , since \omega _{k} is a design variable. Unlike the strong PE given by the third inequality in Assumption 5 , the first two inequalities can be verified online.

Remark 7:

Instead of injecting potentially destabilizing dither signals into the physical system to satisfy the PE condition, virtual excitation can be obtained by using the sample states. Specifically, the sample states x_{k}(t) can be selected from a sampling distribution, such as a normal or uniform distribution, or they can be selected to follow a highly oscillatory trajectory.

Lemma 1:

Provided Assumption 5 is satisfied and \lambda _{\min }\lbrace \Gamma _{0}^{-1}\rbrace >0 , the update law in (23) ensures that the least squares gain matrix \Gamma satisfies \begin{equation*} \underline{\Gamma }I_{L}\leq \Gamma (t)\leq \overline{\Gamma }I_{L} \tag{25} \end{equation*}
View Source \begin{equation*} \underline{\Gamma }I_{L}\leq \Gamma (t)\leq \overline{\Gamma }I_{L} \tag{25} \end{equation*} where the bounds \underline{\Gamma } and \overline{\Gamma } are defined as \begin{align*} \underline{\Gamma } & =\frac{1}{\left(\lambda _{\max }\left\lbrace \Gamma _{0}^{-1}\right\rbrace +\frac{k_{c1}+k_{c2}}{4\gamma _{1}\beta }\right)}\\ \overline{\Gamma } & =\frac{1}{\min \left\lbrace \left(k_{c1}\underline{c}_{3}+k_{c2}\max \left\lbrace \underline{c}_{1}T,\underline{c}_{2}\right\rbrace \right),\lambda _{\min }\left\lbrace \Gamma _{0}^{-1}\right\rbrace \right\rbrace e^{-\beta T}} \end{align*}
View Source \begin{align*} \underline{\Gamma } & =\frac{1}{\left(\lambda _{\max }\left\lbrace \Gamma _{0}^{-1}\right\rbrace +\frac{k_{c1}+k_{c2}}{4\gamma _{1}\beta }\right)}\\ \overline{\Gamma } & =\frac{1}{\min \left\lbrace \left(k_{c1}\underline{c}_{3}+k_{c2}\max \left\lbrace \underline{c}_{1}T,\underline{c}_{2}\right\rbrace \right),\lambda _{\min }\left\lbrace \Gamma _{0}^{-1}\right\rbrace \right\rbrace e^{-\beta T}} \end{align*} where \lambda _{\min }\lbrace \cdot \rbrace, \;\lambda _{\max }\lbrace \cdot \rbrace denote the minimum and maximum eigenvalues, respectively  [22] .

To facilitate the analysis, consider a candidate Lyapunov function V_{L}:\mathbb {R}^{\mathcal {N}+2L}\times \mathbb {R}_{\geq t_{0}}\rightarrow \mathbb {R} given by \begin{align*} V_{L}(Y,t) =&V^{*}\left(\zeta \right)+\frac{1}{2}\tilde{W}_{c}^{T}\Gamma ^{-1}(t)\tilde{W}_{c} \\ & +\frac{1}{2}\tilde{W}_{a}^{T}\Gamma _{a}^{-1}\tilde{W}_{a}+\frac{1}{2}\sum _{i=1}^{M}z_{i}^{T}z_{i} \tag{26} \end{align*}
View Source \begin{align*} V_{L}(Y,t) =&V^{*}\left(\zeta \right)+\frac{1}{2}\tilde{W}_{c}^{T}\Gamma ^{-1}(t)\tilde{W}_{c} \\ & +\frac{1}{2}\tilde{W}_{a}^{T}\Gamma _{a}^{-1}\tilde{W}_{a}+\frac{1}{2}\sum _{i=1}^{M}z_{i}^{T}z_{i} \tag{26} \end{align*} where V^{*} is the optimal value function, and Y=[\zeta ^{T},\tilde{W}_{c}^{T},\tilde{W}_{a}^{T}]^{T}. Since the optimal value function is positive definite, using (25) and [55, Lemma 4.3], (26) can be bounded as \begin{equation*} \underline{\nu }_{l}\left(\left\Vert Y\right\Vert \right)\leq V(Y,t)\leq \overline{\nu }_{l}\left(\left\Vert Y\right\Vert \right) \tag{27} \end{equation*}
View Source \begin{equation*} \underline{\nu }_{l}\left(\left\Vert Y\right\Vert \right)\leq V(Y,t)\leq \overline{\nu }_{l}\left(\left\Vert Y\right\Vert \right) \tag{27} \end{equation*} for all t\in \mathbb {R}_{\geq t_{0}} and for all Y\in \mathbb {R}^{n+1+2~L}, where \underline{\nu }_{l},\overline{\nu }_{l}:\mathbb {R}_{\geq 0}\rightarrow \mathbb {R}_{\geq 0} are class \mathcal {K} functions. To facilitate the following analysis, let \nu _{l}:\mathbb {R}_{\geq 0}\rightarrow \mathbb {R}_{\geq 0} be a class \mathcal {K} function such that \begin{align*} \nu _{l}\left(\left\Vert Y\right\Vert \right) \leq& \frac{\underline{q}}{2}\left\Vert x\right\Vert ^{2}+\frac{\underline{q}_{z}}{4}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2} \\ & +\left(\frac{k_{a1}+k_{a2}}{8}\right)\left\Vert \tilde{W}_{a}\right\Vert ^{2}+\frac{k_{c2}\underline{c}}{8}\left\Vert \tilde{W}_{c}\right\Vert ^{2} \tag{28} \end{align*}
View Source \begin{align*} \nu _{l}\left(\left\Vert Y\right\Vert \right) \leq& \frac{\underline{q}}{2}\left\Vert x\right\Vert ^{2}+\frac{\underline{q}_{z}}{4}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2} \\ & +\left(\frac{k_{a1}+k_{a2}}{8}\right)\left\Vert \tilde{W}_{a}\right\Vert ^{2}+\frac{k_{c2}\underline{c}}{8}\left\Vert \tilde{W}_{c}\right\Vert ^{2} \tag{28} \end{align*} and let \underline{c}\in \mathbb {R}_{>0} be a constant defined as \begin{equation*} \underline{c}\triangleq \frac{\beta }{2k_{c2}\overline{\Gamma }}\,+\,\frac{\underline{c}_{1}}{2}. \tag{29} \end{equation*}
View Source \begin{equation*} \underline{c}\triangleq \frac{\beta }{2k_{c2}\overline{\Gamma }}\,+\,\frac{\underline{c}_{1}}{2}. \tag{29} \end{equation*}

The sufficient conditions for the subsequent analysis are given by \begin{align*} \frac{k_{a1}+k_{a2}}{2} & \geq \max \left\lbrace \varphi _{ac},\frac{\overline{\left\Vert \nabla W\right\Vert }G_{R}}{\lambda _{\min }\left\lbrace \Gamma _{a}\right\rbrace }\overline{\left\Vert \nabla \sigma ^{T}\right\Vert }\right\rbrace \tag{30}\\ k_{c2}\underline{c} & \geq \varphi _{ac}\tag{31}\\ \frac{1}{2}\underline{q}_{z} & \geq L_{z}\tag{32}\\ \nu _{\iota }^{-1}\left(\iota \right) & < \overline{\nu }_{\iota }^{-1}\left(\underline{\nu }_{\iota }\left(\xi \right)\right) \tag{33} \end{align*}
View Source \begin{align*} \frac{k_{a1}+k_{a2}}{2} & \geq \max \left\lbrace \varphi _{ac},\frac{\overline{\left\Vert \nabla W\right\Vert }G_{R}}{\lambda _{\min }\left\lbrace \Gamma _{a}\right\rbrace }\overline{\left\Vert \nabla \sigma ^{T}\right\Vert }\right\rbrace \tag{30}\\ k_{c2}\underline{c} & \geq \varphi _{ac}\tag{31}\\ \frac{1}{2}\underline{q}_{z} & \geq L_{z}\tag{32}\\ \nu _{\iota }^{-1}\left(\iota \right) & < \overline{\nu }_{\iota }^{-1}\left(\underline{\nu }_{\iota }\left(\xi \right)\right) \tag{33} \end{align*} where L_{z} is the Lipschitz constant such that \Vert h_{i}(z_{i})\Vert \leq L_{z}\Vert z_{i}\Vert satisfying assumption ( 2 ) and \varphi _{ac}\in \mathbb {R}_{>0} is defined in the appendix.

Theorem 1:

Consider the augmented dynamic system (4) and the dynamic systems in (1) and (3) . Provided Assumptions 1 – 5 are satisfied along with the sufficient conditions in (30) – (33) , then system state \zeta (t) , input u(t) , and weight approximation errors \tilde{W}_{a} and \tilde{W}_{c} are Uniformly Ultimately Bounded (UUB); furthermore, states \zeta (t) starting outside of \Omega remain outside of \Omega .

Proof:

Consider the Lyapunov function candidate in (26) . The time derivative is given by \begin{align*} \dot{V}_{L} =&\dot{V}^{*}+\tilde{W}_{c}^{T}\Gamma ^{-1}\left(\dot{W}-\dot{\hat{W}}_{c}\right)+\tilde{W}_{a}^{T}\Gamma _{a}^{-1}\left(\dot{W}-\dot{\hat{W}}_{a}\right)\\ & -\frac{1}{2}\tilde{W}_{c}^{T}\Gamma ^{-1}\dot{\Gamma }\Gamma ^{-1}\tilde{W}_{c}+\sum _{i=1}^{M}z_{i}^{T}\left(\mathscr {F}_{i}h_{i}\right). \end{align*}
View Source \begin{align*} \dot{V}_{L} =&\dot{V}^{*}+\tilde{W}_{c}^{T}\Gamma ^{-1}\left(\dot{W}-\dot{\hat{W}}_{c}\right)+\tilde{W}_{a}^{T}\Gamma _{a}^{-1}\left(\dot{W}-\dot{\hat{W}}_{a}\right)\\ & -\frac{1}{2}\tilde{W}_{c}^{T}\Gamma ^{-1}\dot{\Gamma }\Gamma ^{-1}\tilde{W}_{c}+\sum _{i=1}^{M}z_{i}^{T}\left(\mathscr {F}_{i}h_{i}\right). \end{align*} Using the chain rule, the time derivative of the ideal weights \dot{W} can be expressed as \begin{equation*} \dot{W}=\nabla W\left(F+Gu\right). \tag{34} \end{equation*}
View Source \begin{equation*} \dot{W}=\nabla W\left(F+Gu\right). \tag{34} \end{equation*} Substituting in (22) – (24) with (34) yields \begin{align*} \dot{V}_{L} =&\nabla V^{*}F+\nabla V^{*}Gu+\sum _{i=1}^{M}z_{i}^{T}\left(\mathscr {F}_{i}h_{i}\right)\\ & +\tilde{W}_{c}^{T}\Gamma ^{-1}\left (k_{c1}\Gamma \frac{\omega }{\rho }\delta _{t}+\frac{k_{c2}}{N}\Gamma \sum _{k=1}^{N}\frac{\omega _{k}}{\rho _{k}}\delta _{k}\right)\\ & +\tilde{W}_{a}^{T}k_{a1}\left(\hat{W}_{a}-\hat{W}_{c}\right)+\tilde{W}_{a}^{T}k_{a2}\hat{W}_{a}(t)\\ & +\tilde{W}_{a}^{T}\left(k_{c1}G_{a1}\frac{\omega ^{T}}{\rho }-\frac{k_{c2}}{N}\sum _{k=1}^{N}G_{a1,k}\frac{\omega _{k}^{T}}{\rho _{ik}}\right)\hat{W}_{c}(t)\\ & +\left(\tilde{W}_{c}^{T}\Gamma ^{-1}+\tilde{W}_{a}^{T}\right)\nabla W\left(F+Gu\right)-\frac{1}{2}\tilde{W}_{c}^{T}\Gamma ^{-1} \\ &\times \left (\beta \Gamma-k_{c1}\Gamma \frac{\omega \omega ^{T}}{\rho ^{2}}\Gamma -\frac{k_{c2}}{N}\Gamma \sum _{k=1}^{N}\frac{\omega _{k}\omega _{k}^{T}}{\rho _{k}^{2}}\Gamma \right)\Gamma ^{-1}\tilde{W}_{c}. \end{align*}
View Source \begin{align*} \dot{V}_{L} =&\nabla V^{*}F+\nabla V^{*}Gu+\sum _{i=1}^{M}z_{i}^{T}\left(\mathscr {F}_{i}h_{i}\right)\\ & +\tilde{W}_{c}^{T}\Gamma ^{-1}\left (k_{c1}\Gamma \frac{\omega }{\rho }\delta _{t}+\frac{k_{c2}}{N}\Gamma \sum _{k=1}^{N}\frac{\omega _{k}}{\rho _{k}}\delta _{k}\right)\\ & +\tilde{W}_{a}^{T}k_{a1}\left(\hat{W}_{a}-\hat{W}_{c}\right)+\tilde{W}_{a}^{T}k_{a2}\hat{W}_{a}(t)\\ & +\tilde{W}_{a}^{T}\left(k_{c1}G_{a1}\frac{\omega ^{T}}{\rho }-\frac{k_{c2}}{N}\sum _{k=1}^{N}G_{a1,k}\frac{\omega _{k}^{T}}{\rho _{ik}}\right)\hat{W}_{c}(t)\\ & +\left(\tilde{W}_{c}^{T}\Gamma ^{-1}+\tilde{W}_{a}^{T}\right)\nabla W\left(F+Gu\right)-\frac{1}{2}\tilde{W}_{c}^{T}\Gamma ^{-1} \\ &\times \left (\beta \Gamma-k_{c1}\Gamma \frac{\omega \omega ^{T}}{\rho ^{2}}\Gamma -\frac{k_{c2}}{N}\Gamma \sum _{k=1}^{N}\frac{\omega _{k}\omega _{k}^{T}}{\rho _{k}^{2}}\Gamma \right)\Gamma ^{-1}\tilde{W}_{c}. \end{align*} Using (6) with (10) , (18) – (21) , Young's inequality, and Lemma 1 , the Lyapunov derivative can be bounded as \begin{align*} \dot{V}_{L} \leq& -\underline{q}_{x}\left\Vert x\right\Vert ^{2}-\frac{\underline{q}_{z}}{2}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2}\\ & -2\left(\frac{k_{a1}+k_{a2}}{8}\right)\left\Vert \tilde{W}_{a}\right\Vert ^{2}-2\left(\frac{k_{c2}\underline{c}}{8}\right)\left\Vert \tilde{W}_{c}\right\Vert ^{2}\\ & -\left[\begin{array}{cc}\left\Vert \tilde{W}_{c}\right\Vert & \left\Vert \tilde{W}_{a}\right\Vert \end{array}\right]\left[\begin{array}{cc}\frac{k_{c2}\underline{c}}{2} & -\frac{\varphi _{ac}}{2}\\ -\frac{\varphi _{ac}}{2} & \frac{k_{a1}+k_{a2}}{4} \end{array}\right]\left[\begin{array}{c}\left\Vert \tilde{W}_{c}\right\Vert \\ \left\Vert \tilde{W}_{a}\right\Vert \end{array}\right]\\ & -\frac{\underline{q}_{z}}{2}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2}+\sum _{i=1}^{M}z_{i}^{T}\left(\mathscr {F}_{i}h_{i}\right)+\iota \end{align*}
View Source \begin{align*} \dot{V}_{L} \leq& -\underline{q}_{x}\left\Vert x\right\Vert ^{2}-\frac{\underline{q}_{z}}{2}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2}\\ & -2\left(\frac{k_{a1}+k_{a2}}{8}\right)\left\Vert \tilde{W}_{a}\right\Vert ^{2}-2\left(\frac{k_{c2}\underline{c}}{8}\right)\left\Vert \tilde{W}_{c}\right\Vert ^{2}\\ & -\left[\begin{array}{cc}\left\Vert \tilde{W}_{c}\right\Vert & \left\Vert \tilde{W}_{a}\right\Vert \end{array}\right]\left[\begin{array}{cc}\frac{k_{c2}\underline{c}}{2} & -\frac{\varphi _{ac}}{2}\\ -\frac{\varphi _{ac}}{2} & \frac{k_{a1}+k_{a2}}{4} \end{array}\right]\left[\begin{array}{c}\left\Vert \tilde{W}_{c}\right\Vert \\ \left\Vert \tilde{W}_{a}\right\Vert \end{array}\right]\\ & -\frac{\underline{q}_{z}}{2}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2}+\sum _{i=1}^{M}z_{i}^{T}\left(\mathscr {F}_{i}h_{i}\right)+\iota \end{align*} where \iota \in \mathbb {R}_{>0} is the positive constant defined in the appendix. Using (28) , (30) , and (31) , the Lyapunov derivative reduces to \begin{align*} \dot{V}_{L} \leq& -\nu _{l}\left(\left\Vert Y\right\Vert \right)-\left(\nu _{l}\left(\left\Vert Y\right\Vert \right)-\iota \right)\\ & -\frac{\underline{q}_{z}}{2}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2}+\sum _{i=1}^{M}z_{i}^{T}\left(\mathscr {F}_{i}h_{i}\right). \end{align*}
View Source \begin{align*} \dot{V}_{L} \leq& -\nu _{l}\left(\left\Vert Y\right\Vert \right)-\left(\nu _{l}\left(\left\Vert Y\right\Vert \right)-\iota \right)\\ & -\frac{\underline{q}_{z}}{2}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2}+\sum _{i=1}^{M}z_{i}^{T}\left(\mathscr {F}_{i}h_{i}\right). \end{align*}

For the case when x,z_{i}\notin \mathcal {D} \forall i\in \mathcal {M} , the avoidance region dynamics in (3) can be used conclude that, \frac{\underline{q}_{z}}{2}\sum _{i=1}^{M}s_{i}(x,z_{i})\Vert z_{i}\Vert ^{2}+\sum _{i=1}^{M}z_{i}^{T}(\mathscr {F}_{i}h_{i})=0 ; therefore \begin{align*} \dot{V}_{L}\leq -\nu _{l}\left(\left\Vert Y\right\Vert \right)-\left(\nu _{l}\left(\left\Vert Y\right\Vert \right)-\iota \right). \end{align*}
View Source \begin{align*} \dot{V}_{L}\leq -\nu _{l}\left(\left\Vert Y\right\Vert \right)-\left(\nu _{l}\left(\left\Vert Y\right\Vert \right)-\iota \right). \end{align*} Provided the sufficient conditions in (30) , (31) , and (33) are met, then \begin{align*} \dot{V}_{L}\leq -\nu _{l}\left(\left\Vert Y\right\Vert \right),\quad \forall Y\in \chi \;\forall \left\Vert Y\right\Vert \geq \nu _{l}^{-1}\left(\iota \right). \end{align*}
View Source \begin{align*} \dot{V}_{L}\leq -\nu _{l}\left(\left\Vert Y\right\Vert \right),\quad \forall Y\in \chi \;\forall \left\Vert Y\right\Vert \geq \nu _{l}^{-1}\left(\iota \right). \end{align*} For the case when \zeta \in \mathcal {\mathcal {W}}, Assumption 2 is used to conclude that \begin{align*} \dot{V}_{L} \leq &-\nu _{l}\left(\left\Vert Y\right\Vert \right)-\left(\nu _{l}\left(\left\Vert Y\right\Vert \right)-\iota \right)\\ & -\frac{\underline{q}_{z}}{2}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2}+\sum _{i\in \mathcal {M}}L_{z}\left\Vert z_{i}\right\Vert ^{2}. \end{align*}
View Source \begin{align*} \dot{V}_{L} \leq &-\nu _{l}\left(\left\Vert Y\right\Vert \right)-\left(\nu _{l}\left(\left\Vert Y\right\Vert \right)-\iota \right)\\ & -\frac{\underline{q}_{z}}{2}\sum _{i=1}^{M}s_{i}\left(x,z_{i}\right)\left\Vert z_{i}\right\Vert ^{2}+\sum _{i\in \mathcal {M}}L_{z}\left\Vert z_{i}\right\Vert ^{2}. \end{align*} Using the fact that \inf _{x,z_{i}\in \mathcal {W}_{i}}s_{i}(x,z_{i})=1 for any i\in \mathcal {M}, and provided the sufficient conditions in (30) – (33) hold, \begin{equation*} \dot{V}_{L}\leq -\nu _{l}\left(\left\Vert Y\right\Vert \right)\quad \forall \left\Vert Y\right\Vert \geq \nu _{l}^{-1}\left(\iota \right). \tag{35} \end{equation*}
View Source \begin{equation*} \dot{V}_{L}\leq -\nu _{l}\left(\left\Vert Y\right\Vert \right)\quad \forall \left\Vert Y\right\Vert \geq \nu _{l}^{-1}\left(\iota \right). \tag{35} \end{equation*} Hence, (26) is nonincreasing.

If \Vert x-z_{i}\Vert \rightarrow r_{a} for some i\in \mathcal {M}, then P(\zeta)\rightarrow \infty, and V^{*}(\zeta)\rightarrow \infty . If V^{*}(\zeta)\rightarrow \infty then V_{L}(Y)\rightarrow \infty . Since this is a contradiction to (26) being nonincreasing, then \forall \zeta (t_{0})\notin \Omega , \zeta (t)\notin \Omega \forall t\geq t_{0} . Hence, V^{*}(\zeta) is finite and \nabla V^{*}(\zeta) exists for all \Vert x-z_{i}\Vert \ne r_{a}.

After using (27) , (33) , and (35) , [55, Th. 4.18] can be invoked to conclude that Y is uniformly ultimately bounded such that \lim \sup _{t\rightarrow \infty }\Vert Y(t)\Vert \leq \underline{\nu _{\iota }}^{-1}(\overline{\nu _{\iota }}(\nu _{\iota }^{-1}(\iota))). Since Y\in L_{\infty }, it follows that \zeta,\tilde{W}_{c},\tilde{W}_{a}\in L_{\infty }. Since W is a continuous function of \zeta , W\circ \zeta \in L_{\infty }. Hence, \hat{W}_{a},\hat{W}_{c}\in L_{\infty } which implies u\in L_{\infty }. \blacksquare

Remark 8:

The sufficient condition in (30) can be satisfied by increasing the gain k_{a2} and selecting a gain \Gamma _{a} such that \lambda _{\min }\lbrace \Gamma _{a}\rbrace is large. This will not affect the sufficient conditions in (31) and (32) . Selecting extrapolated trajectories x_{k} such that \underline{c} is sufficiently large will aid in satisfying (31) without affecting (30) or (32) . In addition, selecting StaF basis such that \overline{\Vert \nabla \sigma \Vert } is small will help satisfy the conditions in (30) and (31) . To satisfy the sufficient condition in (32) without affecting (30) or (31) , it suffices to select a function Q_{z} according to Assumption 4 such that \underline{q}_{x} is larger than the Lipschitz constant L_{z} . Provided the StaF basis functions are selected such that \overline{\Vert \epsilon \Vert } , \overline{\Vert \nabla \epsilon \Vert } , and \overline{\Vert \nabla W\Vert } are small, and k_{a2} and \underline{c} are selected to be sufficiently large, then the sufficient condition in (33) can be satisfied.

Remark 9:

The value function V^{*} is dependent on the no-entry zone states, and since it is used as a candidate Lyapunov function, (26) is also dependent on the states z_{i} . Therefore, through proper construction of (6) , it is shown in Theorem 1 that since V_{L} is nonincreasing there is no collision between the agent x and no-entry zones z_{i} . Other than Assumptions 2 and 3 , there is no restriction on the movement of the obstacles. Rather, the states of the obstacles are included in the candidate Lyapunov function because the controlled agent must move such that collision is avoided, making the candidate Lyapunov function nonincreasing.

SECTION VI.
Simulations
A. Mobile Robot

To demonstrate the developed approach in Sections II – V , a simulation is provided for unicycle kinematic equations, where f(x(t))=0_{3\times 1} and \begin{align*}g(x(t))=\left[\begin{array}{ccc}\cos (x_{3}(t)) & -\sin (x_{3}(t)) & 0\\ \sin (x_{3}(t)) & \cos (x_{3}(t)) & 0\\ 0 & 0 & 1 \end{array}\right]. \end{align*}
View Source \begin{align*}g(x(t))=\left[\begin{array}{ccc}\cos (x_{3}(t)) & -\sin (x_{3}(t)) & 0\\ \sin (x_{3}(t)) & \cos (x_{3}(t)) & 0\\ 0 & 0 & 1 \end{array}\right]. \end{align*} Three heterogeneous no-entry zones are considered with oscillatory linear dynamics; the third state was selected to be stationary for each no-entry zone for the entirety of the simulation. The function \mathscr {F}_{i}(x,z_{i}) is selected as \begin{equation*} \mathscr {F}_{i}\left(x,z_{i}\right)={\begin{cases}0, & \left\Vert x-z_{i}\right\Vert >r_{d}\\ T_{i}\left(x,z_{i}\right), & r_{d}\geq \left\Vert x-z_{i}\right\Vert >\bar{r}\\ 1, & \left\Vert x-z_{i}\right\Vert \leq \bar{r} \end{cases}} \tag{36} \end{equation*}
View Source \begin{equation*} \mathscr {F}_{i}\left(x,z_{i}\right)={\begin{cases}0, & \left\Vert x-z_{i}\right\Vert >r_{d}\\ T_{i}\left(x,z_{i}\right), & r_{d}\geq \left\Vert x-z_{i}\right\Vert >\bar{r}\\ 1, & \left\Vert x-z_{i}\right\Vert \leq \bar{r} \end{cases}} \tag{36} \end{equation*} where T_{i}(x,z_{i})\triangleq \frac{1}{2}+\frac{1}{2}\cos (\pi (\frac{\Vert x-z_{i}\Vert -\bar{r}}{r_{d}-\bar{r}})) with the smooth scheduling function s_{i}(x,z_{i})=\mathscr {F}_{i}(x,z_{i}), and P_{a}(\zeta) is selected as P_{a}(\zeta)=0. For value function approximation, the StaF basis \sigma _{0}(x,c(x))=[\begin{array}{cccc}k_{0,1}, & k_{0,2}, & k_{0,3}, & k_{0,4}\end{array}]^{T} is used where k_{0,i}\triangleq k(x,c_{i}(x))=e^{x^{T}(x+0.05d_{i})}-1 , i=1,2,3,4, and the offsets are selected as d_{1}=[\begin{array}{ccc}1, & 0, & 0\end{array}]^{T}, d_{2}=[\begin{array}{ccc}-0.333, & 0.943, & 0\end{array}]^{T}, d_{3}=[\begin{array}{ccc}-0.333, & -0.471, & 0.471\end{array}]^{T}, and d_{4}=[\begin{array}{ccc}-0.333, & -0.471, & -0.471\end{array}]^{T}. The StaF basis \sigma _{i} for each obstacle is selected to be the same as the agent, except that the state changes from x to z_{i}. To perform BE extrapolation, five points are selected at random each time step from a \text{0.05}\nu (x(t))\times \text{0.05}\nu (x(t)) uniform distribution centered at the current state, where \nu (x(t))\triangleq \frac{x(t)^{T}x(t)}{1+x(t)^{T}x(t)} . The initial critic and actor weights and gains are selected as W_{c}(0)=W_{a}(0)=\text{0.4}\times \text{1}_{16\times 1}, \Gamma _{c}(0)=\text{300}\times I_{16}, and \Gamma _{a}=I_{16}. Table I summarizes the selected parameters.

TABLE I Initial Conditions and Parameters Selected for the Mobile Robot Simulation
B. Results

Figs. 2(a) and (b) shows that the agent and policy converge while detecting and navigating around the no-entry zones. Specifically, Fig. 2(b) shows that the agent's policy changes when the agent detects each no-entry zone shown in Fig. 2(e) , and hence, modifies the agent's trajectory shown in Fig. 2(f) . Fig. 2(c) and (d) shows that the critic and actor weights for value function approximation remain bounded. However, they can not be compared to the ideal values since they are unknown due to the StaF nature of the function approximation method.
Fig. 2.

States, control policy, and weight estimates are shown in addition to the distances between the agent and each avoidance region center and the agent's phase space portrait. Fig. 2(a) shows that the agents states converge to the origin. The input, shown in Fig. 2(b) , causes the agent to steer off course as shown by the trajectory change of x_{2} in Fig. 2(a) . The distance between the agent's center and each avoidance region is shown in Fig. 2(e) ; the solid horizontal line represents r_{a}=0.1, and the two dashed horizontal lines represent r_{d}=0.6 and \overline{r}=0.55 , respectively. (a) The agent states. (b) The agent approximate optimal input. (c) The critic weight estimates. (d) The actor weight estimates. (e) The distance between the agent and avoidance regions. (f) The phase space portrait for x_{1}(t) and x_{2}(t) of the agent.

Show All
C. Nonlinear System

In addition to the mobile robot simulation, a simulation for a nonlinear system is performed with system dynamics (see [56, Ch. 5.2]) given by \begin{align*} f\left(x(t)\right)=\left[\begin{array}{c}-x_{1}(t)+x_{2}(t),\\ \left(\begin{array}{c}-\frac{1}{2}x_{1}(t)\\ -\frac{1}{2}x_{2}(t)\left(1-\left(\cos \left(2x_{1}(t)\right)+2\right)^{2}\right) \end{array}\right) \end{array}\right] \end{align*}
View Source \begin{align*} f\left(x(t)\right)=\left[\begin{array}{c}-x_{1}(t)+x_{2}(t),\\ \left(\begin{array}{c}-\frac{1}{2}x_{1}(t)\\ -\frac{1}{2}x_{2}(t)\left(1-\left(\cos \left(2x_{1}(t)\right)+2\right)^{2}\right) \end{array}\right) \end{array}\right] \end{align*} and \begin{align*} g\left(x(t)\right)=\left[\begin{array}{cc}\sin \left(2x_{1}(t)\right)+2, & 0,\\ 0, & \cos \left(2x_{1}(t)\right)+2 \end{array}\right]. \end{align*}
View Source \begin{align*} g\left(x(t)\right)=\left[\begin{array}{cc}\sin \left(2x_{1}(t)\right)+2, & 0,\\ 0, & \cos \left(2x_{1}(t)\right)+2 \end{array}\right]. \end{align*} Three heterogeneous obstacles are considered. The first and second obstacles were designed to converge to z_{1}^{e}= [\begin{array}{cc}-0.8, & 0.5\end{array}]^{T} and z_{2}^{e}=[\begin{array}{cc}-0.1, & -1.1\end{array}]^{T}, respectively, while the third obstacle oscillated around z_{3}^{e}=[\begin{array}{cc}0, & 0\end{array}]^{T} at a radius of 1.13. The functions \mathscr {F}_{i}(x,z_{i}), s_{i}(x,z_{i}), and P_{a}(\zeta) are selected to be the same as the mobile robot simulation. The basis used for value function approximation for the agent is selected as \sigma _{0}(x,c(x))=[\begin{array}{ccc}k_{\sigma,1}, & k_{\sigma,2}, & k_{\sigma,3}\end{array}]^{T}, where k_{\sigma,i}\triangleq k_{\sigma }(x,c_{i}(x))=e^{x^{T}(x+0.005\nu (x(t))d_{i})}-1, i=1,2, 3 where \nu (x(t)) is defined in the mobile robot simulation and the offsets are selected as d_{1}=[\begin{array}{cc}0, & 1\end{array}]^{T}, d_{2}=[\begin{array}{cc}-0.866, & -0.5\end{array}]^{T}, d_{3}=[\begin{array}{cc}0.866 & -0.5\end{array}]^{T}. Moreover, the basis used for the each obstacles is selected to be the same as for the agent. A single point was selected from a \text{0.005}\nu (x(t))\times \text{0.005}\nu (x(t)) uniform distribution centered at the current state and is used to perform BE extrapolation. A projection algorithm was used on the actor weight estimates. Table II shows the selected parameters, while the initial actor, critic weights, and least-squares gains are selected as W_{c}(0)=W_{a}(0)=\text{0.4}\times \text{1}_{12\times 1}, \Gamma _{c}(0)=\text{1000}\times I_{12}, and \Gamma _{a}=I_{12}, the selected parameters are shown in the table.

TABLE II Initial Conditions and Parameters Selected for the Nonlinear System Simulation
D. Results

Fig. 3(a) and (b) shows that the agent and policy converge to the origin. However, when a no-entry zone comes into the sensing radius of the agent, shown in Fig. 3(e) , the input in Fig. 3(b) steers the agent off course. This is seen by the change in the agent's trajectory and is shown in Fig. 3(f) . Moreover, when the agent senses the no-entry zones, their basis is turned on and the corresponding actor and critic weights are updated as seen in Fig. 3(c) and (d) . It is seen that the weights remain bounded. Similar to the previous simulation, the weights can not be compared to the ideal values since they are unknown.
Fig. 3.

States, control policy, and weight estimates for the nonlinear system simulation are shown in addition to the distances between the agent and each avoidance region center and the agent's phase space portrait. Fig. 3(a) shows that the agents states converge to the origin, but go off course when obstacles are sensed. Fig. 3(b) shows the input for the agent, which acts abruptly as obstacles are sensed. The distance between the agent's center and each avoidance region is shown in Fig. 3(e) ; the solid horizontal line represents r_{a}=0.1, and the two dashed horizontal lines represent r_{d}=0.7 and \overline{r}=0.5 , respectively. (a) The agent states. (b) The agent approximate optimal input. (c) The critic weight estimates. (d) The actor weight estimates. (e) The distance between the agent and avoidance regions. (f) The phase space portrait for x_{1}(t) and x_{2}(t) of the agent.

Show All
SECTION VII.
Extension to Uncertain Number of Avoidance Regions and Uncertain Systems

In Section II – V , the HJB in (10) required the number of no-entry zones in the operating domain to be known, which may not always be available. In this section, an extension is provided which alleviates the need to know how many no-entry zones are in the operating domain. Furthermore, by adding and subtracting P_{a}(x,Z), the following value function is introduced: \begin{equation*} V^{*}\left(x(t),Z(t)\right)=P_{a}\left(x(t),Z(t)\right)+V^{\#}\left(x(t),Z(t)\right) \tag{37} \end{equation*}
View Source \begin{equation*} V^{*}\left(x(t),Z(t)\right)=P_{a}\left(x(t),Z(t)\right)+V^{\#}\left(x(t),Z(t)\right) \tag{37} \end{equation*} where V^{\#}(x(t),Z(t)) is an approximation error of the optimal value function. Furthermore, the function V^{\#}(x,Z) can be interpreted as time-varying map V_{t}^{\#}:\mathbb {R}^{n}\times \mathbb {R}_{\geq t_{0}} such that V_{t}^{\#}(x,t)=V^{\#}(x,Z) [57] . Therefore, (37) is rewritten as \begin{equation*} V^{*}\left(x(t),Z(t)\right)=P_{a}\left(x(t),Z(t)\right)+V_{t}^{\#}\left(x(t),t\right). \tag{38} \end{equation*}
View Source \begin{equation*} V^{*}\left(x(t),Z(t)\right)=P_{a}\left(x(t),Z(t)\right)+V_{t}^{\#}\left(x(t),t\right). \tag{38} \end{equation*} The optimal controller u^{*} is admissible; hence, the value function V^{*}(x,Z) is finite and x,Z\notin \Omega . Therefore, P_{a}(x,Z) is continuous for x,Z\notin \Omega, hence (38) can be approximated via the StaF approximation method. However, because time does not lie on a compact domain, V_{t}^{\#} can not be approximated directly using time as an input to the NN. To address this technical challenge, the mapping \phi :\mathbb {R}_{\geq t_{0}}\rightarrow [0,\alpha ], \alpha \in \mathbb {R}_{>0} is introduced such that V_{t}^{\#}(x(t),t)=V_{t}^{\#}(x(t),\phi ^{-1}(\kappa))=V_{\kappa }^{\#}(x(t),\kappa) where \kappa =\phi (t). Now, \kappa lies on a compact set and the function V_{\kappa }^{\#}(x,\kappa) can be approximated using the StaF method as \begin{align*} &V^{*}\left(x(t),Z(t)\right)=P_{a}\left(x(t),Z(t)\right)\\ &\quad+W^{T}\left(\zeta ^{\#}(t)\right)\sigma \left(y(t),c\left(\zeta ^{\#}(t)\right)\right)+\varepsilon \left(y(t),\zeta ^{\#}(t)\right) \end{align*}
View Source \begin{align*} &V^{*}\left(x(t),Z(t)\right)=P_{a}\left(x(t),Z(t)\right)\\ &\quad+W^{T}\left(\zeta ^{\#}(t)\right)\sigma \left(y(t),c\left(\zeta ^{\#}(t)\right)\right)+\varepsilon \left(y(t),\zeta ^{\#}(t)\right) \end{align*} with \sigma (\zeta ^{\#},c(\zeta ^{\#}))=[{{\sigma _{0}(x,c_{0}(x))}\atop{s_{0}(x)\sigma _{1}(\kappa,c_{1}(\kappa))}}], where \zeta ^{\#}\triangleq [x^{T},\kappa ]^{T}, y\triangleq [y_{x}^{T},y_{\kappa }]^{T}\in \overline{B_{r}(\zeta ^{\#})}, and s_{0}:\mathbb {R}^{n}\rightarrow [0,1] is a smooth function such that s_{0}(0_{2\times 1})=0.

Moreover, since P_{a}(x,Z)=\sum _{i\in \mathcal {M}}P_{a,i}(x,z_{i}) is designed to be a bounded positive semidefinite symmetric function, it follows that \frac{\partial P_{a,i}(x,z_{1},\ldots,z_{m})}{\partial x}=-\frac{\partial P_{a,i}(x,z_{1},\ldots,z_{m})}{\partial z_{i}} for all i\in \mathcal {M}; hence, the HJB is represented as \begin{align*} 0 =&r(x,Z,u)+\frac{\partial V_{\kappa }^{\#}\left(\zeta ^{\#}\right)}{\partial \zeta ^{\#}}\left(F^{\#}\left(\zeta ^{\#}\right)+G^{\#}\left(\zeta ^{\#}\right)u\right) \\ & +\sum _{i=1}^{M}\frac{\partial P_{a,i}}{\partial x}\left(f(x)+g(x)u-\mathscr {F}_{i}\left(x,z_{i}\right)h_{i}\left(z_{i}\right)\right) \tag{39} \end{align*}
View Source \begin{align*} 0 =&r(x,Z,u)+\frac{\partial V_{\kappa }^{\#}\left(\zeta ^{\#}\right)}{\partial \zeta ^{\#}}\left(F^{\#}\left(\zeta ^{\#}\right)+G^{\#}\left(\zeta ^{\#}\right)u\right) \\ & +\sum _{i=1}^{M}\frac{\partial P_{a,i}}{\partial x}\left(f(x)+g(x)u-\mathscr {F}_{i}\left(x,z_{i}\right)h_{i}\left(z_{i}\right)\right) \tag{39} \end{align*} where F^{\#}(\zeta ^{\#})\triangleq [\begin{array}{c}f(x)^{T},\;\frac{\partial \kappa }{\partial t}\end{array}]^{T}, and G^{\#}(\zeta ^{\#})\triangleq [g(x)^{T}, 0_{m\times 1}]^{T}. The HJB in (39) requires the knowledge of the uncertain dynamics f(x) and h_{i}(z_{i}). Using a NN approximator, the time derivative of P_{a} is written as \begin{align*} \dot{P}_{a} & =\sum _{i=1}^{M}\frac{\partial P_{a,i}}{\partial x}\left(f(x)+g(x)u-\mathscr {F}_{i}\left(x,z_{i}\right)h_{i}\left(z_{i}\right)\right)\\ & =Y_{p}(x,Z)\theta +\varepsilon _{p}(x,Z) \end{align*}
View Source \begin{align*} \dot{P}_{a} & =\sum _{i=1}^{M}\frac{\partial P_{a,i}}{\partial x}\left(f(x)+g(x)u-\mathscr {F}_{i}\left(x,z_{i}\right)h_{i}\left(z_{i}\right)\right)\\ & =Y_{p}(x,Z)\theta +\varepsilon _{p}(x,Z) \end{align*} where Y_{p}:\mathbb {R}^{n}\times \mathbb {R}^{Mn}\rightarrow \mathbb {R}^{1\times l_{p}} is a selected basis such that Y_{p}(x,Z)=0_{1\times l_{p}} when \Vert x-z_{i}\Vert >r_{d}, for all i\in \mathcal {M}, \theta \in \mathbb {R}^{l_{p}} is an unknown weight, and \varepsilon _{p}:\mathbb {R}^{n}\times \mathbb {R}^{Mn}\rightarrow \mathbb {R} is the unknown function approximation error. Likewise the agent drift dynamics can be represented as f(x(t))=Y_{f}(x(t))\varXi+\varepsilon _{f}(x(t)) with Y_{f}:\mathbb {R}^{n}\rightarrow \mathbb {R}^{n\times l_{f}} being a known basis, \varXi\in \mathbb {R}^{l_{f}} an unknown weight, and \varepsilon _{f}:\mathbb {R}^{n}\rightarrow \mathbb {R}^{n} the function approximation error. 5

Assumption 6:

There exists constants \overline{\varepsilon }_{p},\overline{\varepsilon }_{f},\overline{Y}_{\!\!f},\overline{Y}_{\!\!p},\overline{\theta },\overline{\varXi}\in \mathbb {R}_{>0} such that \sup _{\zeta \in \chi }\Vert Y_{p}(x,Z)\Vert \leq \overline{Y}_{\!\!p} , \sup _{\zeta \in \chi }\Vert \varepsilon _{p}(x,Z)\Vert \leq \overline{\varepsilon }_{p} , \sup _{x\in \chi }\Vert Y_{f}(x)\Vert \leq \overline{Y}_{f} , \sup _{x\in \chi }\Vert \varepsilon _{f}(x)\Vert \leq \overline{\varepsilon }_{f} , \Vert \theta \Vert \leq \overline{\theta } , and \Vert \varXi\Vert \leq \overline{\varXi} [23] , [58] .

Using the estimates \hat{W}_{c} , \hat{W}_{a} , \hat{\theta } , and \hat{\varXi} in (39) , the approximate BE \hat{\delta }:\mathbb {R}^{n+1}\times \mathbb {R}^{n+1}\times \mathbb {R}^{\mathcal {N}}\times \mathbb {R}^{L}\times \mathbb {R}^{L}\times \mathbb {R}^{l_{f}+l_{p}}\rightarrow \mathbb {R} is defined as \begin{align*} &\hat{\delta }\left(y,\zeta ^{\#},Z,\hat{W}_{c},\hat{W}_{a},\hat{\theta },\hat{\varXi}\right)\triangleq Y_{p}\left(y_{x},Z\right)\hat{\theta } \\ &\quad+\!\omega ^{\#}\left(y,\zeta ^{\#},Z,\hat{W}_{a},\hat{\varXi}\right)^{T}\hat{W}_{c}\!+\!r\left(y_{x},Z,\hat{u}\left(y,\zeta ^{\#},Z,\hat{W}_{a}\right)\right) \tag{40} \end{align*}
View Source \begin{align*} &\hat{\delta }\left(y,\zeta ^{\#},Z,\hat{W}_{c},\hat{W}_{a},\hat{\theta },\hat{\varXi}\right)\triangleq Y_{p}\left(y_{x},Z\right)\hat{\theta } \\ &\quad+\!\omega ^{\#}\left(y,\zeta ^{\#},Z,\hat{W}_{a},\hat{\varXi}\right)^{T}\hat{W}_{c}\!+\!r\left(y_{x},Z,\hat{u}\left(y,\zeta ^{\#},Z,\hat{W}_{a}\right)\right) \tag{40} \end{align*} where \omega ^{\#}(y,\zeta ^{\#},Z,\hat{W}_{a},\hat{\varXi})\triangleq \nabla \sigma (y,c(\zeta ^{\#}))(Y^{\#}(y)\hat{\varXi}^{\#}+G^{\#}(y)\hat{u}(y,\zeta ^{\#},Z,\hat{W}_{a})) , Y^{\#}(y)\triangleq [\begin{array}{cc}Y_{f}(y_{x})^{T}, & \frac{\partial y_{\kappa }}{\partial t}\end{array}]^{T} , \hat{\varXi}^{\#}\triangleq [{{\hat{\varXi}}\atop{0_{1\times 1}}} {{0_{l_{f}\times 1}}\atop{1}}] , and \begin{align*} &\hat{u}\left(y,\zeta ^{\#},Z,\hat{W}_{a}\right)\triangleq -\mu _{\text{sat}}\operatorname{Tanh}\left (\frac{1}{2\mu _{\text{sat}}}R^{-1}G^{\#T}(y)\right. \\ &\times \Big (\nabla P_{a}^{T}\left(y_{x},Z\right)+\nabla \sigma ^{T}\left(y,c\left(\zeta ^{\#}\right)\right)\hat{W}_{a}\Big)\Big) \tag{41} \end{align*}
View Source \begin{align*} &\hat{u}\left(y,\zeta ^{\#},Z,\hat{W}_{a}\right)\triangleq -\mu _{\text{sat}}\operatorname{Tanh}\left (\frac{1}{2\mu _{\text{sat}}}R^{-1}G^{\#T}(y)\right. \\ &\times \Big (\nabla P_{a}^{T}\left(y_{x},Z\right)+\nabla \sigma ^{T}\left(y,c\left(\zeta ^{\#}\right)\right)\hat{W}_{a}\Big)\Big) \tag{41} \end{align*} where \nabla P_{a}(y_{x},Z)\!\triangleq\! [\frac{\partial P_{a}(y_{x},Z)}{\partial x},\;\frac{\partial P_{a}(y_{x},Z)}{\partial \kappa }]\!=\![\frac{\partial P_{a}(y_{x},Z)}{\partial x},\;0]. Using \hat{\delta } , the instantaneous BEs and approximate policies in (18) – (21) are redefined as \delta _{t}(t)\triangleq \hat{\delta }(\zeta ^{\#}(t),\zeta ^{\#}(t), Z(t),\hat{W}_{c}(t),\hat{W}_{a}(t),\hat{\theta }(t),\hat{\varXi}(t)) , \delta _{k}(t)\!\triangleq\! \hat{\delta }(\zeta _{k}^{\#}(t),\zeta ^{\#}(t),Z(t),\hat{W}_{c}(t),\hat{W}_{a}(t),\hat{\theta }(t),\hat{\varXi}(t)) , u(t)\triangleq \hat{u}(\zeta ^{\#}(t),\zeta ^{\#}(t),Z(t),\hat{W}_{a}(t)) , and \hat{u}_{k}(t)\triangleq \hat{u}(\zeta _{k}^{\#}(t),\zeta ^{\#}(t),Z(t),\hat{W}_{a}(t)) , respectively.

Assumption 7. [22] , [23] :

There exists a compact set \varTheta\subset \mathbb {R}^{l_{p}+l_{f}} , known a priori , which contains the unknown parameter vectors \theta and \varXi. Let \tilde{X}\triangleq [\tilde{\varXi}^{T},\tilde{\theta }^{T}]^{T}=[(\varXi-\hat{\varXi})^{T},(\theta -\hat{\theta })^{T}]^{T} and \hat{X}=[\hat{\varXi}^{T},\hat{\theta }^{T}]^{T} denote the total concatenated vector of parameter estimate errors and parameter estimates, respectively. The estimates \hat{X}:\mathbb {R}_{\geq t_{0}}\rightarrow \mathbb {R}^{l_{p}+l_{f}} are updated based on switched update laws of the form \begin{equation*} \dot{\hat{X}}(t)=f_{Xs}\left(\hat{X}(t),t\right),\quad \hat{X}\left(t_{0}\right)\in \varTheta \tag{42} \end{equation*}
View Source \begin{equation*} \dot{\hat{X}}(t)=f_{Xs}\left(\hat{X}(t),t\right),\quad \hat{X}\left(t_{0}\right)\in \varTheta \tag{42} \end{equation*} where s\in \mathbb {N} is the switching index with \lbrace f_{Xs}:\mathbb {R}^{l_{p}+l_{f}}\times \mathbb {R}_{\geq t_{0}}\rightarrow \mathbb {R}^{l_{p}+l_{f}}\rbrace _{s\in \mathbb {N}} being a family of continuously differentiable functions. There exist a continuously differentiable function V_{\theta }:\mathbb {R}^{l_{p}+l_{f}}\times \mathbb {R}_{\geq t_{0}}\rightarrow \mathbb {R}_{\geq 0} that satisfies \begin{align*} &\underline{\nu }_{\theta }\left(\left\Vert \tilde{X}\right\Vert \right)\leq V_{\theta }\left(\tilde{X},t\right)\leq \overline{\nu }_{\theta }\left(\left\Vert \tilde{X}\right\Vert \right) \tag{43}\\ &\frac{\partial V_{\theta }\left(\tilde{X},t\right)}{\partial \tilde{X}}\left(-f_{Xs}\left(\tilde{X}(t),t\right)\right)+\frac{\partial V_{\theta }\left(\tilde{X},t\right)}{\partial t} \\ &\quad\leq -K_{\theta }\left\Vert \tilde{X}\right\Vert ^{2}+D\left\Vert \tilde{X}\right\Vert \tag{44} \end{align*}
View Source \begin{align*} &\underline{\nu }_{\theta }\left(\left\Vert \tilde{X}\right\Vert \right)\leq V_{\theta }\left(\tilde{X},t\right)\leq \overline{\nu }_{\theta }\left(\left\Vert \tilde{X}\right\Vert \right) \tag{43}\\ &\frac{\partial V_{\theta }\left(\tilde{X},t\right)}{\partial \tilde{X}}\left(-f_{Xs}\left(\tilde{X}(t),t\right)\right)+\frac{\partial V_{\theta }\left(\tilde{X},t\right)}{\partial t} \\ &\quad\leq -K_{\theta }\left\Vert \tilde{X}\right\Vert ^{2}+D\left\Vert \tilde{X}\right\Vert \tag{44} \end{align*} for all t\in \mathbb {R}_{\geq t_{0}} , s\in \mathbb {N} , and \tilde{X}\in \mathbb {R}^{l_{p}+l_{f}}. In (43) , \underline{\nu }_{\theta },\overline{\nu }_{\theta }:\mathbb {R}_{\geq 0}\rightarrow \mathbb {R}_{\geq 0} are class \mathcal {K} functions. In (44) , K_{\theta }\in \mathbb {R}_{>0} is an adjustable parameter, D\in \mathbb {R}_{>0} is a positive constant, and the ratio \frac{D}{K_{\theta }} is sufficiently small. 6

Remark 10:

If f(x(t))=0_{n\times 1} , then Y^{\#}(y) and \hat{\varXi}^{\#} simplify to Y^{\#}(y)\triangleq [\begin{array}{cc}0_{l_{f}\times n}, & \frac{\partial y_{\kappa }}{\partial t}\end{array}]^{T} and \hat{\varXi}^{\#}\triangleq [ {{0_{l_{f}\times 1}}\atop{0_{1\times 1}}} {{0_{l_{f}\times 1}}\atop{1}}] , respectively. Furthermore, \varXi does not need to be estimated for single integrator dynamics and the concatenated systems then reduce to \tilde{X}\triangleq \tilde{\theta } and f_{Xs}(\hat{X}(t),t)\triangleq f_{\theta s}(\hat{\theta }(t),t).

The conditions (43) and (44) in Assumption 7 imply that V_{\theta } can be used as a candidate Lyapunov function to show the parameter estimates \hat{\theta } and \hat{\varXi} converge to a neighborhood of the true values. Update laws using CL-based methods can be designed to satisfy Assumption 7 ; examples of such update laws can be found in  [59] – [60] [61] [62] . The main result for the extension to systems with uncertainties and an unknown number of avoidance regions uses V_{\theta }+V_{L} as a candidate Lyapunov function and is summarized in the following theorem.

Theorem 2:

Provided Assumptions 2 – 7 along with the sufficient conditions in (30) – (33) are satisfied, and StaF kernels are selected such that \nabla W , \varepsilon , \nabla \varepsilon , are sufficiently small, then the update laws in (22) – (24) with (41) , \delta _{t}(t) , and \delta _{k}(t) ensure that the state x and input u(t) , and weight approximation errors \tilde{W}_{a} , \tilde{W}_{c} , \tilde{\theta } , \tilde{\varXi} are UUB; furthermore, states x(t) , z_{i}(t) starting outside of \Omega remain outside of \Omega .

Proof:

The proof is a combination of Assumption 7 with Theorem 1 by using V_{L}+V_{\theta } as a candidate Lyapunov function; hence, the proof is omitted to alleviate redundancy. \blacksquare

SECTION VIII.
Simulation-in-the-Loop Experiments

In Section VI , simulations where performed to demonstrate the developed approach. To demonstrate the robustness of the developed method, experiments are performed on a quadcopter avoiding virtual obstacles. Specifically, three experiments are conducted to demonstrate the ability of an aerial vehicle to be autonomously regulated to the origin while avoiding dynamic avoidance regions. For each experiment, a Parrot Bebop 2.0 quadcopter is used as the aerial vehicle. The developed quadcopter controller requires feedback of its and the obstacle's position and orientation (pose). The pose of the quadcopter is obtained by a NaturalPoint, Inc. OptiTrack motion capture system at 120 Hz. Using the robotic operating system (ROS) Kinetic framework and the bebop_ autonomy package developed by  [63] running on Ubuntu 16.04, the control policies are calculated for the quadcopter. The control policy is communicated from a ground station which broadcasts velocity commands at 120 Hz over the 5-GHz Wi-Fi channel. The developed control policy is implemented as a velocity command to the quadcopter. While this allows an effective demonstration of the underlying strategy, improved performance could be obtained by implementing the policies through acceleration commands that do not rely on the onboard velocity tracking controller. Such an implementation could also have additional implications due to input constraints for acceleration commands.

The experiments are performed using two-dimensional (2-D) Euclidean coordinates (without the inclusion of altitude) for the state x(t) for ease of experimental execution and implementation. However, since the development does not restrict the state dimension, experiments can also be extended to use 3-D Euclidean coordinates. All three experiments use simplified quadcopter dynamics represented by (1) with f(x(t))=0_{2\times 1} and g(x(t))=I_{2} so that \dot{x}=u , where, without a loss of generality, x(t)\in \mathbb {R}^{2} is the composite vector of the 2-D Euclidean coordinates, with respect to the inertial frame and u\in \mathbb {R}^{2} are velocity commands broadcast to the quadcopter. A supplementary video of the experiment accompanies this article, available for download 7 (included in the submitted files). For the first two experiments, virtual spheres are used as the dynamic avoidance regions. The virtual spheres, which evolve according to linear oscillatory dynamics, are generated using ROS via Ubuntu on the ground station. The positions of the virtual spheres in the inertial frame are used in the designed method to interact with the vehicle, only when each position is within the detection radius of the quadcopter. The supplementary video shows how the quadcopter interacts with the virtual spheres. For the third experiment, one of the virtual spheres is replaced by a remotely controlled (i.e., human-piloted) quadcopter.
A. Experiment One

The first experiment is performed using the method developed in Sections II – V . Three virtual avoidance regions are generated using heterogeneous oscillatory linear dynamics. The functions \mathscr {F}_{i}(x,z_{i}) , s_{i}(x,z_{i})=\mathscr {F}_{i}(x,z_{i}) , are selected to be the same as in Section VI while P_{a}(\zeta) is selected to be P_{a}(\zeta)=\sum _{i=1}^{M}(\min \lbrace 0,\frac{\Vert x-z_{i}\Vert ^{2}-r_{d}^{2}}{(\Vert x-z_{i}\Vert ^{2}-r_{a}^{2})^{2}+r_{\varepsilon }}\rbrace)^{2}. For value function approximation, the agent is selected to have the StaF basis \sigma _{0}(x,c(x))=[x^{T}c_{1}(x),x^{T}c_{2}(x),x^{T}c_{3}(x)]^{T} , where c_{i}(x)=x+\nu (x)d_{i} , i=1,2,3 , where \nu (x) is redefined as \nu (x)\triangleq \frac{0.5x^{T}x}{1+x^{T}x} and the offsets are selected as d_{1}=[\begin{array}{cc}0, & -1\end{array}]^{T} , d_{2}=[\begin{array}{cc}0.866, & -0.5\end{array}]^{T} , and d_{3}=[\begin{array}{cc}-0.866, & -0.5\end{array}]^{T}. The StaF basis \sigma _{i} for each obstacle is selected to be the same as the agent, except that the state changes from x to z_{i}. Assumption 5 discussed how the extrapolated regressors \omega _{k} are design variables. Thus, instead of using input–output data from a persistently exciting system, the dynamic model can be used and evaluated at a single time-varying extrapolated state to achieve sufficient excitation. It was shown in [22, Sec. 6.3] that the use of a single time-varying extrapolated point results in improved computational efficiency when compared using a large number of stationary extrapolated states. Motivated by this insight, at each time a single point is selected at random from a \text{0.2}\nu (x(t))\times \text{0.2}\nu (x(t)) uniform distribution centered at the current state. The initial critic and actor weights and gains are selected as W_{c}(0)=U[0,4]1_{12\times 1} , W_{a}(0)=1_{12\times 1} , \Gamma _{c}(0)=I_{12} , and \Gamma _{a}=I_{12} , and the selected parameters are shown in Table III .
TABLE III Initial Conditions and Parameters Selected for the Experiment
B. Experiment Two

The second experiment is performed using the extension in Section VII and similar to experiment one, three virtual avoidance regions are generated with heterogeneous oscillatory linear dynamics. The agent has the same basis \sigma _{0}(x) as the first experiment, while the basis \sigma _{1}(\kappa,c(\kappa)) is selected as \sigma _{1}(\kappa,c(\kappa))=[\kappa ^{T}c_{1}(\kappa),\kappa ^{T}c_{2}(\kappa),]^{T} , where \kappa =\phi (t)\triangleq \frac{0.25}{0.01t+1} and c_{i}(\kappa)=\kappa +\nu (\kappa)d_{i} , i=1,2 where \nu (\kappa) is the same function as in the first experiment except evaluated at \kappa and the offsets are selected as d_{1}=0.25 , and d_{2}=0.05. For the total basis \sigma (\zeta ^{\#},c(\zeta ^{\#})) , the function s_{0}(x) is selected as s_{0}(x)=\frac{\nu (x)}{0.5}. The initial critic and actor weights and adaptive gains are selected as as W_{c}(0)=U[0,4]1_{5\times 1} , W_{a}(0)=1_{5\times 1} , and \Gamma _{a}=I_{5}. The rest of the parameters are selected to remain the same as in the first experiment and are shown in Table III . Since the agent dynamics are modeled as single integrator dynamics with f(x(t))=0_{2\times 1} , system identification was not performed on the agent. However, to approximate \theta in Section VII , the ICL method in [62, Sec. IV.B] was utilized with the basis Y_{p}(x,Z)=\operatorname{Tanh}(V_{p}^{T}\nabla P_{a}(y_{x},Z)^{T}) , where V_{p}=U[-5,5]1_{3\times 10} is a constant weight matrix. To keep the weight estimates bounded, a projection algorithm was used similar to [62, Sec. IV.B] and the update laws were turned off when no avoidance regions were sensed. Not performing system identification on the agent reduces redundancy in parameter identification because the unknown weight \theta in the function in the time derivative of P_{a} is already being approximated. Furthermore, as stated in Footnote 5, if the agent is implemented using single integrator dynamics, then system identification can be ignored on the agent drift dynamics f(x(t)).
C. Experiment Three

The third experiment is performed using the extension in Section VII where the first avoidance region, denoted by the state z_{1} and represented by another Parrot Bebop quadcopter, is flown/controlled manually by hand. The virtual avoidance regions with states z_{2} and z_{3} are simulated as in the previous experiments. The radii were changed to r_{d}=1.0 , \bar{r}=0.7 , and r_{a}=0.45 meters (m) to reduce the chance of the quadcopters colliding, the gains q_{x} , q_{z} where changed to q_{x}=\operatorname{diag}\lbrace 0.5,0.5\rbrace and q_{z}=\operatorname{diag}\lbrace 3.0,3.0\rbrace , and the rest of the parameters remained the same as in the second experiment.
D. Results

The first experimental validation for the development in Sections II – V are shown in Figs. 4 and 5 . Fig. 4(a) and (b) shows that the agent, as well as the agent's control policy, remains bounded around the origin. Fig. 4(b) shows that the control of the agent is bounded by 0.5 \frac{m}{s} even in the presence of the mobile avoidance regions. The input does not converge to zero because of aerodynamic disturbances, when the quadcopter reaches the origin. The critic and actor weight estimates remain bounded and converge to steady-state values, as shown in Fig. 4(c) and (d) . However, because of the StaF nature of the StaF approximation method, the ideal weights are unknown; hence the estimate cannot be compared to their ideal values. Even though the agent enters the detection region as shown by Figs. 4(e) and 5(a) , the developed method drives the agent away from the avoidance regions and toward the origin. When encountering avoidance region z_{2} between the \text{8}{\text{th}} and \text{12}{\text{th}} seconds, the agent was able to maneuver around the avoidance region without collision despite multiple encounters with it because the avoidance region was moving close to the origin and obstructing the path. Moreover, Fig. 6(a) shows the change in velocity when the agent encounters the avoidance region.
Fig. 4.

States, control policy, and weight estimates are shown in addition to the distances between the agent and each avoidance region center for the first experiment. Fig. 4(a) shows that the agents states converge to a close neighborhood of the origin. When the agent detects the avoidance regions, the commanded input, shown in Fig. 4(b) , causes the agent to steer off course as shown by the change in the trajectory of x_{2} in Fig. 4(a) . The distance between the center of the agent and each avoidance region is shown in Fig. 4(e) ; the two dashed horizontal lines represent the detection radius and conflict radius denoted by r_{d}=\text{0.7} m and \overline{r}=\text{0.45} m, respectively, while the solid horizontal line represents the radius of the avoidance region denoted by r_{a}=\text{0.2} m. (a) The agent states. (b) The agent approximate optimal input. (c) The critic weight estimates. (d) The actor weight estimates. (e) The distance between the agent and avoidance regions.

Show All
Fig. 5.

Phase-space portrait for the agent and the positions of the agent and avoidance regions for each experiment. In each figure, the left plot shows the agent's phase-space portrait where the green circle is the agent's final position. The plots on the right of each figure show the agent's and avoidance regions positions at certain time instances where the diamond represents the agent state and the circles represent the avoidance regions. (a) The agent phase-space portrait (left) and the positions of the agent and avoidance regions (right) for the first experiment. (b) The agent phase-space portrait (left) and positions of the agent and avoidance region (right) for the second experiment. (c) The agent phase-space portrait (left) and positions of the agent and avoidance region (right) for the third experiment.

Show All
Fig. 6.

Relative velocities for each experiment. The relative velocities were numerically computed and filtered using a moving average filter with a window size of ten time-steps. In each figure, the blue line represents the relative velocity of the first state and the red line represents the relative velocity of the second state for each obstacle (i.e., \dot{x}_{1}(t)-\dot{z}_{i,1}(t) and \dot{x}_{2}(t)-\dot{z}_{i,2}(t) for i=1,2,3 , respectively). (a) Relative velocities for z_{1} (left), z_{2} (middle), and z_{3} (right) for the first experiment. (b) Relative velocities for z_{1} (left), z_{2} (middle), and z_{3} (right) for the second experiment. (c) Relative velocities for z_{1} (left), z_{2} (middle), and z_{3} (right) for the third experiment.

Show All

The second and third experiments were performed to validate the development in Section VII with the results shown in Figs. 5 – 8 . Specifically, the second experiment was performed using similar conditions and parameters as in the first experiment. Fig. 5(b) shows that the agent is capable of adjusting its path when it encounters the avoidance regions and the agent is regulated to the origin without colliding with the avoidance regions, while Fig. 6(b) shows the relative velocities between the agent and each avoidance region. The approximate value function and total cost for the first two experiments are shown in Fig. 7 . Both experiments resulted in similar costs and approximate value functions. Specifically, Fig. 7(a) shows that the approximate value function remains positive and converges to zero when the agent reaches the origin.
Fig. 7.

Approximate value functions and total costs for the three experiments. (a) Approximate value function. (b) Total cost.

Show All
Fig. 8.

States, control policy, and weight estimates of the agent are shown in addition to the distances between the agent and each avoidance region center for the third experiment. (a) The agent states. (b) The agent approximate optimal input. (c) The estimates of \theta . (d) The distance between the agent and avoidance regions. (e) The critic weight estimates. (f) The actor weight estimates.

Show All

Furthermore, the third experiment extends the second experiment further by substituting one of the autonomous avoidance regions for a nonautonomous one. Specifically, a manually controlled avoidance region is used, which is controlled to approach the agent throughout the experiment. Figs. 5(c) – 8 show the results of the experiment. In Fig. 5(c) , the agent is forced away from the direction of the origin, but still manages to redirect itself without colliding with the avoidance regions. The relative velocity for the third experiment is shown in Fig. 6(c) , which changes abruptly as each avoidance regions is sensed. The approximate value function and total cost for the third experiment are also shown in Fig. 7 . Since one of the avoidance regions was remotely controlled, its trajectory was nonautonomous; hence, the agent's trajectory differed when interacting with it and the applied control policy did not saturate as much compared to the first experiment, resulting in a smaller total cost. Fig. 8(a) shows that the agent is regulated to the origin and that its state is adjusted online in real time by the input, as shown in Fig. 8(b) , when it encounters the avoidance regions. The input remains bounded by the controller saturation of \text{0.5} \frac{\text{m}}{\text{s}} and converges to a small bounded residual of the origin. The estimates of the unknown weights \theta are shown in Fig. 8(c) , which remain bounded, but since the ideal basis is unknown and the ideal weights are unknown, the estimates cannot be compared to the actual weights. Fig. 8(d) shows the distance between the agent and each avoidance region center, and shows that the agent does not get within r_{a} of the avoidance regions. Moreover, as soon as the agent gets within \bar{r} of the avoidance region, it moves away from z_{i}. Additionally, when the agent detects the avoidance region, i.e., \Vert x-z_{i}\Vert \leq r_{d} , the control policy is adjusted, which can be seen from Fig. 8(b) and (d) . Moreover, the critic and actor weights estimates using the transformation in Section VII are shown in Fig. 8(e) and (f) , respectively. The figures show that the estimates remain bounded and converge to steady-state values. Similar to the first experiment, the ideal weights are unknown, thus the weight estimates cannot be compared to the ideal weights.

The results in Figs. 4 – 8 show that the developed method is capable of handling uncertain dynamic avoidance regions while regulating an autonomous agent. The agent locally detects the avoidance regions and then adjusts its path online. While experiments one and two used radii selected as r_{d}=0.7 , \overline{r}=0.45 , r_{a}=0.2 meters, the radii in experiment three were increased to r_{d}=1.0 , \bar{r}=0.7 , and r_{a}=\text{0.45} m, respectively. The increase in radii was because one of the obstacles moved at a higher relative velocity, and a larger distance was required to enable the agent to avoid collision. The optimal selection of the size of the detection region (e.g., as a function of the maximum agent speed, the obstacle relative velocity, and the sensing rate) including detection radii changing with relative agent and avoidance region velocities remains a subject for future research.
SECTION IX.
Conclusion

In this article, an online approximate motion planning strategy in the presence of mobile avoidance regions was developed. Because the avoidance regions need to only be known inside a detection radius, they were modeled using local dynamics. Since the avoidance regions were coupled with the agent in the HJB, the basis of the approximation also used the avoidance region state when approximating the value function. Because the states were not always known, a scheduling function was used to turn- off the basis, which then stopped updating the weight approximations for the avoidance regions when they were not detected. Theorem 1 showed the UUB of the states and that the states of the coupled system remain outside of the avoidance set. An extension to systems with uncertain dynamics and an unknown number of avoidance regions was presented, and Theorem 2 summarized the overall stability for the system with uncertainties. Simulations and three experiments were performed that demonstrated successful implementation of the developed motion planning and avoidance region evasion strategy.

Some possible topics of future research include—determining the size of the avoidance region based on the sampling rate of nearby obstacles, investigating the use of collision cones (cf.,  [39] , [40] ) instead of spherical-shaped avoidance regions, investigating methods to alleviate Assumption 3 , and extending the developed approach to multiple agents which cooperate but avoid other mobile obstacles. Investigating the relationships between the speed of the agent and avoidance regions and the respective sensing radius including dynamically changing radii also remains a topic for future research.

Moreover, in the presence of uncertainty, it is unclear how to develop a finite-time convergent update law. However, recent developments, such as  [64] and  [65] , could potentially provide insight into developing finite-time approximate optimal controllers. In addition, during the learning phase of adaptive systems, it is difficult to ensure safety guarantees are met, especially in safety-critical systems. Results such as  [66] – [67] [68] could provide insight into designing RL constrained approaches with safety specifications for motion planning. Such investigations into finite-time and safety-critical approximate optimal controllers are subjects of future research.
ACKNOWLEDGMENT

Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsoring agency.
Appendix A Stability Analysis Constants

In Section V , the positive constants \iota,\varphi _{ac}\in \mathbb {R}_{>0} are introduced, which are defined as \iota \triangleq \frac{\iota _{c}^{2}}{k_{c2}\underline{c}}+\frac{(\iota _{a1}+\iota _{a2})^{2}}{k_{a1}+k_{a2}}+{\overline{\Vert W^{T}\nabla \sigma\, +\,\sigma {}^{T}\nabla W\,+\,\nabla \epsilon\, +\,\nabla P_{a}\Vert }}\frac{G_{R}}{2}\overline{\Vert \nabla W^{T}\sigma +\nabla \varepsilon ^{T}\Vert }m , and \varphi _{ac}\triangleq k_{a1}+\frac{\overline{\Vert \nabla W\Vert }}{\underline{\Gamma }}\frac{G_{R}}{2}\overline{\Vert \nabla \sigma ^{T}\Vert }+k_{c1}{\sqrt{\frac{m}{\gamma _{1}}}}\mu _{\text{sat}}\overline{\Vert \nabla \sigma G\Vert }+\frac{k_{c2}}{N}\sum _{k=1}^{N}{\sqrt{\frac{m}{\gamma _{1}}}}\mu _{\text{sat}}\overline{\Vert \nabla \sigma _{k}G_{k}\Vert } , where \iota _{a1}\triangleq \overline{\Vert W^{T}\nabla \sigma +\sigma {}^{T}\nabla W\,+\,\nabla \epsilon\, +\,\nabla P_{a}\Vert }\frac{G_{R}}{2}\overline{\Vert \nabla \sigma ^{T}\Vert }\,+\frac{\overline{\Vert \nabla WF\Vert }}{\lambda _{\min }\lbrace \Gamma _{a}\rbrace }+ \frac{\overline{\Vert \nabla W\Vert }}{\lambda _{\min }\lbrace \Gamma _{a}\rbrace }\frac{G_{R}}{2}\overline{\Vert \nabla P_{a}\!+\!\nabla \sigma ^{T}W\Vert } , \iota _{a2}\!\triangleq \!k_{a2}\Vert W\Vert \!+\!k_{c1}\mu _{\text{sat}}\overline{\Vert \nabla \sigma G\Vert } {\scriptstyle{\sqrt{\frac{m}{\gamma _{1}}}}}\Vert W\Vert \!+\!\frac{k_{c2}}{N}\!\sum _{k\!=\!1}^{N}\mu _{\text{sat}}\overline{\Vert \nabla \sigma _{k}G_{k}\Vert }{\scriptstyle{\sqrt{\frac{m}{\gamma _{1}}}}}\overline{\Vert W\Vert } , and \iota _{c}\!\triangleq\! \frac{\overline{\Vert \nabla WF\Vert }}{\underline{\Gamma }}+\frac{\overline{\Vert \nabla W\Vert }}{\underline{\Gamma }}\frac{G_{R}}{2}\overline{\Vert \nabla P_{a}\!+\!\nabla \sigma ^{T}W\Vert }\!+\!k_{c1}\frac{1}{2{\sqrt{\gamma _{1}}}}\overline{\Vert \Delta \Vert }\!+\!\frac{k_{c2}}{N}\!\sum _{k=1}^{N}\frac{1}{2{\sqrt{\gamma _{1}}}}\overline{\Vert \Delta _{k}\Vert }.

Authors
Figures
References
Citations
Keywords
Metrics
Media
Footnotes
More Like This
Formation control of multiple nonholonomic mobile robots via dynamic feedback linearization

2009 International Conference on Advanced Robotics

Published: 2009
Event-Triggered Adaptive Optimal Control With Output Feedback: An Adaptive Dynamic Programming Approach

IEEE Transactions on Neural Networks and Learning Systems

Published: 2021
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
