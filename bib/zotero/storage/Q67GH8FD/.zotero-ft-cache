2016 International Conference on Unmanned Aircraft Systems (ICUAS) June 7-10, 2016. Arlington, VA USA

ThCTT2.4

LIDAR-Inertial Integration for UAV Localization and Mapping in Complex
Environments

Roberto Opromolla, Giancarmine Fasano, Giancarlo Rufino, Michele Grassi
Department of Industrial Engineering University of Naples Federico II Naples, Italy roberto.opromolla@unina.it, giancarmine.fasano@unina.it,
giancarlo.rufino@unina.it, michele.grassi@unina.it

Al Savvaris
School of Aerospace, Transport and Manufacturing Cranfield University
Cranfield, United Kingdom a.savvaris@cranfield.ac.uk

Abstract This paper presents customized techniques for autonomous localization and mapping of micro Unmanned Aerial Vehicles flying in complex environments, e.g. unexplored, full of obstacles, GPS challenging or denied. The proposed algorithms are aimed at 2D environments and are based on the integration of 3D data, i.e. point clouds acquired by means of a laser scanner (LIDAR), and inertial data given by a low cost Inertial Measurement Unit (IMU). Specifically, localization is performed by exploiting a scan matching approach based on a customized version of the Iterative Closest Point algorithm, while mapping is done by extracting robust line features from LIDAR measurements. A peculiarity of the line detection method is the use of the Principal Component Analysis which allows computational time saving with respect to traditional least squares techniques for line fitting. Performance of the proposed approaches is evaluated on real data acquired in indoor environments by means of an experimental setup including an UTM-30LX-EW 2D LIDAR, a Pixhawk IMU, and a Nitrogen board.
Keywords LIDAR; inertial sensors; localization; mapping; UAV; line detection
I. INTRODUCTION
Over the last years, great interest has arisen towards the necessity of developing innovative techniques aimed at improving the level of autonomy of Unmanned Aerial Vehicles (UAV), especially when flying in complex environments. This term generally refers to those mission profiles which can be both indoor and outdoor, which require the UAV to fly into unknown areas, potentially dangerous to the human life and could incorporate static and/or mobile obstacles.
In the case of widely open outdoor scenarios, where it is possible to detect reliable satellite signals, autonomous safe navigation of UAVs can be ensured by integrating the Inertial Measurement Unit (IMU) with the Global Navigation Satellite System (GNSS) received signal, e.g. GPS, utilizing sensor fusion architecture typically indicated

as GPS-INS [1, 2]. This solution, usually based on an Extended Kalman Filter (EKF), has been proved reliable by many researchers over the last few decades both in the case of fixed wing and rotary-wing UAVs [3, 4]. On the other hand, there are several scenarios covering both military and civilian applications (e.g. urban surveillance and pipeline monitoring), the GPS signal may be completely absent (i.e. indoor) or unreliable due to multipath, absorption and jamming phenomena (e.g. in urban or natural canyons and under forest foliage). These environments, also known as GPS-denied or GPS-challenging respectively, require the adoption of alternative hardware and algorithmic solutions in order to ensure high-level state estimation and perception capabilities to Micro-UAVs (MAVs). Specifically, this result can be achieved for example by integrating inertial with Electro-Optical sensors or other ranging systems. Nevertheless, it is worth outlining that even though the GPS signal is available, the adoption of these technologies might still be required in order to get a higher level of autonomy according to the ranking provided by Kendoul about Guidance, Navigation and Control capabilities of Rotorcraft Unmanned Aircraft Systems (RUAS) [5].
The sensors potentially suitable for this kind of applications can be active or passive systems. The former category mainly refers to Light Detection and Ranging (LIDAR) instruments, but it also includes ultrasonic rangefinders and RADAR, while the latter one refers to monocular and stereovision cameras operating in the visible band of the electromagnetic spectrum. A hybrid alternative is given by RGB-depth cameras, which are able to simultaneously register passive RGB images as well as depth images of the same scene in an active way, i.e. by sending a structured pattern of infrared (IR) light and analyzing the echo coming from background or measuring time-of-flight [6]. Although passive systems are certainly lighter in terms of weight, less expensive and less power consuming than their active counter-parts, this work focuses on active sensors. Indeed, they are less sensitive to ambient light

978-1-4673-9333-1/16/$31.00 Â©2016 IEEE

649

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:53:56 UTC from IEEE Xplore. Restrictions apply.

variations and can work day and night, thus providing an higher degree of autonomy compared to passive cameras. Moreover, they can provide directly 3D information about the observed scene without requiring any computationally expensive image processing approach. Among the previously mentioned active sensors, LIDARs represent a convenient choice since they can provide measurements at farther operating ranges than ultrasonic rangefinders and depth cameras. However, if one consider radar systems, they are certainly able to provide, almost in any weather condition, high resolution and wide-area coverage thus representing an optimal solution for collision detection for larger UAVs. The problem is that they are still too heavy and power consuming, and produce large amount of data, to be installed on board any kind of MAV. However, it is worth mentioning that great amount of research efforts are currently in progress towards the integration of compact radar systems on board MAVs [7].
In this framework, Simultaneous Localization And Mapping (SLAM) is a key enabling function to perform missions in complex environments, both indoor and outdoor. Although the SLAM problem can be considered completely solvable from the theoretical point of view [8], and several improvements were carried out in the last two decades [9], there are still many open issues regarding its real-time implementation which are particularly relevant to flying robots [10]. Hence, this paper presents innovative techniques, for the two main steps of SLAM (localization and mapping), which are based on the integration of laser scan data provided by a two-dimensional (2D) LIDAR, and inertial measurements, i.e. vehicle's acceleration, angular velocity and attitude given by a low cost IMU. At this stage of the research activity the two steps are separated meaning that no feedback on the localization solution is foreseen from mapping information.
With regards to the localization step of the SLAM process, it consists in estimating the vehicle's trajectory, i.e. the time evolution of its attitude and position parameters (pose). One possible solution to this problem is to perform LIDAR odometry through scan matching, which means to recursively compute the pose variation between two successive time instants by comparing the corresponding LIDAR scans. Three different approaches to scan matching exist, i.e. feature-to-feature, point-to-feature and point-topoint algorithms [11]. The attention here is focused on the latter category since it is more robust in cluttered environments where it is difficult to identify peculiar structures in the acquired datasets, and it allows saving on the required computational load for the feature extraction step. Hence, a customized version of the Iterative Closest Point (ICP) algorithm [12] is here proposed to carry out point-to-point scan matching. With respect to previous ICP applications for SLAM, mainly addressed at optimizing the computational time required by the matching step [13], peculiarities of this approach are the introduction of an outlier rejection step to cope with the errors caused by wrong point correspondence determination as well as of a strategy for autonomous failure detection. Other examples of point-

to-point scan matching techniques for autonomous localization of MAVs can be found in [10] and [14].
The mapping step of the SLAM process aims at building a map of the observed environment as well as at using the collected information to improve the accuracy of the estimated trajectory. However, mapping can also be used as an independent processing step that complements localization/odometry In addition to the raw data representation [15] which can lead to a huge amount of data to be stored, two approaches are mainly adopted in the literature to carry out this task, namely the occupancy grid [14, 16] and the feature-based ones [17, 18]. In this work, the latter approach is preferred to the occupancy grid method since it leads to compact representations of the environment with a high speed of execution and little memory requirement [18], thus being more compliant with MAVs' limitation both in terms of weight and power consumption. Of course, the use of feature-based mapping involves the associated drawbacks to be considered, e.g. possible loss of information due to sparse representation of the environment, difficulties intrinsic to the feature extraction and the data association processes. An advantage of the proposed approach is the use of the of PCA [19] for line fitting, which ensures a good improvement in terms of computational load when compared to the classical least squares method [18, 20, 21].
Hereinafter, the paper is organized as follows. Section II describes in detail the solutions envisaged for LIDAR/inertial localization and line-based mapping. Section III presents the setup and the results of the experimental tests carried out for performance evaluation. Finally, section IV contains the conclusion.
II. LIDAR/INERTIAL LOCALIZATION AND MAPPING
Before entering the details of the techniques developed for localization and mapping, some preliminary information regarding the mathematical notation are provided. Specifically, italic type is used for scalar quantities and quaternion, italic type with a single underline is used for vectors, and italic type with double underline is used for matrices. The pose of the MAV is described by a set of 6 parameters representing the position and the attitude of its body reference frame (BRF) with respect to a local inertial reference frame, i.e. the East-North-Up (ENU). Specifically, T is the 3D position vector of the MAV with respect to the ENU and expressed in the ENU, while a 321 sequence of Euler angles, i.e. heading (), pitch (), and roll (), is used to represent the attitude of the BRF with respect to the ENU. For the sake of clarity, two assumptions are made. Firstly, the MAV's BRF is considered coincident and aligned with the LIDAR reference frame (LRF, having its x-axis in the boresight direction, its z-axis perpendicular to the scan plane and the y-axis oriented to obtain a right-handed reference frame). Secondly, the ENU origin is assumed to be coincident with the initial position of the MAV. However, the proposed approach can be easily extended to a more general case.

650 Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:53:56 UTC from IEEE Xplore. Restrictions apply.

A. Localization by scan matching
Localization is carried out by a hybrid LIDAR/Inertial Odometry algorithm (L/I-O). This means that the flying vehicle pose is tracked by integrating attitude information from an IMU with position information obtained by recursively registering two consecutive scans provided by a 2D LIDAR by means of a scan matching approach. Of course, the initial pose of the MAV must be fully known at the start of the trajectory, in order to be compliant with the concept of odometry [22]. Generally speaking, 2D scan matching is the problem of registering two sets of 2D data (i.e. a reference scan and a current scan) by looking, in the pose search space, for the optimal rotation and translation, i.e. the ones that provide the best alignment by minimizing a purposely defined error function [23]. Typically, the reference scan represents the environment in which the vehicle moves and it can be a pre-built map or a previous scan, while the current scan is the measurement dataset provided by the available range sensor at the time of interest.
The L/I-O algorithm extracts the attitude directly from an IMU at high frequency (about 90 Hz), while the position is estimated at lower frequency (up to 36 Hz) using a LIDAR odometry technique based on a point-to-point scan matching algorithm. A block diagram to describe this system architecture is provided in Fig. 1.

Fig. 1. Block diagram describing the system architecture for localization.
The scan matching algorithm is described in detail as follows. Let tk and tk+1 be two successive time instants at which two LIDAR scans are acquired and let PLRF(tk) and PLRF(tk+1) be the corresponding point clouds in LRF. Firstly, a coordinate transformation is applied to convert these point clouds from LRF to the Vehicle Reference Frame (VRF), which is a new frame aligned to ENU with the origin at the current position of the MAV (and so coincident to the LRF origin). This is done as shown in (1)

VRF
P

(tk )



R VRFto LRF

( ,  , )T kkk

LRF
P

(tk )

(1)

VRF
P

(t k 1 )



R VRFto LRF

( k 1 ,  k 1 ,  k 1 )T

LRF
P

(t k 1 )

where RVRFtoLRF is the rotation matrix representing the attitude of LRF with respect to VRF. Secondly, a customized
version of the ICP algorithm is applied to find the best
estimate of the rotation and translation necessary to align PVRF(tk+1), i.e. the current scan, to PVRF(tk), i.e. the reference scan. Specifically, the algorithm provides in output an estimate of the variation of the Euler angles (, , ) and of the position vector () between the two VRFs, occurred
during the time interval from tk to tk+1.

This ICP algorithm is characterized by a sequence of steps, i.e. initialization, matching, outliers rejection, selection and minimization of an error metric function, which are iteratively repeated until a convergence criterion is met. Each time a new scan is available, the ICP algorithm is initialized by setting to zero all the previously defined parameters, i.e. , ,  and . With regards to the matching step, which mainly determines the algorithm's computational load and the accuracy level, the classical Nearest Neighbor (NN) approach (i.e. each current scan point is associated to the closest one in the reference scan according to the Euclidean metric) is adopted. In addition, the reference scan is preprocessed to build a K-D tree [24], in order to accelerate the NN search. The outliers rejection step is introduced to compensate for wrong point associations that may arise if there is a poor overlap between the scenes observed in two LIDAR acquisitions (i.e. a large number of measurements in the actual scan do not have real correspondences in the reference scan), since they can lead to significant error in the ICP pose solution. Hence, if d is the set of distances between correspondent points, the reference/actual scan matches characterized by a relative distance which is outside the interval defined by the mean and mode values of d are considered as outliers. As regards the error metric function (f), it is selected as the mean squared distance of corresponding points between the two scans, see (2), and it is minimized thanks to a closed form solution based on quaternion representation [25].

 (2) f (q)



1

N (tk 1 )

N (tk 1) i1

2

VRF
Pi

(tk

)



R (VRF

(tk

) to VRF

( t k  1 ))

(q)T

(

P

VRF i

(tk 1 )



T

)

In (2), PiVRF(tk+1) and PiVRF(tk) are respectively the ith point of the actual scan and the corresponding one in the
reference scan, N(tk+1) is the number of points in the current scan, R(VRF(tk)toVRF(tk+1)) and q are respectively the rotation matrix representing the attitude variation of the VRF
between the time instants of the two scans and the
corresponding quaternion. Once q is estimated, the corresponding values of , , and  can be extracted, and, finally,  is computed as the difference between the
centroids of the two scans, as given by (3).

(3) T



R

(q ) Mean {P VRF

(VRF ( tk ) toVRF ( tk 1 ))

(t k )}  Mean {P VRF

(t k 1 )}

At this point, the value of f can be updated and the procedure is repeated until the variation between two successive iterations goes below a threshold (e.g. 10-6 m2).

It is now fundamental to point out that, since by definition the two VRFs have the same orientation and a different position (unless the MAV is not moving), the estimated values of , , and  are not used. On the other hand, the output of (3) can be used to update the MAV's position vector using (4).

T tk 1  T tk   T

(4)

The logic behind the proposed L/I-O algorithm is summarized by the flow diagram shown in Fig. 2.

651 Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:53:56 UTC from IEEE Xplore. Restrictions apply.

aims at looking for those locations i at which either (6) or (7) is satisfied. These two conditions, where N is the number of points in the cluster, compare the local value of the x and y derivatives with respect to , to the sum of their mean and standard deviation computed over the cluster.

   " dx ( i

)



 

1

N

dx ( i ) 

1

N

 

dx

(

i

)



1

N

dx

(

i

)

# $

2

& '

") *

(6)

"! d

 N i 1 d

N  1 i1  d

N i1 d % '( "+

Fig. 2. Flow diagram describing the proposed L/I-O algorithm for localization. The red circle contains the final output.

An additional peculiar feature of the presented ICP algorithm is its strategy for autonomous failure detection. Specifically, since the value at convergence of the ICP cost function (fCONV) is a direct measure of the algorithm's accuracy level [26], it is possible to determine whether the algorithm has failed or not by comparing it to a given threshold (fLIM). In case fCONV is larger than fLIM the MAV's position is updated by integrating the inertial acceleration provided by the IMU, according to (5)

T t k 1

 T tk



v tk

1 (tk

1



tk

)



1 2

a tk

(tk 1



tk

)2

(5)

where v and a represent velocity and acceleration of the MAV, respectively. These latter quantities are converted in ENU thanks to the attitude estimates.

B. Mapping: line detection (PCA vs. LS fitting method)
The feature-based mapping technique presented in this paper exploits lines identified in the observed environment by means of a line detection algorithm which processes the 3D data provided by the 2D laser scanner. The algorithm's operation can be divided into three steps: clusterization, line identification, and line storage.
The first step is the clusterization process and is characterized by two hierarchical levels. Firstly, the scan is subdivided into separate clusters by looking for those locations, indicated as break-points, at which the inter-point distance (i.e. the distance between consecutive points in the scan) is larger than a specific threshold (DTh). This part of the algorithm is similar to the clusterization process proposed in [27], where the radial distance between consecutive points is compared to a threshold. However, the proposed approach foresees a second level of clusterization, meaning that additional break-points are identified within each cluster, by exploiting the polar structure of the point cloud acquired by the 2D laser scanner. Basically, both the x and y coordinates of the measured points can be considered function of the scan angle . Hence, the sub-cluster search

   "

dy

(

i

)



 

1

N

dy ( i ) 

1

N

 

dy

(

i

)



1

N

dy

(

i

)

# $

2

& '

") *

(7)

"! d

 N i1 d

N  1 i1  d

N i1 d % '( "+

After clusterization, the identification step can start. Unlike the techniques listed in [27], this approach relies on the PCA, which allows finding the principal directions of a multidimensional dataset by analyzing the eigenvectors of its covariance matrix [19]. In this case, the PCA is applied by assigning to each cluster the ratio between the maximum and minimum eigenvalues (r) associated with its covariance matrix. Hence, if r is larger than a fixed threshold (ETh), a cluster is considered as line feature and its direction is given by the eigenvector corresponding to the maximum eigenvalue (whose components are x, y and z). Specifically, this line passes through the centroid of the cluster and it is oriented as the eigenvector corresponding to the maximum eigenvalue of the covariance matrix. Finally, the line detection algorithm foresees a storage step aimed at assigning to each detected line a list of 8 parameters to be stored in memory. Firstly, L is the angle associated to the line direction, according to (8).

 L



tan

1

  

 

y x

# $$ %

(8)

Secondly, xC and yC are the coordinates of the centroid of the cluster, while xE1 and yE1, as well as xE2 and yE2, are the coordinates of the two ends of the line segments. They are
found by projecting the first and the last elements of the
cluster on the edge direction according to (9),

 xE1 & '  yE1(



 x

 



y

& '  E1 (



  

xC yC

& ' (

(9)

  

xE 2 yE2

& ' (



x

 



y

& 'E2 (



 xC   yC

& ' (

where E1 and E2, i.e. the distances of the two ends from the line segment centroid, can be computed using (10).

 E1 

 

x E1



xC

2



y E1



yC

2

 

  

x E1

 

x E1

& '



 

xC

& '

 y E1 (  yC (

 xC 2  y E1 

2
yC

#

$





 



x y

&
' (

$ $ $

$

%

E2 

 

x E 2



xC

2



yE2



yC

2

 

  

x E 2

 

xE2

& '



 

xC

& '

yE2 ( yC (

 xC 2  y E2 

2
yC

#

$





 



x y

&
' (

$ $ $

$

%

(10)

652

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:53:56 UTC from IEEE Xplore. Restrictions apply.

Finally, dL is the distance of the line segment from the origin of the reference frame in which it is represented, and it
can be computed using (11).

dL





yC  tan  L xC 1  tan 2  

(11)

L

Since the sub-cluster search could generate break-points
within a real edge (due to the LIDAR measurement noise), an intermediate merging step is implemented. It allows merging two consecutive clusters if the corresponding lines satisfy the condition defined by (12), where CC is the orientation of the direction of the segment which links the two centroids and T is a very small angular threshold (e.g. 0.05Â°).

( L1

  L2)

  T   ( L1

 )

CC

T

  ( L2

  )   
 

CC

T

( L1

 (180

  )) CC

 T

  ( L2

 (180

  )) CC

 T


 (12)

This means that the line detection algorithm is applied to the aggregate cluster.
The PCA method, adopted to perform line fitting, is compared to the classical Least Squares (LS) approach, by means of numerical simulations. Firstly, a set of n points, randomly distributed along a direction identified by a fixed angular coefficient (ml), and whose coordinates (xi and yi) are given by (13), is defined in the 2D space.

xi  xSi

, i  1...n

(13)

yi  ml xi   i

In (13), xS is the fixed step between points along the x*@2; *6- Hi is the ith extraction from a normal distribution with zero mean and standard deviation equal to . Secondly, both the PCA and LS methods are applied obtaining the corresponding lines, each one identified by an angular coefficient (mPCA and mLS), and a constant term (nPCA and nLS). Finally, the line fitting accuracy is evaluated as the mean squared distance of the assigned points from the estimated line (ErrPCA and ErrLS), according to (14).

C. Feature-based map generation
The feature-based mapping algorithm is exploited to obtain a synthetic representation of the environment in which the MAV is moving, thus limiting the amount of data storage compared to occupancy grid methods. To this aim, the first step is to apply the updated position solution to translate the measured point cloud from VRF to ENU. Then, the line detection algorithm, described previously in sub-section II-B of this paper, is applied to find robust features which become candidates to be added to the updating map. Specifically, each candidate has to be compared to every line in the map in terms of two parameters, i.e. L and dL. If a correspondence is not found, the candidate line becomes a new element of the map. On the other hand, if the candidate represents a visualization of the same feature of the environment from a different position, the two lines must be merged. This is done by projecting the two ends of the candidate line on the direction of the corresponding one in the map, and by comparing these projections to the pre-existing ends. The updated ends are the ones that determine the maximum length of the updated line in the map.

 1
Err PCA 

n
( y i  m PCA x i  n PCA ) 2

n i1

 1
Err LS 

n

( y i  m LS x i  n LS ) 2

n i1

(14)

Results are averaged on 10000 Monte-Carlo simulations and are shown in Table I, in terms of line fitting accuracy and computational load. With regards to the simulation inputs, ml is set to 5, xS is set to 4 cm, and H is equal to 5 cm.

TABLE I.

COMPARISON BETWEEN PCA AND LS FOR LINE FITTING

N
25 50 100 250 500 1000 10000

PCA time saving (%) 83 83 83 83 82 80 57

ErrPCA - rms (m2)
2.4G10-3 2.4G10-3 2.5G10-3 2.5G10-3 2.5G10-3 2.5G10-3 2.5G10-3

ErrLS - rms (m2)
2.4G10-3 2.4G10-3 2.5G10-3 2.5G10-3 2.G -3 2.5G10-3 2.5G10-3

ErrPCA - ErrLS (%)
1.2G10-1 2.9G10-2 7.1G10-3 1.1G10-3 3G10-4 1G10-4 <10-4

III. EXPERIMENTAL TESTS
In order to assess the performance of the algorithms developed for localization and mapping, an experimental setup and a test area are prepared for data recording and testing. The setup (see Fig. 3-a) is composed of the following items: one LIDAR - UTM-30LX-EW, produced by Hokuyo whose specifications can be found in [28]; one autopilot Pixhawk, produced by 3drobotics; one embedded board Nitrogen6X, produced by Boundary-Devices; one battery; two voltage regulators, LM2596 Adjustable DC/DC Power Converter. The latter components are needed since the battery has to power both the LIDAR (at 12 V) and the Nitrogen board (5 V). The test area (see Fig. 3-b) is a 2D maze in which the experimental setup is carried by hand.

This analysis proves that the PCA-based line fitting is almost twice faster than the LS approach while being able to provide the same level of accuracy.

Fig. 3. Experimental setup on the left (a). Test area on the right, with indication of the start and ending points of the travelled path (b).

653

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:53:56 UTC from IEEE Xplore. Restrictions apply.

The Nitrogen board (not in sight in Fig. 3-a, as it is located below the metallic plate on which the LIDAR and the Pixhawk are mounted) is used to register data from both the Pixhawk (using USB connection) and the LIDAR (using Ethernet connection) by exploiting the corresponding nodes of the Robot Operating System (ROS) [29], i.e. the mavros and the urg_node, respectively. In this way the IMU data from the Pixhawk and the range data from the LIDAR can be simultaneously recorded, together with their timestamps, within the same bag-file. This makes it possible to run the proposed algorithms offline in MATLAB environment by directly reading from the generated bag-files. Since IMU data are collected at higher update rate (about 90 Hz) than the LIDAR data (about 35 Hz), the attitude parameters corresponding to the LIDAR timestamps are obtained through linear interpolation. In order to analyze the algorithms' accuracy level in terms of trajectory estimation, a ground truth is required. To this aim, once the indoor scenario is selected, a reference trajectory to be followed is defined, whose length (approximately 9 m) is computed by taking measurements from a single point Laser Range Finder (BOSCH DLR130 Distance Measurer).
Several runs of the L/I-O algorithm are realized to evaluate the effect on performance of its tuning parameters, which are defined hereinafter. The Range Limit (RL) is the value of distance over which LIDAR measurements are disregarded from the acquired scan. The Angular Resolution (AR) is the angle between two consecutive LIDAR measurements which are not deleted from the acquired scan (minimum value for AR is 0.25Â°). The Odometry rate (OR) is the frequency at which the L/I-O algorithm is applied (maximum value for OR is 36 Hz which is the LIDAR measurement rate). On the other hand, other parameters of the algorithm are kept constant and are listed hereinafter. The ICP maximum number of iteration is set to 30. The minimum value for the time derivative of the ICP cost function at convergence is set to 10-6 m2. The value of fLIM is set to 0.5 m2. Firstly, the effect of RL is evaluated by considering different values (from 60 m to 3 m) while keeping the AR (0.25Â°) and the OR (6 Hz) fixed. Results in Table II show that the RL should always be set below 30 m. This is not highlighted by the error on the estimated length of the overall trajectory (LEST), which is almost the same for any value of RL, but by looking at the sum of fCONV during the test (fSUM) and at its mean (fMEAN). Indeed, these parameters represent an index of how well two consecutive scans are aligned by the algorithm, meaning that the lower their value is, the larger the accuracy of localization becomes. When RL is 60 m, fSUM and fMEAN reach the values of 8.8 m2 and 0.149 m2, respectively, which are one order of magnitude larger than for the other runs. This degradation in performance is explained by the fact that LIDAR measurements longer than 30 m are not reliable and can cause wrong point-to-point matching by the ICP thus compromising its operation. Indeed, when the laser intensity reflected back at the detector is below an internal threshold of the sensor (this happens, for instance, when the laser shot passes through a window), a value of range around 60 m is the output. Below 30 m, a reduction of RL gives advantages in terms of computational load (less number of points to be matched by the ICP routine) and localization accuracy. However, if RL is too low (3 m), it may produce an increase in the error in LEST, which

shows that it is convenient to neglect part of the scan to reduce the computing time, provided that it does not cause excessive loss of information. It is finally worth mentioning that the attained very low values of fSUM and fMEAN do not disprove the previous statements since they are caused by the significant reduction in the amount of points analyzed in the scan.

TABLE II.

L/I-O ALGORITHM PERFORMANCE ANALYSIS. EFFECT OF RL

RL

ICP

(m failure

)

(%)

60

10

30

7

15

5

11

5

7

2

3

0

fSUM (m2)
8.799 1.202 0.973 0.984 0.282 0.171

fMEAN (m2)
0.149 0.020 0.017 0.017 0.005 0.003

Mean comp. time (s) 0.134 0.136 0.149 0.130 0.160 0.159

LEST Error on

(m)

LEST (%)

8.16

9

8.20

9

8.30

8

8.44

6

8.50

6

8.02

11

Secondly, the effect of AR is analyzed considering three values (0.25Â°, 0.5Â° e 1Â°) while keeping fixed the RL (11 m) and the OR (6 Hz). Results in Table III show that it is convenient to change AR from 0.25Â° to 0.5Â° since it causes a faster ICP solution (about 30 %) while ensuring the same accuracy level. It is also possible to state that a further reduction of the resolution (AR set to 1Â°) is not advisable, not only because it generates a performance worsening but also because it compromises the applicability of the line extraction algorithm for mapping (the analyzed scans become too sparse to obtain robust line features).

TABLE III.

L/I-O ALGORITHM PERFORMANCE ANALYSIS. EFFECT OF AR

AR (Â°)

ICP failure
(%)

fSUM (m2)

fMEAN (m2)

Mean comp. time (s)

LEST Error on

(m)

LEST (%)

0.25

5

0.984 0.017 0.130

8.44

6

0.5

3

0.947 0.016 0.094

8.48

6

1

7

2.603 0.044 0.063

8.40

7

Thirdly, the effect of OR is analyzed considering four values (36 Hz, 18 Hz, 6 Hz, 3 Hz) while keeping fixed the RL (11 m) and the AR (0.5Â°). Results in Table IV show that low values of OR (3 Hz and 6 Hz) provide better performance than by applying the localization algorithm at larger frequencies (18 Hz and 36 Hz). This is because the lower rate of execution reduces the propagation of the error, which is bonded to the concept of odometry. Since pose variation is computed by comparing two successive datasets without considering the history of the trajectory, there is no way to correct any mistake committed during the application of the algorithm.

TABLE IV.

L/I-O ALGORITHM PERFORMANCE ANALYSIS. EFFECT OF OR

OR

ICP

(Hz failure

)

(%)

fSUM (m2)

fMEAN (m2)

Mean comp. time (s)

LEST Error on

(m)

LEST (%)

36

1

2.751 0.008 0.059

7.21

20

18

1

1.682 0.010 0.077

7.93

12

6

3

0.947 0.016 0.094

8.48

6

3

13

1.357 0.047 0.118

8.49

6

654

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:53:56 UTC from IEEE Xplore. Restrictions apply.

This same principle can be used to understand why the accuracy of the algorithm proposed for localization is also affected by the velocity at which the experimental system is moved along the same path. Table V contains the results obtained for the L/I-O algorithm with the same tuning parameters (RL set to 11 m, AR set to 0.5Â° and OR set to 6 Hz) applied to two different datasets, respectively, which were recorded by moving along the assigned trajectory within the test area but with different velocities.

TABLE V.

L/I-O ALGORITHM PERFORMANCE ANALYSIS. EFFECT OF
MOTION VELOCITY

Time length
(s)

LIDAR / IMU measurements

fSUM (m2)

fMEAN (m2)

Mean comp. time
(s)

LEST (m)

Error on LEST
(%)

26.36 961 / 2441 1.180 0.007 0.013 7.79

13

9.72

355 / 903 0.947 0.016 0.018 8.48

6

It is clear that, moving faster within the test area limits the error propagation of the odometry approach.
An example of application of the proposed localization and mapping algorithms is shown in Fig. 4. It is worth outlining that mapping can be carried out at different rate with respect to localization, e.g. in this case the mapping rate is 3 Hz while the localization one is 6 Hz.
1

0

-1

North (m)

-2

-3

-4

-5

-6

-4 -3 -2 -1

0

1

2

3

4

East (m)

Fig. 4. Example of application of localization and mapping algorithm to the analyzed test case: reference trajectory (dashed-dot green); estimated trajectory red; vertices of the real map (black dots);real map (black lines);estimated map (blue lines).
First of all, it is important to outline that the reference trajectory, depicted in green in Fig. 4, is an approximation of the real travelled trajectory and it is used to determine the error in LEST. From the figure, it is possible to state that the line-based mapping technique is able to provide a sparse but accurate representation of the travelled environment. Specifically, all the edges of the real map are accurately extracted in terms of length, location and inclination in the ENU. However, an exception is given by the two lines

identified by blue circles at their ends, since they have the same inclination as the corresponding real edges but are displaced of some centimeters from the real lines. This can be justified as a consequence of the error propagated in the estimated trajectory along the travelled path. Future work will be aimed at solving this issue by improving the accuracy of the proposed localization and mapping algorithms. This can be done by exploiting the information given by the map itself in real-time to correct the solution given by the odometry algorithm.
IV. CONCLUSION
This paper addressed the problem of localization and mapping for micro Unmanned Aerial Vehicles flying in twodimensional environments (e.g. office-like). Solutions to these problems were obtained by aiding inertial sensors with active EO systems, which globally ensure higher degree of autonomy when compared to passive sensors. Specifically, the presented approaches were based on the integration of inertial data provided by an Inertial Measurement Unit and three-dimensional data acquired by means of a twodimensional laser scanner (LIDAR). The proposed localization technique, namely LIDAR/Inertial-Odometry algorithm, was used to propagate the vehicle's attitude by relying only on inertial data, while the position was recursively updated by exploiting a customized scan matching approach, based on the iterative Closest Point Algorithm, which integrated inertial and LIDAR data available at different update rates. An additional innovative aspect was represented by the criterion introduced for autonomous failure detection of the scan matching approach. With regards to the mapping issue, an accurate but sparse representation of the observed environment was obtained by introducing an innovative method to quickly and robustly extract lines from LIDAR scans based on the Principal Component Analysis. Indeed, it showed capability to perform line fitting about 75% faster than classical least squares approaches while ensuring the same accuracy level. An experimental setup (including a LIDAR, an Inertial Measurement Unit, and a processing unit) and a test area (two-dimensional maze) were conceived and prepared to assess the performance of the proposed approaches, by means of off-line processing of the acquired data. The effect on performance of localization algorithm parameter tuning was analyzed, and it allowed finding the most convenient parameter settings. Results show the effectiveness of the proposed approach for localization and mapping, with observed errors of order of centimeters. . Performance can be improved by feeding back the mapping information to the localization algorithm, since this can reduce the error in trajectory estimation. This will be the objective of further investigations.
ACKNOWLEDGMENT
This research was carried out in the frame of Program STAR - Linea 2 -, financially supported by UniNA and Compagnia di San Paolo.

655

Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:53:56 UTC from IEEE Xplore. Restrictions apply.

REFERENCES
[1] R. L. Greenspan, D! *6- 26.:<2*4 26<.0:*<276F 47+*4 7;2<276260 System: Theory and applications. 2, pp. 187-220, 1996.
[2] *::.44D2-.-6*>20*<276!?2<11201:*<.;.6;7:;F,:*?Hill, Inc., 2008.
[3] K. Jong-A=3 ! !=33*:2.1 *6- !%2;1*:< D .*4-time navigation, guidance, and control of a UAV using low-,7;< ;.6;7:;F 2.4- *6- Service Robotics, Springer Berlin Heidelberg, 2006.
[4] J. Wendel, O. Meister, C. Schlaile, and G. F. ":755.: D6 integrated GPS/MEMS-IMU navigation system for an autonomous 1.42,78<.:F.:7space Science and Technology, Vol. 10 (6), pp. 527533, 2006.
[5] .6-7=4D!=:>.A7/*->*6,.;260=2-*6,.6*>20*<276*6-,76<:74 7/=65*66.-:7<7:,:*/<;A;<.5;F7=:6*47/2.4- 7+7<2,;$74
 (2), pp. 315-378, 2012.
[6] K. Khoshelham, and S. O. Elberink D,,=:*,A *6- :.;74=<276 7/ 326.,<-.8<1-*<*/7:26-77:5*88260*8842,*<276;F!.6;7:;$74 
 (2), pp. 1437-1454, 2012
[7] A. !,*66*82.,7 .60**6-7,,2*D:.42526*:A!<=-A7/* 24425.<.: %*>. % 6!  /7: #! 6-77: *>20*<276F Sensors, Vol. 15 (2), pp. 2309-2335, 2015.
[8] H. Durrant-%1A<. *6- " *24.A D!25=4<*6.7=; 47,*42B*<276 *6- 5*88260 8*:< F  7+7<2cs & Automation Magazine, Vol. 13 (2), pp. 99-110, 2006.
[9] T. Bailey, and H. Durrant-%1A<. D!25=4<*6.7=; 47,*42B*<276 *6- 5*88260!*:<F 7+7<2,;=<75*<276*0*B26. Vol. 13 (3), pp. 108-117, 2006.
[10]  *,1:*,1  . *6-  7A D=<7675ous flight in unknown 26-77: .6>2:765.6<;F 6<.:6*<276*4 7=:6*4 7/ 2,:7 2: $.12,4.; Vol. 1 (4), pp. 217-228, 2009.
[11] ' = ' = *6-  #,125=:* D! .;<25*<276 26 -A6*52, 7=<-77: .6>2:765.6<;  :.>2.?F 6<.4420.6< 7+7<2,; *6- Applications, Springer Berlin Heidelberg, pp. 255-267,2009.
[12]   .;4 *6-   ,*A D.<17- /7: :.02;<:*<276 7/ -D shapesF Proc. SPIE Sens. Fusion IV: Control Paradig. Data Struct. 1611, pp. 586C606, 1992.
[13] S. %263>2;< D7? ,758=<*<276*4 ! /7: *6 *=<76757=; 26-77: *.:2*4 26;8.,<276 >.12,4.F 7,<7:*4 -2;;.:<*<276 #62>.:;2<A 7/ Warwick, 2013.
[14] D. :7.;,1.4  !<=,34.: *6- ! .163. D7,*4 5=4<2-resolution representation for 6D motion estimation and mapping with a ,76<26=7=;4A :7<*<260  4*;.: ;,*66.:F  6<.:national Conference on Robotics and Automation (ICRA), pp. 5221-5226, 2014
[15] J. '1*60 *6- ! !2601 D 2-*: 7-75.<:A *6- 5*88260 26 real-<25.F 7+7<2,;!,2.6,.*6-!A;<.5;76/.:.6,. !!
 .

[16] S. Kohlbrecher, O. Von Stryk, J. Meyer, and U. Klingau/D/4.@2+4. *6- ;,*4*+4. ;4*5 ;A;<.5 ?2<1 /=44 - 57<276 .;<25*<276F  International Symposium on Safety, Security, and Rescue Robotics (SSRR), pp. 155-160, 2011.
[17] L. '1*60 *6-  17;1 D26. ;.05.6< +*;.- 5*8+=24-260*6- localization using 2D 4*;.: :*60./26-.:F :7,..-260; 7/ <1.  International Conference on Robotics and Automation ICRA'00, Vol. 3, pp. 2538-2543, 2000.
[18] P. de la Puente, D. Rodriguez-Losada, A. Valero, and F. Matia, D feature based mapping towards mobile robots' enhanced performance 26 :.;,=. 52;;276;F Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 11381143. 2009.
[19] !%74-;+.6;.6*6-.4*-2D:26,28*4,75876.6<*6*4A;2;F Chemometrics and intelligent laboratory systems, Vol. 2 (1), pp. 3752, 1987.
[20] K.  ::*; *6-  !2.0?*:< D.*<=:. @<:*,<276 *6- !,.6. Interpretation for Map-*;.- *>20*<276 *6- *8 =24-260F Proceedings of the Symposium on Intelligent Systems and Advanced Manufacturing, 1997.
[21] C. .:0.:DToward rich geometric map for SLAM: Online Detection 7/ 4*6.; 26 
  F 7=:6*4 7/ =<75*<276 7+24. 7+7<2,; and Intelligent Systems, 2007.
[22] D. !,*:*5=BB* *6-  :*=6-7:/.: D$2;=*4 7-75.<:A (<=<7:2*4) F IEEE Robotics & Automation Magazine, Vol. 18 (4), pp. 80-92, 2011.
[23] A. .6;2  7,,12 *6-  :2;.<<2 D!,*6 5*<,1260 26 <1. 7=01 -75*26F :7,..-260; 7/ <1. 
  6<.:6*<276*4 76/.:.6,. 76 Robotics and Automation (ICRA'05), pp. 2739-2744, 2005.
[24] J. K. Friedman, J. Bentely, and R. A. Finke, D6 407:2<15 /7: 26-260 .;< *<,1.; 26 70*:2<152, @8.,<.- "25.F  Transactions on Mathematical Software, Vol. 3 (3), pp. 209-226, 1977.
[25] B. 7:6D47;.--form solution of absolute orientation using unit 9=*<.:6276;F 7=:6*4 7/ <1. 8<2,*4!7,2.<y of America, Vol. 4 (4), pp. 629-642, 1987.
[26] R. 8:75744**;*67 =/267*6-:*;;2D#6,778.:*<2>. pose estimation with a LIDAR-+*;.- ;A;<.5F ,<* ;<:76*=<2,* vol. 110, pp.287-297, May-June 2015.
[27] V. Nguyen, A. Martinelli, N. Tomatis, and R. !2.0?*:< D comparison of line extraction algorithms using 2D laser rangefinder /7: 26-77: 57+24. :7+7<2,;F  ! 6<.:6*<276*4 76/.:.6,. 76 Intelligent Robots and Systems (IROS 2005), pp. 1929-1934, 2005.
[28] https://www.hokuyoaut.jp/02sensor/07scanner/download/products/utm-30lx-ew/, accessed on 07/01/2016.
[29] M. Quigley, K. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs, R. %1..4.: *6-  & 0 D ! *6 78.6-source Robot Operating !A;<.5F   %7:3;178 76 8.6 !7=:,. !7/<?*:. 
.

656 Authorized licensed use limited to: Technische Hochschule Ingolstadt. Downloaded on April 20,2022 at 11:53:56 UTC from IEEE Xplore. Restrictions apply.

