IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/jax/output/HTML-CSS/fonts/TeX/Math/BoldItalic/Main.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Conferences > 2012 IEEE/RSJ International C...
Camera-based navigation of a low-cost quadrocopter
Publisher: IEEE
Cite This
PDF
  << Results   
Jakob Engel ; Jürgen Sturm ; Daniel Cremers
All Authors
View Document
212
Paper
Citations
3323
Full
Text Views

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Abstract
Document Sections

    I.
    Introduction
    II.
    Related Work
    III.
    Hardware Platform
    IV.
    APPROACH
    V.
    Experiments and Results

Show Full Outline
Authors
Figures
References
Citations
Keywords
Metrics
More Like This
Footnotes

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Abstract: In this paper, we describe a system that enables a low-cost quadrocopter coupled with a ground-based laptop to navigate autonomously in previously unknown and GPS-denied ... View more
Metadata
Abstract:
In this paper, we describe a system that enables a low-cost quadrocopter coupled with a ground-based laptop to navigate autonomously in previously unknown and GPS-denied environments. Our system consists of three components: a monocular SLAM system, an extended Kalman filter for data fusion and state estimation and a PID controller to generate steering commands. Next to a working system, the main contribution of this paper is a novel, closed-form solution to estimate the absolute scale of the generated visual map from inertial and altitude measurements. In an extensive set of experiments, we demonstrate that our system is able to navigate in previously unknown environments at absolute scale without requiring artificial markers or external sensors. Furthermore, we show (1) its robustness to temporary loss of visual tracking and significant delays in the communication process, (2) the elimination of odometry drift as a result of the visual SLAM system and (3) accurate, scale-aware pose estimation and navigation.
Published in: 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems
Date of Conference: 7-12 Oct. 2012
Date Added to IEEE Xplore : 24 December 2012
ISBN Information:
ISSN Information:
INSPEC Accession Number: 13194796
DOI: 10.1109/IROS.2012.6385458
Publisher: IEEE
Conference Location: Vilamoura-Algarve, Portugal
Contents
SECTION I.
I ntroduction

In recent years, research interest in autonomous micro-aerial vehicles (MAVs) has grown rapidly. Significant progress has been made, recent examples include aggressive flight maneuvers [1] , [2] , ping-pong [3] and collaborative construction tasks [4] . However, all of these systems require external motion capture systems. Flying in unknown, GPS-denied environments is still an open research problem. The key challenges here are to localize the robot purely from its own sensor data and to robustly navigate it even under potential sensor loss. This requires both a solution to the so-called simultaneous localization and mapping (SLAM) problem as well as robust state estimation and control methods. These challenges are even more expressed on low-cost hardware with inaccurate actuators, noisy sensors, significant delays and limited onboard computation resources.

For solving the SLAM problem on MAVs, different types of sensors such laser range scanners [5] , monocular cameras [6] , [7] , stereo cameras [8] and RGB-D sensors [9] have been explored in the past. In our point of view, monocular cameras provide two major advantages above other modalities: (1) the amount of information that can be acquired is immense compared to their low weight, power consumption, size and cost, which are unmatched by any other type of sensor and (2) in contrast to depth measuring devices, the range of a monocular camera is virtually unlimited - allowing a monocular SLAM system to operate both in small, confined and large, open environments. The drawback however is, that the scale of the environment cannot be determined from monocular vision alone, such that additional sensors (such as an IMU) are required.
Fig. 1. A low-cost quadcopter navigates in unstructured environments using the front camera as its main sensor. The quadrocopter is able to hold a position, fly figures with absolute scale, and recover from temporary tracking loss. Picture taken at the TUM open day.

Show All

The motivation behind our work is to showcase that robust, scale-aware visual navigation is feasible and safe on low-cost robotic hardware. As a platform, we use the Parrot AR. Drone which is available for $300 and, with a weight of only 420g and a protective hull, safe to be used in public places (see Fig. 1 ). As the onboard computational resources are utterly limited, all computations are performed externally.

The contribution of this paper is two-fold: first, we derive a maximum-likelihood estimator to determine the map scale in closed-form from metric distance measurements. Second, we provide a robust technique to deal with large delays in the controlled system, which facilitates the use of a ground station in the control loop. Two videos demonstrating the robustness of our approach, its ability to eliminate drift effectively and to estimate the absolute scale of the map are available online:

http://youtu.be/tZxlDly7lno

http://youtu.be/eznMokFQmpc
SECTION II.
R elated W ork

Previous work on autonomous flight with quadrocopters can be categorized into different research areas. One part of the community focuses on accurate quadrocopter control and a number of impressive results have been published [10] , [1] , [3] . These works however rely on advanced external tracking systems, restricting their use to a lab environment. A similar approach is to distribute artificial markers in the environment, simplifying pose estimation [11] . Other approaches learn a map offline from a previously recorded, manual flight and thereby enable a quadrocopter to again fly the same trajectory [12] . For outdoor flights where GPS-based pose estimation is possible, complete solutions are available as commercial products [13] .

In this work we focus on autonomous flight without previous knowledge about the environment nor GPS signals, while using only onboard sensors. First results towards this goal have been presented using a lightweight laser scanner [5] , a Kinect [9] or a stereo rig [8] mounted on a quadrocopter as primary sensor. While these sensors provide absolute scale of the environment, their drawback is a limited range and large weight, size and power consumption when compared to a monocular setup [14] , [7] .

In our work we therefore focus on a monocular camera for pose estimation. Stabilizing controllers based on optical flow were presented in [15] , and similar methods are integrated in commercially available hardware [16] . Such systems however are subject to drift over time, and hence not suited for long-term navigation.

To eliminate drift, various monocular SLAM methods have been investigated on quadrocopters, both with off-board [14] , [5] and on-board processing [7] . A particular challenge for monocular SLAM is, that the scale of the map needs to be estimated from additional metric sensors such as IMU or GPS, as it cannot be recovered from vision alone. This problem has been addressed in recent publications such as [17] , [18] . The current state of the art is to estimate the scale using an extended Kalman filter (EKF), which contains scale and offset in its state. In contrast to this, we propose a novel approach which is based on direct computation: Using a statistical formulation, we derive a closed-form, consistent estimator for the scale of the visual map. Our method yields accurate results both in simulation and practice, and requires less computational resources than filtering. It can be used with any monocular SLAM algorithm and sensors providing metric position or velocity measurements, such as an ultrasonic or pressure altimeter or occasional GPS measurements.

In contrast to the systems presented in [14] , [7] , we deliberately refrain from using expensive, customized hardware: the only hardware required is the AR. Drone, which comes at a costs of merely $300 - a fraction of the cost of quadrocopters used in previous work. Released in 2010 and marketed as high-tech toy, it has been used and discussed in several research projects [19] , [20] , [21] . To our knowledge, we are the first to present a complete implementation of autonomous, camera-based flight in unknown, unstructured environments using the AR. Drone.
SECTION III.
H ardware P latform

As platform we use the Parrot AR. Drone, a commercially available quadrocopter. Compared to other modern MAV's such as Ascending Technology's Pelican or Hummingbird quadrocopters, its main advantage is the very low price, its robustness to crashes and the fact that it can safely be used indoor and close to people. This however comes at the price of flexibility: Neither the hardware itself nor the software running onboard can easily be modified, and communication with the quadrocopter is only possible over wireless LAN. With battery and hull, the AR. Drone measures 53 cm × 52 cm and weights 420g.
Fig. 2. Approach Outline: Our navigation system consists of three major components: a monocular SLAM implementation for visual tracking, an EKF for data fusion and prediction, and PID control for pose stabilization and navigation. All computations are performed offboard, which leads to significant, varying delays which our approach has to compensate.

Show All
A. Sensors

The AR. Drone is equipped with a 3-axis gyroscope and accelerometer, an ultrasound altimeter and two cameras. The first camera is aimed forward, covers a field of view of 73.5° × 58.5°, has a resolution of 320 ⨯ 240 and a rolling shutter with a delay of 40 ms between the first and the last line captured. The video of the first camera is streamed to a laptop at 18 fps, using lossy compression. The second camera aims downward, covers a field of view of 47.5° × 36.5° and has a resolution of 176 × 144 at 60fps. The onboard software uses the down-looking camera to estimate the horizontal velocity. The quadcopter sends gyroscope measurements and the estimated horizontal velocity at 200Hz, the ultrasound measurements at 25 Hz to the laptop. The raw accelerometer data cannot be accessed directly.
B. Control

The onboard software uses these sensors to control the roll \Phi and pitch \Theta , the yaw rotational speed \dot{\Psi} and the vertical velocity \dot{z} of the quadrocopter according to an external reference value. This reference is set by sending a new control command {\bf u}=(\bar{\Phi},\bar{\Theta},\bar{\dot{x}},\bar{\dot{\Psi}})\in[-1,1]^{4} every 10 ms.
SECTION IV.
APPROACH

Our approach consists of three major components running on a laptop connected to the quadrocopter via wireless LAN, an overview is given in Fig. 2 .

    Monocular SLAM

    For monocular SLAM, our solution is based on Parallel Tracking and Mapping (PTAM) [22] . After map initialization, we rotate the visual map such that the xy -plane corresponds to the horizontal plane according to the accelerometer data, and scale it such that the average keypoint depth is 1. Throughout tracking, the scale of the map \lambda\in {\BBR} is estimated using a novel method described in Section IV-A . Furthermore, we use the pose estimates from the EKF to identify and reject falsely tracked frames.

    Extended Kalman Filter

    In order to fuse all available data, we employ an extended Kalman filter (EKF). We derived and calibrated a full motion model of the quadro-copter's flight dynamics and reaction to control commands, which we will describe in more detail in Section IV-B . This EKF is also used to compensate for the different time delays in the system, arising from wireless LAN communication and computationally complex visual tracking.

    We found that height and horizontal velocity measurements arrive with the same delay, which is slightly larger than the delay of attitude measurements. The delay of visual pose estimates \Delta t_{\rm vis} is by far the largest. Furthermore we account for the time required by a new control command to reach the drone \Delta t_{\rm control} . All timing values given subsequently are typical values for a good connection, the exact values depend on the wireless connection quality and are determined by a combination of regular ICMP echo requests sent to the quadrocopter and calibration experiments.

    Our approach works as follows: first, we time-stamp all incoming data and store it in an observation buffer. Control commands are then calculated using a prediction for the quadrocopter's pose at t+\Delta t_{\rm control} . For this prediction, we start with the saved state of the EKF at t-\Delta t_{\rm vis} (i.e., after the last visual observation/unsuccessfully tracked frame). Subsequently, we predict ahead up to t+\Delta t_{\rm control} , using previously issued control commands and integrating stored sensor measurements as observations. This is illustrated in Fig. 3 . With this approach, we are able to compensate for delayed and missing observations at the expense of recalculating the last cycles of the EKF.

    PID Control

    Based on the position and velocity estimates from the EKF at t+\Delta t_{\rm control} , we apply PID control to steer the quadrocopter towards the desired goal location {\bf p}=(\hat{x},\hat{y},\hat{z},\hat{\Psi})^{T}\in{\BBR}^{4} in a global coordinate system. According to the state estimate, we rotate the generated control commands to the robot-centric coordinate system and send them to the quadrocopter. For each of the four degrees-of-freedom, we employ a separate PID controller for which we experimentally determined suitable controller gains.

Fig. 3. Pose Prediction: Measurements and control commands arrive with significant delays. To compensate for these delays, we keep a history of observations and sent control commands between t-\Delta t_{\rm vis} and t+\Delta t_{\rm control} and re-calculate the EKF state when required. Note the large timespan with no or only partial odometry observations.

Show All
A. Scale Estimation

One of the key contributions of this paper is a closed-form solution for estimating the scale \lambda\in {\BBR}^{+} of a monocular SLAM system. For this, we assume that the robot is able to make noisy measurements of absolute distances or veloci- ties from additional, metric sensors such as an ultrasound altimeter.

As a first step, the quadrocopter measures in regular intervals the d-dimensional distance traveled both using only the visual SLAM system (subtracting start and end position) and using only the metric sensors available (subtracting start and end position, or integrating over estimated speeds). Each interval gives a pair of samples {\bf x}_{i},{\bf y}_{i}\in {\BBR}^{d} , where {\bf x}_{i} is scaled according to the visual map and {\bf y}_{i} is in metric units. As both {\bf x}_{i} and {\bf y}_{i} measure the motion of the quadrocopter, they are related according to {\bf x}_{i}\approx\lambda {\bf y}_{i} .

More specifically, if we assume Gaussian noise in the sensor measurements with constant variance , we obtain \eqalignno{ & {\bf x}_{i}\sim {\cal N}(\lambda{\mbi\mu}_{i},{\sigma}_{x}^{2}{\bf I}_{3\times 3}) & \hbox{(1)}\cr & {\bf y}_{i}\sim {\cal N}({\mbi\mu}_{i}, {\sigma}_{y}^{2}{\bf I}_{3\times 3}) & \hbox{(2)}}
View Source \eqalignno{ & {\bf x}_{i}\sim {\cal N}(\lambda{\mbi\mu}_{i},{\sigma}_{x}^{2}{\bf I}_{3\times 3}) & \hbox{(1)}\cr & {\bf y}_{i}\sim {\cal N}({\mbi\mu}_{i}, {\sigma}_{y}^{2}{\bf I}_{3\times 3}) & \hbox{(2)}} where the {\mbi\mu}_{i}\in {\BBR}^{d} denote the true (unknown) distances covered and \sigma_{x}^{2}, \sigma_{y}^{2}\in {\BBR}^{+} the variances of the measurement errors. Note that the individual {\mbi\mu}_{i} are not constant but depend on the actual distances traveled by the quadrocopter in the measurement intervals.

One possibility to estimate \lambda is to minimize the sum of squared differences (SSD) between the re-scaled measurements, i.e., to compute one of the following: \eqalignno{ & \lambda_{y}^{\ast}:=\arg\min_{\lambda}\sum_{i}\Vert {\bf x}_{i}-\lambda {\bf y}_{i}\Vert^{2}={\sum_{i}{\bf x}_{i}^{T}{\bf y}_{i}\over \sum_{i}{\bf y}_{i}^{T}{\bf y}_{i}} & \hbox{(3)}\cr \lambda_{x}^{\ast}:= & \left(\arg\min_{\lambda}\sum_{i}\Vert\lambda {\bf x}_{i}-{\bf y}_{i}\Vert^{2}\right)^{-1}={\sum_{i}{\bf x}_{i}^{T}{\bf x}_{i}\over\sum_{i}{\bf x}_{i}^{T}{\bf y}_{i}}. & \hbox{(4)}}
View Source \eqalignno{ & \lambda_{y}^{\ast}:=\arg\min_{\lambda}\sum_{i}\Vert {\bf x}_{i}-\lambda {\bf y}_{i}\Vert^{2}={\sum_{i}{\bf x}_{i}^{T}{\bf y}_{i}\over \sum_{i}{\bf y}_{i}^{T}{\bf y}_{i}} & \hbox{(3)}\cr \lambda_{x}^{\ast}:= & \left(\arg\min_{\lambda}\sum_{i}\Vert\lambda {\bf x}_{i}-{\bf y}_{i}\Vert^{2}\right)^{-1}={\sum_{i}{\bf x}_{i}^{T}{\bf x}_{i}\over\sum_{i}{\bf x}_{i}^{T}{\bf y}_{i}}. & \hbox{(4)}}

The difference between these two lines is whether one aims at scaling the {\bf x}_{i} to the {\bf y}_{i} or vice versa. However, both approaches lead to different results, none of which converges to the true scale \lambda when adding more samples. To resolve this, we propose a maximum likelihood (ML) approach, that is estimating \lambda by minimizing the negative log-likelihood {\cal L}({\mbi\mu}_{1}\ldots{\mbi\mu}_{n},\lambda)\propto{1\over 2}\sum_{i=1}^{n}\left({\Vert {\bf x}_{i}-\lambda{\mbi\mu}_{i}\Vert^{2} \over \sigma_{x}^{2}}+{\Vert {\bf y}_{i}-\mu_{i}\Vert^{2} \over \sigma_{y}^{2}}\right)\eqno{\hbox{(5)}}
View Source {\cal L}({\mbi\mu}_{1}\ldots{\mbi\mu}_{n},\lambda)\propto{1\over 2}\sum_{i=1}^{n}\left({\Vert {\bf x}_{i}-\lambda{\mbi\mu}_{i}\Vert^{2} \over \sigma_{x}^{2}}+{\Vert {\bf y}_{i}-\mu_{i}\Vert^{2} \over \sigma_{y}^{2}}\right)\eqno{\hbox{(5)}} By first minimizing over the {\mbi\mu}_{i} and then over \lambda , it can be shown analytically that (5) has a unique, global minimum at \eqalignno{{\mbi\mu}_{i}^{\ast}= & {\lambda^{\ast}\sigma_{y}^{2}{\bf x}_{i}+\sigma_{x}^{2}{\bf y}_{i}\over\lambda^{\ast 2}\sigma_{y}^{2}+\sigma_{x}^{2}} & \hbox{(6)}\cr \lambda^{\ast} = & {s_{xx}-s_{yy}+{\rm sing}(s_{xy})\sqrt{(s_{xx}-s_{yy})^{2}+4s_{xy}^{2}}\over 2\sigma_{x}^{-1}\sigma_{y}s_{xy}} & \hbox{(7)}}
View Source \eqalignno{{\mbi\mu}_{i}^{\ast}= & {\lambda^{\ast}\sigma_{y}^{2}{\bf x}_{i}+\sigma_{x}^{2}{\bf y}_{i}\over\lambda^{\ast 2}\sigma_{y}^{2}+\sigma_{x}^{2}} & \hbox{(6)}\cr \lambda^{\ast} = & {s_{xx}-s_{yy}+{\rm sing}(s_{xy})\sqrt{(s_{xx}-s_{yy})^{2}+4s_{xy}^{2}}\over 2\sigma_{x}^{-1}\sigma_{y}s_{xy}} & \hbox{(7)}} with s_{xx}:=\sigma_{y}^{2}\sum_{i=1}^{n}{\bf x}_{i}^{T}{\bf x}_{i},\ s_{yy}:=\sigma_{x}^{2}\sum_{i=1}^{n}{\bf y}_{i}^{T}{\bf y}_{i} and s_{xy}:= \sigma_{y}\sigma_{x}\sum_{i=1}^{n}{\bf x}_{i}^{T}{\bf y}_{i} . Together, these equations give a closed-form solution for the ML estimator of \lambda , assuming the measurement error variances \sigma_{x}^{2} and \sigma_{y}^{2} are known. By analyzing this result, it can be concluded that

    \lambda^{\ast} always lies in between \lambda_{x}^{\ast} and \lambda_{y}^{\ast} , and

    \lambda^{\ast}\rightarrow\lambda_{x}^{\ast} for \sigma_{x}^{2}\rightarrow 0 , and \lambda^{\ast}\rightarrow\lambda_{y}^{\ast} for \sigma_{y}^{2}\rightarrow 0 , i.e., these naïve estimators correspond to the case when one of the measurement sources is noise-free.

Fig. 4. Comparison of \lambda^{\ast} with Other Estimators: The plot shows the estimated scale as more samples are added. It can be seen that the proposed estimator \lambda^{\ast} is the only consistent estimator, i.e., the only one converging to the correct value. For this plot we used \lambda=2, \sigma_{x}=1, \sigma_{y}=0.3 and {\mbi\mu}_{i}\sim {\cal N}({\bf 0}_{3)}{\bf 1}_{3\times 3}) .

Show All

We extensively tested our approach on artificially generated data according to (2) and compared it to other, simple estimators, that is the arithmetic mean, geometric mean and the median of the set of quotients {\Vert {\bf x}_{i}\Vert \over \Vert {\bf y}_{i}\Vert} . It can be observed that out of all presented possibilities, our approach is the only consistent estimator, i.e., the only one converging to the true scale for all dimensions d , values for \sigma_{x}^{2}, \sigma_{y}^{2} and values for {\mbi\mu}_{i} . An example is shown in Fig. 4 . Furthermore, \lambda^{\ast} can be computed efficiently, as each new sample pair only requires one update of the three sums, and the re-evaluation (7) . Note that in practice approximations for \sigma_{x}^{2} and \sigma_{y}^{2} are sufficient, as their influence on \lambda^{\ast} decreases rapidly the more accurate the measured distances are. More results on the accuracy of this method will be presented in Section V-A .
B. State Prediction and Observation

In this section, we describe the state space, the observation models and the motion model used in the EKF. The state space consists of a total of ten state variables {\bf x}_{t}:=(x_{t},y_{t},z_{t},\dot{x}_{t},\dot{y}_{t},\dot{z}_{t},\Phi_{t},\Theta_{t},\Psi_{t},\dot{\Psi}_{t})^{T}\in {\BBR}^{10},\eqno{\hbox{(8)}}
View Source {\bf x}_{t}:=(x_{t},y_{t},z_{t},\dot{x}_{t},\dot{y}_{t},\dot{z}_{t},\Phi_{t},\Theta_{t},\Psi_{t},\dot{\Psi}_{t})^{T}\in {\BBR}^{10},\eqno{\hbox{(8)}} where (x_{t},y_{t},z_{t}) denotes the position of the quadrocopter in m and (\dot{x}_{t},\dot{y}_{f},\dot{z}_{t}) the velocity in m/s, both in world coordinates. Further, the state contains the roll \Phi_{t} , pitch \Theta_{t} and yaw \Psi_{t} angle of the drone in deg, as well as the yaw-rotational speed \dot{\Psi}_{t} in deg/s. In the following, we define for each sensor an observation function h({\bf x}_{t}) and describe how the respective observation vector {\bf z}_{t} is composed from the sensor readings.

1) Odometry Observation Model

The quadrocopter measures its horizontal speed \hat{v}_{x,t} and \hat{v}_{y,t} in its local coordinate system, which we transform into the global frame \dot{x}_{t} and \dot{y}_{t} . The roll and pitch angles \hat{\Phi}_{t} and \hat{\Theta}_{t} measured by the accelerometer are direct observations of \Phi_{t} and \Theta_{t} . To account for yaw-drift and uneven ground, we differentiate the height measurements \hat{h}_{t} and yaw measurements \hat{\Psi}_{t} and treat them as observations of the respective velocities. The resulting observation function h_{{\bf I}}({\bf x}_{t}) and measurement vector {\bf z}_{{\bf I},t} is hence given by \eqalignno{ & h_{{\rm I}}({\bf x}_{t}):=\left(\matrix{ \dot{x}_{t}\cos\Psi_{t}-\dot{y}_{t}\sin\Psi_{t}\cr \dot{x}_{t}\sin\Psi_{t}+\dot{y}_{t}\cos\Psi_{t}\cr \dot{z}_{t}\cr \Phi_{t}\cr \Theta_{t}\cr \dot{\Psi}_{t} }\right) & \hbox{(9)}\cr & {\bf z}_{{\rm I},t}:=(\hat{v}_{x,t},\hat{v}_{y,t},(\hat{h}_{t}-\hat{h}_{t-1}),\hat{\Phi}_{t},\hat{\Theta}_{t},(\hat{\Psi}_{t}-\hat{\Psi}_{t-1}))^{T} & \hbox{(10)}}
View Source \eqalignno{ & h_{{\rm I}}({\bf x}_{t}):=\left(\matrix{ \dot{x}_{t}\cos\Psi_{t}-\dot{y}_{t}\sin\Psi_{t}\cr \dot{x}_{t}\sin\Psi_{t}+\dot{y}_{t}\cos\Psi_{t}\cr \dot{z}_{t}\cr \Phi_{t}\cr \Theta_{t}\cr \dot{\Psi}_{t} }\right) & \hbox{(9)}\cr & {\bf z}_{{\rm I},t}:=(\hat{v}_{x,t},\hat{v}_{y,t},(\hat{h}_{t}-\hat{h}_{t-1}),\hat{\Phi}_{t},\hat{\Theta}_{t},(\hat{\Psi}_{t}-\hat{\Psi}_{t-1}))^{T} & \hbox{(10)}}

2) Visual Observation Model

When PTAM success- fully tracks a video frame, we scale the pose estimate by the current estimate for the scaling factor \lambda^{\ast} and transform it from the coordinate system of the front camera to the coordinate system of the quadrocopter, leading to a direct observation of the quadrocopter's pose given by \eqalignno{h_{{\rm P}}({\bf x}_{t}): & =(x_{t},y_{t},z_{t}, \Phi_{t}, \Theta_{t},\Psi_{t})^{T} & \hbox{(11)}\cr {\bf z}_{{\rm P},t}: & =f({\bf E}_{\cal DC}{\bf E}_{{\cal C},t}) & \hbox{(12)}}
View Source \eqalignno{h_{{\rm P}}({\bf x}_{t}): & =(x_{t},y_{t},z_{t}, \Phi_{t}, \Theta_{t},\Psi_{t})^{T} & \hbox{(11)}\cr {\bf z}_{{\rm P},t}: & =f({\bf E}_{\cal DC}{\bf E}_{{\cal C},t}) & \hbox{(12)}} where {\bf E}_{{\cal C},t}\in SE(3) is the estimated camera pose (scaled with \lambda ), {\bf E}_{\cal DC}\in SE(3) the constant transformation from the camera to the quadrocopter coordinate system, and f: {\rm SE}(3)\rightarrow {\BBR}^{6} the transformation from an element of SE(3) to our roll-pitch-yaw representation.

3) Prediction Model

The prediction model describes how the state vector {\bf x}_{t} evolves from one time step to the next. In particular, we approximate the quadrocopter's horizontal acceleration \ddot{x},\dot{y} based on its current state {\bf x}_{t} , and estimate its vertical acceleration \ddot{z} , yaw-rotational acceleration \ddot{\Psi} and roll/pitch rotational speed \dot{\Phi},\dot{\Theta} based on the state {\bf x}_{t} and the active control command {\bf u}_{t} .

The horizontal acceleration is proportional to the horizontal force acting upon the quadrocopter, which is given by {\ddot{x}\choose \ddot{y}}\propto {\bf f}_{\rm acc}-{\bf f}_{\rm drag}\eqno{\hbox{(13)}}
View Source {\ddot{x}\choose \ddot{y}}\propto {\bf f}_{\rm acc}-{\bf f}_{\rm drag}\eqno{\hbox{(13)}} where {\bf f}_{\rm drag} denotes the drag and {\bf f}_{\rm acc} denotes the accelerating force. The drag is approximately proportional to the horizontal velocity of the quadrocopter, while {\bf f}_{\rm acc} depends on the tilt angle. We approximate it by projecting the quadrocopter's z-axis onto the horizontal plane, which leads to \eqalignno{\ddot{x}({\bf x}_{t}) & =c_{1}(\cos\Psi_{t}\sin\Phi_{t}\cos\Theta_{t}-\sin\Psi_{t}\sin\Theta_{t})-c_{2}\dot{x}_{t} & \hbox{(14)}\cr \ddot{y}({\bf x}_{t}) & =c_{1}(-\sin\Psi_{t}\sin\Phi_{t}\cos\Theta_{t}-\cos\Psi_{t}\sin\Theta_{t})-c_{2}\dot{y}_{t} & \hbox{(15)}}
View Source \eqalignno{\ddot{x}({\bf x}_{t}) & =c_{1}(\cos\Psi_{t}\sin\Phi_{t}\cos\Theta_{t}-\sin\Psi_{t}\sin\Theta_{t})-c_{2}\dot{x}_{t} & \hbox{(14)}\cr \ddot{y}({\bf x}_{t}) & =c_{1}(-\sin\Psi_{t}\sin\Phi_{t}\cos\Theta_{t}-\cos\Psi_{t}\sin\Theta_{t})-c_{2}\dot{y}_{t} & \hbox{(15)}} We estimated the proportionality coefficients c_{1} and c_{2} from data collected in a series of test flights. Note that this model assumes that the overall thrust generated by the four rotors is constant. Furthermore. we describe the influence of sent control commands {\bf u}_{t}=(\bar{\Phi}_{t},\bar{\Theta}_{t},\bar{\dot{z}}_{t},\bar{\dot{\Psi}}_{t}) by a linear model: \eqalignno{\dot{\Phi}({\bf x}_{t}, {\bf u}_{t}) & =c_{3}\bar{\Phi}_{t}-c_{4}\Phi_{t} & \hbox{(16)}\cr \dot{\Theta}({\bf x}_{t}, {\bf u}_{t}) & =c_{3}\bar{\Theta}_{t}-c_{4}\Theta_{t} & \hbox{(17)}\cr \ddot{\Psi}({\bf x}_{t}, {\bf u}_{t}) & =c_{5}\bar{\dot{\Psi}}_{t}-c_{6}\dot{\Psi}_{t} & \hbox{(18)}\cr \ddot{z}({\bf x}_{t}, {\bf u}_{t}) & =c_{7}\bar{\dot{z}}_{t}-c_{8}\dot{z}_{t} & \hbox{(19)}}
View Source \eqalignno{\dot{\Phi}({\bf x}_{t}, {\bf u}_{t}) & =c_{3}\bar{\Phi}_{t}-c_{4}\Phi_{t} & \hbox{(16)}\cr \dot{\Theta}({\bf x}_{t}, {\bf u}_{t}) & =c_{3}\bar{\Theta}_{t}-c_{4}\Theta_{t} & \hbox{(17)}\cr \ddot{\Psi}({\bf x}_{t}, {\bf u}_{t}) & =c_{5}\bar{\dot{\Psi}}_{t}-c_{6}\dot{\Psi}_{t} & \hbox{(18)}\cr \ddot{z}({\bf x}_{t}, {\bf u}_{t}) & =c_{7}\bar{\dot{z}}_{t}-c_{8}\dot{z}_{t} & \hbox{(19)}}

Again, we estimated the coefficients c_{3}, \ldots,c_{8} from test flight data. The overall state transition function is now given by \left(\matrix{ x_{t+1}\cr y_{t+1}\cr z_{t+1}\cr \dot{x}_{t+1}\cr \dot{y}_{t+1}\cr \dot{z}_{t+1}\cr \Phi_{t+1}\cr \Theta_{t+1}\cr \Psi_{t+1}\cr \dot{\Psi}_{t+1} }\right)\leftarrow\left(\matrix{ x_{t}\cr y_{t}\cr z_{t}\cr \dot{x}_{t}\cr \dot{y}_{t}\cr \dot{z}_{t}\cr \Phi_{t}\cr \Theta_{t}\cr \Psi_{t}\cr \dot{\Psi}_{t} }\right)+\delta_{t}\left(\matrix{ \dot{x}_{t}\cr \dot{y}_{t}\cr \dot{z}_{t}\cr \ddot{x}({\bf x}_{t})\cr \ddot{y}({\bf x}_{t})\cr \ddot{z}({\bf x}_{t},{\bf u}_{t})\cr \dot{\Phi}({\bf x}_{t},{\bf u}_{t})\cr \dot{\Theta}({\bf x}_{t},{\bf u}_{t})\cr \dot{\Psi}_{t}\cr \ddot{\Psi}({\bf x}_{t},{\bf u}_{t})}\right)\eqno{\hbox{(20)}}
View Source \left(\matrix{ x_{t+1}\cr y_{t+1}\cr z_{t+1}\cr \dot{x}_{t+1}\cr \dot{y}_{t+1}\cr \dot{z}_{t+1}\cr \Phi_{t+1}\cr \Theta_{t+1}\cr \Psi_{t+1}\cr \dot{\Psi}_{t+1} }\right)\leftarrow\left(\matrix{ x_{t}\cr y_{t}\cr z_{t}\cr \dot{x}_{t}\cr \dot{y}_{t}\cr \dot{z}_{t}\cr \Phi_{t}\cr \Theta_{t}\cr \Psi_{t}\cr \dot{\Psi}_{t} }\right)+\delta_{t}\left(\matrix{ \dot{x}_{t}\cr \dot{y}_{t}\cr \dot{z}_{t}\cr \ddot{x}({\bf x}_{t})\cr \ddot{y}({\bf x}_{t})\cr \ddot{z}({\bf x}_{t},{\bf u}_{t})\cr \dot{\Phi}({\bf x}_{t},{\bf u}_{t})\cr \dot{\Theta}({\bf x}_{t},{\bf u}_{t})\cr \dot{\Psi}_{t}\cr \ddot{\Psi}({\bf x}_{t},{\bf u}_{t})}\right)\eqno{\hbox{(20)}} using the model specified in (14) to (19) . Note that, due to the many assumptions made, we do not claim the physical correctness of this model. It however performs very well in practice, which is mainly due to its completeness: the behavior of all state parameters and the effect of all control commands is approximated, allowing “blind” prediction, i.e., prediction without observations for a brief period of time (∼ 125 ms in practice, see Fig. 3 ).

SECTION V.
E xperiments and R esults

We conducted a series of real-world experiments to analyze the properties of the resulting system. The experiments were conducted in different environments, i.e., both indoor in rooms of varying size and visual appearance as well as outdoor under the influence of sunlight and wind. A selection of these environments is depicted in Fig. 5 .

In the following, we present our results on the convergence behavior and accuracy of scale estimation in Section IV-A , the accuracy of the motion model in Section V-B , the responsiveness and accuracy of pose control in Section V-C , and the long-term stability and drift elimination in Section V-D .

As ground truth at time t we use the state of the EKF after all odometry and visual pose information up to t have been received and integrated. It can only be calculated at t+\Delta t_{\rm vis} , and therefore is not used for drone control - in practice it is available ∼ 250 ms after a control command for t has been computed and sent to the quadrocopter.
A. Scale Estimation Accuracy

To analyze the accuracy of the scale estimation method derived in IV-A, we instructed the quadrocopter to fly a fixed figure, while every second a new sample is taken and the scale re-estimated. In the first set of flights, the quadrocopter was commanded to move only vertically, such that the samples mostly consist of altitude measurements. In the second set, the quadrocopter was commanded to fly a horizontal rectangle, such that primarily the IMU-based velocity information is used. After each flight, we measured the ground truth \hat{\lambda} by manually placing the quadrocopter at two measurement points, and comparing the known distance between these points with the distance measured by the visual SLAM system. Note that due to the initial scale normalization, the values for \hat{\lambda} roughly correspond to the mean feature depth in meters of the first keyframe, which in our experiments ranges from 2 m to 10m. To provide better comparability, we analyze and visualize the estimation error e: ={\lambda^{\ast}\over \hat{\lambda}} , corresponding to the estimated length of 1m.
Fig. 5. Testing Environments: The top row shows an image of the quadrocopter flying, the bottom row the corresponding image from the quadrocopter's frontal camera. This shows that our system can operate robustly in different, real-world environments.

Show All
Fig. 6. Scale Estimation Accuracy: The plots show the mean and standard deviation of the the estimation error e , corresponding to the estimated length of 1 m, from horizontal and vertical motion. It can be seen that the scale can be estimated accurately in both cases, it is however more accurate and converges faster if the quadrocopter moves vertically.

Show All
Fig. 7. Comparison of Predicted and Real State. The black curve shows the ground truth, it can only be computed with a delay of ~250 ms (dashed curve). At t=5\ {\rm s} , the quadrocopter is manually pushed away which cannot be predicted - hence the brief deviation. This plot shows (1) that the prediction approximates the ground truth well and in particular without notable delay and (2) that using visual information, the EKF rapidly recovers from large external disturbances - however with a small delay.

Show All

Fig. 6 gives the mean error as well as the standard deviation spread over 10 flights. As can be seen, our method quickly and accurately estimates the scale from both types of motion. Due to the superior accuracy of the altimeter compared to the horizontal velocity estimates, the estimate converges faster and is more accurate if the quadrocopter moves vertically, i.e., convergence after 2 s versus 15 s, and to a final accuracy ± 1.7 % versus ± 5 %. Note that in practice, we allow for (and recommend) arbitrary motions during scale estimation so that information from both sensor modalities can be used to improve convergence. Large, sudden changes in measured relative height can be attributed to uneven ground, and removed automatically from the data set.
B. State Prediction Accuracy

In this section we give a qualitative evaluation of the accuracy of the predicted state of the quadrocopter, used for control. Fig. 7 shows both the predicted state for time t as well as the ground truth, i.e., the state computed after all sensor measurements have been evaluated. This is only possible ∼ 250 ms after the respective control command has been issued. It can be observed that the prediction approximates the ground truth very well and without notable delay, which is crucial for oscillation-free control.
TABLE I Convergence Speed IN Position Control
relative motion (x,y,z)\ [{\rm m}] 	(1,0,0) 	(4,0,0) 	(0,0,1) 	(1,1,1)
t_{\rm conv}\ [{\rm s}] 	3. 1\pm 1.3 	4.5\pm 0.8 	3.1\pm 0.1 	3.9\pm 0.5
C. Positioning Accuracy and Convergence Speed

In this Section, we evaluated the performance of the complete system in terms of position control. In particular, we measured the average time to approach a given goal location and the average positioning error while holding this position. Considering the large delay in our system, the pose stability of the quadrocopter heavily depends on an accurate prediction from the EKF: the more accurate the pose estimates and in particular the velocity estimates are, the higher the gains can be set without leading to oscillations.

To determine the stability, we instructed the quadrocopter to hold a target position over 60 s in different environments and measure the root mean square error (RMSE). The results are given in Fig. 10 : the measured RMSE lies between 4.9 cm (indoor) and 18.0 cm (outdoor).

To evaluate the convergence speed, we repeatedly let the quadrocopter fly a given distance and measure the convergence time t_{\rm conv} , corresponding to the time required to reach the target position and hold it for at least 5 s. We consider the quadrocopter to be at the target position if the Euclidean distance is less than 10 cm. An example of flying a long distance in x-direction is shown in Fig. 8 : the plot clearly shows that the quadrocopter accelerates initially with maximum pitch, and de-accelerates before reaching the target location at t= 3.5\ {\rm s} . Fig. 9 shows an example trajectory in all three dimensions. We repeated this experiment ten times and summarized the results in Tab. I . Reaching a target location at a distance of 4 m took on average 4.5 s.
Fig. 8. Flying a Large Distance: The plot shows the behavior of the controller for a large distance. As can be seen, the quadrocopter accelerates with maximum pitch for the first second and decelerates before converging on the setpoint.

Show All
Fig. 9. Example Flight: Flying a simple figure consisting of four waypoints. This plot illustrates the typical behavior of the quadrocopter when holding and approaching waypoints ( t_{\rm conv} is indicated, see also Tab. I ).

Show All
Fig. 10. Flight Stability: Path taken and RMSE of the quadrocopter when instructed to hold a target position for 60 s, in three of the environments depicted in Fig. 5 . It can be seen that the quadrocopter can hold a position very accurately, even when perturbed by wind (right).

Show All
Fig. 11. Elimination of Odometry Drift: Horizontal path taken by the quadrocopter as estimated by the EKF compared to the raw odometry (i.e., the integrated velocity estimates). Left: when flying a figure; right: when being pushed away repeatedly from its target position. The odometry drift is clearly visible, in particular when the quadrocopter is being pushed away. When incorporating visual pose estimates, it is eliminated completely.

Show All
D. Drift Elimination

To verify that the incorporation of a visual SLAM system eliminates odometry drift, we compare the estimated trajectory with and without the visual SLAM system. Fig. 11 shows the resulting paths, both for flying a fixed figure (left) and for holding a target position while the quadrocopter is being pushed away (right). Both flights took approximately 35 s, and the quadrocopter landed no more than 15 cm away from its takeoff position. In contrast, the raw odometry accumulated an error of 2.1 m for the fixed figure and 6m when being pushed away. This experiment demonstrates that the visual SLAM system efficiently eliminates pose drift during maneuvering.
E. Robustness to Temporary Loss of Visual Tracking

The system as a whole is robust to temporary loss of visual tracking, e.g. due to occlusions or large rotations, as it continues to navigate based only on odometry measurements. As soon as visual tracking recovers, the EKF state is updated with the absolute pose estimate, eliminating accumulated estimation error. This is demonstrated in the attached video.
SECTION VI.
C onclusion

In this paper, we presented a visual navigation system for a low-cost quadrocopter using offboard processing. Our system enables the quadrocopter to visually navigate in unstructured, GPS-denied environments and does not require artificial landmarks nor prior knowledge about the environment. The contribution of this paper is two-fold: first, we presented a robust solution for visual navigation with a low-cost quadrocopter. Second, we derived a maximum-likelihood estimator in closed-form to recover the absolute scale of the visual map, providing an efficient and consistent alternative to predominant filtering-based methods. Our system was able to estimate the map scale up to \pm 1.7\hbox{\%} of its true value, with which we achieved an average positioning accuracy of 4.9cm (indoor) to 18.0cm (outdoor). Furthermore, our approach is able to robustly deal with communication delays of up to 400 ms. We tested our system in a set of extensive experiments in different real-world indoor and outdoor environments. With these experiments, we demonstrated that accurate, robust and drift-free visual navigation is feasible even with low-cost robotic hardware.

Authors
Figures
References
Citations
Keywords
Metrics
Footnotes
   Back to Results   
More Like This
Pose estimation of unmanned ground vehicle based on dead-reckoning/GPS sensor fusion by unscented Kalman filter

2009 6th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology

Published: 2009
Mobile robot localization via sensor fusion algorithms

2017 Intelligent Systems Conference (IntelliSys)

Published: 2017
Show More
References
References is not available for this document.
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
