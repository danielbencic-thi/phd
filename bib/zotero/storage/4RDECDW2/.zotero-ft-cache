IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our Privacy Policy.
Accept & Close
Loading [MathJax]/extensions/MathMenu.js

Skip to Main Content

    IEEE.org
    IEEE Xplore
    IEEE SA
    IEEE Spectrum
    More Sites 

    Cart 
    Create Account
    Personal Sign In

IEEE Xplore logo - Link to home

    Browse
    My Settings
    Help

Access provided by:
Technische Hochschule Ingolstadt
Sign Out
IEEE logo - Link to IEEE main site homepage
Access provided by:
Technische Hochschule Ingolstadt
Sign Out
ADVANCED SEARCH
Journals & Magazines > IEEE Photonics Journal > Volume: 12 Issue: 3
Fast Visibility Restoration Using a Single Degradation Image in Scattering Media
Publisher: IEEE
Cite This
PDF
  << Results   
Yingbo Wang ; Jie Cao ; Chengqiang Xu ; Saad Rizvi ; Kun Li ; Qun Hao
All Authors
View Document
5
Paper
Citations
418
Full
Text Views
Open Access

    Alerts
    Alerts
    Manage Content Alerts
    Add to Citation Alerts

Under a Creative Commons License
Abstract
Document Sections

    1.
    Introduction
    2.
    Quadtree Theory Combined With Transmission Optimization
    3.
    Experimental Comparisons and Analyses
    4.
    Conclusion

Authors
Figures
References
Citations
Keywords
Metrics
More Like This

    Download PDF
    View References
    Request Permissions
    Save to
    Alerts 

Impact Statement: The visibility restoration method of prior-based is a typical method for improving degraded image quality in scattering condition, especially dark channel prior method. H... View more
Abstract: Visibility restoration of an image degraded by scattering media (such as haze, fog, smog) is a challenging task which requires careful design of computational methods. Th... View more
Metadata
Impact Statement:
The visibility restoration method of prior-based is a typical method for improving degraded image quality in scattering condition, especially dark channel prior method. However, this method has the high time cost, poor recovery result in sky region, and the robustness of method is insufficient. Here, a method based on quadtree theory combined transmission optimized method (QTCTO) is presented to fast visibility restoration, and it is robust and effective for turbid water outside the atmosphere.
Abstract:
Visibility restoration of an image degraded by scattering media (such as haze, fog, smog) is a challenging task which requires careful design of computational methods. The fundamental problem which limits clear image reconstruction is the random diffusion of light in the scattering medium. Recently, the dark channel prior method has emerged as a powerful approach for haze removal under adverse weather conditions. However, this method requires high computational time, and fails to achieve haze-free image in the sky region. To overcome these problems, we propose a novel imaging method based on quadtree theory combined with transmission optimization (QTCTO) to restore the visibility of images degraded by scattering medium. The proposed method uses the quadtree theory (for region segmentation) to estimate the global atmospheric light, while the transmission in the media is estimated by coefficient modification for the dark channel. The experimental results show that the proposed method is robust against scattering (under smoke and turbid water conditions) and has the advantage of low computational time. The proposed method can be used for security surveillance, remote sensing, and various military applications.
Published in: IEEE Photonics Journal ( Volume: 12 , Issue: 3 , June 2020 )
Article Sequence Number: 6100813
Date of Publication: 20 April 2020
ISSN Information:
INSPEC Accession Number: 19682448
DOI: 10.1109/JPHOT.2020.2988279
Publisher: IEEE
Funding Agency:
Hide Full Abstract
Contents
CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.
SECTION 1.
Introduction

Image restoration under scattering (turbid) media has been a topic of constant research recently. The reconstruction of image through turbid media finds numerous applications in remote sensing, intelligent vehicles, and military surveillance. Studies on image visibility restoration have been conducted in the past from two aspects. One of these methods is the contrast enhancement method based on image processing [1] – [5] , and the other is the image restoration method based on physical model [6] – [9] . The contrast enhancement method simply applies image processing algorithm to rectify the degraded image without taking into account the physical phenomena and reasons behind degradation. Therefore, this method loses more information and fails to recover images efficiently. Compared to this method, the image restoration method based on physical model proves to be more efficient by considering light propagation in the medium and recovering more details about the target image. It is observed that these restoration methods lose less information during restoration process to efficiently recover the original image. Therefore, the proposed method is a type of image restoration method which considers physical model for fast and efficient image restoration.

The visibility restoration method based on physical model can further be categorized as polarization-based methods [10] – [12] , depth-based methods [8, 13]– [15] , and prior-based methods [16] – [25] . Amongst these methods, the prior based methods are extensively used. Tan [16] observes that a haze-free image must have higher contrast compared to an input hazy image, and the haze is removed by maximizing the local contrast of the restored image. The results are visually compelling but the model may not be physically valid [18] . Similarly, Fattal et al. [17] assumes the albedo of an image local patch to be a constant vector, and uses independent component analysis to estimate the albedo. This method further uses atmospheric physical model to restore the colors of image. This approach is physical model-based and produces a natural haze-free image together with a good depth map. However, this method strictly adheres to model assumptions and fails when these assumptions become invalid. The method also fails when the haze is heavy [18, 22]. He et al. [18] proposed a dark channel prior method to estimate the atmospheric light and transmission. The method employs a soft matting algorithm to refine transmission maps. The haze-free images can be recovered by dark channel prior with haze imaging model. However, this method has a high computational cost and complexity associated with soft matting algorithm, which lowers its haze removal efficiency. He et al. [19] also proposed a fast guided filter algorithm with edge-preserving capability. The guided filter can be used to refine the transmission and achieve dehazing. However, due to inaccurate estimation of atmospheric light, it is difficult to use the model for dehazing in the sky region. Tarel et al. [21] proposed a fast image dehazing algorithm which estimates atmospheric veil by using a median filter. The main advantages of this algorithm are fast computation and remarkable dehazing capability. However, the edge-preserving ability of this algorithm is poor due to median filtering, and the algorithm produces Halo effect when the filter size settings are inappropriate.

To summarize, in order to obtain more useful information from the degraded image, the visibility restoration of an image (degraded by scattering medium) can be achieved by different methods, amongst which the prior-based method is most commonly used. The dark channel prior method is a prime example of prior-based method. However, the dark channel prior method suffers from high computational (time) cost, and poor recovery performance for the sky region of degraded image.

In this paper, we report a fast visibility restoration method based on quadtree theory combined with transmission optimization (QTCTO) to solve aforementioned problems. The core ideas of our approach are to estimate global atmospheric light and transmission, and to refine the transmission rapidly. Our method offers three advantages: ( 1 ) the global atmospheric light can efficiently be estimated by the quadtree method of region segmentation, ( 2 ) the transmission of the degraded image is optimized by coefficient modification for the dark channel and the refined transmission can be quickly obtained by the guided filtering, ( 3 ) the proposed method is effective for the sky region, and the visibility can be quickly recovered under different scattering conditions. The recovered image acquired from the proposed algorithm retains all fine details in the original image.
SECTION 2.
Quadtree Theory Combined With Transmission Optimization

QTCTO involves three processes. First, the global atmospheric light is estimated by the quadtree method of region segmentation. Second, atmospheric transmission is optimized by coefficient modification for the dark channel, and the refined transmission is calculated by guided filtering. Finally, the color correction algorithm is used to restore colors and obtain recovered image.
2.1 Estimation of Global Atmospheric Light

The classical atmospheric scattering model based on Mie scattering theorem is shown in Fig. 1 . The degraded image received by the camera through scattering media can be written as:
I ( x ) = J ( x ) e − β d ( x ) + A ( 1 − e − β d ( x ) ) , (1)
View Source Right-click on figure for MathML and additional features. \begin{equation*} I(x) = J(x){e^{ - \beta d(x)}} + A(1 - {e^{ - \beta d(x)}}),\tag{1} \end{equation*} where I ( x ) is the observed intensity, J ( x ) is the scene radiance, t ( x ) = e − βd ( x ) is the transmission describing the portion of the light that is not scattered and reaches the camera, β is the scattering coefficient of the atmosphere, d ( x ) is the depth of field of the scene, and A denotes the global atmospheric light. Once the A and t ( x ) are estimated, the scene radiance J ( x ) can be restored.

Fig. 1. - Atmospheric scattering model
Fig. 1.

Atmospheric scattering model

Show All

The atmospheric light can be estimated by the quadtree method of region segmentation. The sky region is found by the proposed regional segmentation method shown as blue outline in Fig. 2 (a 5 ). First, the gradient of the degraded image is calculated, and the gradient map D ( x ) is obtained as shown in Fig. 2 (a 1 ). Then the maximum gradient value D max ( x ) in the gradient map is found out and the parameter α (0 < α ≤ 1) is used to control the gradient enhancement, expressed as:
{ D ( x ) = D ( x ) , ( D ( x ) < α ⋅ D max ( x ) ) D ( x ) = α ⋅ D ( x ) , ( D ( x ) > α ⋅ D max ( x ) ) , D ′ ( x ) = 255 ⋅ D ( x ) α ⋅ D max ( x ) , (2) (3)
View Source Right-click on figure for MathML and additional features. \begin{align*} &\left\{ \begin{array}{l} D(x) = D(x),(D(x) < \alpha \cdot {D_{\max }}(x))\\ D(x) = \alpha \cdot D(x),(D(x) > \alpha \cdot {D_{\max }}(x)) \end{array} \right.,\tag{2}\\ &D'(x) = 255 \cdot \frac{{D(x)}}{{\alpha \cdot {D_{\max }}(x)}},\tag{3} \end{align*} when the value of α is large: the degree of edge enhancement reduces and the accuracy of edge detection is affected. In case the α is small, the noise in the sky region is amplified and this leads to edge detection errors. The optimal value of α is calculated to be between 0.05–0.2. The enhanced gradient map computed through Gaussian filter is shown in Fig. 2 (a 2 ). The Canny algorithm is used for edge detection (results in Fig. 2 (a 3 )), and morphological dilation operation is used to increase the stability of edge connections (results in Fig. 2 (a 4 )). Finally, the sky region can be extracted by finding the maximal connected domain as shown in Fig. 2 (a 5 ).

Fig. 2. - Schematic of the quadtree method of region segmentation. (a) The processes of regional segmentation: (a0) degraded image, (a1) gradient map, (a2) gradient enhancement map, (a3) edge detection, (a4) morphological dilation, and (a5) sky region is segmented. (b) The estimation of atmospheric light using rectangular windows with red box as the selected region by quadtree method. (c)–(e) The atmospheric light estimation from the red rectangular window with size of 10 × 10 pixels.
Fig. 2.

Schematic of the quadtree method of region segmentation. (a) The processes of regional segmentation: (a 0 ) degraded image, (a 1 ) gradient map, (a 2 ) gradient enhancement map, (a 3 ) edge detection, (a 4 ) morphological dilation, and (a 5 ) sky region is segmented. (b) The estimation of atmospheric light using rectangular windows with red box as the selected region by quadtree method. (c)–(e) The atmospheric light estimation from the red rectangular window with size of 10 × 10 pixels.

Show All

After finding the sky region by regional segmentation method shown as blue line in Fig. 2 (a 5 ), the information is subsequently utilized by quadtree decomposition to estimate the atmospheric light as shown in Fig. 2 (b). The intensity of sky region can be expressed as:
I u p ( x ) = O m ( I ( x ) ) , (4)
View Source Right-click on figure for MathML and additional features. \begin{equation*} {I_{up}}(x) = {O_m}(I(x)),\tag{4} \end{equation*} where I up is the sky region intensity used to calculate the atmospheric light, O m ( I ) represents the regional segmentation operation of degraded image I . The atmospheric light can be calculated by quadtree theory in the range of I up , written as:
⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ I 1 = I u p ( 1 : h 2 ⋅ 2 i ; 1 : w 2 ⋅ 2 i ) , I 2 = I u p ( 1 : h 2 ⋅ 2 i ; w 2 ⋅ 2 i : w ) I 3 = I u p ( h 2 ⋅ 2 i : h ; 1 : w 2 ⋅ 2 i ) , I 4 = I u p ( h 2 ⋅ 2 i : h ; w 2 ⋅ 2 i : w ) A = max ( I ¯ 1 , I ¯ 2 , I ¯ 3 , I ¯ 4 ) I u p = I ′ u p ( A ) , (5)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \left\{ \begin{array}{l} {I_{1}}{\rm{ = }}{I_{up}}(1:\frac{h}{{2 \cdot {2^i}}};1:\frac{w}{{2 \cdot {2^i}}}),{I_{2}}{\rm{ = }}{I_{up}}(1:\frac{h}{{2 \cdot {2^i}}};\frac{w}{{2 \cdot {2^i}}}:w)\\ {I_{3}}{\rm{ = }}{I_{up}}(\frac{h}{{2 \cdot {2^i}}}:h;1:\frac{w}{{2 \cdot {2^i}}}),{I_{4}}{\rm{ = }}{I_{up}}(\frac{h}{{2 \cdot {2^i}}}:h;\frac{w}{{2 \cdot {2^i}}}:w)\\ A = \max ({{\bar{I}}_1},{{\bar{I}}_2},{{\bar{I}}_3},{{\bar{I}}_4})\\ {I_{up}} = {{I'}_{up}}(A) \end{array} \right.,\tag{5} \end{equation*} where h , w denotes the number of rows and the number of columns of the segmented region I up , respectively; h /2 i > L , w /2 i > L , i = 0, 1, 2, …, n . L denotes the window size of the quadtree method. I' up ( A ) represents the region in which atmospheric light A is calculated after the i -th segmentation. The process of estimating the atmospheric light is shown in Fig. 2 . The atmospheric light is calculated by averaging the intensity values inside the red boxed area shown in Fig. 2 (b). For Fig. 2 (c)–2(e), the red box is located in the sky region where the gray intensity values are high.

2.2 Estimation of Refined Transmission

For an arbitrary image I , the dark channel I dark can be written as follows [18] :
I d a r k ( x ) = m i n c ∈ ( r , g , b ) ( m i n y ∈ Ω ( x ) ( I c ( y ) ) ) , (6)
View Source Right-click on figure for MathML and additional features. \begin{equation*} {I^{dark}}(x) = \mathop {min}\limits_{c \in (r,g,b)} (\mathop {min}\limits_{y \in \Omega (x)} ({I^c}(y))),\tag{6} \end{equation*} where I c is the color channel of image I , and Ω( x ) denotes a local patch centered at x . A dark channel is the outcome of two minimum operators, and the minimum operators are commutative. If I is an outdoor haze-free image, except for the sky region, the intensity of I 's dark channel is low and tends to be zero, i.e., I dark →0. To overcome the problem of the reduced visibility in the sky regions, the transmission is estimated by coefficient modification for the dark channel. The difference between the extreme values of dark channel serves as the correction coefficient for media transmission to compensate the transmission of the sky region. The optimized transmission can be written as:
⎧ ⎩ ⎨ I d a r k max ( x ) = m a x ( I d a r k ( x ) ) , I d a r k min ( x ) = m i n ( I d a r k ( x ) ) t ( x ) = ( I d a r k max ( x ) − I d a r k min ( x ) ) ⋅ ( A ∘ U ) − I d a r k ( x ) A ⋅ ( I d a r k max ( x ) − I d a r k min ( x ) ) ∘ U − ( I d a r k ( x ) − I d a r k min ( x ) ) ∘ I d a r k ( x ) , (7)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \left\{ \begin{array}{l} I_{\max }^{dark}(x) = max({I^{dark}}(x)),I_{\min }^{dark}(x) = min({I^{dark}}(x))\\ t(x) = \frac{{(I_{\max }^{dark}(x) - I_{\min }^{dark}(x)) \cdot (A \circ U) - {I^{dark}}(x)}}{{A \cdot (I_{\max }^{dark}(x) - I_{\min }^{dark}(x)) \circ U - ({I^{dark}}(x) - I_{\min }^{dark}(x)) \circ {I^{dark}}(x)}} \end{array} \right.,\tag{7} \end{equation*} where max(·) denotes the maximum extraction operation, U represents the identity matrix, and ‘ ∘ ’ denotes the Hadamard product. The transmission of degraded image can be estimated by Eq. (7), shown in Fig. 3 (c). Since the estimated transmission contains noise, it causes halo artifacts and block effects in the restored image. Therefore, the transmission is refined by guided filtering, which can be expressed as follows:
⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ a ¯ i = 1 | w | ∑ k ∈ w i a k , a k = 1 | w | ∑ i ∈ w k I i p i − μ k p k σ 2 k + ε b ¯ i = 1 | w | ∑ k ∈ w i b k , b k = p ¯ k − a k μ k , p ¯ k = 1 | w | ∑ i ∈ w k p i t i = a ¯ i I i + b ¯ i , (8)
View Source Right-click on figure for MathML and additional features. \begin{equation*} \left\{ \begin{array}{l} {{\bar{a}}_i} = \frac{1}{{\left| w \right|}}\sum\limits_{k \in {w_i}} {{a_k}},{a_k} = \frac{{\frac{1}{{\left| w \right|}}\sum\nolimits_{i \in {w_k}} {{I_i}{p_i} - {\mu _k}{p_k}} }}{{\sigma _k^2 + \varepsilon }}\\ {{\bar{b}}_i} = \frac{1}{{\left| w \right|}}\sum\limits_{k \in {w_i}} {{b_k}},{b_k} = {{\bar{p}}_k} - {a_k}{\mu _k},{{\bar{p}}_k} = \frac{1}{{\left| w \right|}}\sum\limits_{i \in {w_k}} {{p_i}} \\ \\ {t_i} = {{\bar{a}}_i}{I_i} + {{\bar{b}}_i} \end{array} \right.,\tag{8} \end{equation*} where i and k are pixel indexes, w k is a window centered at the pixel k , ε is a regularization parameter penalizing large a k , μ k and σ 2 k are the mean and variance of I in w k , ( a ¯ i , b ¯ i ) are the average coefficients of all windows overlapping k in w k , I denotes the guided image, and p i (that is the t ( x )) represents the rough transmission estimated by the Eq. (7) . The refined transmission map after guided filtering is shown in Fig. 3 (d).

Fig. 3. - Visibility restoration results for the degraded images under scattering conditions. (a) degraded images. (b) dark channel. (c) estimated transmission maps. (d) refined transmission maps after guided filtering. (e) depth maps. (f) recovered images.
Fig. 3.

Visibility restoration results for the degraded images under scattering conditions. (a) degraded images. (b) dark channel. (c) estimated transmission maps. (d) refined transmission maps after guided filtering. (e) depth maps. (f) recovered images.

Show All

2.3 Restoring the Scene Radiance

According to Eq. (1) , the scene radiance of degraded image is restored by:
J ( x ) = I ( x ) − A max ( t , t 0 ) + A , (9)
View Source Right-click on figure for MathML and additional features. \begin{equation*} J(x) = \frac{{I(x) - A}}{{\max (t,{t_0})}} + A,\tag{9} \end{equation*} where t 0 is the adjustment factor used to improve the visual effect of restored image. The value of t 0 is set as 0.1. Finally, the correction algorithm [21] is used to obtain the restored image without any color distortion.

SECTION 3.
Experimental Comparisons and Analyses
3.1 Image Visibility Restoration Analysis
3.1.1 Image Visibility Restoration

To validate the mathematical model presented above, the proposed method is tested through numerical simulations. The visibility restoration results for different degraded scenes (in Fig. 3 (a)) are presented in Fig. 3 (f). It can be seen from Fig. 3 (f) that the proposed method efficiently restores the visibility of images (scenes) degraded by the scattering medium. The scene depth is estimated by setting β = 0.1, shown in Fig. 3 (e). The recovered images and corresponding depth maps, before and after transmission, refined by guided filtering are shown in Fig. 4 . For qualitative comparison, the portions of Fig. 4 marked with red boxes are shown as image insets (inside red dotted box, and magnified 2.5 times). Compared to Fig. 4 (b), the depth map of Fig. 4 (d) is more refined (smooth). Fig. 4 (e) validates the efficient performance of the proposed method which recovers degraded image by applying guided filter to remove halo effect and retain scene colors.
Fig. 4. - The visibility restoration comparison before and after guided filtering. (a) degraded image and zoomed image insets. (b) depth map before refined transmission. (c) image restored before refined transmission along with zoomed insets. (d) depth map after transmission refined by guided filtering. (e) image restored after transmission refined by guided filtering and zoomed image insets.
Fig. 4.

The visibility restoration comparison before and after guided filtering. (a) degraded image and zoomed image insets. (b) depth map before refined transmission. (c) image restored before refined transmission along with zoomed insets. (d) depth map after transmission refined by guided filtering. (e) image restored after transmission refined by guided filtering and zoomed image insets.

Show All

The atmospheric light can be estimated from the degraded image by dividing the image into smaller regions and selecting the area with high average intensity values (dense zone). The proposed method uses three window sizes (10 × 10, 15 × 15, and 30 × 30) for quadtree sub division which can be predefined in the algorithm. The degraded images and their recovered counterparts are shown in Fig. 5 , along with image insets (red dotted lines), zoomed in for visualizing reconstruction quality for far regions in the image. From Fig. 5 (b)–(d), it can be seen that the results of recovered images using different window sizes are the same. This result demonstrates the consistency of our algorithm on different window sizes. Therefore, when the window size changes within a small range, it has no effect on the visibility restoration of degraded image, which is beneficial for fast estimation of atmospheric light and makes the proposed method robust against variations.
Fig. 5. - The visibility restoration results in different atmospheric light conditions and window sizes. (a) degraded images. (b) image restoration using 10 × 10 pixel window size. (c) using 15 × 15 pixel window size. (d) using 30 × 30 pixel window size. Image insets shown with red dotted boxes in between for the zoomed parts in the images.
Fig. 5.

The visibility restoration results in different atmospheric light conditions and window sizes. (a) degraded images. (b) image restoration using 10 × 10 pixel window size. (c) using 15 × 15 pixel window size. (d) using 30 × 30 pixel window size. Image insets shown with red dotted boxes in between for the zoomed parts in the images.

Show All

3.1.2 Qualitative and Quantitative Analysis of Image Restoration Methods

The visibility restoration of the proposed method is analyzed both qualitatively and quantitatively. For the qualitative analysis, the proposed method is compared with three classical haze removal algorithms, shown in Fig. 6 . The images used for testing are diverse scenes degraded by haze (fog). Fig. 6 (b) shows that the He's algorithm produces some block effects because of invariable transmission through a patch. The results of Fig. 6 (c) and Fig. 6 (d) show that there is color distortion under heavy fog (haze), resulting in dark restored images. Furthermore, the Tarel's method ( Fig. 6 (d)) produces halo effect in the restored images. The results for the proposed method are shown in Fig. 6 (e). The proposed method employs quadtree decomposition method based on sky region segmentation to estimate atmospheric light (details of calculation shown in Fig. 2 ). The atmospheric light can be accurately estimated from the sky region through high intensity values, and helps in effectively restoring degraded images. The proposed method also takes the advantage of optimized transmission (given by Eq.(7)) and color correction algorithm to give clear reconstructions with no color distortions and halo effects. The superior performance of the proposed algorithm can be observed from the qualitative comparison in Fig. 6 (e).
Fig. 6. - Qualitative comparison for different haze removal methods. (a) input degraded images. (b) He's results. (c) Fattal's results. (d) Tarel's results. (e) our results.
Fig. 6.

Qualitative comparison for different haze removal methods. (a) input degraded images. (b) He's results. (c) Fattal's results. (d) Tarel's results. (e) our results.

Show All

To quantitatively compare the restoration results of proposed method with existing methods, four evaluation metrics are used i.e., recovery time (T), information entropy (E), mean gradient (Ag), and rate of new visible edges (e). The recovery time (T) is calculated on a computer with: Windows 10 64bit, CPU(i7-7700HQ @2.8 GHz), RAM(8G, DDR4 2667 MHz), MATLAB2017b. Furthermore, the information entropy (E) [26] is introduced to quantify the amount of information in the image, and the mean gradient (Ag) is used to evaluate image definition. Unlike image quality assessment, it is difficult to compare our results with a standard image, so we choose the rate of new visible edges (e) of blind assessment method [27] for quantitative evaluation of our method. The quantitative results for different methods are shown in Fig. 7 . Compared to He's and Tarel's methods, the processing time of our method is very small. From Fig. 7 , it can be clearly seen that the proposed method has better restoration capability showing high information entropy and mean gradient values compared to other methods. Although the value of index e for the proposed method is not the highest, the proposed method still outperforms other methods in terms of restoration quality.
Fig. 7. - Quantitative comparison of four visibility restoration methods (on four images from Fig. 6) using the metrics of (a) recovery time, (b) information entropy, (c) mean gradient, and (d) the rate of new visible edges.
Fig. 7.

Quantitative comparison of four visibility restoration methods (on four images from Fig. 6 ) using the metrics of (a) recovery time, (b) information entropy, (c) mean gradient, and (d) the rate of new visible edges.

Show All

To evaluate color retention capability of the proposed method, the color histogram similarity method [28] is used to calculate color similarity before and after image restoration. The color histograms (for the degraded and recovered images) before and after image restoration are shown in Fig. 8 . Fig. 8 (a) shows that the histogram trend for the three channels remains consistent before and after restoration. Similarly, through quantitative comparison between different algorithms based on color similarity histograms (calculated on before and after recovery images) (in Fig. 8 (b)), it can be seen that the proposed method efficiently retains colors with the highest percentile value of 63.5%. Overall, the visibility of degraded image can quickly be restored by the proposed method.
Fig. 8. - Color analysis of the degraded image “C” before and after restoration. (a) quantitative comparison of color histograms before and after restoration by the proposed method; (b) comparison of color similarity histograms between different algorithms.
Fig. 8.

Color analysis of the degraded image “C” before and after restoration. (a) quantitative comparison of color histograms before and after restoration by the proposed method; (b) comparison of color similarity histograms between different algorithms.

Show All

3.2 Experimental Results

Experiments are carried out to verify the visibility restoration capability of QTCTO on the degraded images. Experiments are performed under different scattering mediums, as shown in Fig. 9 . The targets used in the experiments are: 3D print model, wire stripper, and cup. The degraded images are captured using a camera. The experiments are carried out during the daytime to avoid the color distortion due to uneven lighting at night.
Fig. 9. - Experiments are carried out under different conditions. (a) experimental setup of testing under smoke scattering conditions; (b) experimental setup of testing under turbid water.
Fig. 9.

Experiments are carried out under different conditions. (a) experimental setup of testing under smoke scattering conditions; (b) experimental setup of testing under turbid water.

Show All

3.2.1 Restoration Comparisons for Targets Under Smoke Scattering Conditions

The visibility restoration results for the degraded images under smoke scattering using the proposed method are shown in Fig. 10 . The scattering medium is created by generating smoke from a smoke machine. The concentration of scattering medium cannot be quantified due to smoke's non-uniformity. For comparison, we divide the density of smoke into three grades by visual inspection shown in Fig. 10 . Fig. 10 shows that the visibility of degraded image is efficiently restored by the proposed method, and the colors in the recovered image are not distorted. The colors are corrected by the color correction algorithm [21] . More importantly, the visibility of the degraded image is well restored even under dense scattering conditions shown in Fig. 10 (c).
Fig. 10. - Visibility restoration results for the degraded images under smoke conditions (left-degraded image, right-recovered image). (a) low scattering conditions, (b) medium scattering conditions, and (c) dense scattering conditions.
Fig. 10.

Visibility restoration results for the degraded images under smoke conditions (left-degraded image, right-recovered image). (a) low scattering conditions, (b) medium scattering conditions, and (c) dense scattering conditions.

Show All

Quantitative results before and after visibility restoration for the degraded images under smoke scattering using the proposed method are presented in Table 1 . The recovery time (T) for all the images is same due to similar image resolution. By accurately estimating the atmospheric light and transmission, the proposed method efficiently recovers fine details from the degraded image. The metrics of E and Ag indicate that the values of information entropy and mean gradient for the restored images are greatly improved. The rate of new visible edges (e) is also greatly improved considering the superior performance of the proposed method as well as the simplicity of the target scene.
TABLE 1 Quantitative comparison for visibility restoration under smoke scattering conditions
Table 1- Quantitative comparison for visibility restoration under smoke scattering conditions

To verify the superiority of our method, we compare its performance with three well-known existing methods. We capture degraded images under different smoke scattering conditions (each with a different target). We compare the restoration results of our method with the results of three existing algorithms (namely He's, Fattal's and Tarel's), shown in Fig. 11 . From Fig. 11 (b) and Fig. 11 (c), it can be seen that both He and Fattal algorithms produce color distortion. Similarly, the results in Fig. 11 (c) presents halo effects at the edges. Fig. 11 (e) shows that the visibility restoration results of the proposed method are better compared to other methods. We further present quantitative results using different metrics for the four methods under smoke scattering conditions, shown in Fig. 12 . From quantitative comparison in Fig. 12 , it can be seen that the proposed method clearly outperforms existing methods over different metrics.
Fig. 11. - Qualitative comparison of visibility restoration methods under smoke scattering conditions. (a) input degraded images, (b) He's results, (c) Fattal's results, (d) Tarel's results, and (e) our results.
Fig. 11.

Qualitative comparison of visibility restoration methods under smoke scattering conditions. (a) input degraded images, (b) He's results, (c) Fattal's results, (d) Tarel's results, and (e) our results.

Show All
Fig. 12. - Quantitative comparison of different visibility restoration methods under smoke scattering conditions using the metrics of (a) similarity of color histograms, (b) information entropy, (c) mean gradient, and (d) the rate of new visible edges.
Fig. 12.

Quantitative comparison of different visibility restoration methods under smoke scattering conditions using the metrics of (a) similarity of color histograms, (b) information entropy, (c) mean gradient, and (d) the rate of new visible edges.

Show All

3.2.2 Restoration Comparisons for Targets Under Turbid Water

The proposed method is based on the atmospheric scattering model and it is necessary to test it under different mediums. Therefore, we extend the testing of QTCTO under turbid water to evaluate its robustness. The visibility restoration results for the images degraded under turbid water are shown in Fig. 13 . In Fig. 13 (a)–(c), the left and right side for each taget represents degraded and restored images, respectively.The scattering medium is generated by adding milk in water. From Fig. 13 , it can be seen that the proposed method efficiently restores the degraded image captured under turbid water. Furthermore, it can be observed that the colors of restored images in Fig. 13 are brighter than those in Fig. 10 . This is due to the fact that the water turbidity is unifom compared to smoke. Quantitative results for the degraded images before and after visibility restoration under turbid water are shown in Table 2 . Table 2 shows that the values of metrics E, Ag and e for the restored image have been improved, particularly the values for the rate of new visible edges are greatly improved. Overall, the proposed method is robust and effective for turbid water.
Fig. 13. - Visibility restoration results for targets under turbid water. (a) low scattering condition, (b) medium scattering condition, and (c) dense scattering condition.
Fig. 13.

Visibility restoration results for targets under turbid water. (a) low scattering condition, (b) medium scattering condition, and (c) dense scattering condition.

Show All
TABLE 2 Quantitative comparison for visibility restoration under turbid water
Table 2- Quantitative comparison for visibility restoration under turbid water

To further verify the robustness of the proposed method, its performance is compared with three other methods under turbid water with different scattering conditions. The visibility restoration results for different dehaze algorithms are shown in Fig. 14 . The results in Fig. 14 (c) show severe color distortion using the Fattal's algorithm. In case where the restoration is clear, the colors are very dim, shown in Fig. 14 (b) and Fig. 14 (d). Similarly, the visibility restoration of Fig. 14 (d) is insufficient. The visibility restoration results of the proposed method, shown in Fig. 14 (e), indicate that the proposed method outpeforms other methods under turbid water. The performance of the proposed method can quantitatively be analyzed using different mertrics shown in Fig. 15 . The color histogram similarity results indicate that the proposed method performs better compared to existing methods in preserving colors ( Fig. 15 (a)). Overall, the proposed method retains more details in the recovered images compared to other methods, shown in Fig. 15 (b)–(d).
Fig. 14. - Qualitative comparison of visibility restoration methods under turbid water. (a) input degraded images. (b) He's results. (c) Fattal's results. (d) Tarel's results. (e) our results.
Fig. 14.

Qualitative comparison of visibility restoration methods under turbid water. (a) input degraded images. (b) He's results. (c) Fattal's results. (d) Tarel's results. (e) our results.

Show All
Fig. 15. - Quantitative comparison of different visibility restoration methods under turbid water using (a) similarity of color histograms, (b) information entropy, (c) mean gradient, and (d) the rate of new visible edges.
Fig. 15.

Quantitative comparison of different visibility restoration methods under turbid water using (a) similarity of color histograms, (b) information entropy, (c) mean gradient, and (d) the rate of new visible edges.

Show All

3.3 Discussion

According to the above experimental results, to further improve the restoration quality of degraded images under different conditions for practical applications, we discuss some potential limitations that affect restoration performance. We also provide solutions as future direction which would improve the performance of the proposed algorithm.

First, the real-time performance of QTCTO method needs to be improved for the visibility restoration of degraded video. Although the image recovery time for the proposed method is very small, it is still difficult to achieve higher frame rates required by video processing. Therefore, to cater real-time needs, as future work, an advanced model can be developed, or the support of special hardware such as GPU can be used for acceleration. Similarly, executing the algorithm on fast platforms (such as C using SSE instructions) can also achieve real-time performance.

Second, the proposed method has poor restoration capability under non-uniform scattering medium. The experimental results reveal that the restoration performance under turbid water is better compared to a smoky medium. This can be attributed to the uniformity of turbid water and non-uniformity of the smoke distribution in our experiments. However, in practice, images are captured from diverse inhomogeneous medium (i.e., under dense fog) which may degrade the performance of QTCTO. Therefore, the performance of the proposed method can be improved in the future by computing atmospheric light and transmission from different regions within the degraded image to achieve good restoration.

Third, although the proposed method is based on physical atmospheric model, it achieves better results under turbid water compared to a smoky medium. By improving the color correction algorithm used in our work, the proposed method can easily be extended to restore degraded images under marine environment. In general, the restoration capability of the proposed method can be improved by capturing the target within the camera's depth of field, and without defocus blur.
SECTION 4.
Conclusion

The paper presents QTCTO algorithm for the visibility restoration of degraded images under scattering medium. The proposed method is based on computing atmospheric light and transmission for efficient restoration capability. The method is tested for different scattering conditions such as sky, turbid water, and smoke. The experimental results show that the proposed method outperforms other existing methods over different metrics achieving good restoration results. The proposed method restores degraded image with high contrast and sharpness without any color distortion. Similarly, the halo artifacts that appear in other methods are also removed by the proposed method. Although the proposed method is based on the atmospheric scattering model, it can handle complex scattering mediums as shown by its robustness under turbid water. To achieve real-time performance for video processing, the algorithm can be written in C language (using SSE instructions) to cater degraded video. The proposed method can effectively be used for different applications such as security monitoring, remote sensing, and various military applications.

Authors
Figures
References
Citations
Keywords
Metrics
   Back to Results   
More Like This
Color Image Restoration Exploiting Inter-Channel Correlation With a 3-Stage CNN

IEEE Journal of Selected Topics in Signal Processing

Published: 2021
An Experimental-Based Review of Image Enhancement and Image Restoration Methods for Underwater Imaging

IEEE Access

Published: 2019
Show More
References
1. T. K. Kim, J. K. Paik and B. S. Kang, "Contrast enhancement system using spatially adaptive histogram equalization with temporal filtering", IEEE Trans. Consum. Electron. , vol. 44, no. 1, pp. 82-87, 1998.
Show in Context View Article Full Text: PDF (1011) Google Scholar
2. J. A. Stark, "Adaptive image contrast enhancement using generalizations of histogram equalization", IEEE Trans. Image Process. , vol. 9, no. 5, pp. 889-896, 2000.
View Article Full Text: PDF (1397) Google Scholar
3. F. Russo, "An image enhancement technique combining sharpening and noise reduction", IEEE Trans. Instrum. Meas. , vol. 51, no. 4, pp. 824-828, 2002.
View Article Full Text: PDF (784) Google Scholar
4. M.-J. Seow and V. K. Asari, "Ratio rule and homomorphic filter for enhancement of digital colour image", Neurocomputing , vol. 69, no. 7, pp. 954-958, 2006.
CrossRef Google Scholar
5. E. H. Land and J. J. McCann, "Lightness and retinex theory", J. Opt. Soc. Am. , vol. 61, no. 1, pp. 1-11, 1971.
Show in Context CrossRef Google Scholar
6. J. P. Oakley and B. L. Satherley, "Improving image quality in poor visibility conditions using a physical model for contrast degradation", IEEE Trans. Image Process. , vol. 7, no. 2, pp. 167-179, 1998.
Show in Context View Article Full Text: PDF (347) Google Scholar
7. S. K. Nayar and S. G. Narasimhan, "Vision in bad weather", Proceedings of the Seventh IEEE International Conference on Computer Vision , pp. 820-827, 1999.
View Article Full Text: PDF (292) Google Scholar
8. S. G. Narasimhan and S. K. Nayar, "Chromatic framework for vision in bad weather", Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 598-605, 2000.
View Article Full Text: PDF (338) Google Scholar
9. S. G. Narasimhan and S. K. Nayar, "Contrast restoration of weather degraded images", IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25, no. 6, pp. 713-724, 2003.
Show in Context View Article Full Text: PDF (3039) Google Scholar
10. Y. Y. Schechner, S. G. Narasimhan and S. K. Nayar, "Instant dehazing of images using polarization", Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pp. 325-332, 2001.
Show in Context View Article Full Text: PDF (1059) Google Scholar
11. Y. Y. Schechner, S. G. Narasimhan and S. K. Nayar, "Polarization-based vision through haze", Appl. Optics , vol. 42, no. 3, pp. 511-525, 2003.
CrossRef Google Scholar
12. S. Shwartz, E. Namer and Y. Y. Schechner, "Blind haze separation", Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pp. 1984-1991, 2006.
Show in Context View Article Full Text: PDF (1369) Google Scholar
13. S. G. Narasimhan and S. K. Nayar, "Vision and the atmosphere", Int. J. Comput. Vis. , vol. 48, no. 3, pp. 233-254, 2002.
CrossRef Google Scholar
14. J. Kopf et al., "Deep photo: Model-based photograph enhancement and viewing", ACM Trans. Graph. , vol. 27, no. 5, pp. 111-119, 2008.
CrossRef Google Scholar
15. S. G. Narasimhan and S. K. Nayar, "Interactive (de) weathering of an image using physical models", Proceedings of the 2003 IEEE Conference on Workshop Color and Photometric Methods in computer Vision , pp. 1-8, 2003.
Show in Context Google Scholar
16. R. T. Tan, "Visibility in bad weather from a single image", Proceedings of the 2008 IEEE Conference on Computer Vision and Pattern Recognition , pp. 1-8, 2008.
Show in Context View Article Full Text: PDF (1599) Google Scholar
17. R. Fattal, "Single image dehazing", ACM Trans. Graph. , vol. 27, no. 3, pp. 721-729, 2008.
Show in Context CrossRef Google Scholar
18. K. He, J. Sun and X. Tang, "Single image haze removal using dark channel prior", IEEE Trans. Pattern Anal. Mach. Intell. , vol. 33, no. 12, pp. 2341-2353, 2010.
Show in Context Google Scholar
19. K. He, J. Sun and X. Tang, "Guided image filtering", IEEE Trans. Pattern Anal. Mach. Intell. , vol. 35, no. 6, pp. 1397-1409, 2012.
Show in Context View Article Full Text: PDF (8315) Google Scholar
20. L. Kratz and K. Nishino, "Factorizing scene albedo and depth from a single foggy image", Proceedings of the 2009 IEEE 12th International Conference on Computer Vision , pp. 1701-1708, 2009.
View Article Full Text: PDF (3121) Google Scholar
21. J.-P. Tarel and N. Hautiere, "Fast visibility restoration from a single color or gray level image", Proceedings of the 2009 IEEE 12th International Conference on Computer Vision , pp. 2201-2208, 2009.
Show in Context View Article Full Text: PDF (4768) Google Scholar
22. R. Fattal, "Dehazing using color-lines", ACM Trans. Graph. , vol. 34, no. 1, pp. 131-1314, 2014.
CrossRef Google Scholar
23. L. Bao, Y. Song, Q. Yang and N. Ahuja, "An edge-preserving filtering framework for visibility restoration", Proceedings of the 21th IEEE International Conference on Pattern Recognition , pp. 384-387, 2012.
Google Scholar
24. Q. Yan, L. Xu and J. Jia, "Dense scattering layer removal", Proceedings of the Siggraph Asia 2013 Technial Briefs , pp. 1-10, 2013.
CrossRef Google Scholar
25. L. Caraffa and J.-P. Tarel, "Markov random field model for single image defogging", Proceedings of the 2013 IEEE Conference on Intelligent Vehicles Symposium , pp. 994-999, 2013.
Show in Context View Article Full Text: PDF (2809) Google Scholar
26. S. R. Gadre, "Information entropy and Thomas-Fermi theory", Phys. Rev. A , vol. 30, no. 1, pp. 620-621, 1984.
Show in Context CrossRef Google Scholar
27. N. Hautiere, J.-P. Tarel, D. Aubert and E. Dumont, "Blind contrast enhancement assessment by gradient ratioing at visible edges", Image Anal. Stereol. , vol. 27, no. 2, pp. 87-95, 2008.
Show in Context CrossRef Google Scholar
28. M. Chambah, A. Rizzi, C. Gatta, B. Besserer and D. Marini, "Perceptual approach for unsupervised digital color restoration of cinematographic archives", Proceedings of the Proc. SPIE 5008 Color Imaging VIII: Processing Hardcopy and Applications , pp. 138-149, 2003.
Show in Context CrossRef Google Scholar
IEEE Personal Account

    Change username/password 

Purchase Details

    Payment Options
    View Purchased Documents 

Profile Information

    Communications Preferences
    Profession and Education
    Technical interests 

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support 

Follow

About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

© Copyright 2022 IEEE - All rights reserved.
IEEE Account

    Change Username/Password
    Update Address

Purchase Details

    Payment Options
    Order History
    View Purchased Documents

Profile Information

    Communications Preferences
    Profession and Education
    Technical Interests

Need Help?

    US & Canada: +1 800 678 4333
    Worldwide: +1 732 981 0060
    Contact & Support

    About IEEE Xplore
    Contact Us
    Help
    Accessibility
    Terms of Use
    Nondiscrimination Policy
    Sitemap
    Privacy & Opting Out of Cookies

A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.
© Copyright 2022 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
